**You**

ğŸ§  Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), pÃ¡gina por pÃ¡gina.

Para cada pÃ¡gina, siga este modelo de resposta:

ğŸ“„ PÃ¡gina X

ğŸ” Resumo dos principais pontos:

\[bullet points com os tÃ³picos abordados na pÃ¡gina\]

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

\[explique os conceitos em portuguÃªs claro, com boa didÃ¡tica, como um professor\]

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

\[comandos ou trechos de cÃ³digo aplicÃ¡veis no Windows, usando Docker Desktop e Visual Studio Code\]

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

\[comparaÃ§Ãµes simples ou observaÃ§Ãµes para reforÃ§ar o entendimento\]

âœ… Importante: percorra automaticamente todas as pÃ¡ginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "prÃ³xima".

Quando terminar todas as pÃ¡ginas, diga:

"âœ… Fim da leitura. Todas as pÃ¡ginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

OlÃ¡ Alexandre! Combinado, farei a leitura completa do PDF "Learn Docker in a Month of Lunches", pÃ¡gina por pÃ¡gina, seguindo o modelo que vocÃª pediu. Vamos comeÃ§ar!

---

ğŸ“„ PÃ¡gina 1

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco, provavelmente uma pÃ¡gina de rosto ou separadora inicial do livro.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina estÃ¡ vazia. Em livros impressos ou digitais, pÃ¡ginas em branco como esta sÃ£o frequentemente usadas no inÃ­cio, no fim ou entre seÃ§Ãµes importantes para separar conteÃºdos, melhorar a diagramaÃ§Ã£o ou marcar o comeÃ§o de uma nova parte.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Pense nela como a capa de uma pasta antes de vocÃª ver os documentos dentro.

---

ğŸ“„ PÃ¡gina 2

ğŸ” Resumo dos principais pontos:

*   Define contÃªineres como ambientes isolados e eficientes.
*   Menciona que mÃºltiplos contÃªineres podem rodar em um Ãºnico computador, compartilhando o mesmo SO, CPU e memÃ³ria.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A ideia central aqui Ã© apresentar o conceito fundamental dos contÃªineres Docker. Eles sÃ£o como "caixas" isoladas onde seus aplicativos podem rodar. O ponto chave Ã© a eficiÃªncia: diferente de mÃ¡quinas virtuais (que precisam de um sistema operacional completo para cada uma), os contÃªineres compartilham o sistema operacional da mÃ¡quina hospedeira. Isso significa que eles usam muito menos recursos (CPU, memÃ³ria), permitindo que vocÃª rode muitos mais contÃªineres em comparaÃ§Ã£o com mÃ¡quinas virtuais no mesmo hardware. O isolamento garante que o que acontece dentro de um contÃªiner nÃ£o afeta os outros.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando especÃ­fico nesta pÃ¡gina, mas o conceito Ã© a base do Docker Desktop que vocÃª usa. Ao rodar `docker ps` no PowerShell ou terminal do VS Code, vocÃª verÃ¡ contÃªineres rodando, todos compartilhando seu Windows (ou o kernel Linux dentro do Docker Desktop, dependendo do modo) mas isolados entre si.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Analogia: Pense em apartamentos em um prÃ©dio. Cada apartamento Ã© um contÃªiner isolado, mas todos compartilham a estrutura do prÃ©dio (sistema operacional), eletricidade (CPU) e Ã¡gua (memÃ³ria). MÃ¡quinas virtuais seriam como casas separadas, cada uma com sua prÃ³pria fundaÃ§Ã£o, estrutura e utilidades.

---

ğŸ“„ PÃ¡gina 3

ğŸ” Resumo dos principais pontos:

*   TÃ­tulo do livro: Learn Docker in a Month of Lunches.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina exibe apenas o tÃ­tulo principal do livro. A sÃ©rie "Month of Lunches" da Manning Ã© conhecida por dividir tÃ³picos complexos em liÃ§Ãµes diÃ¡rias que podem ser aprendidas durante o horÃ¡rio de almoÃ§o, focando na prÃ¡tica. Este livro aplica essa metodologia ao Docker.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Ã‰ a "capa" do conteÃºdo que estamos prestes a explorar.

---

ğŸ“„ PÃ¡gina 4

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Assim como a pÃ¡gina 1, esta Ã© uma pÃ¡gina em branco, servindo provavelmente como separador visual ou para ajuste de paginaÃ§Ã£o antes do inÃ­cio do conteÃºdo mais formal do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 5

ğŸ” Resumo dos principais pontos:

*   Repete o tÃ­tulo do livro: Learn Docker in a Month of Lunches.
*   Nome do autor: ELTON STONEMAN.
*   Nome da editora: MANNING.
*   LocalizaÃ§Ã£o da editora: SHELTER ISLAND.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta Ã© a pÃ¡gina de tÃ­tulo formal do livro. Ela apresenta as informaÃ§Ãµes essenciais: o que Ã© o livro (tÃ­tulo), quem o escreveu (autor) e quem o publicou (editora e local). Elton Stoneman Ã© uma figura conhecida na comunidade Docker.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 6

ğŸ” Resumo dos principais pontos:

*   InformaÃ§Ãµes de direitos autorais e publicaÃ§Ã£o da Manning Publications Co. (Â©2020).
*   Detalhes sobre como obter o livro e descontos por quantidade.
*   DeclaraÃ§Ã£o sobre o uso de marcas registradas.
*   PolÃ­tica de uso de papel reciclado e sem cloro.
*   CrÃ©ditos da equipe editorial (editores, revisores, designers, etc.).
*   ISBN: 9781617297052.
*   InformaÃ§Ã£o de que foi impresso nos EUA.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina contÃ©m os detalhes legais e editoriais do livro. Inclui informaÃ§Ãµes sobre direitos autorais (copyright), como comprar o livro, polÃ­ticas ambientais da editora, e reconhece as vÃ¡rias pessoas envolvidas na produÃ§Ã£o do livro (ediÃ§Ã£o, revisÃ£o tÃ©cnica, design, etc.). O ISBN Ã© um identificador Ãºnico para esta ediÃ§Ã£o do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina. O ISBN pode ser Ãºtil se vocÃª quiser procurar exatamente esta ediÃ§Ã£o em livrarias ou bibliotecas.

---

ğŸ“„ PÃ¡gina 7

ğŸ” Resumo dos principais pontos:

*   DedicatÃ³ria do autor (Elton Stoneman).
*   O autor dedica o livro Ã  sua esposa, Nikki, e aos seus filhos, Jackson e Eris.
*   Menciona que escreveu o livro em um celeiro em Gloucestershire, Inglaterra.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta Ã© a pÃ¡gina de dedicatÃ³ria. Ã‰ um espaÃ§o pessoal onde o autor agradece ou homenageia pessoas importantes para ele, frequentemente familiares que o apoiaram durante o processo de escrita. TambÃ©m dÃ¡ um toque humano, mencionando o local onde o livro foi escrito.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 8

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Mais uma pÃ¡gina em branco, provavelmente usada para separar a dedicatÃ³ria do inÃ­cio do sumÃ¡rio ou conteÃºdo principal.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 9

ğŸ” Resumo dos principais pontos:

*   InÃ­cio do sumÃ¡rio breve ("brief contents").
*   **PARTE 1: UNDERSTANDING DOCKER CONTAINERS AND IMAGES** (Entendendo ContÃªineres e Imagens Docker).
    *   CapÃ­tulo 1: Before you begin (Antes de comeÃ§ar).
    *   CapÃ­tulo 2: Understanding Docker and running Hello World (Entendendo Docker e rodando Hello World).
    *   CapÃ­tulo 3: Building your own Docker images (Construindo suas prÃ³prias imagens Docker).
    *   CapÃ­tulo 4: Packaging applications from source code into Docker Images (Empacotando aplicaÃ§Ãµes a partir do cÃ³digo-fonte em Imagens Docker).
    *   CapÃ­tulo 5: Sharing images with Docker Hub and other registries (Compartilhando imagens com Docker Hub e outros registros).
    *   CapÃ­tulo 6: Using Docker volumes for persistent storage (Usando volumes Docker para armazenamento persistente).
*   **PARTE 2: RUNNING DISTRIBUTED APPLICATIONS IN CONTAINERS** (Rodando AplicaÃ§Ãµes DistribuÃ­das em ContÃªineres).
    *   CapÃ­tulo 7: Running multi-container apps with Docker Compose (Rodando apps multi-contÃªiner com Docker Compose).
    *   CapÃ­tulo 8: Supporting reliability with health checks and dependency checks (Suportando confiabilidade com verificaÃ§Ãµes de saÃºde e dependÃªncia).
    *   CapÃ­tulo 9: Adding observability with containerized monitoring (Adicionando observabilidade com monitoramento conteinerizado).
    *   CapÃ­tulo 10: Running multiple environments with Docker Compose (Rodando mÃºltiplos ambientes com Docker Compose).
    *   CapÃ­tulo 11: Building and testing applications with Docker and Docker Compose (Construindo e testando aplicaÃ§Ãµes com Docker e Docker Compose).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina inicia o sumÃ¡rio resumido, dando uma visÃ£o geral da estrutura do livro. Ele Ã© dividido em partes temÃ¡ticas, e aqui vemos as duas primeiras.
    *   A Parte 1 foca nos fundamentos do Docker: o que sÃ£o contÃªineres e imagens, como criar suas prÃ³prias imagens (inclusive a partir do cÃ³digo-fonte), como compartilhÃ¡-las e como lidar com dados que precisam persistir (volumes). Ã‰ a base essencial.
    *   A Parte 2 avanÃ§a para aplicaÃ§Ãµes mais complexas, compostas por mÃºltiplos contÃªineres que precisam trabalhar juntos. Introduz o Docker Compose como ferramenta para orquestrar esses apps, e aborda temas importantes para rodÃ¡-los de forma confiÃ¡vel, como verificaÃ§Ãµes de saÃºde, monitoramento e gerenciamento de diferentes ambientes (desenvolvimento, teste, etc.).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando direto aqui, mas este sumÃ¡rio te ajuda a localizar rapidamente onde encontrar informaÃ§Ãµes sobre tÃ³picos especÃ­ficos, como `Docker Compose` (CapÃ­tulo 7) ou `Volumes` (CapÃ­tulo 6).

---

ğŸ“„ PÃ¡gina 10

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o do sumÃ¡rio breve ("brief contents").
*   **PARTE 3: RUNNING AT SCALE WITH A CONTAINER ORCHESTRATOR** (Rodando em Escala com um Orquestrador de ContÃªineres).
    *   CapÃ­tulo 12: Understanding orchestration: Docker Swarm and Kubernetes (Entendendo orquestraÃ§Ã£o: Docker Swarm e Kubernetes).
    *   CapÃ­tulo 13: Deploying distributed applications as stacks in Docker Swarm (Implantando aplicaÃ§Ãµes distribuÃ­das como stacks no Docker Swarm).
    *   CapÃ­tulo 14: Automating releases with upgrades and rollbacks (Automatizando lanÃ§amentos com atualizaÃ§Ãµes e reversÃµes).
    *   CapÃ­tulo 15: Configuring Docker for secure remote access and CI/CD (Configurando Docker para acesso remoto seguro e CI/CD).
    *   CapÃ­tulo 16: Building Docker images that run anywhere: Linux, Windows, Intel, and Arm (Construindo imagens Docker que rodam em qualquer lugar: Linux, Windows, Intel e Arm).
*   **PARTE 4: GETTING YOUR CONTAINERS READY FOR PRODUCTION** (Preparando Seus ContÃªineres para ProduÃ§Ã£o).
    *   CapÃ­tulo 17: Optimizing your Docker images for size, speed, and security (Otimizando suas imagens Docker para tamanho, velocidade e seguranÃ§a).
    *   CapÃ­tulo 18: Application configuration management in containers (Gerenciamento de configuraÃ§Ã£o de aplicaÃ§Ã£o em contÃªineres).
    *   CapÃ­tulo 19: Writing and managing application logs with Docker (Escrevendo e gerenciando logs de aplicaÃ§Ã£o com Docker).
    *   CapÃ­tulo 20: Controlling HTTP traffic to containers with a reverse proxy (Controlando trÃ¡fego HTTP para contÃªineres com um proxy reverso).
    *   CapÃ­tulo 21: Asynchronous communication with a message queue (ComunicaÃ§Ã£o assÃ­ncrona com uma fila de mensagens).
    *   CapÃ­tulo 22: Never the end (Nunca o fim).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina conclui o sumÃ¡rio resumido, apresentando as Partes 3 e 4.
    *   A Parte 3 trata de rodar aplicaÃ§Ãµes em mÃºltiplos servidores (um cluster) usando orquestradores como Docker Swarm e Kubernetes. Cobre como implantar, atualizar, reverter e acessar remotamente essas aplicaÃ§Ãµes em escala. O CapÃ­tulo 16 Ã© particularmente interessante, pois aborda como criar imagens que funcionam em diferentes sistemas operacionais e arquiteturas de CPU.
    *   A Parte 4 foca em refinar as aplicaÃ§Ãµes e imagens para o ambiente de produÃ§Ã£o. TÃ³picos incluem otimizaÃ§Ã£o de imagens, gerenciamento de configuraÃ§Ãµes e logs de forma eficiente, controle de trÃ¡fego com proxies reversos e introduÃ§Ã£o a padrÃµes de comunicaÃ§Ã£o assÃ­ncrona (filas de mensagens). O Ãºltimo capÃ­tulo oferece conselhos para continuar a jornada com Docker.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Novamente, sem comandos diretos. Este sumÃ¡rio mostra onde aprender sobre `Docker Swarm` (CapÃ­tulo 12-15), `Kubernetes` (CapÃ­tulo 12), otimizaÃ§Ã£o de imagens (CapÃ­tulo 17), gerenciamento de logs (CapÃ­tulo 19) e proxies reversos (CapÃ­tulo 20).

---

ğŸ“„ PÃ¡gina 11

ğŸ” Resumo dos principais pontos:

*   InÃ­cio do sumÃ¡rio detalhado ("contents").
*   Links para prefÃ¡cio (xvii), agradecimentos (xviii), sobre este livro (xix), sobre o autor (xxiii).
*   **PARTE 1: UNDERSTANDING DOCKER CONTAINERS AND IMAGES** (detalhado).
    *   **CapÃ­tulo 1: Before you begin**
        *   1.1 Por que contÃªineres dominarÃ£o o mundo (cenÃ¡rios: migraÃ§Ã£o para nuvem, modernizaÃ§Ã£o de apps legados, construÃ§Ã£o de apps cloud-native, inovaÃ§Ã£o tÃ©cnica como Serverless, transformaÃ§Ã£o digital com DevOps).
        *   1.2 Este livro Ã© para vocÃª?
        *   1.3 Criando seu ambiente de laboratÃ³rio (Instalando Docker, verificando a instalaÃ§Ã£o, baixando o cÃ³digo-fonte, lembrando comandos de limpeza).
        *   1.4 Sendo imediatamente eficaz.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina inicia o sumÃ¡rio detalhado, comeÃ§ando com os links para as seÃ§Ãµes introdutÃ³rias (prefÃ¡cio, agradecimentos, etc.) e detalhando o conteÃºdo da Parte 1, especificamente o CapÃ­tulo 1.
*   O CapÃ­tulo 1 serve como introduÃ§Ã£o, justificando a importÃ¢ncia dos contÃªineres em diversos cenÃ¡rios de TI modernos, desde mover aplicaÃ§Ãµes existentes para a nuvem atÃ© criar novas aplicaÃ§Ãµes e adotar prÃ¡ticas DevOps. Ele tambÃ©m define o pÃºblico-alvo do livro e guia o leitor na configuraÃ§Ã£o do ambiente necessÃ¡rio (instalaÃ§Ã£o do Docker, download dos exemplos) para acompanhar os exercÃ­cios prÃ¡ticos. A seÃ§Ã£o final reforÃ§a a abordagem prÃ¡tica do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O CapÃ­tulo 1.3 guiarÃ¡ na instalaÃ§Ã£o do **Docker Desktop para Windows**, na verificaÃ§Ã£o com comandos como `docker version` e `docker-compose version` no PowerShell ou terminal do VS Code, e no download do cÃ³digo-fonte do livro (provavelmente via `git clone` ou download de ZIP). Comandos de limpeza como `docker container rm -f $(docker container ls -aq)` tambÃ©m serÃ£o apresentados.

---

ğŸ“„ PÃ¡gina 12

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o do sumÃ¡rio detalhado da Parte 1.
    *   **CapÃ­tulo 2: Understanding Docker and running Hello World**
        *   Rodando Hello World em um contÃªiner.
        *   O que Ã© um contÃªiner?
        *   Conectando a um contÃªiner como um computador remoto.
        *   Hospedando um site em um contÃªiner.
        *   Entendendo como o Docker roda contÃªineres.
        *   Lab: Explorando o sistema de arquivos do contÃªiner.
    *   **CapÃ­tulo 3: Building your own Docker images**
        *   Usando uma imagem de contÃªiner do Docker Hub.
        *   Escrevendo seu primeiro Dockerfile.
        *   Construindo sua prÃ³pria imagem de contÃªiner.
        *   Entendendo imagens Docker e camadas de imagem.
        *   Otimizando Dockerfiles para usar o cache de camadas.
        *   Lab.
    *   **CapÃ­tulo 4: Packaging applications from source code into Docker Images**
        *   Quem precisa de um servidor de build quando se tem um Dockerfile?
        *   Passo a passo da aplicaÃ§Ã£o: cÃ³digo-fonte Java.
        *   Passo a passo da aplicaÃ§Ã£o: cÃ³digo-fonte Node.js.
        *   Passo a passo da aplicaÃ§Ã£o: cÃ³digo-fonte Go.
        *   Entendendo Dockerfiles multi-estÃ¡gio.
        *   Lab.
    *   **CapÃ­tulo 5: Sharing images with Docker Hub and other registries**
        *   Trabalhando com registros, repositÃ³rios e tags de imagem.
        *   Enviando (push) suas prÃ³prias imagens para o Docker Hub.
        *   Rodando e usando seu prÃ³prio registro Docker.
        *   Usando tags de imagem eficazmente.
        *   Transformando imagens oficiais em "golden images".
        *   Lab.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina detalha os CapÃ­tulos 2 a 5, o nÃºcleo da Parte 1.
    *   O CapÃ­tulo 2 introduz a prÃ¡tica com Docker, comeÃ§ando com o clÃ¡ssico "Hello World", explicando o que Ã© um contÃªiner de forma mais concreta, como interagir com ele e como o Docker funciona por baixo dos panos.
    *   O CapÃ­tulo 3 ensina a criar suas prÃ³prias imagens usando o `Dockerfile`, abordando a estrutura das imagens em camadas e como otimizar o processo de build usando o cache.
    *   O CapÃ­tulo 4 mostra um uso poderoso do Docker: compilar e empacotar aplicaÃ§Ãµes diretamente do cÃ³digo-fonte (Java, Node.js, Go) usando `Dockerfiles multi-estÃ¡gio`, eliminando a necessidade de ter ferramentas de build instaladas localmente.
    *   O CapÃ­tulo 5 foca em como compartilhar as imagens criadas, explicando o papel do Docker Hub e outros registros, a importÃ¢ncia das tags para versionamento e o conceito de "golden images" (imagens base customizadas).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Cap. 2:** Comandos como `docker container run`, `docker ps`, `docker exec -it <container> powershell` (ou `cmd`), `docker logs`, `docker inspect`.
*   **Cap. 3:** EdiÃ§Ã£o de `Dockerfile` no VS Code, comandos `docker image build -t <nome> .`, `docker image ls`, `docker history`.
*   **Cap. 4:** Uso de `Dockerfiles multi-estÃ¡gio` para compilar cÃ³digo Java, Node.js, Go dentro do contÃªiner de build.
*   **Cap. 5:** Comandos `docker login`, `docker image tag <origem> <destino>`, `docker image push <imagem>`, `docker run` para um registro local.

---

ğŸ“„ PÃ¡gina 13

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o do sumÃ¡rio detalhado.
    *   **CapÃ­tulo 6: Using Docker volumes for persistent storage**
        *   Por que dados em contÃªineres nÃ£o sÃ£o permanentes.
        *   Rodando contÃªineres com volumes Docker.
        *   Rodando contÃªineres com montagens de sistema de arquivos (filesystem mounts).
        *   LimitaÃ§Ãµes das montagens de sistema de arquivos.
        *   Entendendo como o sistema de arquivos do contÃªiner Ã© construÃ­do.
        *   Lab.
*   **PARTE 2: RUNNING DISTRIBUTED APPLICATIONS IN CONTAINERS** (detalhado).
    *   **CapÃ­tulo 7: Running multi-container apps with Docker Compose**
        *   A anatomia de um arquivo Docker Compose.
        *   Rodando uma aplicaÃ§Ã£o multi-contÃªiner com Compose.
        *   Como o Docker conecta os contÃªineres.
        *   ConfiguraÃ§Ã£o de aplicaÃ§Ã£o no Docker Compose.
        *   Entendendo o problema que o Docker Compose resolve.
        *   Lab.
    *   **CapÃ­tulo 8: Supporting reliability with health checks and dependency checks**
        *   Construindo verificaÃ§Ãµes de saÃºde (health checks) nas imagens Docker.
        *   Iniciando contÃªineres com verificaÃ§Ãµes de dependÃªncia.
        *   Escrevendo utilitÃ¡rios customizados para lÃ³gica de verificaÃ§Ã£o da aplicaÃ§Ã£o.
        *   Definindo verificaÃ§Ãµes de saÃºde e dependÃªncia no Docker Compose.
        *   Entendendo como as verificaÃ§Ãµes potencializam apps auto-reparÃ¡veis (self-healing).
        *   Lab.
    *   **CapÃ­tulo 9: Adding observability with containerized monitoring**
        *   A pilha de monitoramento para aplicaÃ§Ãµes conteinerizadas.
        *   Expondo mÃ©tricas da sua aplicaÃ§Ã£o.
        *   Rodando um contÃªiner Prometheus para coletar mÃ©tricas.
        *   Rodando um contÃªiner Grafana para visualizar mÃ©tricas.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina finaliza o detalhamento da Parte 1 com o CapÃ­tulo 6 e inicia a Parte 2 com os CapÃ­tulos 7 a 9.
    *   O CapÃ­tulo 6 aborda um ponto crucial: como lidar com dados que precisam sobreviver alÃ©m da vida de um contÃªiner. Explica por que o armazenamento padrÃ£o do contÃªiner Ã© temporÃ¡rio e introduz `volumes` e `bind mounts` como soluÃ§Ãµes para persistÃªncia de dados.
    *   A Parte 2 (CapÃ­tulos 7-11) foca em aplicaÃ§Ãµes distribuÃ­das. O CapÃ­tulo 7 introduz o `Docker Compose` para definir e gerenciar mÃºltiplos contÃªineres como uma Ãºnica aplicaÃ§Ã£o.
    *   O CapÃ­tulo 8 discute como tornar essas aplicaÃ§Ãµes mais robustas usando `health checks` (para monitorar a saÃºde interna do app no contÃªiner) e `dependency checks` (para garantir que serviÃ§os dos quais o app depende estejam disponÃ­veis na inicializaÃ§Ã£o).
    *   O CapÃ­tulo 9 trata de `observabilidade`, mostrando como coletar `mÃ©tricas` das aplicaÃ§Ãµes (usando `Prometheus`) e visualizÃ¡-las (usando `Grafana`) para entender o desempenho e identificar problemas.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Cap. 6:** Comandos `docker volume create`, `docker run -v <volume>:<caminho_container>`, `docker run -v C:\caminho\local:<caminho_container>`. ExploraÃ§Ã£o do sistema de arquivos via `docker exec`.
*   **Cap. 7:** EdiÃ§Ã£o de arquivos `docker-compose.yml` no VS Code, comandos `docker-compose up`, `docker-compose down`, `docker-compose ps`, `docker network create`.
*   **Cap. 8:** AdiÃ§Ã£o da instruÃ§Ã£o `HEALTHCHECK` no Dockerfile, uso de `depends_on` e `healthcheck` no `docker-compose.yml`.
*   **Cap. 9:** Rodar contÃªineres do `Prometheus` e `Grafana`, configurar o Prometheus para coletar mÃ©tricas, acessar dashboards do Grafana.

---

ğŸ“„ PÃ¡gina 14

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o do sumÃ¡rio detalhado da Parte 2.
    *   **CapÃ­tulo 9 (continuaÃ§Ã£o): Adding observability with containerized monitoring**
        *   9.5 Entendendo os nÃ­veis de observabilidade.
        *   9.6 Lab.
    *   **CapÃ­tulo 10: Running multiple environments with Docker Compose**
        *   10.1 Implantando muitas aplicaÃ§Ãµes com Docker Compose.
        *   10.2 Usando arquivos de override (sobreposiÃ§Ã£o) do Docker Compose.
        *   10.3 Injetando configuraÃ§Ã£o com variÃ¡veis de ambiente e secrets.
        *   10.4 Reduzindo duplicaÃ§Ã£o com campos de extensÃ£o (extension fields).
        *   10.5 Entendendo o fluxo de trabalho de configuraÃ§Ã£o com Docker.
        *   10.6 Lab.
    *   **CapÃ­tulo 11: Building and testing applications with Docker and Docker Compose**
        *   11.1 Como o processo de CI (IntegraÃ§Ã£o ContÃ­nua) funciona com Docker.
        *   11.2 Criando infraestrutura de build com Docker.
        *   11.3 Capturando configuraÃ§Ãµes de build com Docker Compose.
        *   11.4 Escrevendo jobs de CI sem dependÃªncias exceto Docker.
        *   11.5 Entendendo contÃªineres no processo de CI.
        *   11.6 Lab.
*   **PARTE 3: RUNNING AT SCALE WITH A CONTAINER ORCHESTRATOR** (detalhado).
    *   **CapÃ­tulo 12: Understanding orchestration: Docker Swarm and Kubernetes**
        *   12.1 O que Ã© um orquestrador de contÃªineres?
        *   12.2 Configurando um cluster Docker Swarm.
        *   12.3 Rodando aplicaÃ§Ãµes como serviÃ§os Docker Swarm.
        *   12.4 Gerenciando trÃ¡fego de rede no cluster.
        *   12.5 Entendendo a escolha entre Docker Swarm e Kubernetes.
        *   12.6 Lab.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina conclui o detalhamento da Parte 2 (CapÃ­tulos 9-11) e inicia a Parte 3 (CapÃ­tulo 12).
    *   O CapÃ­tulo 10 aprofunda o uso do `Docker Compose`, mostrando como gerenciar diferentes ambientes (dev, teste, prod) usando `arquivos de override`, `variÃ¡veis de ambiente`, `secrets` e `campos de extensÃ£o` para evitar duplicaÃ§Ã£o e customizar configuraÃ§Ãµes.
    *   O CapÃ­tulo 11 integra o Docker ao processo de `IntegraÃ§Ã£o ContÃ­nua (CI)`, demonstrando como usar Docker e Compose para construir, testar e atÃ© mesmo hospedar a prÃ³pria infraestrutura de CI (como Jenkins).
    *   A Parte 3 (CapÃ­tulos 12-16) introduz a `orquestraÃ§Ã£o` para rodar aplicaÃ§Ãµes em mÃºltiplos servidores (clusters). O CapÃ­tulo 12 explica o conceito de orquestraÃ§Ã£o e apresenta os dois principais players: `Docker Swarm` e `Kubernetes`, focando em como configurar e usar o Swarm.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Cap. 10:** Uso de mÃºltiplos arquivos `-f` com `docker-compose`, definiÃ§Ã£o de variÃ¡veis de ambiente (`$env:VAR="valor"` no PowerShell), uso de `secrets` e `x-` (extension fields) no `docker-compose.yml`.
*   **Cap. 11:** ConfiguraÃ§Ã£o de um pipeline de CI (ex: Jenkins) para executar comandos `docker build` e `docker-compose build/up/push`.
*   **Cap. 12:** Comandos do Docker Swarm como `docker swarm init`, `docker node ls`, `docker service create`, `docker service ls`, `docker service ps`, `docker network create --driver overlay`.

---

ğŸ“„ PÃ¡gina 15

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o do sumÃ¡rio detalhado da Parte 3.
    *   **CapÃ­tulo 13: Deploying distributed applications as stacks in Docker Swarm**
        *   13.1 Usando Docker Compose para implantaÃ§Ãµes de produÃ§Ã£o.
        *   13.2 Gerenciando configuraÃ§Ã£o de app com objetos de configuraÃ§Ã£o (config objects).
        *   13.3 Gerenciando configuraÃ§Ãµes confidenciais com secrets.
        *   13.4 Armazenando dados com volumes no Swarm.
        *   13.5 Entendendo como o cluster gerencia stacks.
        *   13.6 Lab.
    *   **CapÃ­tulo 14: Automating releases with upgrades and rollbacks**
        *   14.1 O processo de atualizaÃ§Ã£o de aplicaÃ§Ã£o com Docker.
        *   14.2 Configurando lanÃ§amentos (rollouts) de produÃ§Ã£o com Compose.
        *   14.3 Configurando reversÃµes (rollbacks) de serviÃ§o.
        *   14.4 Gerenciando tempo de inatividade (downtime) para seu cluster.
        *   14.5 Entendendo alta disponibilidade (high availability) em clusters Swarm.
        *   14.6 Lab.
    *   **CapÃ­tulo 15: Configuring Docker for secure remote access and CI/CD**
        *   15.1 OpÃ§Ãµes de endpoint para a API Docker.
        *   15.2 Configurando Docker for acesso remoto seguro.
        *   15.3 Usando Contextos Docker para trabalhar com engines remotos.
        *   15.4 Adicionando implantaÃ§Ã£o contÃ­nua (CD) ao seu pipeline de CI.
        *   15.5 Entendendo o modelo de acesso para Docker.
        *   15.6 Lab.
    *   **CapÃ­tulo 16: Building Docker images that run anywhere: Linux, Windows, Intel, and Arm**
        *   16.1 Por que imagens multi-arquitetura sÃ£o importantes.
        *   16.2 Construindo imagens multi-arch a partir de um ou mais Dockerfiles.
        *   16.3 Enviando (push) imagens multi-arch para registros com manifestos.
        *   16.4 Construindo imagens multi-arch com Docker Buildx.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina continua detalhando a Parte 3, cobrindo os CapÃ­tulos 13 a 16.
    *   O CapÃ­tulo 13 mostra como usar arquivos `Docker Compose` para implantar aplicaÃ§Ãµes como `stacks` no `Docker Swarm`, introduzindo `config objects` e `secrets` para gerenciamento de configuraÃ§Ã£o em cluster, alÃ©m de volumes especÃ­ficos do Swarm.
    *   O CapÃ­tulo 14 foca no ciclo de vida das aplicaÃ§Ãµes no Swarm, detalhando como configurar `atualizaÃ§Ãµes (upgrades)` e `reversÃµes (rollbacks)` automÃ¡ticas de forma segura, e como gerenciar a disponibilidade do cluster.
    *   O CapÃ­tulo 15 aborda o `acesso remoto seguro` Ã  API Docker (essencial para administraÃ§Ã£o e `CI/CD`), usando TLS ou SSH, e como gerenciar mÃºltiplas conexÃµes com `Docker Contexts`. Mostra como integrar a ImplantaÃ§Ã£o ContÃ­nua (CD) ao pipeline.
    *   O CapÃ­tulo 16 retorna Ã  construÃ§Ã£o de imagens, explicando como criar `imagens multi-arquitetura` que rodam em diferentes sistemas operacionais (Linux, Windows) e CPUs (Intel, Arm) usando `manifestos` e a ferramenta `Docker Buildx`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Cap. 13:** Comando `docker stack deploy -c <arquivo.yml> <nome_stack>`, `docker config create`, `docker secret create`, uso de `configs` e `secrets` no Compose.
*   **Cap. 14:** ConfiguraÃ§Ã£o da seÃ§Ã£o `deploy` (com `update_config`, `rollback_config`) no Compose, comandos `docker service update --rollback`.
*   **Cap. 15:** ConfiguraÃ§Ã£o do Docker Engine para acesso remoto (nÃ£o recomendado diretamente no Docker Desktop, mas o conceito Ã© explicado), uso de `docker context create/use/ls`, configuraÃ§Ã£o de pipeline de CI/CD (ex: Jenkins) com credenciais para deploy remoto.
*   **Cap. 16:** Uso do `docker buildx build`, `docker manifest create/push/inspect`, construÃ§Ã£o de imagens para diferentes `--platform`.

---

ğŸ“„ PÃ¡gina 16

ğŸ” Resumo dos principais pontos:

*   ConclusÃ£o do sumÃ¡rio detalhado da Parte 3 e inÃ­cio da Parte 4.
    *   **CapÃ­tulo 16 (continuaÃ§Ã£o): Building Docker images that run anywhere: Linux, Windows, Intel, and Arm**
        *   16.5 Entendendo onde imagens multi-arch se encaixam no seu roadmap.
        *   16.6 Lab.
*   **PARTE 4: GETTING YOUR CONTAINERS READY FOR PRODUCTION** (detalhado).
    *   **CapÃ­tulo 17: Optimizing your Docker images for size, speed, and security**
        *   17.1 Como otimizar imagens Docker.
        *   17.2 Escolhendo as imagens base corretas.
        *   17.3 Minimizando a contagem e o tamanho das camadas da imagem.
        *   17.4 Levando seus builds multi-estÃ¡gio para o prÃ³ximo nÃ­vel.
        *   17.5 Entendendo por que a otimizaÃ§Ã£o conta.
        *   17.6 Lab.
    *   **CapÃ­tulo 18: Application configuration management in containers**
        *   18.1 Uma abordagem multi-camadas para configuraÃ§Ã£o de app.
        *   18.2 Empacotando configuraÃ§Ã£o para cada ambiente.
        *   18.3 Carregando configuraÃ§Ã£o do runtime.
        *   18.4 Configurando apps legados da mesma forma que apps novos.
        *   18.5 Entendendo por que um modelo de configuraÃ§Ã£o flexÃ­vel compensa.
        *   18.6 Lab.
    *   **CapÃ­tulo 19: Writing and managing application logs with Docker**
        *   19.1 Bem-vindo ao stderr e stdout!
        *   19.2 Retransmitindo logs de outros coletores (sinks) para stdout.
        *   19.3 Coletando e encaminhando logs de contÃªiner.
        *   19.4 Gerenciando sua saÃ­da e coleta de logs.
        *   19.5 Entendendo o modelo de logging de contÃªiner.
        *   19.6 Lab.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina finaliza a Parte 3 com o restante do CapÃ­tulo 16 e inicia a Parte 4 (CapÃ­tulos 17-19).
    *   O restante do CapÃ­tulo 16 discute consideraÃ§Ãµes estratÃ©gicas sobre o uso de imagens multi-arquitetura.
    *   A Parte 4 foca em preparar os contÃªineres para produÃ§Ã£o. O CapÃ­tulo 17 aborda tÃ©cnicas para `otimizar imagens Docker`, tornando-as menores, mais rÃ¡pidas de construir e mais seguras, incluindo a escolha de imagens base adequadas e o uso eficiente de camadas e builds multi-estÃ¡gio.
    *   O CapÃ­tulo 18 revisita o `gerenciamento de configuraÃ§Ã£o`, detalhando modelos flexÃ­veis para carregar configuraÃ§Ãµes de diferentes fontes (arquivos, variÃ¡veis de ambiente) e como aplicar isso atÃ© mesmo a aplicaÃ§Ãµes legadas.
    *   O CapÃ­tulo 19 explora o `logging em contÃªineres`, explicando o padrÃ£o de usar `stdout/stderr`, como lidar com apps que logam em arquivos, e como centralizar logs usando ferramentas como Fluentd e Elasticsearch (pilha EFK).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Cap. 17:** AnÃ¡lise do tamanho das imagens (`docker image ls`), uso de `.dockerignore`, tÃ©cnicas para reduzir o tamanho das camadas (combinar comandos `RUN`), comparaÃ§Ã£o de imagens base (ex: `-slim`, `-alpine` para Linux; `nanoserver` para Windows).
*   **Cap. 18:** ImplementaÃ§Ã£o de padrÃµes de configuraÃ§Ã£o em cÃ³digo (Node.js, .NET Core, Go), uso de variÃ¡veis de ambiente e montagem de volumes/secrets para injetar configuraÃ§Ã£o.
*   **Cap. 19:** ConfiguraÃ§Ã£o de aplicaÃ§Ãµes para logar em `stdout`, uso de `docker logs`, configuraÃ§Ã£o de `logging drivers` (como `fluentd`), execuÃ§Ã£o da pilha EFK com Docker Compose.

---

ğŸ“„ PÃ¡gina 17

ğŸ” Resumo dos principais pontos:

*   ConclusÃ£o do sumÃ¡rio detalhado da Parte 4.
    *   **CapÃ­tulo 20: Controlling HTTP traffic to containers with a reverse proxy**
        *   20.1 O que Ã© um proxy reverso?
        *   20.2 Lidando com roteamento e SSL no proxy reverso.
        *   20.3 Melhorando desempenho e confiabilidade com o proxy.
        *   20.4 Usando um proxy reverso cloud-native.
        *   20.5 Entendendo os padrÃµes que um proxy reverso habilita.
        *   20.6 Lab.
    *   **CapÃ­tulo 21: Asynchronous communication with a message queue**
        *   21.1 O que Ã© mensageria assÃ­ncrona?
        *   21.2 Usando uma fila de mensagens cloud-native.
        *   21.3 Consumindo e tratando mensagens.
        *   21.4 Adicionando novas funcionalidades com manipuladores de mensagem (message handlers).
        *   21.5 Entendendo padrÃµes de mensageria assÃ­ncrona.
        *   21.6 Lab.
    *   **CapÃ­tulo 22: Never the end**
        *   22.1 Rode sua prÃ³pria prova de conceito (Proof-of-Concept - PoC).
        *   22.2 Apresente um caso para Docker na sua organizaÃ§Ã£o.
        *   22.3 Planeje o caminho para produÃ§Ã£o.
        *   22.4 ConheÃ§a a comunidade Docker.
*   Link para o Ã­ndice (429).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina finaliza o sumÃ¡rio detalhado com os CapÃ­tulos 20, 21 e 22.
    *   O CapÃ­tulo 20 introduz o conceito de `proxy reverso` (como Nginx ou Traefik) para gerenciar o trÃ¡fego HTTP/S que chega aos contÃªineres, permitindo hospedar mÃºltiplos sites na mesma porta, lidar com certificados SSL e melhorar o desempenho com caching.
    *   O CapÃ­tulo 21 explora a `comunicaÃ§Ã£o assÃ­ncrona` usando `filas de mensagens` (como Redis ou NATS), um padrÃ£o que desacopla componentes, melhora a escalabilidade e permite adicionar funcionalidades sem alterar o cÃ³digo existente.
    *   O CapÃ­tulo 22 serve como conclusÃ£o, incentivando o leitor a aplicar o conhecimento em uma Prova de Conceito (PoC), a advogar pelo uso do Docker em sua organizaÃ§Ã£o, a planejar a adoÃ§Ã£o em produÃ§Ã£o e a se conectar com a comunidade Docker.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Cap. 20:** Rodar Nginx ou Traefik em contÃªineres, configurar roteamento baseado em host ou path, configurar terminaÃ§Ã£o SSL, habilitar caching ou sticky sessions.
*   **Cap. 1:** Rodar Redis ou NATS em contÃªineres, usar clientes (CLI ou cÃ³digo de aplicaÃ§Ã£o) para publicar e subscrever mensagens, implementar message handlers.
*   **Cap. 22:** Nenhum comando especÃ­fico, mas oferece um guia prÃ¡tico para aplicar o conhecimento adquirido.

---

ğŸ“„ PÃ¡gina 18

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Uma pÃ¡gina em branco, provavelmente separando o sumÃ¡rio detalhado do prefÃ¡cio.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 19

ğŸ” Resumo dos principais pontos:

*   **PrefÃ¡cio**
*   O autor relata sua experiÃªncia com Docker desde 2014 e a falta de um livro abrangente que pudesse recomendar.
*   O objetivo do livro Ã© preencher essa lacuna, sendo inclusivo para desenvolvedores e operadores, usuÃ¡rios de Linux e Windows.
*   Destaca o conceito central do Docker: empacotar uma aplicaÃ§Ã£o com suas dependÃªncias para rodar da mesma forma em qualquer lugar.
*   Menciona que essa portabilidade quebra barreiras entre equipes de desenvolvimento e operaÃ§Ãµes e habilita projetos de TI modernos.
*   Enfatiza a abordagem prÃ¡tica da sÃ©rie "Month of Lunches", com exercÃ­cios e laboratÃ³rios.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   No prefÃ¡cio, Elton Stoneman contextualiza a criaÃ§Ã£o do livro, motivado pela necessidade de um guia prÃ¡tico e acessÃ­vel sobre Docker para um pÃºblico amplo (devs, ops, Linux, Windows). Ele reitera o benefÃ­cio principal do Docker - a portabilidade ("build, share, run anywhere") - e como isso facilita a colaboraÃ§Ã£o e modernizaÃ§Ã£o de TI. Ele tambÃ©m justifica a escolha do formato "Month of Lunches" pela sua Ãªnfase no aprendizado prÃ¡tico atravÃ©s de exercÃ­cios.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando especÃ­fico, mas reforÃ§a que os exercÃ­cios do livro sÃ£o projetados para funcionar no seu ambiente Windows com Docker Desktop.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   O conceito de "empacotar tudo junto" Ã© como preparar uma marmita completa para o almoÃ§o: vocÃª leva a comida (aplicaÃ§Ã£o) e todos os utensÃ­lios necessÃ¡rios (dependÃªncias) em uma Ãºnica caixa (contÃªiner), garantindo que vocÃª possa comer (rodar) em qualquer lugar (ambiente) sem problemas.

---

ğŸ“„ PÃ¡gina 20

ğŸ” Resumo dos principais pontos:

*   **Agradecimentos (Acknowledgments)**
*   O autor agradece Ã  equipe da Manning Publications pelo suporte na criaÃ§Ã£o do livro.
*   Agradece aos revisores e aos participantes do programa de acesso antecipado (Early Access Program - MEAP) pelo feedback e contribuiÃ§Ãµes.
*   Lista nominalmente vÃ¡rios revisores que ajudaram a melhorar o livro.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina Ã© dedicada aos agradecimentos. O autor expressa sua gratidÃ£o Ã  editora Manning, aos revisores tÃ©cnicos e editoriais, e especialmente Ã  comunidade que participou do processo de desenvolvimento do livro atravÃ©s do programa MEAP, fornecendo feedback valioso. Ã‰ um reconhecimento padrÃ£o da colaboraÃ§Ã£o envolvida na publicaÃ§Ã£o de um livro tÃ©cnico.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 21

ğŸ” Resumo dos principais pontos:

*   **Sobre este livro (About this book)**
*   Objetivo claro: capacitar o leitor a rodar suas prÃ³prias aplicaÃ§Ãµes em Docker e saber os passos para levÃ¡-las Ã  produÃ§Ã£o.
*   Foco em tarefas do mundo real e construÃ§Ã£o incremental de experiÃªncia com Docker, aplicaÃ§Ãµes distribuÃ­das, orquestraÃ§Ã£o e ecossistema.
*   PÃºblico-alvo: usuÃ¡rios novos e em aprimoramento de Docker, sem exigir muito conhecimento prÃ©vio, abrangendo arquitetura, desenvolvimento e operaÃ§Ãµes.
*   ÃŠnfase na importÃ¢ncia de realizar os exercÃ­cios e laboratÃ³rios prÃ¡ticos.
*   Compatibilidade: os exemplos funcionam em Windows, Mac, Linux (incluindo Raspberry Pi).
*   CÃ³digo-fonte dos exemplos disponÃ­vel no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o define claramente o propÃ³sito e o pÃºblico do livro. O autor quer que, ao final da leitura (e prÃ¡tica), Alexandre (vocÃª!) se sinta confiante para usar Docker no dia a dia, desde a conteinerizaÃ§Ã£o de uma aplicaÃ§Ã£o atÃ© o planejamento para produÃ§Ã£o. O livro Ã© projetado para ser acessÃ­vel, mesmo sem profundo conhecimento prÃ©vio em Ã¡reas especÃ­ficas, e a chave para o aprendizado Ã© a execuÃ§Ã£o dos exemplos prÃ¡ticos fornecidos, que sÃ£o multiplataforma e estÃ£o disponÃ­veis online.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   ReforÃ§a que vocÃª poderÃ¡ usar o Docker Desktop no seu Windows para todos os exercÃ­cios.
*   Menciona a necessidade de baixar o cÃ³digo do GitHub (provavelmente via `git clone https://github.com/sixeyed/diamol.git` no PowerShell ou terminal do VS Code, ou baixando o ZIP).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   O livro Ã© como um curso prÃ¡tico de culinÃ¡ria: a teoria Ã© importante, mas vocÃª sÃ³ aprende de verdade a cozinhar (usar Docker) colocando a mÃ£o na massa (fazendo os exercÃ­cios).

---

ğŸ“„ PÃ¡gina 22

ğŸ” Resumo dos principais pontos:

*   **Como usar este livro (How to use this book)**
    *   Segue os princÃ­pios "Month of Lunches": capÃ­tulos projetados para serem concluÃ­dos em cerca de uma hora (leitura + exercÃ­cios + lab).
    *   Enfatiza a importÃ¢ncia de "trabalhar" nos capÃ­tulos, nÃ£o apenas ler.
*   **Sua jornada de aprendizado (Your learning journey)**
    *   Estrutura do livro dividida em partes:
        *   **Cap. 1:** IntroduÃ§Ã£o, instalaÃ§Ã£o do Docker, download dos recursos.
        *   **Cap. 2-6:** Fundamentos (rodar contÃªineres, empacotar apps, compartilhar imagens, armazenamento persistente com volumes).
        *   **Cap. 7-11:** AplicaÃ§Ãµes distribuÃ­das (Docker Compose, healthchecks, monitoramento, mÃºltiplos ambientes, CI com Docker).
        *   **Cap. 12-16:** Rodando em escala com orquestrador (Docker Swarm, deploy em cluster, builds multi-arquitetura - Linux, Windows, Intel, Arm).
        *   **Cap. 17-21:** TÃ³picos avanÃ§ados para produÃ§Ã£o (otimizaÃ§Ã£o de imagem, configuraÃ§Ã£o, logging, proxy reverso, filas de mensagens).
        *   **Cap. 22:** PrÃ³ximos passos (PoC, adoÃ§Ã£o organizacional, caminho para produÃ§Ã£o).
    *   Objetivo final: confianÃ§a para usar Docker no trabalho diÃ¡rio.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina detalha a metodologia do livro e a estrutura do aprendizado. A ideia Ã© dedicar cerca de uma hora por dia durante um mÃªs para cobrir todo o conteÃºdo, com foco na execuÃ§Ã£o prÃ¡tica. A jornada de aprendizado Ã© progressiva: comeÃ§a com os conceitos bÃ¡sicos do Docker, avanÃ§a para aplicaÃ§Ãµes com mÃºltiplos contÃªineres usando Docker Compose, depois aborda a execuÃ§Ã£o em escala com orquestradores (Swarm), e finaliza com tÃ³picos essenciais para preparar as aplicaÃ§Ãµes para o ambiente de produÃ§Ã£o, alÃ©m de orientaÃ§Ãµes para os prÃ³ximos passos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando especÃ­fico, mas estrutura o que esperar em termos de comandos e conceitos em cada parte do livro. Por exemplo, vocÃª sabe que `Docker Compose` serÃ¡ introduzido nos capÃ­tulos 7-11 e `Docker Swarm` nos capÃ­tulos 12-16.

---

ğŸ“„ PÃ¡gina 23

ğŸ” Resumo dos principais pontos:

*   **Try-it-nows (Experimente agora)**
    *   ExercÃ­cios guiados em todos os capÃ­tulos.
    *   CÃ³digo-fonte no GitHub: https://github.com/sixeyed/diamol.
    *   Os capÃ­tulos se baseiam nos anteriores, mas nÃ£o Ã© estritamente necessÃ¡rio segui-los em ordem, pois as imagens necessÃ¡rias jÃ¡ estÃ£o publicadas no Docker Hub.
    *   Recomenda fazer os exercÃ­cios para melhor aprendizado.
*   **Hands-on labs (LaboratÃ³rios prÃ¡ticos)**
    *   No final de cada capÃ­tulo, com desafios menos guiados.
    *   InstruÃ§Ãµes e dicas sÃ£o fornecidas.
    *   SoluÃ§Ãµes disponÃ­veis no repositÃ³rio GitHub.
*   **Recursos adicionais (Additional resources)**
    *   DocumentaÃ§Ã£o oficial do Docker: https://docs.docker.com.
    *   MÃ­dias sociais: Docker no Twitter/Facebook, autor Elton Stoneman (@EltonStoneman no Twitter, blog https://blog.sixeyed.com, YouTube https://youtube.com/eltonstoneman).
*   **Sobre o cÃ³digo (About the code)**
    *   Exemplos de Dockerfiles e manifestos no livro.
    *   CÃ³digo disponÃ­vel para download no site da Manning e no GitHub (https://github.com/sixeyed/diamol).
*   **FÃ³rum de discussÃ£o liveBook (liveBook discussion forum)**
    *   Acesso gratuito a um fÃ³rum privado para compradores do livro para tirar dÃºvidas e interagir com o autor e outros usuÃ¡rios.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina descreve os recursos de aprendizado prÃ¡tico do livro e onde encontrar ajuda adicional.
    *   Os `Try-it-nows` sÃ£o exercÃ­cios passo a passo integrados aos capÃ­tulos.
    *   Os `Labs` sÃ£o desafios mais abertos ao final de cada capÃ­tulo para consolidar o conhecimento.
    *   AlÃ©m do livro, a `documentaÃ§Ã£o oficial` do Docker e os `canais do autor` (blog, Twitter, YouTube) sÃ£o fontes valiosas.
    *   Todo o `cÃ³digo` usado nos exemplos estÃ¡ disponÃ­vel online.
    *   HÃ¡ um `fÃ³rum` exclusivo para quem comprou o livro interagir e tirar dÃºvidas.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `git clone https://github.com/sixeyed/diamol.git` (ou download do ZIP) para obter os arquivos dos exercÃ­cios e labs.
*   Os arquivos `Dockerfile` e `docker-compose.yml` serÃ£o editados no VS Code.
*   VocÃª rodarÃ¡ comandos `docker` e `docker-compose` no PowerShell ou terminal integrado do VS Code.
*   A documentaÃ§Ã£o em `docs.docker.com` serÃ¡ uma referÃªncia constante.

---

ğŸ“„ PÃ¡gina 24

ğŸ” Resumo dos principais pontos:

*   **FÃ³rum de discussÃ£o liveBook (continuaÃ§Ã£o)**
    *   Link para acesso ao fÃ³rum: https://livebook.manning.com/#!/book/learn-docker-in-a-month-of-lunches/discussion.
    *   Link para regras de conduta: https://livebook.manning.com/#!/discussion.
    *   O objetivo do fÃ³rum Ã© proporcionar diÃ¡logo significativo.
    *   A participaÃ§Ã£o do autor Ã© voluntÃ¡ria.
    *   O fÃ³rum permanecerÃ¡ acessÃ­vel enquanto o livro estiver em catÃ¡logo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina continua detalhando o fÃ³rum de discussÃ£o online associado ao livro (liveBook). Fornece os links diretos para acessar o fÃ³rum e suas regras. Esclarece que o fÃ³rum Ã© um benefÃ­cio para facilitar a interaÃ§Ã£o e o aprendizado, mas a participaÃ§Ã£o do autor nÃ£o Ã© garantida em tempo integral. O acesso ao fÃ³rum estÃ¡ vinculado Ã  compra do livro e dura enquanto ele for comercializado pela editora.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando, mas um recurso Ãºtil (o fÃ³rum) para tirar dÃºvidas que possam surgir ao longo dos exercÃ­cios no seu ambiente Windows.

---

ğŸ“„ PÃ¡gina 25

ğŸ” Resumo dos principais pontos:

*   **Sobre o autor (About the author)**
    *   Elton Stoneman Ã© Docker Captain e Microsoft MVP.
    *   Autor de cursos online na Pluralsight.
    *   ExperiÃªncia como consultor em sistemas .NET empresariais.
    *   Trabalhou na Docker Inc. por trÃªs anos.
    *   Atualmente Ã© consultor e instrutor freelancer focado em contÃªineres.
    *   Escreve sobre Docker e Kubernetes em seu blog (https://blog.sixeyed.com) e Twitter (@EltonStoneman).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina apresenta o autor, Elton Stoneman. Destaca suas credenciais e experiÃªncia na Ã¡rea de Docker, Kubernetes e desenvolvimento de software (especialmente .NET). Ser um Docker Captain e Microsoft MVP indica reconhecimento pela comunidade e pelas empresas. Sua experiÃªncia prÃ¡tica, tanto como consultor quanto trabalhando diretamente na Docker, confere credibilidade ao conteÃºdo do livro. Seus canais online (blog, Twitter) sÃ£o mencionados como locais para encontrar mais conteÃºdo dele.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando. Saber que o autor tem experiÃªncia com .NET e Windows (sendo MVP) pode ser relevante para vocÃª, indicando que ele provavelmente entende os desafios especÃ­ficos desse ambiente ao trabalhar com Docker.

---

ğŸ“„ PÃ¡gina 26

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, servindo de separador antes do inÃ­cio da Parte 1 do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 27

ğŸ” Resumo dos principais pontos:

*   **InÃ­cio da Parte 1: Understanding Docker containers and images**
*   Boas-vindas ao livro.
*   Objetivo da Parte 1: familiarizar o leitor rapidamente com os conceitos centrais do Docker (contÃªineres, imagens, registros).
*   ConteÃºdo da Parte 1: aprender a rodar aplicaÃ§Ãµes em contÃªineres, empacotar suas prÃ³prias aplicaÃ§Ãµes e compartilhÃ¡-las.
*   TambÃ©m abordarÃ¡ o armazenamento de dados com volumes Docker para aplicaÃ§Ãµes stateful (que guardam estado).
*   Promessa: ao final da Parte 1, o leitor estarÃ¡ confortÃ¡vel com os fundamentos e terÃ¡ aprendido boas prÃ¡ticas desde o inÃ­cio.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina serve como introduÃ§Ã£o Ã  Parte 1 do livro. Ela estabelece o foco desta seÃ§Ã£o nos conceitos fundamentais do Docker: o que sÃ£o contÃªineres, como sÃ£o construÃ­dos a partir de imagens e como essas imagens sÃ£o gerenciadas e compartilhadas atravÃ©s de registros. A parte prÃ¡tica envolverÃ¡ rodar, criar e compartilhar contÃªineres/imagens. Um ponto importante mencionado Ã© o tratamento de dados persistentes (usando volumes), essencial para aplicaÃ§Ãµes que nÃ£o podem perder informaÃ§Ãµes quando o contÃªiner Ã© recriado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Antecipa os comandos que serÃ£o usados nesta parte: `docker run`, `docker build`, `docker push`, `docker pull`, `docker volume create`, etc., todos executÃ¡veis via PowerShell/terminal no seu Docker Desktop.

---

ğŸ“„ PÃ¡gina 28

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, provavelmente separando a introduÃ§Ã£o da Parte 1 do inÃ­cio do CapÃ­tulo 1.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 29

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 1: Before you begin**
*   Introduz Docker como plataforma para rodar aplicaÃ§Ãµes em contÃªineres.
*   Menciona a popularidade crescente do Docker (citando pesquisa Stack Overflow 2019).
*   ReforÃ§a a simplicidade de aprender Docker e a abordagem prÃ¡tica do livro (exemplos e labs multiplataforma).
*   Destaca a importÃ¢ncia de entender os casos de uso e problemas que contÃªineres resolvem antes de comeÃ§ar a usÃ¡-los.
*   Lista os cinco principais cenÃ¡rios de sucesso com Docker que serÃ£o abordados no capÃ­tulo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina inicia o CapÃ­tulo 1. Ela reafirma o que Ã© Docker (uma plataforma para executar aplicaÃ§Ãµes em unidades leves chamadas contÃªineres) e sua relevÃ¢ncia no mercado atual. O capÃ­tulo tem como objetivo contextualizar o Docker, explicando *por que* ele Ã© importante e *quais problemas* ele resolve no mundo real, antes de mergulhar nos aspectos tÃ©cnicos nos capÃ­tulos seguintes. A ideia Ã© motivar o aprendizado mostrando o valor prÃ¡tico da tecnologia.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ainda, mas prepara o terreno para a instalaÃ§Ã£o e verificaÃ§Ã£o do Docker no seu ambiente Windows nas prÃ³ximas pÃ¡ginas.

---

ğŸ“„ PÃ¡gina 30

ğŸ” Resumo dos principais pontos:

*   **1.1 Why containers will take over the world** (Por que contÃªineres dominarÃ£o o mundo)
*   Relato da jornada pessoal do autor com Docker desde 2014.
*   Exemplo de projeto onde Docker simplificou desenvolvimento, testes, produÃ§Ã£o e handover.
*   Docker centraliza ferramentas e facilita a colaboraÃ§Ã£o entre desenvolvedores e administradores.
*   VisÃ£o de que Docker se tornaria ubÃ­quo devido Ã  facilidade e flexibilidade (apps antigos/novos, Windows/Linux).
*   **1.1.1 Migrating apps to the cloud** (Migrando apps para a nuvem)
    *   Desafios da migraÃ§Ã£o para nuvem: opÃ§Ãµes IaaS (Infrastructure as a Service) vs PaaS (Platform as a Service).
    *   IaaS: VMs, portabilidade entre nuvens, mas custo alto e ineficiÃªncia.
    *   PaaS: ServiÃ§os gerenciados, custo menor, mas aprisionamento tecnolÃ³gico (lock-in) e esforÃ§o de migraÃ§Ã£o.
    *   Docker como terceira opÃ§Ã£o: conteinerizar a aplicaÃ§Ã£o e rodar em serviÃ§os de contÃªineres (AKS, ECS) ou cluster prÃ³prio.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O autor comeÃ§a compartilhando sua prÃ³pria experiÃªncia positiva com Docker, enfatizando como a tecnologia simplificou drasticamente um projeto complexo, desde o desenvolvimento atÃ© a entrega e manutenÃ§Ã£o, unificando as ferramentas usadas por desenvolvedores e operadores.
*   Em seguida, aborda o primeiro grande cenÃ¡rio de uso: a migraÃ§Ã£o de aplicaÃ§Ãµes para a nuvem. Explica as limitaÃ§Ãµes das abordagens tradicionais: IaaS (basicamente alugar VMs na nuvem) que Ã© flexÃ­vel mas cara, e PaaS (usar serviÃ§os especÃ­ficos da nuvem) que Ã© mais barata mas exige reescrita e prende vocÃª a um provedor. Docker surge como uma soluÃ§Ã£o intermediÃ¡ria ideal: vocÃª empacota sua aplicaÃ§Ã£o em contÃªineres e pode rodÃ¡-la de forma eficiente e portÃ¡til em qualquer nuvem (ou atÃ© mesmo no seu prÃ³prio datacenter) usando serviÃ§os de orquestraÃ§Ã£o de contÃªineres.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando direto, mas contextualiza por que usar Docker (instalado via Docker Desktop) Ã© vantajoso ao pensar em rodar suas aplicaÃ§Ãµes na Azure, AWS ou Google Cloud.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Migrar para nuvem: IaaS Ã© como alugar uma casa vazia (VM), vocÃª traz tudo e cuida de tudo. PaaS Ã© como alugar um quarto mobiliado num hotel especÃ­fico, conveniente mas limitado. Docker Ã© como colocar seus mÃ³veis (aplicaÃ§Ã£o) num contÃªiner de mudanÃ§a, vocÃª pode levar para qualquer casa ou hotel que aceite o contÃªiner.

---

ğŸ“„ PÃ¡gina 31

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 1.1) ilustrando as opÃ§Ãµes IaaS vs PaaS para migraÃ§Ã£o de um app distribuÃ­do para a nuvem.
    *   Mostra a ineficiÃªncia (muitas VMs) do IaaS e o aprisionamento (lock-in) do PaaS.
*   IntroduÃ§Ã£o ao Docker como alternativa que combina benefÃ­cios de custo (PaaS) e portabilidade (IaaS).
*   Diagrama (Figura 1.2) ilustrando a migraÃ§Ã£o via Docker para a nuvem.
    *   Mostra a mesma aplicaÃ§Ã£o rodando em contÃªineres em poucos servidores, destacando portabilidade e eficiÃªncia.
*   Menciona que os CapÃ­tulos 7, 13 e 14 detalharÃ£o como empacotar e rodar aplicaÃ§Ãµes distribuÃ­das em contÃªineres e em escala.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina continua a discussÃ£o sobre migraÃ§Ã£o para a nuvem, usando diagramas para visualizar as opÃ§Ãµes.
    *   A Figura 1.1 mostra um aplicativo com vÃ¡rias partes (web, API, banco de dados). Em IaaS, cada parte roda em sua prÃ³pria VM, o que Ã© caro. Em PaaS, cada parte Ã© substituÃ­da por um serviÃ§o especÃ­fico da nuvem, o que dificulta a mudanÃ§a para outra nuvem.
    *   A Figura 1.2 mostra a mesma aplicaÃ§Ã£o onde cada parte foi colocada em um contÃªiner Docker. Esses contÃªineres rodam de forma eficiente em menos mÃ¡quinas virtuais na nuvem (ou onde vocÃª quiser), combinando baixo custo com a liberdade de escolher ou trocar de provedor de nuvem. O livro promete ensinar como fazer isso nos capÃ­tulos subsequentes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando. Os diagramas ajudam a visualizar os benefÃ­cios conceituais do Docker que vocÃª explorarÃ¡ com comandos prÃ¡ticos mais tarde.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Relembrando a analogia da mudanÃ§a: A Figura 1.1 mostra ou alugar vÃ¡rias casas (caro - IaaS) ou se adaptar totalmente a um hotel especÃ­fico (restritivo - PaaS). A Figura 1.2 mostra usar seus contÃªineres de mudanÃ§a (Docker) para morar eficientemente em qualquer lugar que os aceite.

---

ğŸ“„ PÃ¡gina 32

ğŸ” Resumo dos principais pontos:

*   Reconhece que migrar para contÃªineres exige investimento: criar `Dockerfiles` (scripts de build) e `manifestos de aplicaÃ§Ã£o` (Docker Compose ou Kubernetes).
*   Vantagem: nÃ£o Ã© necessÃ¡rio mudar o cÃ³digo da aplicaÃ§Ã£o.
*   Resultado: a aplicaÃ§Ã£o roda da mesma forma em qualquer ambiente (laptop, nuvem), usando a mesma pilha tecnolÃ³gica.
*   **1.1.2 Modernizing legacy apps** (Modernizando apps legados)
    *   ContÃªineres rodam bem aplicaÃ§Ãµes monolÃ­ticas (grandes e antigas), mas elas limitam a agilidade (ex: longos ciclos de teste/release).
    *   Docker facilita a modernizaÃ§Ã£o gradual sem reescrever tudo.
    *   Abordagem:
        1.  Colocar o monolito em um Ãºnico contÃªiner.
        2.  ComeÃ§ar a extrair funcionalidades para seus prÃ³prios contÃªineres.
        3.  Usar a rede virtual do Docker para comunicaÃ§Ã£o entre os novos contÃªineres e o monolito.
*   Diagrama (Figura 1.3) ilustrando a decomposiÃ§Ã£o gradual de um monolito.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s discutir a migraÃ§Ã£o para nuvem, a pÃ¡gina aborda outro cenÃ¡rio: modernizar aplicaÃ§Ãµes antigas (monolitos). Rodar um monolito em um contÃªiner jÃ¡ traz benefÃ­cios de portabilidade, mas nÃ£o resolve problemas de agilidade (um pequeno bug exige testar e reimplantar toda a aplicaÃ§Ã£o gigante).
*   O Docker oferece um caminho para modernizar essas aplicaÃ§Ãµes gradualmente:
    1.  **Empacotar:** Primeiro, coloque o monolito como estÃ¡ dentro de um contÃªiner (usando um Dockerfile).
    2.  **Decompor:** Identifique uma funcionalidade e crie um novo serviÃ§o (em seu prÃ³prio contÃªiner) para ela. A comunicaÃ§Ã£o entre o monolito e o novo serviÃ§o acontece pela rede interna do Docker.
    3.  **Iterar:** Repita o processo, extraindo mais funcionalidades para contÃªineres separados.
*   A Figura 1.3 visualiza isso: um "roteador" (geralmente um proxy reverso, como veremos no Cap. 20) direciona algumas requisiÃ§Ãµes para o monolito original e outras para os novos microsserviÃ§os em contÃªineres. Isso permite modernizar sem parar tudo para uma reescrita completa.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker build` para criar a imagem do monolito a partir de um `Dockerfile`.
*   VocÃª usarÃ¡ `Docker Compose` (Cap. 7) para definir como o monolito e os novos microsserviÃ§os rodam juntos e se comunicam pela rede Docker.
*   Os CapÃ­tulos 20 e 21 mostrarÃ£o exemplos prÃ¡ticos dessa decomposiÃ§Ã£o.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Modernizar um monolito com Docker Ã© como reformar uma casa antiga quarto por quarto, em vez de demolir tudo e reconstruir. VocÃª continua morando na casa (rodando a aplicaÃ§Ã£o) enquanto moderniza partes dela (extrai funcionalidades para contÃªineres).

---

ğŸ“„ PÃ¡gina 33

ğŸ” Resumo dos principais pontos:

*   BenefÃ­cios da modernizaÃ§Ã£o gradual com Docker:
    *   Agilidade: Testes rÃ¡pidos focados no componente alterado.
    *   Escalabilidade: Escalar funcionalidades especÃ­ficas independentemente.
    *   Flexibilidade: Usar diferentes tecnologias para novos componentes.
*   **1.1.3 Building new cloud-native apps** (Construindo novos apps cloud-native)
    *   Docker tambÃ©m acelera projetos novos baseados em princÃ­pios cloud-native.
    *   DefiniÃ§Ã£o da CNCF (Cloud Native Computing Foundation): usar software open source, implantar como microsserviÃ§os, empacotar em contÃªineres e orquestrar dinamicamente.
*   Diagrama (Figura 1.4) mostrando uma arquitetura tÃ­pica de microsserviÃ§os.
    *   Exemplo: app de demonstraÃ§Ã£o `microservices-demo` no GitHub (https://github.com/microservices-demo).
    *   Cada componente tem seus prÃ³prios dados e API.
    *   Usa vÃ¡rias linguagens e bancos de dados.
    *   Cada componente tem um Dockerfile e o app Ã© definido em um Docker Compose.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s discutir apps legados, a pÃ¡gina foca em como o Docker beneficia a construÃ§Ã£o de aplicaÃ§Ãµes *novas*, especialmente as "cloud-native". Essas aplicaÃ§Ãµes sÃ£o projetadas desde o inÃ­cio para rodar bem na nuvem, geralmente seguindo o padrÃ£o de `microsserviÃ§os`.
*   A CNCF define as caracterÃ­sticas chave: uso de tecnologias open source, divisÃ£o da aplicaÃ§Ã£o em pequenos serviÃ§os independentes (microsserviÃ§os), cada um rodando em seu prÃ³prio contÃªiner, e gerenciamento desses contÃªineres por um orquestrador.
*   A Figura 1.4 exemplifica isso com uma aplicaÃ§Ã£o de e-commerce de demonstraÃ§Ã£o. Cada caixinha (carrinho, catÃ¡logo, pedidos, frontend) Ã© um microsserviÃ§o rodando em um contÃªiner. Eles se comunicam entre si (provavelmente via APIs), e cada um pode usar a tecnologia mais adequada (linguagem, banco de dados). O Docker Ã© fundamental aqui, pois cada microsserviÃ§o Ã© empacotado com seu `Dockerfile`, e a aplicaÃ§Ã£o como um todo Ã© definida com `Docker Compose`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O CapÃ­tulo 4 ensinarÃ¡ como usar `Dockerfiles multi-estÃ¡gio` para compilar e empacotar esses microsserviÃ§os (escritos em Java, Go, Node.js, etc.) sem precisar das ferramentas de build instaladas no seu Windows.
*   VocÃª usarÃ¡ `Docker Compose` para rodar toda essa aplicaÃ§Ã£o de microsserviÃ§os localmente com um Ãºnico comando.

---

ğŸ“„ PÃ¡gina 34

ğŸ” Resumo dos principais pontos:

*   Docker permite compilar cÃ³digo dentro do Dockerfile (CapÃ­tulo 4), eliminando a necessidade de ferramentas de desenvolvimento instaladas localmente. Desenvolvedores sÃ³ precisam do Docker.
*   Facilidade de integrar software de terceiros (bancos de dados, filas, ferramentas de monitoramento) disponÃ­veis como imagens no Docker Hub.
*   Mapa de projetos open source da CNCF disponÃ­veis como imagens Docker.
*   **1.1.4 Technical innovation: Serverless and more** (InovaÃ§Ã£o tÃ©cnica: Serverless e mais)
    *   ConsistÃªncia como motor da TI moderna: usar as mesmas ferramentas/processos para todos os projetos.
    *   Docker permite isso para apps antigos (.NET Framework/Windows) e novos (Go/Linux).
    *   Um Ãºnico cluster Docker pode rodar toda a variedade de aplicaÃ§Ãµes da empresa.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina continua destacando os benefÃ­cios do Docker para aplicaÃ§Ãµes novas e introduz o cenÃ¡rio de inovaÃ§Ã£o tÃ©cnica.
*   Um ponto importante Ã© que o Docker simplifica o ambiente de desenvolvimento. Como o build da aplicaÃ§Ã£o pode ocorrer *dentro* do processo de criaÃ§Ã£o da imagem Docker (usando builds multi-estÃ¡gio), o desenvolvedor nÃ£o precisa mais instalar compiladores, SDKs, etc., em sua mÃ¡quina local. Apenas o Docker Ã© necessÃ¡rio.
*   AlÃ©m disso, o ecossistema Docker (especialmente o Docker Hub) facilita enormemente a incorporaÃ§Ã£o de componentes de terceiros (como bancos de dados, ferramentas de cache, etc.), pois eles jÃ¡ vÃªm prontos para rodar como contÃªineres.
*   O Docker tambÃ©m se posiciona no centro de inovaÃ§Ãµes como `Serverless`. A ideia do Serverless Ã© rodar cÃ³digo em resposta a eventos, sem gerenciar servidores. Por baixo dos panos, as plataformas Serverless (como AWS Lambda, Azure Functions) usam contÃªineres.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker build` com Dockerfiles multi-estÃ¡gio (Cap. 4).
*   VocÃª usarÃ¡ `docker pull` para baixar imagens de softwares como bancos de dados (Postgres, Redis) ou ferramentas (Prometheus, Grafana) do Docker Hub.
*   VocÃª poderÃ¡ rodar frameworks Serverless open source (como OpenFaaS) localmente usando Docker.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Usar imagens do Docker Hub Ã© como usar blocos de Lego prÃ©-fabricados: vocÃª nÃ£o precisa construir cada peÃ§a (software) do zero; vocÃª pega as peÃ§as prontas (imagens) e as encaixa para montar sua aplicaÃ§Ã£o.

---

ğŸ“„ PÃ¡gina 35

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 1.5) mostrando um Ãºnico cluster Docker rodando monolitos legados, apps cloud-native e funÃ§Ãµes serverless.
*   Explica o conceito de Serverless: desenvolvedor escreve a funÃ§Ã£o, a plataforma cuida do build, empacotamento e execuÃ§Ã£o sob demanda.
*   Plataformas Serverless na nuvem usam Docker internamente, mas criam lock-in.
*   Alternativa: rodar plataformas Serverless open source (Nuclio, OpenFaaS, Fn Project) no seu prÃ³prio cluster Docker.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 1.5 ilustra a versatilidade de um cluster Docker: ele pode hospedar diferentes tipos de aplicaÃ§Ãµes simultaneamente â€“ desde as antigas e monolÃ­ticas atÃ© as modernas baseadas em microsserviÃ§os e as funÃ§Ãµes Serverless. Isso promove a consistÃªncia no gerenciamento da infraestrutura.
*   O conceito de `Serverless` Ã© explicado: o desenvolvedor foca apenas no cÃ³digo da funÃ§Ã£o, e a plataforma se encarrega de todo o resto (construir, empacotar, escalar, executar quando necessÃ¡rio). Embora as grandes nuvens (AWS, Azure, Google) ofereÃ§am serviÃ§os Serverless populares, eles prendem o usuÃ¡rio Ã quela plataforma especÃ­fica (lock-in). Uma alternativa Ã© usar frameworks Serverless `open source` que rodam sobre Docker (ou Kubernetes), permitindo ter Serverless no seu prÃ³prio datacenter ou em qualquer nuvem, sem lock-in.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª poderÃ¡ rodar contÃªineres do OpenFaaS ou Fn Project usando `docker run` ou `Docker Compose` no seu Docker Desktop para experimentar Serverless localmente.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Serverless Ã© como ter um cozinheiro sob demanda: vocÃª sÃ³ entrega a receita (cÃ³digo da funÃ§Ã£o), e ele aparece com todos os ingredientes e equipamentos (infraestrutura gerenciada pela plataforma), cozinha o prato (executa a funÃ§Ã£o) e desaparece, cobrando apenas pelo tempo que cozinhou.

---

ğŸ“„ PÃ¡gina 36

ğŸ” Resumo dos principais pontos:

*   Outras inovaÃ§Ãµes como Machine Learning (TensorFlow), Blockchain (Hyperledger) e IoT tambÃ©m se beneficiam do empacotamento e deploy consistentes do Docker.
*   Docker + Arm: parceria para tornar contÃªineres o padrÃ£o para dispositivos Edge e IoT.
*   **1.1.5 Digital transformation with DevOps** (TransformaÃ§Ã£o digital com DevOps)
    *   Problema operacional comum: silos entre desenvolvedores ("dev") e operadores ("ops"), levando a ciclos de culpa e processos de release lentos e arriscados.
    *   DevOps: visa agilidade atravÃ©s de uma Ãºnica equipe responsÃ¡vel por todo o ciclo de vida da aplicaÃ§Ã£o.
    *   DevOps Ã© principalmente cultural, mas tecnologia ajuda. Docker como ferramenta comum que une devs e ops (Dockerfiles, Compose files).
    *   Framework CALMS (Culture, Automation, Lean, Metrics, Sharing) e como Docker se encaixa em cada iniciativa.
*   **1.2 Is this book for you?** (Este livro Ã© para vocÃª?)
    *   ConfirmaÃ§Ã£o: Se vocÃª quer usar Docker para resolver problemas reais como os descritos, sim.
    *   Leva do zero ao deploy em cluster de produÃ§Ã£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina menciona brevemente outras Ã¡reas de inovaÃ§Ã£o (ML, Blockchain, IoT) onde Docker Ã© relevante, destacando a parceria com a Arm para dispositivos de borda (Edge) e Internet das Coisas (IoT).
*   O foco principal muda para o `DevOps`. O problema clÃ¡ssico sÃ£o as equipes separadas de desenvolvimento e operaÃ§Ãµes, com atritos e processos lentos. DevOps busca quebrar esses silos, promovendo colaboraÃ§Ã£o e automaÃ§Ã£o. O Docker atua como um catalisador tecnolÃ³gico para o DevOps, fornecendo uma linguagem e ferramentas comuns (`Dockerfile`, `Docker Compose`) que tanto desenvolvedores quanto operadores usam, facilitando a comunicaÃ§Ã£o e a automaÃ§Ã£o (um dos pilares do CALMS).
*   Finalmente, a seÃ§Ã£o 1.2 confirma que o livro Ã© destinado a quem quer aplicar Docker nesses cenÃ¡rios prÃ¡ticos, prometendo guiar o leitor desde o bÃ¡sico atÃ© a execuÃ§Ã£o em um cluster.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `Dockerfiles` e `docker-compose.yml` extensivamente, ferramentas centrais na colaboraÃ§Ã£o DevOps habilitada por Docker.
*   O CapÃ­tulo 11 mostrarÃ¡ a automaÃ§Ã£o (CI/CD) na prÃ¡tica.
*   O CapÃ­tulo 9 (mÃ©tricas) e o CapÃ­tulo 19 (logs) se relacionam com a parte de "Metrics" do CALMS.
*   O CapÃ­tulo 5 (compartilhamento de imagens) se relaciona com "Sharing".

---

ğŸ“„ PÃ¡gina 37

ğŸ” Resumo dos principais pontos:

*   **O que o livro NÃƒO cobre em detalhes:**
    *   Internals profundos do Docker (containerd, cgroups, namespaces, Host Compute Service). Recomenda "Docker in Action, 2Âª ed." para isso.
    *   Amostras nÃ£o sÃ£o focadas apenas em Windows (.NET Framework), usam linguagens multiplataforma (.NET Core, etc.). Recomenda o blog do autor para Windows Containers.
    *   Foco em Docker Swarm para orquestraÃ§Ã£o de produÃ§Ã£o, nÃ£o Kubernetes em detalhes (embora discuta a escolha no Cap. 12). Tudo aprendido se aplica a Kubernetes.
*   **1.3 Creating your lab environment** (Criando seu ambiente de laboratÃ³rio)
    *   NecessÃ¡rio: Docker e o cÃ³digo-fonte dos exemplos.
    *   **1.3.1 Installing Docker** (Instalando Docker)
        *   Usar Docker Community Edition (gratuito).
        *   OpÃ§Ãµes: Docker Desktop (Windows 10 Pro/Enterprise recente, macOS recente), Docker Toolbox (versÃµes mais antigas), pacotes Linux.
        *   NecessÃ¡rio criar conta no Docker Hub (gratuita).
    *   **Instalando Docker Desktop no Windows 10:**
        *   Requisitos: Windows 10 Pro/Enterprise (mÃ­nimo release 1809), verificar com `winver`.
        *   Baixar instalador estÃ¡vel de www.docker.com/products/docker-desktop.
        *   Rodar instalador com padrÃµes. Ãcone da baleia na barra de tarefas indica que estÃ¡ rodando.
    *   **Instalando Docker Desktop no macOS:**
        *   Requisitos: macOS Sierra 10.12+.
        *   Baixar instalador estÃ¡vel do mesmo site.
        *   Rodar instalador com padrÃµes. Ãcone da baleia na barra de menus.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina define o escopo do livro, esclarecendo o que *nÃ£o* serÃ¡ aprofundado (detalhes internos do Docker, foco exclusivo em Windows Containers, Kubernetes detalhado) e recomendando outros recursos para esses tÃ³picos.
*   Em seguida, inicia a configuraÃ§Ã£o do ambiente prÃ¡tico (lab). O essencial Ã© ter o Docker instalado e acesso ao cÃ³digo dos exemplos.
*   ComeÃ§a a detalhar a instalaÃ§Ã£o do Docker, recomendando a versÃ£o Community Edition (CE), que Ã© gratuita. As opÃ§Ãµes variam conforme o sistema operacional:
    *   **Docker Desktop:** A melhor opÃ§Ã£o para Windows 10 (Pro/Enterprise) e macOS recentes. Ele integra o Docker de forma mais nativa.
    *   **Docker Toolbox:** Uma opÃ§Ã£o mais antiga para versÃµes legadas de Windows/macOS, que usa uma mÃ¡quina virtual (como VirtualBox) por baixo dos panos.
    *   **Pacotes Linux:** InstruÃ§Ãµes especÃ­ficas para distribuiÃ§Ãµes Linux.
*   A pÃ¡gina detalha os passos especÃ­ficos para instalar o **Docker Desktop no Windows 10** (seu caso) e no macOS, incluindo requisitos de sistema e como verificar se a instalaÃ§Ã£o foi bem-sucedida (procurando pelo Ã­cone da baleia). A criaÃ§Ã£o de uma conta no Docker Hub tambÃ©m Ã© mencionada como necessÃ¡ria para downloads.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** Seguir as instruÃ§Ãµes para **instalar o Docker Desktop para Windows**.
    *   Verificar sua versÃ£o do Windows com o comando `winver` (digitar no menu Iniciar ou Executar).
    *   Ir ao site `docker.com`, baixar e executar o instalador do Docker Desktop (versÃ£o Stable).
    *   Criar uma conta no `hub.docker.com` se ainda nÃ£o tiver.
    *   ApÃ³s a instalaÃ§Ã£o, verificar se o Ã­cone da baleia aparece na Ã¡rea de notificaÃ§Ã£o.

---

ğŸ“„ PÃ¡gina 38

ğŸ” Resumo dos principais pontos:

*   **Instalando Docker Toolbox:**
    *   Para versÃµes antigas de Windows/OS X.
    *   Requer software de VM como VirtualBox.
    *   Link para docs: https://docs.docker.com/toolbox.
    *   Docker Desktop Ã© preferÃ­vel se possÃ­vel.
*   **Instalando Docker Community Edition e Docker Compose (Linux):**
    *   NÃ£o usar a versÃ£o que vem com a distribuiÃ§Ã£o (geralmente antiga).
    *   Usar script de instalaÃ§Ã£o do Docker em https://get.docker.com (para ambientes nÃ£o-produÃ§Ã£o).
    *   Instalar Docker Compose separadamente via https://docs.docker.com/compose/install.
*   **Instalando Docker em Windows Server ou Distros Linux de Servidor:**
    *   Pode usar Community Edition (CE) ou Docker Enterprise (EE - versÃ£o comercial suportada).
    *   EE Ã© construÃ­do sobre CE, entÃ£o o livro se aplica a ambos.
    *   VersÃµes EE disponÃ­veis para Linux e Windows Server 2016/2019.
    *   InstruÃ§Ãµes de instalaÃ§Ã£o do EE no Docker Hub: http://mng.bz/K29E.
*   **1.3.2 Verifying your Docker setup** (Verificando sua instalaÃ§Ã£o Docker)
    *   Verificar Docker com `docker version`.
        *   Deve mostrar informaÃ§Ãµes do Cliente (Client) e do Servidor (Server).
    *   Exemplo de saÃ­da do `docker version`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina continua as instruÃ§Ãµes de instalaÃ§Ã£o do Docker para diferentes cenÃ¡rios:
    *   **Docker Toolbox:** Mencionado como alternativa para sistemas mais antigos, mas desencorajado se o Docker Desktop for compatÃ­vel.
    *   **Linux (CE):** InstruÃ§Ãµes especÃ­ficas para usuÃ¡rios Linux, recomendando o script oficial em vez das versÃµes dos repositÃ³rios da distro, e a instalaÃ§Ã£o separada do Docker Compose.
    *   **Servidores (Windows/Linux - CE/EE):** Esclarece que tanto a versÃ£o gratuita (CE) quanto a paga e suportada (EE) podem ser usadas, e que o conteÃºdo do livro vale para ambas. O EE Ã© mais comum em produÃ§Ã£o.
*   Em seguida, passa para a **verificaÃ§Ã£o da instalaÃ§Ã£o**. O comando essencial Ã© `docker version`. Ã‰ crucial que este comando retorne informaÃ§Ãµes tanto para o "Client" (a ferramenta de linha de comando que vocÃª usa) quanto para o "Server" (o Docker Engine que executa os contÃªineres). Se ambos aparecerem, a instalaÃ§Ã£o bÃ¡sica estÃ¡ funcionando.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** Abrir o PowerShell (como administrador, se possÃ­vel) ou o terminal do VS Code e executar:
    ```powershell
    docker version
    ```
*   Analisar a saÃ­da: VocÃª deve ver seÃ§Ãµes "Client" e "Server". A seÃ§Ã£o "Server" no Docker Desktop para Windows provavelmente mostrarÃ¡ "OS/Arch: linux/amd64" se vocÃª estiver no modo Linux (padrÃ£o) ou "windows/amd64" se estiver no modo Windows. O importante Ã© nÃ£o haver erros e ambas as seÃ§Ãµes estarem presentes.

---

ğŸ“„ PÃ¡gina 39

ğŸ” Resumo dos principais pontos:

*   Exemplo de saÃ­da do `docker version` (continuaÃ§Ã£o).
    *   Detalha as informaÃ§Ãµes retornadas (versÃ£o, API, Go, Git commit, OS/Arch, Experimental).
    *   NÃ£o se preocupar com os detalhes de cliente/servidor ainda (serÃ£o explicados no Cap. 2).
*   Verificar Docker Compose com `docker-compose version`.
    *   Deve retornar versÃµes sem erros.
*   Exemplo de saÃ­da do `docker-compose version`.
*   **1.3.3 Downloading the source code for the book** (Baixando o cÃ³digo-fonte do livro)
    *   RepositÃ³rio Git pÃºblico no GitHub: https://github.com/sixeyed/diamol.
    *   Comando para clonar (se tiver Git): `git clone https://github.com/sixeyed/diamol.git`.
    *   Alternativa: Baixar como ZIP do site do GitHub e extrair.
*   **1.3.4 Remembering the cleanup commands** (Lembrando os comandos de limpeza)
    *   Docker nÃ£o limpa automaticamente contÃªineres/imagens.
    *   ContÃªineres parados nÃ£o usam CPU/memÃ³ria, mas ocupam disco.
    *   Comando para remover todos os contÃªineres: `docker container rm -f $(docker container ls -aq)`.
    *   Comando para remover imagens do livro (prefixo 'diamol/*'): `docker image rm -f $(docker image ls -f reference='diamol/*' -q)`.
    *   Docker baixa o que precisa se nÃ£o encontrar localmente.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina continua a verificaÃ§Ã£o da instalaÃ§Ã£o, agora focando no `Docker Compose`. O comando `docker-compose version` confirma que essa ferramenta essencial (que vocÃª usarÃ¡ muito a partir do CapÃ­tulo 7) tambÃ©m estÃ¡ funcionando.
*   Em seguida, instrui como obter o **cÃ³digo-fonte dos exemplos** do livro. A forma preferida Ã© usar o Git com o comando `git clone`, que baixa todo o histÃ³rico do repositÃ³rio. Se vocÃª nÃ£o tiver o Git instalado, pode baixar um arquivo ZIP diretamente do GitHub.
*   Por fim, uma dica importante sobre **limpeza**. O Docker acumula contÃªineres parados e imagens que nÃ£o estÃ£o mais em uso, consumindo espaÃ§o em disco. SÃ£o fornecidos dois comandos Ãºteis para remover todos os contÃªineres e as imagens especÃ­ficas baixadas/criadas para os exercÃ­cios do livro. Ã‰ bom saber desses comandos para manter seu ambiente organizado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  No PowerShell/terminal, execute:
        ```powershell
        docker-compose version
        ```
        Verifique se nÃ£o hÃ¡ erros.
    2.  Escolha um diretÃ³rio para os exemplos (ex: `C:\DockerLabs`). Navegue atÃ© ele no terminal.
    3.  Se vocÃª tem Git instalado (recomendado, pode baixar de `git-scm.com`), execute:
        ```powershell
        git clone https://github.com/sixeyed/diamol.git
        ```
        Se nÃ£o tiver Git, baixe o ZIP do link, extraia-o para `C:\DockerLabs`, e talvez renomeie a pasta resultante de `diamol-main` para `diamol`.
    4.  **Anote** os comandos de limpeza para usar periodicamente (especialmente apÃ³s cada capÃ­tulo ou se ficar sem espaÃ§o):
        ```powershell
        # Remover todos os contÃªineres (parados ou rodando)
        docker container rm -f $(docker container ls -aq)

        # Remover imagens dos exemplos do livro (com prefixo diamol/)
        docker image rm -f $(docker image ls -f reference='diamol/*' -q)
        ```
        *ObservaÃ§Ã£o:* A sintaxe `$(comando)` funciona bem no PowerShell moderno e no Bash (Linux/Mac), executando o comando interno e passando sua saÃ­da como argumento para o comando externo.

---

ğŸ“„ PÃ¡gina 40

ğŸ” Resumo dos principais pontos:

*   **1.4 Being immediately effective** (Sendo imediatamente eficaz)
*   Reitera o princÃ­pio "Month of Lunches": foco em aprender habilidades e colocÃ¡-las em prÃ¡tica.
*   Estrutura dos prÃ³ximos capÃ­tulos: introduÃ§Ã£o -> exercÃ­cios "try-it-now" -> recapitulaÃ§Ã£o/detalhes -> lab prÃ¡tico.
*   TÃ³picos focados em tarefas Ãºteis do mundo real.
*   Convite para comeÃ§ar a rodar contÃªineres.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina conclui o CapÃ­tulo 1, reforÃ§ando a filosofia prÃ¡tica do livro. A estrutura tÃ­pica dos capÃ­tulos seguintes Ã© apresentada: uma breve teoria seguida de exercÃ­cios guiados (`try-it-now`), uma explicaÃ§Ã£o um pouco mais detalhada e, por fim, um desafio prÃ¡tico (`lab`) para aplicar o que foi aprendido. O objetivo Ã© que cada capÃ­tulo ensine uma habilidade Ãºtil e aplicÃ¡vel imediatamente. O convite final Ã© para iniciar a parte prÃ¡tica, comeÃ§ando a rodar contÃªineres no prÃ³ximo capÃ­tulo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando. Prepara vocÃª para a estrutura dos prÃ³ximos capÃ­tulos e a necessidade de executar os comandos propostos.

---

ğŸ“„ PÃ¡gina 41

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 2: Understanding Docker and running Hello World**
*   InÃ­cio da parte prÃ¡tica com Docker.
*   Foco do capÃ­tulo: rodar aplicaÃ§Ãµes em contÃªineres, entender o que Ã© um contÃªiner e por que sÃ£o leves.
*   Uso de exercÃ­cios `try-it-now` com comandos simples.
*   **2.1 Running Hello World in a container** (Rodando Hello World em um contÃªiner)
    *   Primeiro comando prÃ¡tico: `docker container run diamol/ch02-hello-diamol`.
    *   InstruÃ§Ã£o para abrir um terminal (PowerShell recomendado para Windows).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 2, marcando o comeÃ§o da interaÃ§Ã£o prÃ¡tica com o Docker. O objetivo Ã© rodar o primeiro contÃªiner, entender o que acontece nesse processo e comeÃ§ar a se familiarizar com a linha de comando do Docker. O capÃ­tulo usa o exemplo clÃ¡ssico "Hello World" adaptado para contÃªineres. A seÃ§Ã£o 2.1 introduz o comando fundamental `docker container run` para executar um contÃªiner a partir de uma imagem especÃ­fica (`diamol/ch02-hello-diamol`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** Abrir o PowerShell ou terminal do VS Code.
*   Executar o comando:
    ```powershell
    docker container run diamol/ch02-hello-diamol
    ```
*   Observar a saÃ­da (que serÃ¡ analisada nas prÃ³ximas pÃ¡ginas). VocÃª provavelmente verÃ¡ o Docker baixar a imagem ("pulling") antes de executÃ¡-la, pois Ã© a primeira vez que vocÃª a usa.

---

ğŸ“„ PÃ¡gina 42

ğŸ” Resumo dos principais pontos:

*   Apresenta e analisa a saÃ­da completa do comando `docker container run diamol/ch02-hello-diamol` (Figura 2.1).
*   **ExplicaÃ§Ã£o do processo:**
    1.  O comando `docker container run` instrui o Docker a rodar um app em um contÃªiner.
    2.  A imagem (`diamol/ch02-hello-diamol`) contÃ©m o app empacotado e publicado.
    3.  Docker verifica se a imagem existe localmente. Se nÃ£o ("unable to find image locally"), ele baixa ("pulling") a imagem do registro (Docker Hub, por padrÃ£o).
    4.  Docker inicia um contÃªiner usando a imagem. A imagem contÃ©m instruÃ§Ãµes de como iniciar o app.
    5.  O app (um script simples) executa e sua saÃ­da Ã© exibida.
*   **AnÃ¡lise da SaÃ­da do App:**
    *   Mensagem "Hello from Chapter 2!".
    *   InformaÃ§Ãµes sobre o "computador" onde estÃ¡ rodando: nome da mÃ¡quina (ex: `e5943557213b`), sistema operacional (ex: `Linux 4.9.125-linuxkit x86_64`), endereÃ§o de rede (ex: `172.17.0.2`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina disseca o que aconteceu quando vocÃª rodou o primeiro comando. O Docker agiu como um gerenciador inteligente:
    1.  Recebeu a ordem: "rode um contÃªiner com a imagem X".
    2.  Procurou a "receita" (imagem X) localmente.
    3.  NÃ£o achou? Foi buscar na "biblioteca pÃºblica" (Docker Hub).
    4.  Baixou a receita.
    5.  Usou a receita para "montar" e "ligar" o ambiente isolado (contÃªiner).
    6.  Executou o programa que estava dentro do contÃªiner.
    7.  Mostrou o que o programa imprimiu na tela.
*   A saÃ­da do programa mostra que ele estÃ¡ rodando em um ambiente que *parece* ser uma mÃ¡quina separada, com seu prÃ³prio nome, OS e endereÃ§o IP. Isso demonstra o isolamento do contÃªiner.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revisar a saÃ­da que vocÃª obteve ao rodar `docker container run diamol/ch02-hello-diamol` na pÃ¡gina anterior. Compare com a Figura 2.1. VocÃª deve ter visto as etapas de "pulling" seguidas pela saÃ­da do "Hello".

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `docker container run` Ã© como pedir uma pizza (`diamol/ch02-hello-diamol`). Se a pizzaria (`Docker Engine`) nÃ£o tem a receita (`imagem`) na hora, ela busca no livro de receitas central (`Docker Hub`). Depois, ela prepara (`cria o contÃªiner`) e entrega (`executa o app`) a pizza para vocÃª (`mostra a saÃ­da`).

---

ğŸ“„ PÃ¡gina 43

ğŸ” Resumo dos principais pontos:

*   A saÃ­da pode variar dependendo do sistema operacional e arquitetura do host.
*   Exemplo de saÃ­da em Windows Containers: `Microsoft Windows [Version 10.0.17763.557]`.
*   Exemplo de saÃ­da em Raspberry Pi (Arm): `Linux 4.19.42-v7+ armv7l`.
*   Introduz o fluxo Docker: **Build, Share, Run** (Construir, Compartilhar, Rodar).
    *   AlguÃ©m empacota (Build) -> Publica (Share) -> Qualquer pessoa pode usar (Run).
*   ReforÃ§a que o fluxo Ã© o mesmo para apps simples ou complexos (ex: Java com mÃºltiplos componentes).
*   Portabilidade Ã© um benefÃ­cio chave do Docker.
*   **ExercÃ­cio Try-it-now:** Repetir o comando `docker container run diamol/ch02-hello-diamol`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que a informaÃ§Ã£o do sistema operacional dentro do contÃªiner reflete o ambiente real onde o Docker estÃ¡ rodando (ou o kernel que ele estÃ¡ usando, como no caso do Docker Desktop modo Linux no Windows). Isso demonstra que, embora isolado, o contÃªiner compartilha o kernel do SO hospedeiro.
*   O fluxo **Build, Share, Run** Ã© apresentado como o ciclo de vida fundamental do Docker:
    1.  **Build:** Um desenvolvedor cria um `Dockerfile` para empacotar a aplicaÃ§Ã£o e suas dependÃªncias em uma `imagem`.
    2.  **Share:** Essa imagem Ã© enviada (`push`) para um `registro` (como o Docker Hub).
    3.  **Run:** Qualquer outra pessoa (ou sistema) pode baixar (`pull`) essa imagem e executar (`run`) um `contÃªiner` a partir dela, em qualquer mÃ¡quina que tenha Docker, obtendo o mesmo resultado.
*   A `portabilidade` (rodar em qualquer lugar) Ã© um dos maiores trunfos do Docker.
*   O exercÃ­cio pede para rodar o mesmo comando novamente para observar o que muda.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   No seu Docker Desktop modo Linux, a saÃ­da do OS dentro do contÃªiner serÃ¡ Linux (provavelmente `linuxkit`). Se vocÃª mudar para o modo Windows Containers (clicando com o botÃ£o direito no Ã­cone da baleia > Switch to Windows containers...) e rodar uma imagem Windows compatÃ­vel, a saÃ­da refletiria o Windows Server Core.
*   **AÃ§Ã£o principal:** Execute novamente no PowerShell/terminal:
    ```powershell
    docker container run diamol/ch02-hello-diamol
    ```
*   Observe a saÃ­da (analisada na prÃ³xima pÃ¡gina). Note que desta vez nÃ£o deve haver a etapa de "pulling", pois a imagem jÃ¡ estÃ¡ local.

---

ğŸ“„ PÃ¡gina 44

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise da saÃ­da da segunda execuÃ§Ã£o do comando:
    *   Sem download ("pulling") da imagem, pois jÃ¡ existe localmente.
    *   ExecuÃ§Ã£o direta do contÃªiner.
    *   SaÃ­da do app mostra o mesmo SO, mas **nome de mÃ¡quina** e **endereÃ§o IP diferentes** (ex: `858a26ee2741`, `172.17.0.5`).
*   Pergunta levantada: De onde vÃªm esses nomes e IPs diferentes se estÃ£o rodando no mesmo computador?
*   IntroduÃ§Ã£o Ã  prÃ³xima seÃ§Ã£o (2.2) que explicarÃ¡ o que Ã© um contÃªiner.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Ao rodar o comando pela segunda vez, o Docker foi mais rÃ¡pido porque jÃ¡ tinha a "receita" (imagem) guardada. Ele apenas criou e rodou um novo contÃªiner.
*   A observaÃ§Ã£o crucial Ã© que, embora o sistema operacional *dentro* do contÃªiner seja o mesmo (pois compartilham o kernel do host), cada novo contÃªiner obtÃ©m uma identidade *Ãºnica* dentro do ambiente Docker. Isso se manifesta como um nome de mÃ¡quina aleatÃ³rio diferente e um endereÃ§o IP diferente (dentro da rede virtual gerenciada pelo Docker).
*   Isso reforÃ§a a ideia de isolamento: cada contÃªiner, mesmo sendo baseado na mesma imagem, Ã© uma instÃ¢ncia separada e independente com sua prÃ³pria identidade de rede e nome. A pÃ¡gina prepara o terreno para explicar *como* o Docker consegue criar esses ambientes isolados e virtuais.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Compare a saÃ­da da segunda execuÃ§Ã£o com a primeira. Verifique se o nome da mÃ¡quina (ex: `858a26ee2741`) e o endereÃ§o IP (ex: `172.17.0.5`) sÃ£o diferentes dos da primeira execuÃ§Ã£o e diferentes entre si a cada nova execuÃ§Ã£o.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Ã‰ como usar o mesmo molde (imagem) para fazer vÃ¡rios bolos (contÃªineres). Cada bolo Ã© feito com os mesmos ingredientes e formato, mas cada um Ã© um bolo individual. Se vocÃª colocar uma etiqueta em cada um (nome da mÃ¡quina, IP), cada etiqueta serÃ¡ Ãºnica.

---

ğŸ“„ PÃ¡gina 45

ğŸ” Resumo dos principais pontos:

*   **2.2 So what is a container?** (EntÃ£o, o que Ã© um contÃªiner?)
*   Analogia: ContÃªiner fÃ­sico = caixa com uma aplicaÃ§Ã£o dentro.
*   Dentro da "caixa", a aplicaÃ§Ã£o pensa ter um computador sÃ³ para ela:
    *   Nome de mÃ¡quina prÃ³prio.
    *   EndereÃ§o IP prÃ³prio.
    *   Disco (sistema de arquivos) prÃ³prio.
    *   (Para Windows) Registro do Windows prÃ³prio.
*   Diagrama (Figura 2.2) mostrando o app "encaixotado" pelo contÃªiner.
*   Recursos sÃ£o virtuais, criados e gerenciados pelo Docker.
*   O aplicativo dentro da caixa nÃ£o vÃª nada fora dela.
*   Muitas "caixas" (contÃªineres) rodam no mesmo computador host.
*   ContÃªineres compartilham CPU, memÃ³ria e Sistema Operacional do host.
*   Diagrama (Figura 2.3) mostrando mÃºltiplos contÃªineres isolados no mesmo host.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina define formalmente o que Ã© um contÃªiner Docker, usando a analogia de uma caixa.
*   **Isolamento:** O ponto principal Ã© que, para a aplicaÃ§Ã£o *dentro* do contÃªiner, parece que ela estÃ¡ rodando sozinha em sua prÃ³pria mÃ¡quina. Ela tem seus prÃ³prios recursos virtualizados (nome, IP, disco). Isso Ã© o isolamento. A Figura 2.2 ilustra essa "bolha" em torno do app.
*   **Compartilhamento:** No entanto, essa "mÃ¡quina" Ã© virtual. Na realidade (Figura 2.3), vÃ¡rios desses contÃªineres rodam no mesmo computador fÃ­sico (o host) e compartilham os recursos reais: o processador (CPU), a memÃ³ria RAM e, crucialmente, o *kernel* do Sistema Operacional do host.
*   Ã‰ essa combinaÃ§Ã£o de isolamento (a aplicaÃ§Ã£o nÃ£o interfere nas outras) e compartilhamento (usam eficientemente os recursos do host) que torna os contÃªineres tÃ£o poderosos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Os nomes de mÃ¡quina (ex: `858a26ee2741`) e IPs (ex: `172.17.0.5`) que vocÃª viu nas execuÃ§Ãµes anteriores sÃ£o exemplos desses recursos virtuais criados pelo Docker para cada contÃªiner. O sistema de arquivos que o contÃªiner vÃª Ã© uma construÃ§Ã£o virtual sobre o sistema de arquivos do seu Windows (ou da VM Linux no Docker Desktop).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Retomando a analogia dos apartamentos (contÃªineres) em um prÃ©dio (host): cada apartamento tem seu prÃ³prio nÃºmero (nome/IP) e layout interno (disco virtual), mas todos usam a fundaÃ§Ã£o e estrutura do prÃ©dio (kernel do SO), alÃ©m da eletricidade (CPU) e Ã¡gua (memÃ³ria) fornecidas centralmente.

---

ğŸ“„ PÃ¡gina 46

ğŸ” Resumo dos principais pontos:

*   ContÃªineres resolvem o conflito entre **isolamento** e **densidade** na computaÃ§Ã£o.
    *   **Densidade:** Rodar o mÃ¡ximo de apps no mesmo hardware para usar eficientemente CPU/memÃ³ria.
    *   **Isolamento:** NecessÃ¡rio porque apps podem ter conflitos (versÃµes diferentes de Java/.NET, bibliotecas incompatÃ­veis, consumo excessivo de recursos). Isolamento tradicional impede alta densidade.
*   SoluÃ§Ã£o anterior: **MÃ¡quinas Virtuais (VMs)**.
    *   Conceito similar (caixa para app), mas a caixa da VM contÃ©m seu **prÃ³prio Sistema Operacional**.
    *   NÃ£o compartilham o SO do host.
*   Diagrama (Figura 2.4) mostrando mÃºltiplas VMs, cada uma com seu SO, no mesmo host.
*   **ImplicaÃ§Ãµes das VMs:**
    *   Cada SO da VM consome GBs de memÃ³ria e CPU.
    *   Custos de licenciamento de SO.
    *   ManutenÃ§Ã£o de atualizaÃ§Ãµes de SO para cada VM.
    *   VMs fornecem isolamento, mas sacrificam a densidade.
*   **ContÃªineres oferecem ambos:**
    *   Compartilham o SO do host -> extremamente leves ("lightweight").
    *   Iniciam rÃ¡pido, usam poucos recursos.
    *   Permitem rodar muito mais contÃªineres do que VMs no mesmo hardware (5 a 10x mais).
    *   Oferecem densidade E isolamento.
*   **EficiÃªncia** Ã© outro benefÃ­cio chave do Docker.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina aprofunda a comparaÃ§Ã£o entre contÃªineres e a tecnologia anterior para isolamento, as MÃ¡quinas Virtuais (VMs), explicando por que os contÃªineres sÃ£o mais eficientes.
*   O problema fundamental que ambas tentam resolver Ã© o dilema **Densidade vs. Isolamento**: queremos rodar muitos aplicativos (densidade) no mesmo servidor para economizar custos, mas precisamos mantÃª-los separados (isolamento) para evitar conflitos.
*   As VMs resolvem o isolamento dando a cada aplicativo sua prÃ³pria mÃ¡quina virtual completa, *incluindo um sistema operacional inteiro*. A Figura 2.4 ilustra isso. O preÃ§o dessa abordagem Ã© a **baixa densidade**: cada SO da VM consome muitos recursos (disco, memÃ³ria, CPU) que poderiam ser usados pelos aplicativos, alÃ©m de custos de licenÃ§a e manutenÃ§Ã£o.
*   Os contÃªineres, como visto na pÃ¡gina anterior, tambÃ©m fornecem isolamento, mas resolvem o problema da densidade de forma diferente: eles **compartilham o sistema operacional** da mÃ¡quina hospedeira. Isso os torna muito mais leves e eficientes. VocÃª pode iniciar contÃªineres quase instantaneamente e rodar um nÃºmero significativamente maior deles no mesmo hardware em comparaÃ§Ã£o com VMs.
*   A conclusÃ£o Ã© que os contÃªineres oferecem o melhor dos dois mundos: **isolamento E alta densidade (eficiÃªncia)**.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O Docker Desktop no Windows usa uma VM Linux leve (no modo Linux) ou a tecnologia de contÃªineres nativa do Windows (no modo Windows). Mesmo no modo Linux, a sobrecarga da VM do Docker Desktop Ã© muito menor do que rodar mÃºltiplas VMs completas para cada aplicativo. VocÃª pode observar o baixo consumo de recursos dos contÃªineres usando `docker stats` no PowerShell.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   VMs sÃ£o como casas completas, cada uma precisando de seu prÃ³prio terreno, fundaÃ§Ã£o, estrutura, telhado (SO completo). ContÃªineres sÃ£o como apartamentos prÃ©-fabricados; eles oferecem isolamento interno, mas compartilham a estrutura maior do prÃ©dio (SO do host), sendo muito mais rÃ¡pidos de instalar e ocupando menos espaÃ§o geral.

---

ğŸ“„ PÃ¡gina 47

ğŸ” Resumo dos principais pontos:

*   **2.3 Connecting to a container like a remote computer** (Conectando a um contÃªiner como um computador remoto)
*   ContÃªineres podem executar tarefas Ãºnicas e sair (ex: scripts) ou podem rodar continuamente.
*   Caso de uso para tarefas Ãºnicas: empacotar scripts e suas ferramentas em uma imagem Docker, facilitando o compartilhamento e execuÃ§Ã£o sem instalaÃ§Ã£o manual.
*   Outra forma de interagir: rodar um contÃªiner e conectar-se a um terminal *dentro* dele.
*   Comando: `docker container run --interactive --tty diamol/base`.
    *   `--interactive` (ou `-i`): MantÃ©m a entrada padrÃ£o (STDIN) aberta, permitindo interatividade.
    *   `--tty` (ou `-t`): Aloca um pseudo-TTY (terminal), simulando uma sessÃ£o de terminal.
*   AbreviaÃ§Ã£o comum: `-it`.
*   Resultado: O Docker baixa a imagem (se necessÃ¡rio) e apresenta um prompt de comando *de dentro do contÃªiner*.
*   Diagrama (Figura 2.5) mostrando a execuÃ§Ã£o interativa e o prompt do contÃªiner.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   AtÃ© agora, os contÃªineres que rodamos executaram uma tarefa e terminaram. Esta seÃ§Ã£o introduz outra forma de usar contÃªineres: executar um contÃªiner e "entrar" nele para interagir com seu ambiente interno atravÃ©s de um terminal, como se vocÃª estivesse acessando uma mÃ¡quina remota via SSH.
*   Isso Ã© Ãºtil para explorar o ambiente do contÃªiner, depurar problemas ou executar comandos manualmente dentro dele.
*   O comando chave Ã© `docker container run` com as flags `--interactive` (ou `-i`) e `--tty` (ou `-t`), frequentemente combinadas como `-it`.
    *   `-i`: Permite que vocÃª digite comandos no contÃªiner.
    *   `-t`: DÃ¡ a vocÃª uma interface de terminal (prompt de comando) para interagir.
*   A imagem usada (`diamol/base`) Ã© uma imagem base simples que provavelmente contÃ©m um shell (como `sh` no Linux ou `cmd`/`powershell` no Windows) que serÃ¡ iniciado quando vocÃª se conectar.
*   A Figura 2.5 ilustra o resultado: seu terminal local agora estÃ¡ mostrando um prompt que pertence ao ambiente *dentro* do contÃªiner.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** No PowerShell ou terminal do VS Code, execute:
    ```powershell
    docker container run --interactive --tty diamol/base
    # Ou a forma abreviada:
    # docker container run -it diamol/base
    ```
*   Observar o resultado: VocÃª verÃ¡ o Docker baixar a imagem `diamol/base` (se for a primeira vez) e, em seguida, seu prompt mudarÃ¡. Se estiver no modo Linux do Docker Desktop, serÃ¡ algo como `/ #`. Se estiver no modo Windows, serÃ¡ `C:\>`. VocÃª estÃ¡ agora "dentro" do contÃªiner.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Rodar `docker run -it` Ã© como abrir uma conexÃ£o de Acesso Remoto (RDP) ou SSH para uma mÃ¡quina, mas a "mÃ¡quina" Ã© o contÃªiner rodando localmente.

---

ğŸ“„ PÃ¡gina 48

ğŸ” Resumo dos principais pontos:

*   Exemplo de prompt dentro de um contÃªiner Windows: `C:\>`.
*   Dentro do contÃªiner, pode-se executar comandos normais do SO correspondente.
*   **ExercÃ­cio Try-it-now:** Dentro do contÃªiner, rodar `hostname` e `date`.
    *   Exemplo de saÃ­da: `f1695de1f2ec` e `Thu Jun 20 12:18:26 UTC 2019`.
*   A experiÃªncia Ã© similar a conectar via SSH (Linux) ou RDP (Windows Server Core).
*   O contÃªiner compartilha o kernel do SO do host, por isso o shell corresponde (Linux no Linux/Mac/DockerDesktop-Linux; Windows no Windows).
*   Alguns comandos sÃ£o iguais (ex: `ping google.com`), outros diferentes (`ls` vs `dir`).
*   O comportamento do *Docker* (gerenciamento) Ã© o mesmo, independentemente do SO ou CPU. Ã‰ o *app dentro* do contÃªiner que percebe a diferenÃ§a.
*   **ExercÃ­cio Try-it-now:** Abrir um *novo* terminal no *host* e rodar `docker container ls`.
    *   Mostra informaÃ§Ãµes dos contÃªineres *em execuÃ§Ã£o*.
    *   Exemplo de saÃ­da (abreviada): ID, Imagem, Comando, Criado, Status.
    *   ObservaÃ§Ã£o: O ID do contÃªiner Ã© o mesmo que o hostname interno.
*   Docker atribui IDs aleatÃ³rios; partes do ID sÃ£o usadas para hostname.
*   Pode-se usar os primeiros caracteres do ID para referenciar um contÃªiner especÃ­fico nos comandos.
*   **ExercÃ­cio Try-it-now:** Listar processos dentro do contÃªiner com `docker container top <id_curto>`.
    *   Exemplo: `docker container top f1`.
    *   Mostra o processo principal (`/bin/sh`) rodando.
    *   ContÃªineres Windows podem mostrar mais processos de fundo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Uma vez "dentro" do contÃªiner (via `docker run -it`), vocÃª pode usar os comandos do sistema operacional daquele contÃªiner. Os exemplos `hostname` e `date` confirmam que vocÃª estÃ¡ em um ambiente distinto com seu prÃ³prio nome e relÃ³gio.
*   Ã‰ crucial entender que, embora o *ambiente interno* do contÃªiner seja especÃ­fico do SO (Linux ou Windows), os comandos do *Docker* para gerenciar esses contÃªineres (`docker ps`, `docker stop`, `docker rm`, etc.) sÃ£o os mesmos em qualquer plataforma onde o Docker roda.
*   A pÃ¡gina introduz comandos para observar contÃªineres *do lado de fora*, a partir do seu terminal no host:
    *   `docker container ls` (ou `docker ps`): Lista os contÃªineres que estÃ£o *atualmente em execuÃ§Ã£o*. Mostra informaÃ§Ãµes Ãºteis como ID, imagem usada e status.
    *   `docker container top <id>`: Mostra os processos que estÃ£o rodando *dentro* de um contÃªiner especÃ­fico.
*   Uma dica Ãºtil Ã© que vocÃª pode usar apenas os primeiros caracteres Ãºnicos do ID de um contÃªiner para identificÃ¡-lo nos comandos, em vez de digitar o ID completo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **No terminal conectado ao contÃªiner (da pÃ¡gina anterior):**
    ```powershell
    # Se for contÃªiner Linux (prompt / #)
    hostname
    date
    ping google.com
    ls /
    exit # Para sair do contÃªiner
    # Se for contÃªiner Windows (prompt C:\>)
    hostname
    date
    ping google.com
    dir C:\
    exit # Para sair do contÃªiner
    ```
*   **Abrir um NOVO PowerShell/terminal no seu Windows:**
    ```powershell
    # Listar contÃªineres em execuÃ§Ã£o (o que vocÃª iniciou com -it deve aparecer)
    docker container ls
    # Pegue os primeiros caracteres do CONTAINER ID (ex: f169) e use-os:
    docker container top <id_curto>
    ```

---

ğŸ“„ PÃ¡gina 49

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Exibir logs coletados do contÃªiner com `docker container logs <id_curto>`.
    *   Exemplo: `docker container logs f1`.
    *   Mostra a saÃ­da que o app do contÃªiner produziu (neste caso, os comandos executados no terminal interativo).
    *   Para apps reais (ex: web), mostraria logs de requisiÃ§Ã£o HTTP, etc.
*   **ExercÃ­cio Try-it-now:** Mostrar todos os detalhes de um contÃªiner com `docker container inspect <id_curto>`.
    *   Exemplo: `docker container inspect f1`.
    *   SaÃ­da Ã© um JSON longo com informaÃ§Ãµes de baixo nÃ­vel (filesystem, comando, rede, etc.). Ãštil para troubleshooting.
*   Comandos (`run`, `logs`, `top`, `inspect`) sÃ£o consistentes para gerenciar qualquer tipo de aplicaÃ§Ã£o em contÃªiner (Java antigo/Linux, .NET antigo/Windows, Go novo/Raspberry Pi).
*   Docker adiciona camada de gerenciamento consistente sobre qualquer app.
*   InstruÃ§Ã£o para fechar o segundo terminal e sair do primeiro (que estava conectado ao contÃªiner) com `exit`.
*   **2.4 Hosting a website in a container** (Hospedando um site em um contÃªiner)
    *   RevisÃ£o: ContÃªineres executam tarefas e saem, ou rodam interativamente atÃ© serem fechados.
    *   `docker container ls` sÃ³ mostra contÃªineres *em execuÃ§Ã£o*.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Continuando a exploraÃ§Ã£o de comandos de gerenciamento do Docker a partir do host:
    *   `docker container logs <id>`: Recupera toda a saÃ­da (stdout/stderr) que foi produzida pelo processo principal dentro do contÃªiner desde que ele iniciou. Ã‰ essencial para ver o que sua aplicaÃ§Ã£o estÃ¡ fazendo ou por que falhou.
    *   `docker container inspect <id>`: Fornece uma visÃ£o extremamente detalhada da configuraÃ§Ã£o e do estado atual de um contÃªiner, em formato JSON. Ã‰ uma ferramenta poderosa para diagnÃ³stico avanÃ§ado.
*   A pÃ¡gina reforÃ§a a **consistÃªncia** do gerenciamento Docker: nÃ£o importa a tecnologia dentro do contÃªiner, os comandos para gerenciÃ¡-lo sÃ£o os mesmos.
*   Prepara para a prÃ³xima seÃ§Ã£o, que abordarÃ¡ contÃªineres que rodam serviÃ§os continuamente em background, como servidores web.
*   Relembra que `docker ps` (ou `docker container ls`) por padrÃ£o sÃ³ lista contÃªineres ativos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Ainda no segundo terminal (host):**
    ```powershell
    # Use o ID curto do contÃªiner da pÃ¡gina anterior
    docker container logs <id_curto>
    docker container inspect <id_curto> # Observe a quantidade de detalhes em JSON
    ```
*   **No primeiro terminal (que estava conectado ao contÃªiner):** Se ainda nÃ£o saiu, digite `exit`.
*   **No segundo terminal (host):** Execute `docker container ls`. Agora ele nÃ£o deve listar mais nada, pois o contÃªiner interativo foi encerrado quando vocÃª digitou `exit`.

---

ğŸ“„ PÃ¡gina 50

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Listar *todos* os contÃªineres (incluindo os parados) com `docker container ls --all`.
    *   Exemplo de saÃ­da mostrando contÃªineres com status `Exited`.
*   **Pontos chave sobre contÃªineres parados (Exited):**
    1.  ContÃªineres rodam *apenas* enquanto o processo principal dentro deles estÃ¡ rodando. Quando o processo termina, o contÃªiner entra no estado `Exited`.
    2.  ContÃªineres `Exited` nÃ£o usam CPU ou memÃ³ria, mas *ainda existem* e ocupam espaÃ§o em disco (seu sistema de arquivos Ã© mantido).
    3.  Pode-se reiniciar (`docker start`), verificar logs (`docker logs`), copiar arquivos (`docker cp`) de contÃªineres parados.
    4.  Docker *nÃ£o* remove contÃªineres parados automaticamente; Ã© preciso usar `docker rm`.
*   Caso de uso principal do Docker: rodar aplicaÃ§Ãµes de servidor (sites, APIs, bancos de dados) que ficam rodando em background.
*   **ExercÃ­cio Try-it-now:** Rodar um contÃªiner de site em background.
    *   Comando: `docker container run --detach --publish 8088:80 diamol/ch02-hellodiamol-web`.
    *   `--detach` (ou `-d`): Roda o contÃªiner em background e imprime apenas o ID longo do contÃªiner.
    *   `--publish` (ou `-p`): Mapeia uma porta do host para uma porta do contÃªiner (`<porta_host>:<porta_container>`).
*   **ExercÃ­cio Try-it-now:** Listar contÃªineres em execuÃ§Ã£o com `docker container ls` para ver o novo contÃªiner com status `Up`.
    *   Exemplo de saÃ­da mostrando ID, Imagem, Comando, Criado, Status, Portas, Nome (aleatÃ³rio se nÃ£o especificado).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina esclarece o ciclo de vida dos contÃªineres e introduz como rodar serviÃ§os em background.
*   O comando `docker container ls --all` (ou `docker ps -a`) Ã© importante para ver o "histÃ³rico" de contÃªineres que jÃ¡ rodaram e pararam. Eles nÃ£o somem sozinhos e continuam ocupando espaÃ§o em disco atÃ© serem explicitamente removidos (`docker rm`).
*   O foco muda para o cenÃ¡rio mais comum: rodar serviÃ§os como servidores web. Para isso, usamos duas flags importantes no `docker run`:
    *   `--detach` (ou `-d`): Libera seu terminal imediatamente apÃ³s iniciar o contÃªiner, deixando-o rodar em segundo plano.
    *   `--publish` (ou `-p`) `<porta_host>:<porta_container>`: ExpÃµe a aplicaÃ§Ã£o dentro do contÃªiner para o mundo exterior (ou pelo menos para a mÃ¡quina host). O Docker direciona o trÃ¡fego que chega na `<porta_host>` da sua mÃ¡quina para a `<porta_container>` dentro do contÃªiner. No exemplo, o trÃ¡fego na porta 8088 do seu Windows serÃ¡ enviado para a porta 80 do contÃªiner.
*   A imagem `diamol/ch02-hellodiamol-web` contÃ©m um servidor web (Apache) pronto para rodar.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # Listar todos os contÃªineres (vocÃª verÃ¡ os 'hello-diamol' e 'base' como Exited)
    docker container ls --all

    # Rodar o servidor web em background e mapear a porta
    docker container run --detach --publish 8088:80 diamol/ch02-hellodiamol-web
    # Anote o ID longo que Ã© impresso

    # Listar contÃªineres em execuÃ§Ã£o (agora vocÃª verÃ¡ o 'hello-diamol-web')
    docker container ls
    # Observe a coluna PORTS mostrando '0.0.0.0:8088->80/tcp'
    # Observe a coluna NAMES com um nome aleatÃ³rio (ex: reverent_dubinsky)
    ```

---

ğŸ“„ PÃ¡gina 51

ğŸ” Resumo dos principais pontos:

*   A imagem `diamol/ch02-hello-diamol-web` contÃ©m o servidor web Apache e uma pÃ¡gina HTML simples.
*   ExplicaÃ§Ã£o da flag `--publish`:
    *   ContÃªineres tÃªm IP prÃ³prio em uma rede virtual Docker, nÃ£o expostos por padrÃ£o.
    *   `--publish <host>:<container>` faz o Docker ouvir na porta do host e redirecionar o trÃ¡fego para a porta do contÃªiner.
    *   Exemplo: TrÃ¡fego para `host:8088` vai para `container:80`.
*   Diagrama (Figura 2.6) ilustrando redes fÃ­sicas e virtuais:
    *   Computador Host (ex: 192.168.2.150) na rede fÃ­sica.
    *   ContÃªiner (ex: 172.0.5.1) na rede virtual Docker.
    *   Mapeamento de porta (8088 -> 80) permite acesso externo ao contÃªiner via IP/porta do host.
*   ContÃªineres nÃ£o sÃ£o diretamente acessÃ­veis por outros na rede fÃ­sica pelo seu IP Docker.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica com mais detalhes o funcionamento do mapeamento de portas (`--publish`). Ã‰ essencial entender que os contÃªineres operam em sua prÃ³pria rede virtual isolada, criada e gerenciada pelo Docker. Eles recebem endereÃ§os IP nessa rede (como `172.x.x.x`), que nÃ£o sÃ£o diretamente acessÃ­veis de fora da mÃ¡quina host.
*   A flag `--publish` cria uma "ponte" entre a rede fÃ­sica da sua mÃ¡quina (onde ela tem um IP como `192.168.x.x` ou `localhost`) e a rede virtual do contÃªiner. O Docker intercepta o trÃ¡fego que chega na porta especificada do host (8088 no exemplo) e o encaminha para a porta correspondente dentro do contÃªiner (80 no exemplo).
*   A Figura 2.6 ilustra bem essa separaÃ§Ã£o entre a rede fÃ­sica (onde seu PC estÃ¡) e a rede virtual Docker (onde o contÃªiner "vive"), e como o mapeamento de porta conecta as duas para permitir o acesso ao serviÃ§o no contÃªiner.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo, mas a explicaÃ§Ã£o Ã© fundamental para entender por que vocÃª acessarÃ¡ o site (no prÃ³ximo passo) usando `localhost:8088` no seu navegador Windows, mesmo que o servidor web esteja rodando na porta 80 *dentro* do contÃªiner.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Pense na rede virtual Docker como um condomÃ­nio fechado (com seus prÃ³prios endereÃ§os internos). O comando `--publish` Ã© como dizer ao porteiro (Docker) para receber entregas (trÃ¡fego de rede) em um endereÃ§o especÃ­fico do lado de fora (porta 8088 do host) e levÃ¡-las para um apartamento especÃ­fico lÃ¡ dentro (porta 80 do contÃªiner).

---

ğŸ“„ PÃ¡gina 52

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Acessar o site no navegador via `http://localhost:8088`.
    *   Mostra a pÃ¡gina HTML servida pelo contÃªiner (Figura 2.7).
*   BenefÃ­cios mesmo para um site simples: portabilidade e eficiÃªncia.
    *   Imagem contÃ©m tudo (conteÃºdo web + servidor).
    *   Desenvolvedor roda localmente da mesma forma que operador rodaria em produÃ§Ã£o (mesma imagem).
*   ContÃªiner de servidor roda indefinidamente (enquanto o processo do servidor estiver ativo).
*   Pode-se usar comandos `docker container` (ls, logs, inspect, etc.) para gerenciÃ¡-lo.
*   **ExercÃ­cio Try-it-now:** Usar `docker container stats <id_ou_nome>` para ver uso de CPU, memÃ³ria, rede e disco em tempo real.
    *   Exemplo: `docker container stats e53` (usando ID curto).
    *   SaÃ­da ligeiramente diferente entre Linux e Windows.
*   Remover contÃªineres (incluindo os em execuÃ§Ã£o) com `docker container rm --force <id_ou_nome>`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Agora vocÃª testa o mapeamento de porta acessando `http://localhost:8088` no seu navegador. `localhost` refere-se Ã  sua prÃ³pria mÃ¡quina Windows, e a porta `8088` Ã© a que vocÃª mandou o Docker escutar. O Docker entÃ£o encaminha essa requisiÃ§Ã£o para a porta 80 do contÃªiner, que serve a pÃ¡gina web de volta.
*   A pÃ¡gina reforÃ§a que mesmo um exemplo simples como este demonstra os benefÃ­cios do Docker: a imagem Ã© autocontida e garante que a aplicaÃ§Ã£o rode da mesma forma em qualquer ambiente.
*   Introduz outro comando Ãºtil: `docker container stats`. Ele fornece uma visÃ£o ao vivo (atualizada continuamente no terminal) dos recursos (CPU, memÃ³ria, I/O de rede/disco) que um ou mais contÃªineres estÃ£o consumindo. Ã‰ Ã³timo para monitoramento rÃ¡pido.
*   Menciona como remover um contÃªiner, usando a flag `--force` (ou `-f`) se ele ainda estiver rodando.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Abra seu navegador web (Chrome, Edge, Firefox) e acesse `http://localhost:8088`. VocÃª deve ver a pÃ¡gina "Hello World" da Figura 2.7.
    2.  No PowerShell/terminal, execute `docker container ls` para pegar o ID ou o nome aleatÃ³rio do contÃªiner web (ex: `e53085ff0cc4` ou `reverent_dubinsky`).
    3.  Execute `docker container stats <id_ou_nome>` (substitua pelo ID/nome real). Observe as estatÃ­sticas sendo atualizadas. Pressione `Ctrl+C` para sair do `stats`.
    4.  Para remover o contÃªiner:
        ```powershell
        docker container rm --force <id_ou_nome>
        # Ou use o ID curto:
        # docker container rm -f <id_curto>
        ```
        *Nota:* Execute este comando de remoÃ§Ã£o apenas se quiser parar o site agora. O prÃ³ximo exercÃ­cio tambÃ©m farÃ¡ a limpeza.

---

ğŸ“„ PÃ¡gina 53

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Comando para remover *todos* os contÃªineres (parados ou rodando) de uma vez.
    *   Comando: `docker container rm --force $(docker container ls --all --quiet)`.
    *   ExplicaÃ§Ã£o da sintaxe `$()`: envia a saÃ­da de um comando (lista de IDs) para outro (rm). Funciona em PowerShell, Linux, Mac.
    *   Combina `ls --all --quiet` (lista IDs de todos os contÃªineres) com `rm --force` (remove forÃ§adamente).
    *   Aviso: Usar com cautela, pois nÃ£o pede confirmaÃ§Ã£o.
*   **2.5 Understanding how Docker runs containers** (Entendendo como Docker roda contÃªineres)
    *   RevisÃ£o do fluxo "build, share, run". Facilita distribuiÃ§Ã£o de software (exemplos: Elasticsearch, SQL Server, Ghost).
    *   IntroduÃ§Ã£o Ã  arquitetura do Docker para entender o que acontece nos comandos.
    *   Componentes envolvidos (Figura 2.8):
        *   **Docker Engine (ou Daemon):** Componente de gerenciamento. Cuida do cache de imagens, download/reuso, interaÃ§Ã£o com SO para criar contÃªineres, redes, etc. Roda em background (serviÃ§o/daemon).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina apresenta um comando poderoso e conveniente para limpeza geral: `docker container rm -f $(docker container ls -aq)`.
    *   `docker container ls -aq` (ou `--all --quiet`): Lista *apenas* os IDs (`-q`) de *todos* os contÃªineres (`-a`).
    *   `$(...)`: Captura essa lista de IDs.
    *   `docker container rm -f ...`: Remove forÃ§adamente (`-f`) todos os contÃªineres cujos IDs foram capturados.
    *   Ã‰ um atalho Ãºtil para limpar seu ambiente, mas perigoso se vocÃª tiver contÃªineres importantes rodando que nÃ£o queria remover.
*   Inicia a seÃ§Ã£o 2.5, que explica a arquitetura por trÃ¡s dos comandos Docker. O objetivo Ã© entender melhor como o Docker funciona internamente.
*   O primeiro componente chave Ã© o **Docker Engine** (tambÃ©m chamado de Docker Daemon). Ele Ã© o "cÃ©rebro" e "mÃºsculo" do Docker, rodando continuamente em segundo plano. Ã‰ ele quem gerencia as imagens, cria os contÃªineres, configura as redes e volumes, interagindo diretamente com o sistema operacional (ou a VM no caso do Docker Desktop).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal (limpeza):** Execute o comando de remoÃ§Ã£o total:
    ```powershell
    docker container rm --force $(docker container ls -aq)
    ```
    *Confirme executando `docker container ls -a` depois; a lista deve estar vazia.*
*   O Docker Engine no seu caso Ã© o serviÃ§o gerenciado pelo Docker Desktop. VocÃª nÃ£o interage diretamente com ele, mas sim atravÃ©s da API (prÃ³ximo ponto).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   O comando de remoÃ§Ã£o total Ã© como apertar um botÃ£o de "reset" para os contÃªineres, limpando tudo que estava rodando ou parado.
*   O Docker Engine Ã© o "gerente da fÃ¡brica" Docker. Ele fica no escritÃ³rio (background) recebendo ordens e coordenando toda a produÃ§Ã£o (criaÃ§Ã£o/gerenciamento de contÃªineres).

---

ğŸ“„ PÃ¡gina 54

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o da arquitetura Docker (Figura 2.8):
    *   **Docker API:** Interface REST baseada em HTTP usada pelo Engine para expor suas funcionalidades. Pode ser configurada para acesso local (padrÃ£o) ou remoto.
    *   **Docker CLI (Command-Line Interface):** Cliente da API Docker. Comandos como `docker run` sÃ£o enviados pela CLI para a API, e o Engine executa o trabalho.
*   Entender a arquitetura Ã© bom: a Ãºnica forma de interagir com o Engine Ã© via API.
*   CLI pode apontar para uma API remota (outra mÃ¡quina com Docker) para gerenciar contÃªineres remotamente (ex: build servers, test, prod).
*   API Docker Ã© a mesma em qualquer SO, permitindo gerenciamento cross-platform (ex: CLI no Windows gerenciando Docker no Raspberry Pi ou Linux na nuvem).
*   API tem especificaÃ§Ã£o publicada; CLI nÃ£o Ã© o Ãºnico cliente.
*   Existem GUIs (Interfaces GrÃ¡ficas) que conectam Ã  API Docker.
    *   Exemplo: **Universal Control Plane (UCP)** (produto comercial da Docker Inc., Figura 2.9).
    *   Exemplo: **Portainer** (projeto open source).
    *   UCP e Portainer rodam como contÃªineres.
*   Detalhes de baixo nÃ­vel (containerd, OCI) nÃ£o serÃ£o aprofundados.
    *   `containerd`: Componente usado pelo Engine para gerenciar contÃªineres (agora um projeto CNCF).
    *   `OCI (Open Container Initiative)`: EspecificaÃ§Ã£o aberta para formato de imagem e runtime de contÃªiner.
*   Docker Ã© a plataforma mais popular, mas nÃ£o a Ãºnica. O uso de padrÃµes abertos evita lock-in.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina completa a visÃ£o geral da arquitetura Docker.
*   O **Docker Engine** nÃ£o Ã© acessado diretamente. Ele expÃµe uma **API** (Application Programming Interface), que Ã© um conjunto de regras e endpoints (endereÃ§os) baseados no padrÃ£o HTTP/REST, permitindo que outros programas conversem com ele.
*   A **Docker CLI** (o comando `docker` que vocÃª usa no terminal) Ã© apenas um desses programas. Quando vocÃª digita `docker container ls`, a CLI envia uma requisiÃ§Ã£o formatada para a API do Engine, o Engine processa, e a CLI exibe a resposta que o Engine devolveu.
*   Essa separaÃ§Ã£o Cliente-API-Engine Ã© poderosa:
    *   Permite **gerenciamento remoto**: VocÃª pode configurar sua CLI local para se conectar Ã  API de um Docker Engine rodando em outro servidor.
    *   Permite **outras ferramentas**: Qualquer programa pode usar a API, como interfaces grÃ¡ficas (UCP, Portainer) que oferecem uma forma visual de gerenciar contÃªineres.
*   A pÃ¡gina tambÃ©m menciona brevemente `containerd` (o componente de runtime de baixo nÃ­vel) e a `OCI` (padrÃµes abertos), reforÃ§ando que o ecossistema Docker Ã© baseado em tecnologias padronizadas, o que Ã© bom para evitar dependÃªncia de um Ãºnico fornecedor.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª jÃ¡ estÃ¡ usando a CLI (`docker`, `docker-compose`) que interage com a API do Engine gerenciado pelo Docker Desktop.
*   VocÃª pode instalar o **Portainer** facilmente no Docker Desktop para ter uma interface grÃ¡fica:
    ```powershell
    docker volume create portainer_data
    docker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v \\.\pipe\docker_engine:\\.\pipe\docker_engine -v portainer_data:C:\data portainer/portainer-ce:latest
    ```
    Depois acesse `https://localhost:9443` (pode precisar aceitar certificado auto-assinado) e configure o acesso ao ambiente Docker local.
*   O CapÃ­tulo 15 cobrirÃ¡ o acesso remoto Ã  API em detalhes.

---

ğŸ“„ PÃ¡gina 55

ğŸ” Resumo dos principais pontos:

*   **Figura 2.9:** Screenshot da interface grÃ¡fica do Docker Universal Control Plane (UCP).
*   **2.6 Lab: Exploring the container filesystem** (Lab: Explorando o sistema de arquivos do contÃªiner)
    *   Primeiro lab do livro: tarefa para fazer sozinho, cimentar o aprendizado.
    *   InstruÃ§Ãµes e dicas fornecidas, mas a soluÃ§Ã£o Ã© por conta do leitor.
    *   SoluÃ§Ã£o de exemplo disponÃ­vel no GitHub (link fornecido).
    *   **Tarefa:** Rodar o contÃªiner do site (`diamol/ch02-hellodiamol-web`), mas substituir o arquivo `index.html` interno para mostrar um conteÃºdo diferente ao acessar o site no navegador.
    *   Lembrete: O contÃªiner tem seu prÃ³prio sistema de arquivos.
    *   **Dicas:**
        *   Usar `docker container --help` para ver aÃ§Ãµes possÃ­veis.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra um exemplo de interface grÃ¡fica (UCP) e introduz o primeiro **Lab** do livro.
*   Os Labs sÃ£o projetados para serem mais desafiadores que os exercÃ­cios "Try-it-now", exigindo que o leitor combine os conceitos aprendidos para resolver um problema menos guiado. Existe uma soluÃ§Ã£o de referÃªncia no GitHub, mas o ideal Ã© tentar resolver sozinho primeiro.
*   **O desafio do Lab 2:** VocÃª rodou o contÃªiner `diamol/ch02-hellodiamol-web` anteriormente e acessou o site "Hello World". Agora, o objetivo Ã© *modificar* o conteÃºdo desse site *sem reconstruir a imagem*. VocÃª precisa descobrir como copiar um novo arquivo `index.html` (que vocÃª pode criar localmente com qualquer conteÃºdo) para *dentro* do sistema de arquivos do contÃªiner em execuÃ§Ã£o (ou parado) antes de acessÃ¡-lo novamente pelo navegador.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab:**
    1.  Crie um arquivo `index.html` simples na sua mÃ¡quina Windows (ex: em `C:\DockerLabs\diamol\ch02\lab`) usando o VS Code. Coloque um texto diferente, como `<h1>OlÃ¡, Alexandre!</h1>`.
    2.  Rode o contÃªiner (se ainda nÃ£o estiver rodando ou se o removeu):
        ```powershell
        docker run -d --name meu-site-web -p 8088:80 diamol/ch02-hellodiamol-web
        ```
    3.  Use o comando `docker container cp` (introduzido na p. 76, mas aplicÃ¡vel aqui) para copiar seu `index.html` local para o diretÃ³rio correto *dentro* do contÃªiner. A prÃ³xima pÃ¡gina darÃ¡ a dica do diretÃ³rio.
        ```powershell
        # Comando exemplo (caminho do container serÃ¡ dado na p. 56):
        # docker container cp C:\DockerLabs\diamol\ch02\lab\index.html meu-site-web:<caminho_no_container>/index.html
        ```
    4.  Acesse `http://localhost:8088` no navegador e veja se seu novo conteÃºdo aparece.
*   Use `docker container --help` e `docker container cp --help` se precisar de ajuda com os comandos.

---

ğŸ“„ PÃ¡gina 56

ğŸ” Resumo dos principais pontos:

*   **ContinuaÃ§Ã£o das dicas do Lab 2:**
    *   Adicionar `--help` a qualquer comando `docker` mostra ajuda detalhada.
    *   Na imagem `diamol/ch02-hello-diamol-web`, o conteÃºdo do site Ã© servido do diretÃ³rio `/usr/local/apache2/htdocs` (Linux) ou `C:\usr\local\apache2\htdocs` (Windows).
*   Encerramento do CapÃ­tulo 2.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina fornece as dicas finais cruciais para completar o Lab do CapÃ­tulo 2.
    *   A primeira dica reforÃ§a o uso da ajuda integrada do Docker (`--help`).
    *   A segunda dica revela o caminho exato dentro do contÃªiner onde o arquivo `index.html` precisa ser colocado para que o servidor web Apache o sirva: `/usr/local/apache2/htdocs/` (para a versÃ£o Linux da imagem) ou `C:\usr\local\apache2\htdocs\` (para uma versÃ£o Windows, embora a imagem `diamol/ch02-hellodiamol-web` seja provavelmente baseada em Linux). VocÃª usarÃ¡ esse caminho como destino no comando `docker cp`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Completando o Lab (usando o caminho Linux, pois a imagem base Ã© provavelmente Linux):**
    1.  Certifique-se de ter o arquivo `index.html` local (ex: em `C:\DockerLabs\diamol\ch02\lab`).
    2.  Certifique-se de que o contÃªiner estÃ¡ rodando (ex: com nome `meu-site-web`).
    3.  Execute o comando `docker cp`:
        ```powershell
        docker container cp C:\DockerLabs\diamol\ch02\lab\index.html meu-site-web:/usr/local/apache2/htdocs/index.html
        ```
    4.  Acesse `http://localhost:8088` no navegador. VocÃª deve ver o conteÃºdo do seu `index.html` personalizado.

*   **Comandos Ãºteis para explorar (opcional):**
    ```powershell
    # Ver a ajuda para o comando cp
    docker container cp --help

    # Entrar no contÃªiner para verificar o arquivo (se a imagem tiver um shell)
    # docker exec -it meu-site-web sh
    # (dentro do contÃªiner)
    # ls /usr/local/apache2/htdocs/
    # cat /usr/local/apache2/htdocs/index.html
    # exit
    ```

---

ğŸ“„ PÃ¡gina 57

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 3: Building your own Docker images** (Construindo suas prÃ³prias imagens Docker)
*   RevisÃ£o: CapÃ­tulos anteriores rodaram contÃªineres e os gerenciaram.
*   Foco do CapÃ­tulo 3: Mudar de *consumidor* de imagens para *construtor* de imagens.
*   Aprendizado: Sintaxe do `Dockerfile` e padrÃµes chave para conteinerizar seus prÃ³prios apps.
*   **3.1 Using a container image from Docker Hub** (Usando uma imagem de contÃªiner do Docker Hub)
    *   ComeÃ§ar com a versÃ£o finalizada da imagem que serÃ¡ construÃ­da no capÃ­tulo.
    *   Aplicativo de exemplo: `web-ping` (verifica se um site estÃ¡ ativo).
    *   O app roda em um contÃªiner, faz requisiÃ§Ãµes HTTP para um URL (blog do autor) a cada 3 segundos.
    *   `docker container run` baixa a imagem se nÃ£o estiver local.
    *   DistribuiÃ§Ã£o de software Ã© integrada Ã  plataforma Docker.
    *   Pode-se baixar imagens explicitamente com `docker image pull`.
    *   **ExercÃ­cio Try-it-now:** Baixar a imagem `diamol/ch03-web-ping`.
        *   Comando: `docker image pull diamol/ch03-web-ping`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 3, que marca um passo importante: aprender a criar suas prÃ³prias imagens Docker, em vez de apenas usar as que outros fizeram. A ferramenta central para isso Ã© o `Dockerfile`.
*   A seÃ§Ã£o 3.1 comeÃ§a apresentando o objetivo final do capÃ­tulo: construir uma imagem para um aplicativo chamado `web-ping`. Antes de construir, o leitor Ã© instruÃ­do a baixar e usar a imagem pronta do Docker Hub.
*   Relembra que o Docker baixa automaticamente imagens (`pull`) se elas nÃ£o existirem localmente ao executar `docker run`. O comando `docker image pull` permite baixar uma imagem explicitamente, sem rodar um contÃªiner imediatamente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** No PowerShell/terminal, execute o comando para baixar a imagem de exemplo:
    ```powershell
    docker image pull diamol/ch03-web-ping
    ```
*   Observe a saÃ­da (analisada na prÃ³xima pÃ¡gina), que mostrarÃ¡ o download das camadas da imagem.

---

ğŸ“„ PÃ¡gina 58

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise da saÃ­da do `docker image pull` (Figura 3.1).
*   Nome da imagem: `diamol/ch03-web-ping`.
*   Local padrÃ£o: Docker Hub (registro pÃºblico gratuito).
*   Link para detalhes da imagem no Hub: https://hub.docker.com/r/diamol/ch03-web-ping.
*   ExplicaÃ§Ã£o das **camadas da imagem (image layers)**:
    *   Imagem Docker Ã© logicamente uma coisa sÃ³ (contÃ©m todo o app stack - ex: Node.js + cÃ³digo).
    *   Fisicamente, Ã© armazenada como mÃºltiplos arquivos pequenos (camadas).
    *   Docker monta essas camadas para criar o sistema de arquivos do contÃªiner.
*   **ExercÃ­cio Try-it-now:** Rodar um contÃªiner da imagem `web-ping`.
    *   Comando: `docker container run -d --name web-ping diamol/ch03-web-ping`.
    *   `-d`: Roda em background (detached).
    *   `--name web-ping`: Atribui um nome especÃ­fico (`web-ping`) ao contÃªiner, em vez de um aleatÃ³rio. Ãštil para referenciÃ¡-lo depois.
    *   App roda como batch job, sem UI, sem necessidade de publicar portas.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a saÃ­da do `docker image pull`. O ponto mais importante aqui Ã© a introduÃ§Ã£o do conceito de **camadas (layers)**. Embora pensemos em uma imagem como um pacote Ãºnico, o Docker a armazena de forma inteligente como uma pilha de camadas. Cada camada representa uma mudanÃ§a (como adicionar um arquivo, instalar um pacote). Esse sistema de camadas Ã© a chave para a eficiÃªncia do Docker em armazenamento e transferÃªncia.
*   Em seguida, instrui a rodar o aplicativo `web-ping` em um contÃªiner. Desta vez, alÃ©m da flag `-d` (detach) jÃ¡ vista, introduz a flag `--name`. Dar um nome fixo a um contÃªiner (`--name web-ping`) facilita muito o trabalho com ele posteriormente (em comandos como `docker logs`, `docker stop`, `docker rm`), pois vocÃª nÃ£o precisa ficar procurando ou usando IDs aleatÃ³rios.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revise a saÃ­da do `docker image pull` da pÃ¡gina anterior. VocÃª verÃ¡ vÃ¡rias linhas com IDs e status (Downloading, Pull complete), representando as camadas.
*   **AÃ§Ã£o principal:** Execute o comando para rodar o contÃªiner `web-ping` em background com um nome especÃ­fico:
    ```powershell
    docker container run -d --name web-ping diamol/ch03-web-ping
    ```
*   Verifique se ele estÃ¡ rodando:
    ```powershell
    docker container ls
    # Ou docker ps
    ```
    VocÃª deverÃ¡ ver um contÃªiner com o nome `web-ping`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Camadas de imagem sÃ£o como transparÃªncias em um retroprojetor antigo (ou camadas no Photoshop). Cada uma adiciona algo. Juntas, formam a imagem final. Se duas imagens usam a mesma transparÃªncia base (ex: o sistema operacional), elas compartilham essa camada, economizando espaÃ§o.
*   Usar `--name` Ã© como dar um apelido fÃ¡cil de lembrar a um amigo (contÃªiner) em vez de chamÃ¡-lo pelo nÃºmero de identidade (ID aleatÃ³rio).

---

ğŸ“„ PÃ¡gina 59

ğŸ” Resumo dos principais pontos:

*   O app `web-ping` estÃ¡ rodando em background, pingando o blog do autor.
*   Pode-se usar comandos `docker container` (logs, etc.) com o nome (`web-ping`).
*   **ExercÃ­cio Try-it-now:** Ver os logs da aplicaÃ§Ã£o.
    *   Comando: `docker container logs web-ping`.
    *   SaÃ­da (Figura 3.2) mostra o app fazendo requisiÃ§Ãµes HTTP para `blog.sixeyed.com`.
*   Problema: App parece fixo ("hardcoded") para usar o blog do autor, tornando-o inÃºtil para outros.
*   SoluÃ§Ã£o: O app pode ser configurado! LÃª configuraÃ§Ãµes de **variÃ¡veis de ambiente (environment variables)** do sistema.
    *   VariÃ¡veis de ambiente: pares chave/valor simples fornecidos pelo SO (Windows/Linux).
    *   ContÃªineres Docker tambÃ©m tÃªm variÃ¡veis de ambiente, definidas pelo Docker (nÃ£o pelo SO do host).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s iniciar o contÃªiner `web-ping` em background, a pÃ¡gina mostra como verificar o que ele estÃ¡ fazendo usando `docker container logs web-ping`. A saÃ­da confirma que ele estÃ¡ acessando o site `blog.sixeyed.com`.
*   Isso levanta um ponto importante sobre flexibilidade: um aplicativo que sÃ³ faz uma coisa fixa nÃ£o Ã© muito Ãºtil. A pÃ¡gina revela que o `web-ping` pode, na verdade, ser configurado.
*   A forma como ele Ã© configurado Ã© atravÃ©s de **variÃ¡veis de ambiente**. Estas sÃ£o configuraÃ§Ãµes simples (como `NOME=Alexandre` ou `SITE_URL=google.com`) que um processo pode ler do seu ambiente de execuÃ§Ã£o. O Docker permite definir variÃ¡veis de ambiente especÃ­ficas para cada contÃªiner no momento em que ele Ã© iniciado. O aplicativo dentro do contÃªiner pode entÃ£o ler essas variÃ¡veis e ajustar seu comportamento.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** Execute para ver os logs do contÃªiner `web-ping`:
    ```powershell
    docker container logs web-ping
    ```
    Observe as linhas indicando requisiÃ§Ãµes para `blog.sixeyed.com`. VocÃª pode rodar o comando vÃ¡rias vezes para ver novos logs.
*   Para ver as variÃ¡veis de ambiente *do seu Windows* (nÃ£o do contÃªiner), vocÃª pode usar `Get-ChildItem Env:` no PowerShell. As variÃ¡veis de ambiente *dentro* do contÃªiner sÃ£o um conceito separado, gerenciado pelo Docker.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   VariÃ¡veis de ambiente sÃ£o como post-its colados na "caixa" (contÃªiner) antes de ligar o aplicativo. O aplicativo pode ler esses post-its para saber como deve se comportar (ex: qual site pingar, com que frequÃªncia).

---

ğŸ“„ PÃ¡gina 60

ğŸ” Resumo dos principais pontos:

*   A imagem `web-ping` tem valores *padrÃ£o* para as variÃ¡veis de ambiente.
*   Pode-se fornecer valores *diferentes* ao criar o contÃªiner para mudar o comportamento do app.
*   **ExercÃ­cio Try-it-now:** Remover o contÃªiner existente e rodar um novo, especificando um valor para a variÃ¡vel de ambiente `TARGET`.
    *   Comando para remover: `docker rm -f web-ping`.
    *   Comando para rodar novo: `docker container run --env TARGET=google.com diamol/ch03-web-ping`.
    *   `--env` (ou `-e`): Flag para definir uma variÃ¡vel de ambiente (`CHAVE=VALOR`).
*   AnÃ¡lise da saÃ­da da nova execuÃ§Ã£o (Figura 3.3):
    *   ContÃªiner roda interativamente (sem `-d`), mostrando logs no console.
    *   App agora pinga `google.com` em vez de `blog.sixeyed.com`.
    *   ContÃªiner continua rodando atÃ© `Ctrl+C`.
*   **Takeaway principal:** Imagens Docker podem ter configuraÃ§Ãµes padrÃ£o, mas devem permitir overrides via variÃ¡veis de ambiente na execuÃ§Ã£o.
*   Diagrama (Figura 3.4) ilustrando variÃ¡veis de ambiente:
    *   Imagem tem valor padrÃ£o (TARGET=blog.sixeyed.com).
    *   ContÃªiner 1 usa o padrÃ£o.
    *   ContÃªiner 2 usa override (TARGET=google.com).
    *   Host tem suas prÃ³prias variÃ¡veis, separadas dos contÃªineres.
*   App Ã© o mesmo (mesma imagem), comportamento muda devido Ã  configuraÃ§Ã£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina demonstra na prÃ¡tica como usar variÃ¡veis de ambiente para configurar um contÃªiner.
*   Primeiro, remove-se o contÃªiner anterior (`web-ping`).
*   Depois, um novo contÃªiner Ã© iniciado com `docker run`, mas desta vez usando a flag `--env` (ou `-e`) para definir a variÃ¡vel `TARGET` com o valor `google.com`. Isso *sobrescreve* o valor padrÃ£o que estava definido na imagem.
*   Como o comando nÃ£o usou `-d`, o contÃªiner roda em primeiro plano, e vemos imediatamente nos logs que ele estÃ¡ pingando `google.com`. Isso prova que a configuraÃ§Ã£o via variÃ¡vel de ambiente funcionou.
*   A Figura 3.4 ilustra visualmente: a imagem Ã© o blueprint com um valor padrÃ£o, mas cada contÃªiner criado a partir dela pode receber instruÃ§Ãµes diferentes (variÃ¡veis de ambiente) que alteram seu comportamento final, sem modificar a imagem original. O ambiente do host (seu Windows) tambÃ©m tem suas variÃ¡veis, mas elas sÃ£o independentes das dos contÃªineres.
*   A liÃ§Ã£o Ã©: ao criar imagens, pense em tornÃ¡-las configurÃ¡veis via variÃ¡veis de ambiente para que possam ser usadas em diferentes cenÃ¡rios sem precisar de uma imagem diferente para cada caso.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # Remover o contÃªiner anterior
    docker rm -f web-ping

    # Rodar novo contÃªiner configurado via variÃ¡vel de ambiente
    docker container run --env TARGET=google.com diamol/ch03-web-ping
    # Ou forma abreviada:
    # docker container run -e TARGET=google.com diamol/ch03-web-ping
    ```
*   Observe a saÃ­da no terminal mostrando "Pinging google.com...".
*   Pressione `Ctrl+C` para parar o contÃªiner quando quiser.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Usar `--env` Ã© como dar um post-it especÃ­fico (`TARGET=google.com`) para *este* contÃªiner em particular, que terÃ¡ prioridade sobre o post-it padrÃ£o que veio com a receita (imagem).

---

ğŸ“„ PÃ¡gina 61

ğŸ” Resumo dos principais pontos:

*   Cabe ao autor da imagem Docker fornecer essa flexibilidade de configuraÃ§Ã£o.
*   IntroduÃ§Ã£o Ã  construÃ§Ã£o da sua primeira imagem Docker a partir de um `Dockerfile`.
*   **3.2 Writing your first Dockerfile** (Escrevendo seu primeiro Dockerfile)
    *   `Dockerfile`: script simples com instruÃ§Ãµes para empacotar uma aplicaÃ§Ã£o.
    *   SaÃ­da do Dockerfile: uma imagem Docker.
    *   Sintaxe simples; flexÃ­vel (pode usar comandos shell padrÃ£o - Bash/Linux, PowerShell/Windows).
*   **Listagem 3.1:** Dockerfile completo para empacotar o app `web-ping`.
    ```dockerfile
    FROM diamol/node
    ENV TARGET="blog.sixeyed.com"
    ENV METHOD="HEAD"
    ENV INTERVAL="3000"
    WORKDIR /web-ping
    COPY app.js .
    CMD ["node", "/web-ping/app.js"]
    ```
*   ExplicaÃ§Ã£o das instruÃ§Ãµes (convenÃ§Ã£o: maiÃºsculas, mas nÃ£o obrigatÃ³rio):
    *   `FROM`: Toda imagem comeÃ§a de outra imagem base. Aqui, usa `diamol/node` (que tem Node.js).
    *   `ENV`: Define variÃ¡veis de ambiente *padrÃ£o* na imagem (`CHAVE="VALOR"`).
    *   `WORKDIR`: Cria um diretÃ³rio na imagem e define como diretÃ³rio de trabalho atual para instruÃ§Ãµes seguintes (`COPY`, `CMD`, `RUN`). Sintaxe `/` funciona em Linux/Windows.
    *   `COPY`: Copia arquivos/diretÃ³rios do contexto de build (mÃ¡quina local) para a imagem (`<origem_local> <destino_imagem>`). `.` como destino refere-se ao `WORKDIR`.
    *   `CMD`: Especifica o comando *padrÃ£o* a ser executado quando um contÃªiner Ã© iniciado a partir da imagem. Aqui, roda `node` com o script `app.js`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina introduz o `Dockerfile`, o arquivo de texto que contÃ©m as instruÃ§Ãµes passo a passo para o Docker construir uma imagem.
*   A Listagem 3.1 mostra um Dockerfile real para o aplicativo `web-ping`. Vamos analisar cada linha:
    *   `FROM diamol/node`: Diz ao Docker para comeÃ§ar construindo sobre a imagem `diamol/node`. Essa imagem base jÃ¡ contÃ©m o Node.js, que Ã© necessÃ¡rio para rodar o `web-ping`. Isso evita que tenhamos que instalar o Node.js manualmente no nosso Dockerfile.
    *   `ENV ...`: Define trÃªs variÃ¡veis de ambiente (`TARGET`, `METHOD`, `INTERVAL`) com valores padrÃ£o. Estes sÃ£o os valores que o `web-ping` usarÃ¡ se nenhum override for fornecido via `docker run -e`.
    *   `WORKDIR /web-ping`: Cria um diretÃ³rio chamado `/web-ping` dentro da imagem (ou `C:\web-ping` se fosse uma imagem Windows) e define que os prÃ³ximos comandos (como `COPY` e `CMD`) acontecerÃ£o dentro desse diretÃ³rio.
    *   `COPY app.js .`: Copia o arquivo `app.js` (que deve estar na mesma pasta do Dockerfile no seu computador) para o diretÃ³rio `/web-ping` dentro da imagem.
    *   `CMD ["node", "/web-ping/app.js"]`: Define o comando que serÃ¡ executado quando um contÃªiner for iniciado a partir desta imagem. Ele executarÃ¡ o Node.js para rodar o script `app.js` que foi copiado. A forma `["executÃ¡vel", "param1", "param2"]` (formato JSON) Ã© geralmente preferida sobre a forma de shell (`CMD node /web-ping/app.js`).
*   Essas poucas instruÃ§Ãµes sÃ£o suficientes para empacotar a aplicaÃ§Ã£o, definindo seu ambiente base, configuraÃ§Ãµes padrÃ£o, arquivos necessÃ¡rios e como ela deve ser iniciada.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª precisarÃ¡ ter o arquivo `Dockerfile` (com o conteÃºdo da Listagem 3.1) e o arquivo `app.js` (do cÃ³digo fonte do livro) em um mesmo diretÃ³rio no seu Windows para poder construir a imagem (prÃ³ximos passos). Use o VS Code para criar/editar esses arquivos. O diretÃ³rio seria algo como `C:\DockerLabs\diamol\ch03\exercises\web-ping`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Dockerfile Ã© a receita de bolo detalhada.
    *   `FROM`: Qual forma/base de bolo usar.
    *   `ENV`: Ingredientes padrÃ£o (aÃ§Ãºcar, etc.) que jÃ¡ vÃªm na receita, mas podem ser ajustados.
    *   `WORKDIR`: Em qual bancada da cozinha trabalhar.
    *   `COPY`: Trazer ingredientes externos (o `app.js` do seu PC) para a bancada.
    *   `CMD`: A instruÃ§Ã£o final de "assar o bolo por X minutos" (como rodar o app).

---

ğŸ“„ PÃ¡gina 62

ğŸ” Resumo dos principais pontos:

*   Essas 5 instruÃ§Ãµes (`FROM`, `ENV`, `WORKDIR`, `COPY`, `CMD`) cobrem o essencial para empacotar apps.
*   O Dockerfile jÃ¡ segue boas prÃ¡ticas.
*   **ExercÃ­cio Try-it-now:** Navegar atÃ© o diretÃ³rio do cÃ³digo-fonte (`ch03/exercises/web-ping`) e listar os arquivos.
    *   Comandos: `cd ch03/exercises/web-ping`, `ls` (ou `dir` no Windows).
    *   Arquivos esperados: `Dockerfile`, `app.js`, `README.md`.
*   Figura 3.5 ilustra esses arquivos.
*   NÃ£o Ã© necessÃ¡rio entender Node.js/JavaScript para empacotar/rodar o app.
*   O cÃ³digo em `app.js` Ã© bÃ¡sico e usa bibliotecas padrÃ£o do Node.js para fazer chamadas HTTP e ler variÃ¡veis de ambiente.
*   Com esses arquivos, tem-se tudo para construir a imagem `web-ping`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a que as instruÃ§Ãµes vistas (`FROM`, `ENV`, `WORKDIR`, `COPY`, `CMD`) sÃ£o os blocos de construÃ§Ã£o fundamentais para criar a maioria das imagens Docker.
*   Instrui o leitor a navegar, no seu sistema de arquivos local, atÃ© a pasta que contÃ©m os arquivos necessÃ¡rios para construir a imagem `web-ping`. Esses arquivos foram obtidos no CapÃ­tulo 1 ao clonar ou baixar o repositÃ³rio do GitHub.
*   Os arquivos essenciais sÃ£o:
    *   `Dockerfile`: A receita (como visto na pÃ¡gina anterior).
    *   `app.js`: O cÃ³digo da aplicaÃ§Ã£o em si.
    *   `README.md`: Arquivo de documentaÃ§Ã£o (nÃ£o usado no build).
*   Um ponto importante Ã© destacado: vocÃª, como pessoa construindo a imagem, nÃ£o precisa ser um especialista na linguagem de programaÃ§Ã£o do aplicativo (Node.js neste caso). O `Dockerfile` abstrai os detalhes de como o aplicativo Ã© executado. O cÃ³digo `app.js` simplesmente usa funcionalidades padrÃ£o do Node.js para realizar sua tarefa e ler as configuraÃ§Ãµes das variÃ¡veis de ambiente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:**
    1.  Abra o PowerShell ou terminal do VS Code.
    2.  Navegue atÃ© o diretÃ³rio onde vocÃª clonou/extraiu o cÃ³digo do livro. Se vocÃª colocou em `C:\DockerLabs`, o comando seria:
        ```powershell
        cd C:\DockerLabs\diamol\ch03\exercises\web-ping
        ```
    3.  Liste os arquivos para confirmar:
        ```powershell
        dir
        # ou Get-ChildItem
        ```
        VocÃª deve ver `Dockerfile`, `app.js`, e `README.md`.
    4.  (Opcional) Abra os arquivos `Dockerfile` e `app.js` no VS Code para inspecionÃ¡-los.

---

ğŸ“„ PÃ¡gina 63

ğŸ” Resumo dos principais pontos:

*   **3.3 Building your own container image** (Construindo sua prÃ³pria imagem de contÃªiner)
*   Para construir uma imagem, Docker precisa:
    *   Nome para a imagem (`--tag`).
    *   LocalizaÃ§Ã£o dos arquivos a serem empacotados (`contexto`).
*   **ExercÃ­cio Try-it-now:** Construir a imagem a partir do Dockerfile.
    *   Comando: `docker image build --tag web-ping .`
    *   `--tag web-ping`: Define o nome (e tag padrÃ£o `latest`) da imagem como `web-ping`.
    *   `.` (ponto final): Indica que o **contexto de build** Ã© o diretÃ³rio atual (onde estÃ£o o `Dockerfile` e `app.js`).
*   SaÃ­da do build mostra a execuÃ§Ã£o das instruÃ§Ãµes do Dockerfile (Figura 3.6).
*   **Troubleshooting de erros no build:**
    *   Verificar se o Docker Engine estÃ¡ rodando (Ã­cone da baleia).
    *   Verificar se estÃ¡ no diretÃ³rio correto (`ch03/exercises/web-ping`).
    *   Verificar se digitou o comando corretamente (incluindo o `.` no final).
*   **Aviso sobre permissÃµes de arquivo (Windows buildando Linux):**
    *   Pode aparecer um aviso sobre permissÃµes.
    *   Windows e Linux tratam permissÃµes de forma diferente.
    *   Arquivos copiados do Windows para imagem Linux terÃ£o permissÃµes de leitura/escrita completas. Isso geralmente nÃ£o Ã© um problema para os exemplos do livro.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina ensina como executar o processo de construÃ§Ã£o da imagem usando o comando `docker image build` (ou `docker build`).
*   Os dois parÃ¢metros essenciais sÃ£o:
    *   `--tag <nome>:<tag>` (ou `-t`): DÃ¡ um nome e, opcionalmente, uma tag (versÃ£o) Ã  imagem que serÃ¡ criada. Se a tag for omitida, `:latest` Ã© usado por padrÃ£o. Aqui, a imagem serÃ¡ chamada `web-ping:latest`.
    *   `<caminho_para_contexto>`: Especifica o diretÃ³rio no seu computador que contÃ©m o `Dockerfile` e todos os outros arquivos que podem ser necessÃ¡rios durante o build (como o `app.js` que serÃ¡ copiado pela instruÃ§Ã£o `COPY`). Usar `.` significa "use o diretÃ³rio atual como contexto".
*   O Docker lÃª o `Dockerfile` no contexto, executa cada instruÃ§Ã£o em ordem, criando uma camada para cada uma (usando o cache se possÃ­vel), e ao final, monta a imagem final com o nome e tag especificados.
*   A seÃ§Ã£o de troubleshooting Ã© Ãºtil caso o comando falhe.
*   O aviso sobre permissÃµes Ã© comum ao construir imagens Linux a partir do Windows usando Docker Desktop. Para a maioria dos casos, pode ser ignorado, mas em cenÃ¡rios de produÃ§Ã£o mais rigorosos, o gerenciamento de permissÃµes pode exigir atenÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** Certifique-se de que vocÃª estÃ¡ no diretÃ³rio `ch03\exercises\web-ping` no seu terminal PowerShell/VS Code.
*   Execute o comando de build:
    ```powershell
    docker image build --tag web-ping .
    ```
*   Observe a saÃ­da. VocÃª verÃ¡ o Docker passar por cada "Step" do Dockerfile:
    *   Step 1/7 : FROM diamol/node
    *   Step 2/7 : ENV TARGET="blog.sixeyed.com"
    *   ...
    *   Step 6/7 : COPY app.js .
    *   Step 7/7 : CMD ["node", "/web-ping/app.js"]
    *   Successfully built <id_da_imagem>
    *   Successfully tagged web-ping:latest
*   Ignore o aviso de permissÃµes se ele aparecer.

---

ğŸ“„ PÃ¡gina 64

ğŸ” Resumo dos principais pontos:

*   Mensagens "successfully built" e "successfully tagged" indicam sucesso.
*   Imagem construÃ­da Ã© armazenada localmente no cache de imagens.
*   **ExercÃ­cio Try-it-now:** Listar imagens comeÃ§ando com "w".
    *   Comando: `docker image ls 'w*'` (Aspas podem ser necessÃ¡rias dependendo do shell).
    *   SaÃ­da esperada: mostra a imagem `web-ping` recÃ©m-criada, com `TAG=latest`, ID, data de criaÃ§Ã£o e tamanho.
    *   Exemplo de saÃ­da: REPOSITORY=web-ping, TAG=latest, IMAGE ID=f2a5c430ab2a, CREATED=14 minutes ago, SIZE=75.3MB.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s a execuÃ§Ã£o bem-sucedida do `docker image build`, a nova imagem (`web-ping:latest`) estÃ¡ pronta para ser usada e reside no armazenamento local do Docker Engine (o cache de imagens).
*   O comando `docker image ls` (ou `docker images`) Ã© usado para listar as imagens disponÃ­veis localmente. Ele pode ser combinado com filtros (`-f` ou `--filter`) para encontrar imagens especÃ­ficas. O exemplo usa um filtro de nome (`reference=w*`) para listar imagens cujo nome comeÃ§a com "w".
*   A saÃ­da do `docker image ls` mostra informaÃ§Ãµes importantes sobre cada imagem:
    *   `REPOSITORY`: O nome base da imagem.
    *   `TAG`: A versÃ£o ou identificador especÃ­fico (padrÃ£o `latest`).
    *   `IMAGE ID`: Um identificador Ãºnico para o conteÃºdo da imagem.
    *   `CREATED`: Quando a imagem foi construÃ­da.
    *   `SIZE`: O tamanho total (virtual) da imagem.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:** No PowerShell/terminal, execute:
    ```powershell
    docker image ls 'w*'
    # ou simplesmente para ver todas:
    # docker image ls
    ```
*   Confirme que a imagem `web-ping` com a tag `latest` estÃ¡ na lista. Anote o tamanho (SIZE) reportado.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `docker image build` Ã© o processo de seguir a receita (Dockerfile) para assar o bolo (criar a imagem).
*   `docker image ls` Ã© como olhar na sua despensa para ver quais bolos (imagens) vocÃª jÃ¡ tem prontos.

---

ğŸ“„ PÃ¡gina 65

ğŸ” Resumo dos principais pontos:

*   A imagem `web-ping` que vocÃª construiu pode ser usada da mesma forma que a baixada do Docker Hub (`diamol/ch03-web-ping`).
*   O conteÃºdo do app Ã© o mesmo, e a configuraÃ§Ã£o via variÃ¡veis de ambiente funciona igual.
*   **ExercÃ­cio Try-it-now:** Rodar um contÃªiner a partir da *sua* imagem `web-ping`, configurando-o para pingar `docker.com` a cada 5 segundos.
    *   Comando: `docker container run -e TARGET=docker.com -e INTERVAL=5000 web-ping`.
*   SaÃ­da esperada (Figura 3.7): Logs mostrando que o alvo Ã© `docker.com` e o intervalo Ã© `5000` ms.
*   ContÃªiner roda em primeiro plano; parar com `Ctrl+C`.
*   O contÃªiner entrarÃ¡ no estado `Exited`.
*   RecapitulaÃ§Ã£o: Empacotou um app simples; processo Ã© similar para apps complexos.
    *   Escrever Dockerfile com os passos.
    *   Coletar recursos necessÃ¡rios.
    *   Decidir como permitir configuraÃ§Ã£o pelo usuÃ¡rio (ex: via ENV).
*   **3.4 Understanding Docker images and image layers** (Entendendo imagens Docker e camadas de imagem)
    *   IntroduÃ§Ã£o Ã  construÃ§Ã£o de mais imagens e Ã  importÃ¢ncia de entender camadas e sua relaÃ§Ã£o com contÃªineres.
    *   Imagem Docker contÃ©m arquivos (sistema de arquivos do contÃªiner) e metadados (incluindo histÃ³rico de build).
    *   Pode-se ver o histÃ³rico de camadas de uma imagem.
    *   **ExercÃ­cio Try-it-now:** Verificar o histÃ³rico da imagem `web-ping`.
        *   Comando: `docker image history web-ping`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Agora que vocÃª construiu sua prÃ³pria versÃ£o da imagem `web-ping`, esta pÃ¡gina mostra que ela funciona exatamente como a versÃ£o prÃ©-construÃ­da do Docker Hub, incluindo a capacidade de ser configurada via variÃ¡veis de ambiente (`-e`). O exercÃ­cio demonstra isso rodando um contÃªiner da sua imagem local, mas configurado para pingar `docker.com` em vez do padrÃ£o.
*   A seÃ§Ã£o 3.4 reintroduz o conceito de **camadas de imagem (image layers)**, mencionando que o histÃ³rico de como uma imagem foi construÃ­da (quais comandos do Dockerfile geraram quais camadas) pode ser inspecionado. Isso Ã© fundamental para entender como otimizar os builds.
*   O comando `docker image history <imagem>` permite visualizar essa sequÃªncia de camadas e os comandos que as criaram.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Execute o contÃªiner a partir da sua imagem `web-ping` local, com configuraÃ§Ã£o customizada:
        ```powershell
        docker container run -e TARGET=docker.com -e INTERVAL=5000 web-ping
        ```
        Observe os logs confirmando as configuraÃ§Ãµes. Pressione `Ctrl+C` para parar.
    2.  Inspecione o histÃ³rico da sua imagem:
        ```powershell
        docker image history web-ping
        ```
        Observe a saÃ­da (analisada na prÃ³xima pÃ¡gina). Cada linha corresponde (aproximadamente) a uma instruÃ§Ã£o no seu Dockerfile.

---

ğŸ“„ PÃ¡gina 66

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise da saÃ­da do `docker image history` (exemplo abreviado).
    *   Mostra IMAGE ID (da camada), CREATED (quando), CREATED BY (comando Dockerfile), SIZE, COMMENT.
    *   ObservaÃ§Ã£o: `<missing>` pode aparecer para camadas da imagem base que nÃ£o estÃ£o totalmente presentes localmente.
    *   RelaÃ§Ã£o um-para-um: cada linha no Dockerfile cria uma camada de imagem.
*   **Teoria sobre camadas de imagem:**
    *   Imagem Docker Ã© uma coleÃ§Ã£o *lÃ³gica* de camadas.
    *   Camadas sÃ£o os arquivos *fÃ­sicos* armazenados no cache do Docker Engine.
    *   **ImportÃ¢ncia:** Camadas podem ser **compartilhadas** entre diferentes imagens e contÃªineres.
    *   Exemplo: MÃºltiplos apps Node.js compartilharÃ£o as mesmas camadas base do Node.js runtime.
*   Diagrama (Figura 3.8): Ilustra como camadas sÃ£o logicamente empilhadas.
    *   Imagem `diamol/node`: Camada OS + Camada Node.js runtime.
    *   Imagem `web-ping`: ComeÃ§a com as camadas de `diamol/node` (via `FROM`) + adiciona camadas prÃ³prias (ENV, WORKDIR, COPY, CMD).
*   Tamanho da imagem `diamol/node` (Linux: ~75MB, Windows: ~300MB).
*   Arquivo `app.js` Ã© pequeno (KB).
*   Pergunta: Qual o tamanho total da imagem `web-ping`?

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a saÃ­da do `docker image history`, mostrando que cada instruÃ§Ã£o do `Dockerfile` (`FROM`, `ENV`, `COPY`, `CMD`, etc.) resulta na criaÃ§Ã£o de uma **camada** na imagem final.
*   Aprofunda a teoria das camadas:
    *   **Compartilhamento:** Este Ã© o conceito mais crucial. Se vocÃª tem vÃ¡rias imagens que usam a mesma base (ex: `FROM ubuntu:20.04`), o Docker armazena as camadas do Ubuntu apenas *uma vez* no disco. Todas as imagens que derivam dela simplesmente apontam para essas camadas compartilhadas. O mesmo vale para camadas intermediÃ¡rias idÃªnticas. Isso economiza muito espaÃ§o em disco e tempo de download/upload.
    *   **ConstruÃ§Ã£o:** Uma imagem final Ã© formada pelo empilhamento lÃ³gico dessas camadas. A Figura 3.8 mostra isso claramente: a imagem `web-ping` "herda" as camadas da imagem `diamol/node` e adiciona suas prÃ³prias camadas por cima.
*   Levanta a questÃ£o sobre o tamanho da imagem `web-ping`, dado que ela reutiliza a maior parte da imagem `diamol/node`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revise a saÃ­da do `docker image history web-ping` que vocÃª executou. Tente mapear cada linha (especialmente a coluna `CREATED BY`) para as instruÃ§Ãµes no seu `Dockerfile` da Listagem 3.1. Note que algumas instruÃ§Ãµes como `ENV` podem nÃ£o adicionar tamanho significativo (SIZE 0B).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   ReutilizaÃ§Ã£o de camadas Ã© como compartilhar ingredientes entre receitas. Se 10 receitas diferentes comeÃ§am com "use 1kg de farinha base", vocÃª sÃ³ precisa ter 1kg dessa farinha na despensa, nÃ£o 10kg. A imagem `web-ping` usa a "farinha base" do `diamol/node` e sÃ³ adiciona seus ingredientes extras (`app.js`, `ENV`, `CMD`).

---

ğŸ“„ PÃ¡gina 67

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Listar imagens com `docker image ls` (sem filtro) para ver os tamanhos.
    *   SaÃ­da esperada (Figura 3.9): Mostra vÃ¡rias imagens Node.js (`diamol/node`, `diamol/ch03-web-ping`, `web-ping`), todas com tamanho similar (~75MB em Linux).
*   **Tamanho LÃ³gico vs. FÃ­sico:**
    *   A coluna `SIZE` em `docker image ls` mostra o tamanho *lÃ³gico* (ou virtual) da imagem - o tamanho total se todas as camadas fossem descompactadas e nÃ£o houvesse compartilhamento.
    *   Se as camadas sÃ£o compartilhadas, o espaÃ§o *fÃ­sico* real usado no disco Ã© menor.
    *   NÃ£o Ã© possÃ­vel ver o compartilhamento diretamente em `docker image ls`.
*   Comando `docker system df` mostra o uso real de disco pelo Docker.
*   **ExercÃ­cio Try-it-now:** Verificar o uso de disco do Docker.
    *   Comando: `docker system df`.
*   AnÃ¡lise da saÃ­da do `docker system df` (Figura 3.10):
    *   Mostra tamanho total das imagens (lÃ³gico), contÃªineres, volumes locais e cache de build.
    *   Mostra o espaÃ§o *real* usado pelas imagens (coluna SIZE) e quanto espaÃ§o Ã© recuperÃ¡vel (RECLAIMABLE).
    *   Exemplo do autor: Tamanho lÃ³gico 363MB, tamanho real 202MB -> 163MB economizados (45%) devido ao compartilhamento.
*   Economia de disco Ã© maior com muitas imagens compartilhando as mesmas bases.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina aborda a diferenÃ§a entre o tamanho que o `docker image ls` reporta e o espaÃ§o real que as imagens ocupam no disco.
*   O comando `docker image ls` mostra o tamanho **lÃ³gico**, que Ã© a soma dos tamanhos de todas as camadas da imagem, como se nÃ£o houvesse compartilhamento. Isso pode ser enganoso, pois faz parecer que imagens similares ocupam muito mais espaÃ§o do que realmente ocupam.
*   O comando `docker system df` fornece uma visÃ£o mais precisa do uso de disco pelo Docker. Ele mostra separadamente o espaÃ§o ocupado por imagens, contÃªineres, volumes e o cache de build. Crucialmente, para as imagens, ele mostra o tamanho total *real* (considerando o compartilhamento de camadas) e quanto desse espaÃ§o poderia ser liberado removendo imagens nÃ£o utilizadas (reclaimable).
*   O exemplo do autor demonstra uma economia significativa (45%) graÃ§as ao compartilhamento de camadas. Em ambientes com muitas aplicaÃ§Ãµes baseadas nas mesmas tecnologias (ex: muitas aplicaÃ§Ãµes Java, muitas Node.js), essa economia pode ser ainda maior.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # Listar todas as imagens e observar os tamanhos lÃ³gicos
    docker image ls

    # Verificar o uso real de disco e o compartilhamento
    docker system df
    ```
*   Compare o `TOTAL SIZE` das imagens em `docker image ls` com o `SIZE` da linha `Images` em `docker system df`. O segundo valor (real) provavelmente serÃ¡ menor que o primeiro (lÃ³gico), especialmente se vocÃª tiver outras imagens alÃ©m das usadas neste capÃ­tulo.

---

ğŸ“„ PÃ¡gina 68

ğŸ” Resumo dos principais pontos:

*   Economia via reuso Ã© maior com muitas imagens compartilhando bases comuns (Java, .NET Core, PHP, etc.).
*   **Teoria final sobre camadas:**
    *   Como camadas sÃ£o compartilhadas, elas *nÃ£o podem* ser editadas (sÃ£o **read-only / somente leitura**).
    *   Se uma camada pudesse ser alterada, afetaria todas as imagens que a compartilham.
*   Docker garante isso tornando as camadas de imagem imutÃ¡veis apÃ³s criadas.
*   Pode-se usar esse conhecimento para otimizar Dockerfiles, tornando builds mais rÃ¡pidos e imagens menores.
*   **3.5 Optimizing Dockerfiles to use the image layer cache** (Otimizando Dockerfiles para usar o cache de camadas da imagem)
    *   A camada da imagem `web-ping` que contÃ©m `app.js` Ã© especÃ­fica dela.
    *   Se `app.js` for alterado e a imagem reconstruÃ­da, uma *nova* camada serÃ¡ gerada para a instruÃ§Ã£o `COPY`.
    *   Docker assume sequÃªncia definida de camadas. Se uma camada muda no meio, Docker nÃ£o assume que pode reusar camadas *posteriores*.
    *   **ExercÃ­cio Try-it-now:** Modificar `app.js` (ex: adicionar linha vazia) e reconstruir a imagem com uma nova tag (`web-ping:v2`).
        *   Comando: `docker image build -t web-ping:v2 .`

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz uma propriedade fundamental das camadas: elas sÃ£o **imutÃ¡veis (read-only)**. Uma vez que uma camada Ã© criada durante o `docker build`, ela nunca mais Ã© alterada. Isso Ã© essencial para permitir o compartilhamento seguro entre mÃºltiplas imagens.
*   Essa imutabilidade Ã© a base para a **otimizaÃ§Ã£o do cache de build**. O Docker tenta reutilizar camadas de builds anteriores sempre que possÃ­vel para acelerar o processo.
*   A seÃ§Ã£o 3.5 comeÃ§a a explicar como funciona o cache:
    *   Quando vocÃª reconstrÃ³i uma imagem, o Docker compara cada instruÃ§Ã£o do `Dockerfile` (e os arquivos que ela usa, como no `COPY`) com as camadas que ele jÃ¡ tem no cache de builds anteriores.
    *   Se a instruÃ§Ã£o e seus arquivos de entrada nÃ£o mudaram, o Docker reutiliza a camada do cache (vocÃª verÃ¡ "Using cache" na saÃ­da do build).
    *   **Importante:** Assim que uma instruÃ§Ã£o *nÃ£o* pode usar o cache (porque ela mudou, ou um arquivo que ela copia mudou), o Docker executa essa instruÃ§Ã£o *e todas as instruÃ§Ãµes subsequentes* no Dockerfile, mesmo que elas nÃ£o tenham mudado. Isso Ã© chamado de "quebra do cache" (cache busting).
*   O exercÃ­cio prepara para demonstrar isso: modificando o `app.js` e reconstruindo a imagem, a instruÃ§Ã£o `COPY app.js .` nÃ£o poderÃ¡ usar o cache, quebrando-o naquele ponto.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Abra o arquivo `app.js` (em `C:\DockerLabs\diamol\ch03\exercises\web-ping`) no VS Code.
    2.  FaÃ§a uma pequena alteraÃ§Ã£o. Adicionar um comentÃ¡rio (`// comentario`) ou uma linha em branco no final Ã© suficiente. Salve o arquivo.
    3.  No terminal (no mesmo diretÃ³rio), reconstrua a imagem com uma nova tag:
        ```powershell
        docker image build -t web-ping:v2 .
        ```
    4.  Observe atentamente a saÃ­da do build (analisada na prÃ³xima pÃ¡gina). Veja quais steps usam o cache e a partir de qual step o cache Ã© quebrado.

---

ğŸ“„ PÃ¡gina 69

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise da saÃ­da do build `web-ping:v2` (Figura 3.11).
    *   Steps 1-5 (FROM, ENV, WORKDIR) usam o cache (`---> Using cache`).
    *   Step 6 (COPY app.js .) executa novamente, pois `app.js` mudou. Cache quebrado aqui.
    *   Step 7 (CMD) executa novamente, mesmo nÃ£o tendo mudado, porque o cache foi quebrado no step anterior.
*   **Como funciona o cache:**
    *   Docker calcula um hash (impressÃ£o digital) da instruÃ§Ã£o e dos arquivos copiados.
    *   Se o hash coincide com uma camada existente, usa o cache.
    *   Se nÃ£o hÃ¡ correspondÃªncia, executa a instruÃ§Ã£o -> cache quebrado.
    *   ApÃ³s quebra do cache, todas as instruÃ§Ãµes seguintes sÃ£o executadas.
*   **Impacto:** Mesmo pequenas mudanÃ§as podem invalidar o cache para vÃ¡rias etapas.
*   **OtimizaÃ§Ã£o:** Ordenar as instruÃ§Ãµes no Dockerfile pela frequÃªncia de mudanÃ§a.
    *   InstruÃ§Ãµes que mudam raramente (ex: `FROM`, instalar dependÃªncias base) devem vir *primeiro*.
    *   InstruÃ§Ãµes que mudam frequentemente (ex: `COPY` do cÃ³digo fonte da aplicaÃ§Ã£o) devem vir por *Ãºltimo*.
    *   Objetivo: Na maioria dos builds (onde sÃ³ o cÃ³digo da app muda), apenas as Ãºltimas instruÃ§Ãµes sÃ£o executadas, maximizando o uso do cache.
*   Exemplo de otimizaÃ§Ã£o para `web-ping`:
    *   InstruÃ§Ã£o `CMD` nÃ£o precisa ser a Ãºltima; pode ir para perto do topo, pois raramente muda.
    *   InstruÃ§Ãµes `ENV` podem ser combinadas em uma sÃ³.
*   **Listagem 3.2:** Dockerfile otimizado para `web-ping`.
    ```dockerfile
    FROM diamol/node
    CMD ["node", "/web-ping/app.js"] # CMD movido para cima
    ENV TARGET="blog.sixeyed.com" \ # ENV combinado
        METHOD="HEAD" \
        INTERVAL="3000"
    WORKDIR /web-ping
    COPY app.js . # COPY fica por Ãºltimo
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica detalhadamente por que o build `v2` se comportou daquela forma: a mudanÃ§a no `app.js` invalidou o cache para a instruÃ§Ã£o `COPY` (Step 6), e como consequÃªncia, a instruÃ§Ã£o `CMD` (Step 7) tambÃ©m teve que ser reexecutada.
*   A regra de ouro da otimizaÃ§Ã£o do cache Ã© apresentada: **Ordene suas instruÃ§Ãµes da menos volÃ¡til para a mais volÃ¡til.**
    *   Coisas que quase nunca mudam (como a imagem base `FROM`, a instalaÃ§Ã£o de ferramentas bÃ¡sicas do SO) devem vir no inÃ­cio do Dockerfile.
    *   Coisas que mudam com frequÃªncia (como copiar o cÃ³digo-fonte da sua aplicaÃ§Ã£o, que vocÃª altera a todo momento durante o desenvolvimento) devem vir o mais tarde possÃ­vel no Dockerfile.
*   Aplicando isso ao `web-ping`: O comando de inicializaÃ§Ã£o (`CMD`) provavelmente nÃ£o mudarÃ¡, entÃ£o pode ser movido para cima. As variÃ¡veis de ambiente (`ENV`) tambÃ©m sÃ£o relativamente estÃ¡veis e podem ser combinadas. A cÃ³pia do `app.js` (`COPY`) Ã© o que muda mais, entÃ£o deve ficar por Ãºltimo. A Listagem 3.2 mostra essa versÃ£o otimizada.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Compare a Listagem 3.2 (otimizada) com a Listagem 3.1 (original). Note a mudanÃ§a na ordem de `CMD` e `COPY`, e a combinaÃ§Ã£o dos `ENV`.
*   VocÃª testarÃ¡ essa versÃ£o otimizada na prÃ³xima pÃ¡gina.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   O cache do Docker Ã© como seguir uma receita de bolo onde vocÃª jÃ¡ fez partes dela antes. Se vocÃª sÃ³ mudar o Ãºltimo passo (a cobertura), vocÃª pode reutilizar todas as etapas anteriores (massa, assar). Mas se vocÃª mudar um ingrediente no meio da receita (ex: tipo de farinha), vocÃª terÃ¡ que refazer todos os passos daquele ponto em diante. A otimizaÃ§Ã£o ordena a receita para que as mudanÃ§as mais comuns (a cobertura) fiquem no final.

---

ğŸ“„ PÃ¡gina 70

ğŸ” Resumo dos principais pontos:

*   Dockerfile otimizado estÃ¡ no cÃ³digo fonte (`web-ping-optimized`).
*   **ExercÃ­cio Try-it-now:** Navegar para a pasta `web-ping-optimized` e construir a imagem com a nova versÃ£o do Dockerfile, tag `web-ping:v3`.
    *   Comandos: `cd ../web-ping-optimized`, `docker image build -t web-ping:v3 .`
*   Resultado do build `v3`: similar ao anterior (5 steps em vez de 7), imagem final tem o mesmo comportamento.
*   **BenefÃ­cio da otimizaÃ§Ã£o:** Se agora vocÃª alterar `app.js` e reconstruir `v3`, *apenas* o Ãºltimo step (`COPY`) serÃ¡ executado, pois os steps anteriores (`FROM`, `CMD`, `ENV`, `WORKDIR`) usarÃ£o o cache.
*   **ConclusÃ£o do capÃ­tulo sobre build:**
    *   Viu sintaxe do Dockerfile e instruÃ§Ãµes chave.
    *   Aprendeu a construir e trabalhar com imagens via CLI.
*   **Dois pontos importantes:**
    1.  Otimizar Dockerfiles (ordem das instruÃ§Ãµes).
    2.  Tornar a imagem portÃ¡vel/configurÃ¡vel (ex: via ENV) para usar a mesma imagem em diferentes ambientes.
    *   BenefÃ­cios: builds rÃ¡pidos, deploys em produÃ§Ã£o usando a *mesma* imagem testada.
*   **3.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Produzir uma imagem Docker *sem* usar um Dockerfile.
    *   CenÃ¡rio: Simula a necessidade de passos manuais pÃ³s-instalaÃ§Ã£o que nÃ£o podem ser scriptados.
    *   Tarefa:
        1.  Usar a imagem `diamol/ch03-lab` do Docker Hub.
        2.  Essa imagem tem um arquivo `/diamol/ch03.txt`.
        3.  Atualizar esse arquivo *dentro de um contÃªiner* (adicionar seu nome no final).
        4.  Produzir uma *nova imagem* com o arquivo modificado.
        5.  *RestriÃ§Ã£o:* NÃ£o pode usar `docker build` ou `Dockerfile`.
    *   SoluÃ§Ã£o de exemplo no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina instrui a construir a imagem `v3` usando o Dockerfile otimizado. O build em si nÃ£o parecerÃ¡ muito diferente, mas a estrutura agora estÃ¡ melhor preparada para mudanÃ§as futuras. O benefÃ­cio real Ã© percebido ao fazer uma alteraÃ§Ã£o *apÃ³s* esse build: somente a Ãºltima instruÃ§Ã£o (`COPY`) precisarÃ¡ ser executada, tornando o build subsequente muito mais rÃ¡pido.
*   Resume os aprendizados do capÃ­tulo (sintaxe Dockerfile, build de imagens) e reforÃ§a as boas prÃ¡ticas de otimizaÃ§Ã£o e configuraÃ§Ã£o para portabilidade.
*   Introduz o **Lab do CapÃ­tulo 3**. O desafio aqui Ã© interessante e ensina uma tÃ©cnica diferente: como "salvar" o estado de um contÃªiner modificado como uma nova imagem, *sem* passar pelo processo de `docker build` a partir de um Dockerfile. Isso Ã© Ãºtil em cenÃ¡rios especÃ­ficos, como capturar um estado apÃ³s uma configuraÃ§Ã£o manual ou para depuraÃ§Ã£o. A tarefa envolve iniciar um contÃªiner da imagem base, modificar um arquivo dentro dele e, em seguida, criar uma nova imagem a partir desse estado modificado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais (Build v3):**
    ```powershell
    # Navegar para a pasta otimizada
    cd ..\web-ping-optimized
    # Ou C:\DockerLabs\diamol\ch03\exercises\web-ping-optimized

    # Construir a imagem v3
    docker image build -t web-ping:v3 .

    # (Opcional) Modifique app.js novamente e reconstrua v3 para ver o cache em aÃ§Ã£o:
    # (faÃ§a uma pequena ediÃ§Ã£o em app.js e salve)
    # docker image build -t web-ping:v3 . # Observe que sÃ³ o Step 5/5 deve rodar
    ```
*   **EstratÃ©gia para o Lab 3:**
    1.  Inicie um contÃªiner interativo a partir de `diamol/ch03-lab`: `docker run -it --name lab3-mod diamol/ch03-lab sh` (assumindo shell `sh`).
    2.  Dentro do contÃªiner, edite o arquivo `/diamol/ch03.txt` (use um editor como `vi` ou `nano` se disponÃ­vel, ou comandos como `echo "Meu Nome" >> /diamol/ch03.txt`).
    3.  Saia do contÃªiner (`exit`). O contÃªiner estarÃ¡ parado (`Exited`).
    4.  Use o comando `docker container commit <nome_ou_id_container> <nome_nova_imagem>:<tag>` para criar a nova imagem a partir do contÃªiner parado. Ex: `docker container commit lab3-mod minha-imagem-lab3:latest`.
    5.  Verifique com `docker image ls` e `docker run minha-imagem-lab3:latest cat /diamol/ch03.txt`.

---

ğŸ“„ PÃ¡gina 71

ğŸ” Resumo dos principais pontos:

*   **ContinuaÃ§Ã£o das dicas do Lab 3:**
    *   Relembra flags `-it` para rodar contÃªiner interativamente.
    *   Sistema de arquivos de um contÃªiner `Exited` ainda existe.
    *   Usar `docker container --help` para encontrar comandos Ãºteis (dois deles ajudarÃ£o a resolver o lab - provavelmente `docker exec` ou `docker run -it` para modificar, e `docker commit` para salvar).
*   **CapÃ­tulo 4: Packaging applications from source code into Docker Images** (Empacotando aplicaÃ§Ãµes a partir do cÃ³digo-fonte em Imagens Docker)
*   RevisÃ£o: Cap. 3 ensinou a construir imagens com `Dockerfile` e algumas instruÃ§Ãµes.
*   Novidade do Cap. 4: InstruÃ§Ã£o `RUN` para executar comandos *durante* o build.
    *   Comandos `RUN` executam dentro de um contÃªiner temporÃ¡rio.
    *   Quaisquer mudanÃ§as no sistema de arquivos (arquivos criados/modificados) sÃ£o salvas na camada da imagem resultante da instruÃ§Ã£o `RUN`.
*   Flexibilidade: `RUN` permite fazer quase tudo (expandir zips, rodar instaladores, compilar cÃ³digo).
*   Foco do Cap. 4: Usar `RUN` para empacotar aplicaÃ§Ãµes diretamente do cÃ³digo-fonte.
*   **4.1 Who needs a build server when you have a Dockerfile?** (Quem precisa de um servidor de build quando se tem um Dockerfile?)
    *   Processo tradicional de build: cÃ³digo no controle de versÃ£o (ex: GitHub), servidor de build separado que compila/testa o cÃ³digo.
    *   Desafios do servidor de build:
        *   ManutenÃ§Ã£o: Instalar e manter atualizadas todas as ferramentas necessÃ¡rias (SDKs, compiladores, etc.) para todos os projetos.
        *   InconsistÃªncia: DiferenÃ§as entre ferramentas locais dos devs e as do servidor podem causar falhas.
        *   Novos devs levam tempo para configurar o ambiente local.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina finaliza as dicas para o Lab 3, sugerindo os comandos que provavelmente serÃ£o necessÃ¡rios (`run -it` ou `exec` para entrar/modificar, e `commit` para salvar a imagem).
*   Inicia o CapÃ­tulo 4, que introduz uma capacidade crucial do `Dockerfile`: a instruÃ§Ã£o `RUN`. Diferente de `CMD` ou `ENTRYPOINT` (que definem o que roda quando o *contÃªiner* inicia), `RUN` executa um comando *durante o processo de construÃ§Ã£o da imagem*. O resultado desse comando (ex: arquivos criados por um compilador, pacotes instalados) se torna parte da imagem.
*   A seÃ§Ã£o 4.1 apresenta um caso de uso poderoso para a instruÃ§Ã£o `RUN`: realizar o processo de **compilaÃ§Ã£o e build** da aplicaÃ§Ã£o *dentro* do Dockerfile. Isso elimina a necessidade de um servidor de build tradicional separado, com todas as suas ferramentas instaladas. Os problemas comuns de servidores de build (manutenÃ§Ã£o complexa, inconsistÃªncias com ambientes locais) sÃ£o resolvidos, pois o ambiente de build Ã© definido no prÃ³prio Dockerfile e encapsulado em contÃªineres.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   A instruÃ§Ã£o `RUN` no Dockerfile pode executar comandos do sistema operacional da imagem base.
    *   Exemplo (Linux): `RUN apt-get update && apt-get install -y curl`
    *   Exemplo (Windows): `RUN powershell -Command Install-WindowsFeature Web-Server`
*   No CapÃ­tulo 4, vocÃª verÃ¡ `RUN` sendo usado para executar comandos de build como `mvn package` (Java), `npm install` (Node.js), `go build` (Go).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   InstruÃ§Ã£o `RUN`: Pense nisso como passos intermediÃ¡rios na receita (Dockerfile) que modificam os ingredientes ou a massa *antes* do bolo final (imagem) estar pronto. Ex: "Misture a farinha e os ovos" (`RUN compile code`), "Adicione o fermento" (`RUN install dependency`).
*   Build com Dockerfile vs. Servidor de Build: Ã‰ como ter uma cozinha portÃ¡til super equipada (contÃªiner de build definido no Dockerfile) que vocÃª pode levar para qualquer lugar e garante que o bolo sairÃ¡ igual, versus depender de uma cozinha industrial fixa (servidor de build) que pode ter equipamentos diferentes ou desatualizados.

---

ğŸ“„ PÃ¡gina 72

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 4.1): Ilustra a necessidade de ferramentas de build consistentes entre desenvolvedores e servidor de build.
*   Problemas com ferramentas de build tradicionais: sobrecarga de manutenÃ§Ã£o, inconsistÃªncia de versÃµes, dificuldade para novos membros da equipe.
*   SoluÃ§Ã£o Docker: Empacotar o conjunto de ferramentas de build em uma imagem Docker.
    *   Dockerfile define a instalaÃ§Ã£o das ferramentas -> Imagem de ferramentas de build.
    *   Usar essa imagem nos Dockerfiles das aplicaÃ§Ãµes para compilar o cÃ³digo-fonte.
    *   Resultado final: imagem da aplicaÃ§Ã£o empacotada.
*   IntroduÃ§Ã£o a **Dockerfiles multi-estÃ¡gio (multi-stage Dockerfiles)**.
*   **Listagem 4.1:** Exemplo bÃ¡sico de Dockerfile multi-estÃ¡gio.
    ```dockerfile
    FROM diamol/base AS build-stage # EstÃ¡gio 1 nomeado 'build-stage'
    RUN echo 'Building...' > /build.txt

    FROM diamol/base AS test-stage  # EstÃ¡gio 2 nomeado 'test-stage'
    COPY --from=build-stage /build.txt /build.txt # Copia do estÃ¡gio anterior
    RUN echo 'Testing...' >> /build.txt

    FROM diamol/base # EstÃ¡gio final (sem nome)
    COPY --from=test-stage /build.txt /build.txt # Copia do estÃ¡gio anterior
    CMD cat /build.txt
    ```
*   **CaracterÃ­sticas multi-estÃ¡gio:**
    *   MÃºltiplas instruÃ§Ãµes `FROM` no mesmo Dockerfile.
    *   Cada `FROM` inicia um novo estÃ¡gio de build.
    *   EstÃ¡gios podem ser nomeados usando `AS <nome>`.
    *   A imagem final contÃ©m *apenas* o resultado do *Ãºltimo* estÃ¡gio.
    *   Pode-se copiar artefatos (arquivos/diretÃ³rios) de estÃ¡gios anteriores para estÃ¡gios posteriores usando `COPY --from=<nome_estagio> <origem> <destino>`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina continua argumentando contra a complexidade dos servidores de build tradicionais e propÃµe a soluÃ§Ã£o Docker: criar uma "imagem de ferramentas de build" que contenha tudo que Ã© necessÃ¡rio para compilar seus projetos.
*   A tÃ©cnica chave para isso Ã© o **Dockerfile multi-estÃ¡gio**. Em vez de ter um Ãºnico fluxo `FROM ... RUN ... COPY ... CMD`, um Dockerfile multi-estÃ¡gio tem vÃ¡rias seÃ§Ãµes `FROM`, cada uma definindo um estÃ¡gio de build independente.
*   **Listagem 4.1** demonstra isso:
    *   O primeiro `FROM` inicia o estÃ¡gio `build-stage`. Ele usa a imagem `diamol/base` e cria um arquivo `/build.txt`.
    *   O segundo `FROM` inicia o estÃ¡gio `test-stage`. Ele tambÃ©m usa `diamol/base`. Crucialmente, ele usa `COPY --from=build-stage` para copiar o `/build.txt` criado no estÃ¡gio anterior. Depois, adiciona texto a esse arquivo.
    *   O terceiro `FROM` inicia o estÃ¡gio final. Ele copia o `/build.txt` modificado do estÃ¡gio `test-stage`. O `CMD` final exibe o conteÃºdo desse arquivo.
*   **Importante:** A imagem Docker resultante conterÃ¡ *apenas* o sistema de arquivos do *Ãºltimo* estÃ¡gio. Os estÃ¡gios anteriores (`build-stage`, `test-stage`) sÃ£o descartados, a menos que algo deles seja explicitamente copiado para o estÃ¡gio final usando `COPY --from`. Isso Ã© excelente para otimizaÃ§Ã£o, como veremos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª criarÃ¡ um arquivo `Dockerfile` com o conteÃºdo da Listagem 4.1 em um diretÃ³rio (ex: `C:\DockerLabs\diamol\ch04\exercises\multi-stage`).
*   VocÃª usarÃ¡ `docker build -t multi-stage .` para construir a imagem a partir deste Dockerfile multi-estÃ¡gio (prÃ³ximo passo).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Dockerfile multi-estÃ¡gio Ã© como ter vÃ¡rias bancadas de cozinha para preparar um prato complexo:
    *   Bancada 1 (`build-stage`): Picar os vegetais.
    *   Bancada 2 (`test-stage`): Pegar os vegetais picados da Bancada 1 (`COPY --from`), misturar com o molho.
    *   Bancada Final: Pegar a mistura da Bancada 2 (`COPY --from`), colocar no prato final.
    *   SÃ³ o prato final (Ãºltimo estÃ¡gio) Ã© servido (imagem final). As bancadas intermediÃ¡rias sÃ£o limpas e descartadas.

---

ğŸ“„ PÃ¡gina 73

ğŸ” Resumo dos principais pontos:

*   IntroduÃ§Ã£o da instruÃ§Ã£o `RUN`: executa um comando *dentro* de um contÃªiner durante o build.
    *   Qualquer saÃ­da/mudanÃ§a no filesystem Ã© salva na camada da imagem.
    *   Pode executar qualquer comando que *exista* na imagem base usada no `FROM` daquele estÃ¡gio.
    *   Exemplo usou `echo` que estÃ¡ em `diamol/base`.
*   Diagrama (Figura 4.2): Ilustra a execuÃ§Ã£o sequencial dos estÃ¡gios do Dockerfile multi-estÃ¡gio.
*   **Isolamento dos estÃ¡gios:**
    *   Cada estÃ¡gio roda independentemente.
    *   Podem usar imagens base diferentes com ferramentas diferentes.
    *   EstÃ¡gio final sÃ³ contÃ©m o que for explicitamente copiado de estÃ¡gios anteriores.
    *   Se um comando falhar em *qualquer* estÃ¡gio, o build *inteiro* falha.
*   **ExercÃ­cio Try-it-now:** Navegar para a pasta `ch04/exercises/multi-stage` e construir a imagem multi-estÃ¡gio.
    *   Comandos: `cd ch04/exercises/multi-stage`, `docker image build -t multi-stage .`
*   SaÃ­da do build (Figura 4.3) mostra a execuÃ§Ã£o sequencial dos estÃ¡gios.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina formaliza a instruÃ§Ã£o `RUN`, explicando que ela executa comandos *durante o build*, e o resultado (novos arquivos, pacotes instalados, etc.) Ã© capturado na camada da imagem criada por essa instruÃ§Ã£o. Ã‰ fundamental que o comando usado no `RUN` esteja disponÃ­vel na imagem base definida no `FROM` do estÃ¡gio atual.
*   A Figura 4.2 reforÃ§a que os estÃ¡gios sÃ£o executados em sequÃªncia.
*   O **isolamento** entre os estÃ¡gios Ã© reiterado: cada um Ã© um ambiente temporÃ¡rio separado. Apenas o que Ã© explicitamente transferido entre eles usando `COPY --from` persiste no estÃ¡gio seguinte. Isso Ã© crucial para a otimizaÃ§Ã£o: pode-se usar uma imagem base grande com muitas ferramentas de build em um estÃ¡gio inicial, compilar o aplicativo, e entÃ£o copiar *apenas* o executÃ¡vel/binÃ¡rio compilado para um estÃ¡gio final que usa uma imagem base mÃ­nima (sem as ferramentas de build), resultando em uma imagem final muito menor e mais segura.
*   O exercÃ­cio instrui a construir a imagem do Dockerfile multi-estÃ¡gio da Listagem 4.1.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:**
    1.  Certifique-se de ter o `Dockerfile` da Listagem 4.1 salvo em `C:\DockerLabs\diamol\ch04\exercises\multi-stage`.
    2.  No PowerShell/terminal, navegue atÃ© essa pasta:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\multi-stage
        ```
    3.  Construa a imagem:
        ```powershell
        docker image build -t multi-stage .
        ```
    4.  Observe a saÃ­da (Figura 4.3). VocÃª verÃ¡ o Docker processando os 3 estÃ¡gios (provavelmente 9 steps no total). Note as mensagens "Running in ..." para as instruÃ§Ãµes `RUN` e "Copying ..." para as instruÃ§Ãµes `COPY --from`.

---

ğŸ“„ PÃ¡gina 74

ğŸ” Resumo dos principais pontos:

*   Figura 4.3: Exemplo de saÃ­da do build multi-estÃ¡gio.
*   PadrÃ£o multi-estÃ¡gio Ã© aplicÃ¡vel a apps de qualquer complexidade.
*   Diagrama (Figura 4.4): Ilustra um fluxo de trabalho multi-estÃ¡gio para uma aplicaÃ§Ã£o Java.
    *   **EstÃ¡gio `build`:**
        *   Usa imagem base com ferramentas de build Java (ex: JDK, Maven).
        *   Copia cÃ³digo-fonte do host.
        *   Executa comando de build (`RUN mvn package`).
    *   **EstÃ¡gio `test` (opcional):**
        *   Usa imagem base com framework de teste.
        *   Copia binÃ¡rios compilados do estÃ¡gio `build`.
        *   Executa testes (`RUN ./run-tests.sh`).
    *   **EstÃ¡gio final (aplicaÃ§Ã£o):**
        *   Usa imagem base *apenas* com o runtime Java (JRE).
        *   Copia binÃ¡rios testados do estÃ¡gio `build` (ou `test`).
        *   Define `CMD` ou `ENTRYPOINT` para rodar o app.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 4.3 mostra a saÃ­da real do build do exemplo anterior.
*   A pÃ¡gina entÃ£o generaliza o conceito multi-estÃ¡gio para um cenÃ¡rio mais realista: construir uma aplicaÃ§Ã£o Java (Figura 4.4).
*   O fluxo tÃ­pico Ã©:
    1.  **Build Stage:** Usa uma imagem que contÃ©m o Java Development Kit (JDK) e a ferramenta de build (como Maven ou Gradle). O cÃ³digo-fonte Ã© copiado para este estÃ¡gio, e um comando `RUN` executa o build (ex: `mvn package`), gerando os arquivos `.jar` ou `.war`.
    2.  **Test Stage (Opcional):** Pode-se ter um estÃ¡gio intermediÃ¡rio para rodar testes unitÃ¡rios ou de integraÃ§Ã£o. Ele usaria uma imagem com as ferramentas de teste necessÃ¡rias, copiaria os artefatos compilados do estÃ¡gio de build e executaria os testes com um comando `RUN`. Se os testes falharem, o build inteiro falha.
    3.  **Final Stage (Runtime):** Este estÃ¡gio usa uma imagem base *mÃ­nima*, contendo apenas o Java Runtime Environment (JRE), que Ã© necessÃ¡rio para *executar* a aplicaÃ§Ã£o, mas nÃ£o para compilÃ¡-la. Ele copia *apenas* os artefatos compilados e testados (o `.jar`/`.war`) do estÃ¡gio de build (ou teste) e define como iniciar a aplicaÃ§Ã£o (`CMD` ou `ENTRYPOINT`).
*   **BenefÃ­cio:** A imagem final contÃ©m apenas o necessÃ¡rio para rodar a aplicaÃ§Ã£o (JRE + app compilado), sem incluir o pesado JDK e as ferramentas de build, resultando em uma imagem muito menor e mais segura.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Este Ã© o padrÃ£o que serÃ¡ usado nos prÃ³ximos exemplos com Java, Node.js e Go. VocÃª verÃ¡ Dockerfiles que seguem essa estrutura de mÃºltiplos `FROM`, `COPY --from`, e `RUN` para compilar/testar, e um estÃ¡gio final enxuto.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Continuando a analogia da cozinha:
    *   EstÃ¡gio de Build: Uma cozinha industrial completa (imagem com JDK/Maven) onde vocÃª prepara a massa e assa o bolo (compila o JAR).
    *   EstÃ¡gio de Teste: Uma Ã¡rea de controle de qualidade onde o bolo Ã© provado (testes unitÃ¡rios).
    *   EstÃ¡gio Final: Uma embalagem bonita e leve (imagem com JRE mÃ­nimo) onde vocÃª coloca *apenas* o bolo pronto e testado (JAR), sem levar a cozinha industrial junto.

---

ğŸ“„ PÃ¡gina 75

ğŸ” Resumo dos principais pontos:

*   Abordagem multi-estÃ¡gio torna a aplicaÃ§Ã£o verdadeiramente portÃ¡til (construir e rodar em qualquer lugar sÃ³ com Docker).
*   Servidor de build sÃ³ precisa do Docker instalado.
*   ConfiguraÃ§Ã£o de novos membros da equipe simplificada.
*   Ferramentas de build centralizadas em imagens Docker, sem risco de dessincronizaÃ§Ã£o.
*   Muitos frameworks jÃ¡ tÃªm imagens pÃºblicas no Docker Hub com ferramentas de build e runtimes separados (mantidas pelas equipes dos projetos).
*   Pode-se usar essas imagens diretamente ou como base para imagens customizadas.
*   **4.2 App walkthrough: Java source code** (Passo a passo da aplicaÃ§Ã£o: cÃ³digo-fonte Java)
    *   Exemplo real: aplicaÃ§Ã£o Java Spring Boot simples.
    *   NÃ£o Ã© preciso ser dev Java ou ter ferramentas Java instaladas.
    *   Ferramentas usadas: Maven (build, dependÃªncias via `pom.xml`, comando `mvn`) e OpenJDK (runtime/JDK gratuito).
*   **Listagem 4.2:** Dockerfile para construir o app Java com Maven (multi-estÃ¡gio).
    ```dockerfile
    FROM diamol/maven AS builder # EstÃ¡gio builder com Maven/JDK
    WORKDIR /usr/src/iotd
    COPY pom.xml .
    RUN mvn -B dependency:go-offline # OtimizaÃ§Ã£o: baixa dependÃªncias primeiro
    COPY . . # Copia resto do cÃ³digo fonte
    RUN mvn package # Compila e empacota (gera JAR)

    # app # EstÃ¡gio final
    FROM diamol/openjdk # Base apenas com JRE
    WORKDIR /app
    COPY --from=builder /usr/src/iotd/target/iotd-service-0.1.0.jar . # Copia SÃ“ o JAR
    EXPOSE 80 # Informa que o app ouve na porta 80
    ENTRYPOINT ["java", "-jar", "/app/iotd-service-0.1.0.jar"] # Comando para rodar
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a os benefÃ­cios do build multi-estÃ¡gio: portabilidade total (build/run), simplificaÃ§Ã£o da infraestrutura de build e do onboarding de desenvolvedores, e consistÃªncia das ferramentas. Menciona que imagens oficiais para build/runtime jÃ¡ existem no Docker Hub.
*   Inicia o primeiro exemplo prÃ¡tico detalhado (SeÃ§Ã£o 4.2): construir uma aplicaÃ§Ã£o Java.
*   A **Listagem 4.2** mostra o Dockerfile multi-estÃ¡gio para essa aplicaÃ§Ã£o:
    *   **EstÃ¡gio `builder`:**
        *   Usa `diamol/maven` como base (contÃ©m JDK e Maven).
        *   Copia primeiro o `pom.xml` (arquivo de definiÃ§Ã£o do build Maven) e roda `mvn dependency:go-offline`. Isso Ã© uma **otimizaÃ§Ã£o de cache**: baixar as dependÃªncias Ã© demorado e sÃ³ precisa ser refeito se o `pom.xml` mudar. Se apenas o cÃ³digo Java mudar depois, esta camada serÃ¡ reutilizada do cache.
        *   Copia o restante do cÃ³digo (`COPY . .`).
        *   Roda `mvn package` para compilar o cÃ³digo e gerar o arquivo `.jar` final.
    *   **EstÃ¡gio final (`app`):**
        *   Usa `diamol/openjdk` como base (contÃ©m apenas o JRE, mais leve).
        *   Copia *apenas* o arquivo `.jar` gerado no estÃ¡gio `builder`. As ferramentas de build e o cÃ³digo-fonte sÃ£o descartados.
        *   `EXPOSE 80`: Documenta (nÃ£o executa) que a aplicaÃ§Ã£o dentro do contÃªiner ouvirÃ¡ na porta 80.
        *   `ENTRYPOINT`: Define o comando principal para executar a aplicaÃ§Ã£o Java a partir do JAR.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª precisarÃ¡ dos arquivos `Dockerfile` (Listagem 4.2), `pom.xml` e o cÃ³digo-fonte Java (pasta `src`) no diretÃ³rio `ch04\exercises\image-of-the-day`.
*   O comando para construir serÃ¡ `docker image build -t image-of-the-day .` (prÃ³ximo passo).

---

ğŸ“„ PÃ¡gina 76

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise detalhada do Dockerfile Java (Listagem 4.2):
    *   **EstÃ¡gio `builder`:**
        *   Base `diamol/maven` (JDK + Maven).
        *   Copia `pom.xml`.
        *   `RUN mvn ... dependency:go-offline`: OtimizaÃ§Ã£o de cache para dependÃªncias.
        *   `COPY . .`: Copia o resto do cÃ³digo fonte.
        *   `RUN mvn package`: Compila e gera o JAR.
        *   Se build falhar (rede, erro de cÃ³digo), `RUN` falha -> build inteiro falha.
    *   **EstÃ¡gio final (`app`):**
        *   Base `diamol/openjdk` (sÃ³ JRE).
        *   Copia *apenas* o JAR do estÃ¡gio `builder`.
        *   `EXPOSE 80`: Documenta a porta interna.
        *   `ENTRYPOINT`: Define como rodar o JAR com `java -jar`.
*   **ExercÃ­cio Try-it-now:** Navegar para a pasta do cÃ³digo Java e construir a imagem.
    *   Comandos: `cd ch04/exercises/image-of-the-day`, `docker image build -t image-of-the-day .`

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina fornece uma explicaÃ§Ã£o linha a linha mais detalhada do Dockerfile Java apresentado na pÃ¡gina anterior, reforÃ§ando o propÃ³sito de cada estÃ¡gio e cada comando, especialmente a otimizaÃ§Ã£o de cache ao baixar dependÃªncias separadamente e a cÃ³pia seletiva do artefato final (`.jar`) para a imagem de runtime mÃ­nima. Destaca que uma falha em qualquer comando `RUN` interrompe todo o processo de build.
*   O exercÃ­cio instrui a executar o build da imagem a partir deste Dockerfile.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:**
    1.  No PowerShell/terminal, navegue atÃ© o diretÃ³rio:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\image-of-the-day
        ```
    2.  Execute o comando de build:
        ```powershell
        docker image build -t image-of-the-day .
        ```
    3.  Observe a saÃ­da (analisada na prÃ³xima pÃ¡gina). A primeira vez pode demorar um pouco, especialmente no passo `mvn dependency:go-offline`, pois ele baixarÃ¡ as dependÃªncias Java da internet. Builds subsequentes (se nada mudar no `pom.xml`) serÃ£o muito mais rÃ¡pidos nessa etapa devido ao cache.

---

ğŸ“„ PÃ¡gina 77

ğŸ” Resumo dos principais pontos:

*   SaÃ­da do build Java mostra logs do Maven (baixando dependÃªncias, compilando). Figura 4.5 mostra trecho da saÃ­da.
*   **DescriÃ§Ã£o da aplicaÃ§Ã£o construÃ­da (`image-of-the-day`):**
    *   API REST simples que busca a "Astronomy Picture of the Day" da NASA (APOD).
    *   Faz cache da imagem/detalhes para evitar acessos repetidos Ã  NASA.
    *   API Ã© apenas *uma parte* de uma aplicaÃ§Ã£o maior que usarÃ¡ mÃºltiplos contÃªineres comunicando-se entre si.
*   **ComunicaÃ§Ã£o entre contÃªineres:**
    *   ContÃªineres se acessam atravÃ©s de uma **rede virtual Docker**.
    *   Docker atribui um IP virtual a cada contÃªiner na rede.
    *   Pode-se criar e gerenciar redes Docker via linha de comando.
*   **ExercÃ­cio Try-it-now:** Criar uma rede Docker chamada `nat`.
    *   Comando: `docker network create nat`.
    *   Ignorar erro se a rede `nat` jÃ¡ existir (pode ter sido criada no Cap. 4).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina descreve a saÃ­da esperada do build Java (logs do Maven) e explica o que a aplicaÃ§Ã£o `image-of-the-day` (IOTD) faz: Ã© uma API que busca a foto astronÃ´mica do dia da NASA e a armazena localmente (cache).
*   Introduz um conceito fundamental para aplicaÃ§Ãµes distribuÃ­das: a **rede Docker**. Para que mÃºltiplos contÃªineres (como a API IOTD e outros componentes que a consumirÃ£o) possam se comunicar, eles precisam estar conectados Ã  mesma rede virtual gerenciada pelo Docker. O Docker fornece funcionalidades de rede, incluindo a atribuiÃ§Ã£o de IPs e um sistema de DNS interno (que veremos mais tarde) que permite que contÃªineres encontrem uns aos outros pelo nome.
*   O exercÃ­cio instrui a criar manualmente uma rede chamada `nat` usando o comando `docker network create`. Essa rede serÃ¡ usada para conectar os contÃªineres da aplicaÃ§Ã£o. (O nome `nat` Ã© frequentemente usado, especialmente no Windows, pois Ã© o nome da rede padrÃ£o criada pelo Docker, mas aqui estamos criando uma rede personalizada com esse nome para compatibilidade entre exemplos).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revise a saÃ­da do `docker build` da etapa anterior.
*   **AÃ§Ã£o principal:** Crie a rede Docker:
    ```powershell
    docker network create nat
    ```
    Se vocÃª receber uma mensagem como `Error response from daemon: network with name nat already exists`, pode ignorÃ¡-la; significa que a rede jÃ¡ foi criada em um exercÃ­cio anterior.
*   VocÃª pode verificar as redes existentes com:
    ```powershell
    docker network ls
    ```
    A rede `nat` deve aparecer na lista.

---

ğŸ“„ PÃ¡gina 78

ğŸ” Resumo dos principais pontos:

*   Ao rodar contÃªineres, pode-se conectÃ¡-los explicitamente Ã  rede criada usando a flag `--network`.
*   ContÃªineres na mesma rede podem se alcanÃ§ar usando seus nomes como hostname (DNS interno do Docker).
*   **ExercÃ­cio Try-it-now:** Rodar um contÃªiner da imagem `image-of-the-day`.
    *   Comando: `docker container run --name iotd -d -p 800:80 --network nat image-of-the-day`.
    *   `--name iotd`: Nomeia o contÃªiner como `iotd`.
    *   `-d`: Roda em background.
    *   `-p 800:80`: Publica a porta 80 interna do contÃªiner na porta 800 do host.
    *   `--network nat`: Conecta o contÃªiner Ã  rede `nat`.
*   **Teste:** Acessar `http://localhost:800/image` no navegador.
    *   Deve mostrar detalhes em JSON da imagem do dia da NASA (Figura 4.6).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como usar a rede Docker criada: ao iniciar um contÃªiner com `docker run`, a flag `--network nat` o conecta a essa rede especÃ­fica.
*   Um benefÃ­cio crucial das redes Docker Ã© o **DNS interno baseado em nome de contÃªiner/serviÃ§o**. Se dois contÃªineres, `app-a` e `app-b`, estÃ£o na mesma rede Docker, `app-a` pode simplesmente fazer uma requisiÃ§Ã£o para `http://app-b/...`, e o Docker resolverÃ¡ `app-b` para o endereÃ§o IP correto daquele contÃªiner dentro da rede virtual. Isso simplifica enormemente a configuraÃ§Ã£o da comunicaÃ§Ã£o entre serviÃ§os.
*   O exercÃ­cio instrui a rodar o contÃªiner da API IOTD, nomeando-o como `iotd`, publicando a porta 800 e conectando-o Ã  rede `nat`. O nome `iotd` serÃ¡ importante para que outros contÃªineres possam encontrÃ¡-lo nessa rede.
*   O teste final Ã© acessar a API pelo navegador usando a porta publicada no host (`localhost:800`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:**
    ```powershell
    docker container run --name iotd -d -p 800:80 --network nat image-of-the-day
    ```
*   Verifique se o contÃªiner estÃ¡ rodando:
    ```powershell
    docker ps
    ```
*   Abra o navegador e acesse `http://localhost:800/image`. VocÃª deve ver uma resposta JSON com informaÃ§Ãµes sobre a imagem astronÃ´mica do dia.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Conectar contÃªineres a uma rede Docker (`--network nat`) Ã© como dar a eles um ramal telefÃ´nico dentro do mesmo prÃ©dio (`nat`). Eles podem ligar uns para os outros usando apenas o nome do apartamento (`--name iotd`), sem precisar saber o nÃºmero exato do ramal (IP interno).

---

ğŸ“„ PÃ¡gina 79

ğŸ” Resumo dos principais pontos:

*   ImportÃ¢ncia do exemplo Java:
    *   Build pode ser feito em qualquer mÃ¡quina sÃ³ com Docker (sem Java SDK local).
    *   Simples clonar o repo e rodar `docker build`, `docker run`.
*   Ferramentas de build (Maven, JDK) *nÃ£o* fazem parte da imagem final da aplicaÃ§Ã£o (`image-of-the-day`).
    *   SÃ³ o estÃ¡gio final vai para a imagem.
    *   Pode-se verificar rodando um contÃªiner interativo da imagem final e tentando usar `mvn` (nÃ£o estarÃ¡ lÃ¡).
*   **4.3 App walkthrough: Node.js source code** (Passo a passo da aplicaÃ§Ã£o: cÃ³digo-fonte Node.js)
    *   Novo exemplo multi-estÃ¡gio: aplicaÃ§Ã£o Node.js.
    *   Importante para entender diferentes tipos de build (linguagens compiladas vs. interpretadas).
    *   PadrÃ£o similar a Python, PHP, Ruby.
    *   CÃ³digo fonte em `ch04/exercises/access-log`.
*   **DiferenÃ§a Node.js vs. Java/Compilados:**
    *   Java/C#: CÃ³digo fonte -> Compilador (no estÃ¡gio build) -> BinÃ¡rios (.jar, .dll). Imagem final sÃ³ precisa dos binÃ¡rios + runtime.
    *   Node.js (JavaScript): Linguagem interpretada, sem compilaÃ§Ã£o para binÃ¡rio. Imagem final precisa do **runtime Node.js + cÃ³digo fonte (.js)**.
*   **Necessidade de multi-estÃ¡gio em Node.js:** Otimizar carregamento de dependÃªncias (`npm install`).
*   **Listagem 4.3:** Dockerfile para construir app Node.js com `npm`.
    ```dockerfile
    FROM diamol/node AS builder # EstÃ¡gio builder com Node.js + npm
    WORKDIR /src
    COPY src/package.json . # Copia SÃ“ package.json
    RUN npm install # OtimizaÃ§Ã£o: instala dependÃªncias

    # app # EstÃ¡gio final
    FROM diamol/node # Base com Node.js runtime
    EXPOSE 80
    CMD ["node", "server.js"] # Comando para rodar
    WORKDIR /app
    COPY --from=builder /src/node_modules/ /app/node_modules/ # Copia dependÃªncias
    COPY src/ . # Copia cÃ³digo fonte
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a a portabilidade do processo de build Dockerizado e um ponto crucial sobre builds multi-estÃ¡gio: a imagem final Ã© enxuta porque as ferramentas de build (JDK, Maven) ficam apenas no estÃ¡gio intermediÃ¡rio (`builder`) e sÃ£o descartadas.
*   Inicia o segundo exemplo prÃ¡tico (SeÃ§Ã£o 4.3): construir uma aplicaÃ§Ã£o Node.js.
*   Explica a principal diferenÃ§a em relaÃ§Ã£o ao Java: Node.js usa JavaScript, uma linguagem *interpretada*. NÃ£o hÃ¡ um passo de compilaÃ§Ã£o que gera um binÃ¡rio autocontido. Portanto, a imagem Docker final para uma aplicaÃ§Ã£o Node.js precisa conter tanto o **interpretador/runtime (Node.js)** quanto o **prÃ³prio cÃ³digo-fonte (.js)**.
*   Apesar disso, o build multi-estÃ¡gio ainda Ã© Ãºtil para Node.js, principalmente para otimizar o download e instalaÃ§Ã£o de dependÃªncias (gerenciadas pelo `npm`).
*   **Listagem 4.3** mostra o Dockerfile Node.js:
    *   **EstÃ¡gio `builder`:**
        *   Usa `diamol/node` (que tem Node.js e `npm`).
        *   Copia *apenas* o `package.json` (que lista as dependÃªncias).
        *   Roda `npm install`. Similar Ã  otimizaÃ§Ã£o do Maven, isso baixa as dependÃªncias (que ficam na pasta `node_modules`). Se sÃ³ o cÃ³digo `.js` mudar depois, esta camada serÃ¡ cacheada.
    *   **EstÃ¡gio final (`app`):**
        *   Usa a mesma base `diamol/node` (precisa do runtime Node.js).
        *   Define como expor a porta e rodar o app (`CMD`).
        *   Copia a pasta `node_modules` (com as dependÃªncias baixadas) do estÃ¡gio `builder`.
        *   Copia o cÃ³digo-fonte (`.js`) da pasta `src` local.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª precisarÃ¡ dos arquivos `Dockerfile` (Listagem 4.3), `package.json` e do cÃ³digo-fonte Node.js (pasta `src`) no diretÃ³rio `ch04\exercises\access-log`.
*   O comando para construir serÃ¡ `docker image build -t access-log .` (prÃ³ximo passo).

---

ğŸ“„ PÃ¡gina 80

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise do Dockerfile Node.js (Listagem 4.3):
    *   Objetivo: Empacotar e rodar sÃ³ com Docker, sem ferramentas Node.js locais.
    *   Base `diamol/node` (Node.js + npm) usada em ambos os estÃ¡gios.
    *   EstÃ¡gio `builder`: Copia `package.json`, roda `npm install` para baixar dependÃªncias (otimizaÃ§Ã£o de cache). Sem compilaÃ§Ã£o.
    *   EstÃ¡gio final (`app`): ExpÃµe porta, define `CMD`, copia `node_modules` do builder, copia cÃ³digo fonte (`src/`) do host. `server.js` Ã© o ponto de entrada.
*   Mesmo padrÃ£o (multi-estÃ¡gio para dependÃªncias), mas processo diferente (sem compilaÃ§Ã£o, copia cÃ³digo fonte).
*   Processo de build/run Ã© *exatamente o mesmo* para o usuÃ¡rio (sÃ³ comandos Docker).
*   **ExercÃ­cio Try-it-now:** Navegar para a pasta do cÃ³digo Node.js e construir a imagem.
    *   Comandos: `cd ch04/exercises/access-log`, `docker image build -t access-log .`
*   SaÃ­da do build mostra logs do `npm install` (pode ter warnings/errors ignorÃ¡veis). Figura 4.7 mostra trecho da saÃ­da.
*   DependÃªncias baixadas (`node_modules`) ficam cacheadas na camada -> builds futuros rÃ¡pidos se sÃ³ o cÃ³digo mudar.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina detalha o Dockerfile Node.js, explicando cada passo e comparando com o fluxo Java. ReforÃ§a a otimizaÃ§Ã£o de cache vinda da separaÃ§Ã£o da instalaÃ§Ã£o de dependÃªncias (`npm install`) da cÃ³pia do cÃ³digo-fonte. Mesmo sem compilaÃ§Ã£o, o multi-estÃ¡gio Ã© benÃ©fico.
*   Enfatiza que, para quem usa a imagem ou a constrÃ³i, a experiÃªncia Ã© a mesma do exemplo Java: usar comandos `docker build` e `docker run`, mesmo que o conteÃºdo da imagem e os passos internos do build sejam diferentes.
*   O exercÃ­cio instrui a construir a imagem `access-log`. A etapa `npm install` pode levar algum tempo na primeira vez.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:**
    1.  No PowerShell/terminal, navegue atÃ© o diretÃ³rio:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\access-log
        ```
    2.  Execute o comando de build:
        ```powershell
        docker image build -t access-log .
        ```
    3.  Observe a saÃ­da. VocÃª verÃ¡ logs extensos durante o passo `RUN npm install`. Builds subsequentes, se vocÃª alterar apenas arquivos `.js` na pasta `src`, devem ser bem mais rÃ¡pidos nessa etapa.

---

ğŸ“„ PÃ¡gina 81

ğŸ” Resumo dos principais pontos:

*   **DescriÃ§Ã£o da aplicaÃ§Ã£o construÃ­da (`access-log`):**
    *   API REST simples para outros serviÃ§os escreverem logs.
    *   Endpoint `POST` para gravar novo log.
    *   Endpoint `GET` (/stats) para mostrar quantos logs foram gravados.
*   **ExercÃ­cio Try-it-now:** Rodar um contÃªiner da imagem `access-log`.
    *   Comando: `docker container run --name accesslog -d -p 801:80 --network nat access-log`.
    *   `--name accesslog`: Nomeia o contÃªiner.
    *   `-d`: Roda em background.
    *   `-p 801:80`: Publica porta 80 interna na 801 do host.
    *   `--network nat`: Conecta Ã  rede `nat`.
*   **Teste:** Acessar `http://localhost:801/stats` no navegador.
    *   Deve mostrar contagem de logs (inicialmente zero). Figura 4.8 mostra o resultado (Firefox formatando JSON).
*   App roda em Node.js 10.16, mas nÃ£o Ã© preciso ter Node.js instalado localmente.
*   Fluxo do Dockerfile (download dependÃªncias -> copia scripts) Ã© aplicÃ¡vel a outras linguagens interpretadas (Python/Pip, Ruby/Gems).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica o que faz a aplicaÃ§Ã£o `access-log`: Ã© uma API simples onde outros serviÃ§os podem enviar mensagens de log para serem armazenadas (embora o mecanismo de armazenamento nÃ£o seja detalhado aqui). O endpoint `/stats` permite verificar quantas mensagens foram recebidas.
*   O exercÃ­cio instrui a rodar esta API em um contÃªiner, nomeando-o `accesslog`, publicando a porta 801 e conectando-o Ã  mesma rede `nat` onde o contÃªiner `iotd` (da API Java) estÃ¡ rodando. Isso Ã© importante porque eles precisarÃ£o se comunicar mais tarde.
*   O teste acessa o endpoint `/stats` via navegador para confirmar que a API estÃ¡ rodando.
*   A pÃ¡gina conclui reforÃ§ando a vantagem de nÃ£o precisar ter o Node.js instalado localmente e que o padrÃ£o de build multi-estÃ¡gio focado em dependÃªncias Ã© Ãºtil para outras linguagens interpretadas.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:**
    ```powershell
    docker container run --name accesslog -d -p 801:80 --network nat access-log
    ```
*   Verifique se estÃ¡ rodando:
    ```powershell
    docker ps
    ```
*   Abra o navegador e acesse `http://localhost:801/stats`. VocÃª deve ver algo como `{"count":0}`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   A aplicaÃ§Ã£o `access-log` Ã© como um livro de visitas centralizado para os outros serviÃ§os (contÃªineres). Eles podem "assinar" o livro (enviar um log via POST), e vocÃª pode perguntar quantos assinaram (GET /stats).

---

ğŸ“„ PÃ¡gina 82

ğŸ” Resumo dos principais pontos:

*   **4.4 App walkthrough: Go source code** (Passo a passo da aplicaÃ§Ã£o: cÃ³digo-fonte Go)
    *   Ãšltimo exemplo multi-estÃ¡gio: aplicaÃ§Ã£o web escrita em Go.
    *   Go: linguagem moderna, multiplataforma, compila para **binÃ¡rios nativos**.
    *   BinÃ¡rio nativo: executÃ¡vel completo, *nÃ£o* precisa de runtime separado instalado (diferente de Java, .NET Core, Node.js, Python).
    *   Resulta em imagens Docker extremamente pequenas.
    *   Outras linguagens com compilaÃ§Ã£o nativa: Rust, Swift. Go tem suporte amplo e Ã© popular para apps cloud-native (Docker Ã© escrito em Go).
    *   Build multi-estÃ¡gio para Go Ã© similar ao Java, mas com diferenÃ§as importantes.
*   **Listagem 4.4:** Dockerfile para construir app Go do cÃ³digo fonte.
    ```dockerfile
    FROM diamol/golang AS builder # EstÃ¡gio builder com ferramentas Go
    COPY main.go .
    RUN go build -o /server # Compila para binÃ¡rio nativo '/server'

    # app # EstÃ¡gio final
    FROM diamol/base # Base MÃNIMA (sÃ³ OS base, sem Go runtime)
    ENV IMAGE_API_URL="http://iotd/image" \ # ConfiguraÃ§Ã£o via ENV
        ACCESS_API_URL="http://accesslog/access-log"
    CMD ["/web/server"] # Roda o binÃ¡rio compilado
    WORKDIR web
    COPY index.html . # Copia HTML do host
    COPY --from=builder /server . # Copia SÃ“ o binÃ¡rio do builder
    RUN chmod +x server # Torna o binÃ¡rio executÃ¡vel (Linux)
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Introduz o terceiro exemplo: uma aplicaÃ§Ã£o escrita em Go.
*   A caracterÃ­stica chave do Go (e Rust, Swift, etc.) Ã© que ele compila para um **binÃ¡rio nativo Ãºnico**. Isso significa que o arquivo executÃ¡vel resultante jÃ¡ contÃ©m tudo que precisa para rodar naquele sistema operacional e arquitetura especÃ­ficos, sem a necessidade de instalar um "runtime" separado (como o JRE para Java ou o Node.js para JavaScript).
*   Isso tem uma implicaÃ§Ã£o direta e muito positiva para imagens Docker: a imagem final pode ser baseada em uma imagem de sistema operacional *extremamente mÃ­nima* (como `scratch` ou uma imagem base muito pequena como `alpine` ou a `diamol/base` usada aqui), contendo apenas o binÃ¡rio compilado. Isso resulta em imagens finais muito, muito pequenas e seguras (menos software = menos superfÃ­cie de ataque).
*   **Listagem 4.4** mostra o Dockerfile Go:
    *   **EstÃ¡gio `builder`:**
        *   Usa `diamol/golang` (que tem as ferramentas de compilaÃ§Ã£o Go).
        *   Copia o cÃ³digo fonte (`main.go`).
        *   Roda `go build -o /server` para compilar o cÃ³digo e gerar o binÃ¡rio executÃ¡vel `/server`.
    *   **EstÃ¡gio final (`app`):**
        *   Usa `diamol/base` (uma imagem mÃ­nima, *sem* Go instalado).
        *   Define variÃ¡veis de ambiente para configurar os URLs das APIs Java (`iotd`) e Node.js (`accesslog`) que este app Go consumirÃ¡.
        *   Define `CMD` para executar o binÃ¡rio `/web/server`.
        *   Copia um `index.html` (interface web) do host.
        *   Copia *apenas* o binÃ¡rio `/server` do estÃ¡gio `builder`.
        *   `RUN chmod +x server`: No Linux, arquivos precisam de permissÃ£o de execuÃ§Ã£o para serem rodados como programas. Este comando concede essa permissÃ£o ao binÃ¡rio copiado. (No Windows, isso nÃ£o Ã© necessÃ¡rio da mesma forma).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª precisarÃ¡ dos arquivos `Dockerfile` (Listagem 4.4), `main.go` e `index.html` no diretÃ³rio `ch04\exercises\image-gallery`.
*   O comando para construir serÃ¡ `docker image build -t image-gallery .` (prÃ³ximo passo).
*   A instruÃ§Ã£o `RUN chmod +x server` funcionarÃ¡ sem erro mesmo ao construir no Windows (para a imagem Linux final), mas nÃ£o teria efeito se a imagem final fosse Windows.

---

ğŸ“„ PÃ¡gina 83

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Navegar para a pasta do cÃ³digo Go e construir a imagem.
    *   Comandos: `cd ch04/exercises/image-gallery`, `docker image build -t image-gallery .`
*   SaÃ­da do build Go Ã© curta (sÃ³ loga falhas). Figura 4.9 mostra trecho.
*   App Go Ã© Ãºtil; antes de rodar, verificar tamanho das imagens.
*   **ExercÃ­cio Try-it-now:** Comparar tamanho da imagem Go final (`image-gallery`) com a imagem de ferramentas Go (`diamol/golang`).
    *   Comando: `docker image ls -f reference=diamol/golang -f reference=image-gallery`.
    *   `-f reference=...`: Filtra a lista de imagens pelo nome/tag.
*   Exemplo de resultado: `image-gallery` (~25MB Linux), `diamol/golang` (~770MB Linux).
*   Tamanho reportado Ã© virtual; reuso de camadas economiza espaÃ§o real.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio instrui a construir a imagem `image-gallery` a partir do Dockerfile Go. O processo de build tende a ser rÃ¡pido e com pouca saÃ­da no console.
*   Antes de executar a aplicaÃ§Ã£o, a pÃ¡gina sugere uma verificaÃ§Ã£o importante: comparar o tamanho da imagem final da aplicaÃ§Ã£o (`image-gallery`) com o tamanho da imagem que continha as ferramentas de build (`diamol/golang`).
*   O comando `docker image ls` com mÃºltiplos filtros `-f reference=...` Ã© usado para mostrar apenas essas duas imagens.
*   A diferenÃ§a de tamanho esperada Ã© drÃ¡stica (ex: 25MB vs 770MB). Isso demonstra vividamente o benefÃ­cio do build multi-estÃ¡gio para linguagens compiladas nativamente: a imagem final Ã© minÃºscula porque nÃ£o carrega o peso das ferramentas de desenvolvimento.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  No PowerShell/terminal, navegue atÃ© o diretÃ³rio:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\image-gallery
        ```
    2.  Execute o comando de build:
        ```powershell
        docker image build -t image-gallery .
        ```
    3.  Compare os tamanhos das imagens:
        ```powershell
        docker image ls -f reference=diamol/golang -f reference=image-gallery
        ```
        Observe a grande diferenÃ§a nos valores da coluna `SIZE`. (VocÃª precisarÃ¡ ter a imagem `diamol/golang` baixada para vÃª-la; se nÃ£o tiver, o Docker pode baixÃ¡-la ou o comando mostrarÃ¡ apenas `image-gallery`).

---

ğŸ“„ PÃ¡gina 84

ğŸ” Resumo dos principais pontos:

*   BenefÃ­cio nÃ£o Ã© sÃ³ espaÃ§o em disco, mas **seguranÃ§a**: imagem final tem muito menos software -> menor superfÃ­cie de ataque.
*   App Go (`image-gallery`) usa as APIs dos outros apps (Java `iotd`, Node.js `accesslog`).
*   Verificar se os contÃªineres `iotd` e `accesslog` (dos exercÃ­cios anteriores) estÃ£o rodando com esses nomes.
    *   Comando: `docker container ls`.
*   App Go usarÃ¡ os nomes dos contÃªineres (`iotd`, `accesslog`) para encontrÃ¡-los via DNS Docker na rede `nat`.
*   **ExercÃ­cio Try-it-now:** Rodar o app Go (`image-gallery`).
    *   Comando: `docker container run -d -p 802:80 --network nat image-gallery`.
    *   `-d`: Background.
    *   `-p 802:80`: Publica porta 80 interna na 802 do host.
    *   `--network nat`: Conecta Ã  mesma rede dos outros contÃªineres.
*   **Teste:** Acessar `http://localhost:802` no navegador.
    *   Deve mostrar a "Astronomy Picture of the Day" da NASA (Figura 4.10).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a que a principal vantagem de imagens finais pequenas nÃ£o Ã© apenas economizar disco, mas sim a **seguranÃ§a**. Quanto menos software (especialmente ferramentas de desenvolvimento ou utilitÃ¡rios nÃ£o essenciais) houver na imagem que vai para produÃ§Ã£o, menor a chance de um invasor encontrar uma vulnerabilidade ou ferramenta para explorar caso consiga acesso ao contÃªiner.
*   Explica que a aplicaÃ§Ã£o Go (`image-gallery`) que acabamos de construir funciona como um frontend web que, por sua vez, consome as APIs que rodamos anteriormente: a API Java (`iotd`) para buscar a imagem da NASA e a API Node.js (`accesslog`) para registrar o acesso.
*   Ã‰ crucial que os contÃªineres `iotd` e `accesslog` estejam rodando e conectados Ã  mesma rede Docker (`nat`) para que o contÃªiner `image-gallery` possa encontrÃ¡-los usando seus nomes (`http://iotd/...` e `http://accesslog/...`), conforme configurado nas variÃ¡veis de ambiente no Dockerfile do Go.
*   O exercÃ­cio instrui a iniciar o contÃªiner `image-gallery`, conectando-o Ã  rede `nat` e publicando a porta 802.
*   O teste final Ã© acessar `http://localhost:802`, o que deve exibir a pÃ¡gina web com a imagem da NASA.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Verifique se os contÃªineres `iotd` e `accesslog` estÃ£o rodando:
        ```powershell
        docker ps
        ```
        Se nÃ£o estiverem, inicie-os novamente (comandos nas pÃ¡ginas 78 e 81).
    2.  Inicie o contÃªiner `image-gallery`:
        ```powershell
        docker container run -d -p 802:80 --network nat image-gallery
        ```
    3.  Abra o navegador e acesse `http://localhost:802`. VocÃª deverÃ¡ ver a imagem astronÃ´mica do dia.

---

ğŸ“„ PÃ¡gina 85

ğŸ” Resumo dos principais pontos:

*   Figura 4.10: Mostra a aplicaÃ§Ã£o web Go exibindo a imagem buscada da API Java.
*   RecapitulaÃ§Ã£o: AplicaÃ§Ã£o distribuÃ­da rodando em 3 contÃªineres (Go web, Java API, Node.js log API).
*   BenefÃ­cio: Conseguiu construir e rodar tudo *sem* instalar ferramentas Java, Node.js ou Go localmente, apenas com Docker e cÃ³digo fonte.
*   Dockerfiles multi-estÃ¡gio tornam o projeto *inteiramente portÃ¡til*.
*   Pipeline de build (ex: Jenkins, AppVeyor, Azure DevOps) sÃ³ precisa do Docker; a complexidade do build estÃ¡ no Dockerfile (`docker image build`).
*   **4.5 Understanding multi-stage Dockerfiles** (Entendendo Dockerfiles multi-estÃ¡gio)
    *   RevisÃ£o dos pontos chave e benefÃ­cios:
        1.  **PadronizaÃ§Ã£o:** Builds consistentes independentemente do ambiente/mÃ¡quina, usando as mesmas ferramentas definidas nas imagens base dos estÃ¡gios de build. Simplifica onboarding e manutenÃ§Ã£o de servidores de build.
        2.  **Desempenho:** Cada *estÃ¡gio* tem seu prÃ³prio cache. Se um estÃ¡gio intermediÃ¡rio nÃ£o muda, ele Ã© reutilizado do cache, acelerando o build. Estruturar Dockerfiles otimiza o uso do cache.
        3.  **Lean Images (Imagens Enxutas):** Permite isolar ferramentas (compiladores, downloaders como `curl`) em estÃ¡gios iniciais, garantindo que *nÃ£o* estejam presentes na imagem final. Reduz tamanho e superfÃ­cie de ataque. Exemplo: usar `curl` para baixar algo em um estÃ¡gio, mas nÃ£o incluir `curl` na imagem final.
*   **4.6 Lab** (LaboratÃ³rio)

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina celebra o sucesso: vocÃª acabou de rodar uma aplicaÃ§Ã£o distribuÃ­da com trÃªs tecnologias diferentes (Go, Java, Node.js) usando apenas Docker! Isso demonstra o poder do Docker para gerenciar ambientes complexos.
*   A seÃ§Ã£o 4.5 resume os **benefÃ­cios cruciais dos builds multi-estÃ¡gio**:
    1.  **ConsistÃªncia/PadronizaÃ§Ã£o:** O ambiente de build Ã© definido no Dockerfile, garantindo que o resultado seja o mesmo em qualquer mÃ¡quina (dev local, servidor de CI), eliminando problemas de "funciona na minha mÃ¡quina".
    2.  **Velocidade (Desempenho):** O cache funciona por estÃ¡gio. Se vocÃª alterar apenas o cÃ³digo fonte (copiado no final do estÃ¡gio de build), os estÃ¡gios anteriores (ex: baixar dependÃªncias) podem vir do cache, tornando o build muito mais rÃ¡pido.
    3.  **Tamanho/SeguranÃ§a (Imagens Enxutas):** Permite usar ferramentas pesadas ou temporÃ¡rias em estÃ¡gios iniciais e descartÃ¡-las, copiando apenas o essencial (o app compilado/processado) para a imagem final mÃ­nima.
*   Prepara para o Lab do CapÃ­tulo 4.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo, mas consolida o entendimento do padrÃ£o multi-estÃ¡gio que vocÃª acabou de usar para Java, Node.js e Go.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Revisitando a cozinha multi-estÃ¡gio:
    *   **PadronizaÃ§Ã£o:** A receita (Dockerfile) especifica exatamente quais facas, panelas, forno usar (imagens base dos estÃ¡gios), garantindo o mesmo resultado sempre.
    *   **Desempenho (Cache):** Se vocÃª sÃ³ mudou a decoraÃ§Ã£o final do bolo, nÃ£o precisa refazer a massa ou assar de novo; pega essas partes prontas da "geladeira" (cache).
    *   **Imagens Enxutas:** VocÃª usa a batedeira industrial (ferramentas de build) para fazer a massa, mas na hora de embalar o bolo para vender (imagem final), vocÃª leva sÃ³ o bolo, nÃ£o a batedeira.

---

ğŸ“„ PÃ¡gina 86

ğŸ” Resumo dos principais pontos:

*   **ContinuaÃ§Ã£o do Lab 4:**
    *   Objetivo: Praticar builds multi-estÃ¡gio e otimizaÃ§Ã£o de Dockerfile.
    *   Ponto de partida: Pasta `ch04/lab` com app web Go simples e um Dockerfile *nÃ£o otimizado*.
    *   **Sua tarefa:** Otimizar o Dockerfile existente para atingir metas especÃ­ficas.
*   **Metas do Lab:**
    1.  Construir imagem com Dockerfile atual, depois otimizar e construir nova imagem.
    2.  Imagem atual ~800MB (Linux) / 5.2GB (Windows). Imagem otimizada deve ser ~15MB (Linux) / 260MB (Windows).
    3.  Build atual executa 7 steps se HTML mudar.
    4.  Build otimizado deve executar *1 Ãºnico step* se HTML mudar.
*   SoluÃ§Ã£o de exemplo no GitHub. Incentiva a tentar fazer, pois otimizar Dockerfiles Ã© habilidade valiosa.
*   Dica: App do lab Ã© similar a um jÃ¡ construÃ­do no capÃ­tulo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina detalha o **Lab do CapÃ­tulo 4**. O foco Ã© aplicar os conceitos de build multi-estÃ¡gio e otimizaÃ§Ã£o de cache/tamanho aprendidos.
*   VocÃª recebe um Dockerfile "ruim" para uma aplicaÃ§Ã£o Go e precisa refatorÃ¡-lo.
*   As metas sÃ£o claras e quantificÃ¡veis:
    *   **ReduÃ§Ã£o drÃ¡stica de tamanho:** Isso serÃ¡ alcanÃ§ado usando um build multi-estÃ¡gio onde a imagem final Ã© baseada em uma imagem mÃ­nima (como `scratch` ou `alpine` ou `diamol/base`) e contÃ©m apenas o binÃ¡rio Go compilado e o arquivo HTML, sem as ferramentas de build Go.
    *   **OtimizaÃ§Ã£o mÃ¡xima do cache:** Isso serÃ¡ alcanÃ§ado ordenando as instruÃ§Ãµes corretamente no Dockerfile otimizado. A cÃ³pia do arquivo HTML (que pode mudar) deve ser um dos Ãºltimos passos, idealmente o Ãºltimo passo relevante antes do `CMD`/`ENTRYPOINT`, para que mudanÃ§as apenas no HTML invalidem o mÃ­nimo possÃ­vel do cache.
*   A dica sugere que o padrÃ£o de build multi-estÃ¡gio para Go visto na SeÃ§Ã£o 4.4 serÃ¡ diretamente aplicÃ¡vel aqui.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 4:**
    1.  Navegue para `C:\DockerLabs\diamol\ch04\lab`.
    2.  Construa a imagem com o Dockerfile existente: `docker build -t lab4-v1 .` e anote o tamanho com `docker image ls lab4-v1`.
    3.  Analise o `Dockerfile` atual. Provavelmente ele estÃ¡ usando a imagem `golang` como base final e nÃ£o Ã© multi-estÃ¡gio.
    4.  Crie um novo arquivo (ou modifique o existente, talvez salvando como `Dockerfile.optimized`) seguindo o padrÃ£o multi-estÃ¡gio do exemplo Go da SeÃ§Ã£o 4.4 (Listagem 4.4):
        *   EstÃ¡gio `builder` (usando `diamol/golang` ou `golang:alpine`): Copia o cÃ³digo Go (`main.go`), roda `go build`.
        *   EstÃ¡gio final (usando `diamol/base` ou `alpine` ou `scratch`): Copia o binÃ¡rio compilado do estÃ¡gio `builder`, copia o `index.html`, define `WORKDIR`, `EXPOSE`, `CMD`. **Crucial:** Coloque o `COPY index.html .` *depois* do `COPY --from=builder ...` para otimizar o cache para mudanÃ§as no HTML.
    5.  Construa a imagem otimizada: `docker build -t lab4-v2 -f Dockerfile.optimized .` e verifique o tamanho.
    6.  Teste o cache: Modifique `index.html`, reconstrua `lab4-v2` e veja quantos steps usam o cache (deve ser quase todos, exceto o `COPY index.html`).

---

ğŸ“„ PÃ¡gina 87

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 5: Sharing images with Docker Hub and other registries** (Compartilhando imagens com Docker Hub e outros registros)
*   RevisÃ£o: CapÃ­tulos anteriores focaram nas partes "build" e "run" do fluxo Docker.
*   Foco do Cap. 5: Parte "share" (compartilhar).
*   ImportÃ¢ncia do compartilhamento: Empacotar software + dependÃªncias facilita o uso por qualquer pessoa, em qualquer mÃ¡quina, eliminando problemas de ambiente e configuraÃ§Ã£o.
*   **5.1 Working with registries, repositories, and image tags** (Trabalhando com registros, repositÃ³rios e tags de imagem)
    *   DistribuiÃ§Ã£o de software Ã© integrada ao Docker (download automÃ¡tico de imagens).
    *   Servidor central que armazena imagens: **Registro Docker (Docker registry)**.
    *   **Docker Hub:** Registro mais popular, hospeda milhÃµes de imagens, padrÃ£o do Docker Engine.
    *   Nome completo da imagem (**referÃªncia da imagem**) contÃ©m informaÃ§Ãµes para localizaÃ§Ã£o exata.
    *   Exemplos usados atÃ© agora: `image-gallery`, `diamol/golang`.
    *   **4 partes da referÃªncia completa da imagem** (Figura 5.1, usando `docker.io/diamol/golang:latest` como exemplo):
        1.  **Registry Domain (DomÃ­nio do Registro):** Ex: `docker.io` (Docker Hub), `gcr.io` (Google), `my-registry.local:5000`. PadrÃ£o: `docker.io`.
        2.  **User/Organization (UsuÃ¡rio/OrganizaÃ§Ã£o):** Ex: `diamol`. No Docker Hub, Ã© sua conta ou uma organizaÃ§Ã£o da qual faz parte.
        3.  **Repository Name (Nome do RepositÃ³rio):** Ex: `golang`. O nome especÃ­fico da imagem dentro da conta/organizaÃ§Ã£o.
        4.  **Tag:** Ex: `latest`. Identifica uma versÃ£o especÃ­fica da imagem dentro do repositÃ³rio. PadrÃ£o: `latest`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 5, focado na etapa de **compartilhamento** de imagens Docker. O compartilhamento Ã© vital porque permite que o trabalho de empacotar uma aplicaÃ§Ã£o seja feito uma vez e, depois, qualquer pessoa possa rodÃ¡-la facilmente.
*   A seÃ§Ã£o 5.1 define a terminologia essencial para o compartilhamento:
    *   **Registro (Registry):** O servidor onde as imagens sÃ£o armazenadas (ex: Docker Hub, Google Container Registry, ou um registro privado).
    *   **ReferÃªncia da Imagem (Image Reference):** O nome completo e Ãºnico que identifica uma imagem especÃ­fica em um registro.
*   A estrutura completa de uma referÃªncia de imagem Ã© explicada usando `docker.io/diamol/golang:latest` como exemplo:
    *   `docker.io`: O endereÃ§o do registro (Docker Hub). Se omitido, o Docker assume `docker.io`.
    *   `diamol`: O nome do usuÃ¡rio ou organizaÃ§Ã£o proprietÃ¡rio da imagem no registro.
    *   `golang`: O nome do repositÃ³rio (basicamente, o nome da aplicaÃ§Ã£o/imagem).
    *   `latest`: A tag, que geralmente indica uma versÃ£o ou variante. Se omitida, o Docker assume `latest`.
*   Compreender essa estrutura Ã© fundamental para poder baixar imagens de registros especÃ­ficos e, principalmente, para nomear e enviar suas prÃ³prias imagens para um registro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Relembre os comandos anteriores:
    *   `docker pull diamol/ch02-hello-diamol`: Aqui, `docker.io` Ã© implÃ­cito, `diamol` Ã© a organizaÃ§Ã£o, `ch02-hello-diamol` Ã© o repositÃ³rio, e `latest` Ã© a tag implÃ­cita.
    *   `docker build -t web-ping .`: Aqui, `web-ping` Ã© apenas um nome local (repositÃ³rio), sem registro ou usuÃ¡rio/organizaÃ§Ã£o. Para compartilhar, precisaremos adicionar essa informaÃ§Ã£o (prÃ³ximos passos).

---

ğŸ“„ PÃ¡gina 88

ğŸ” Resumo dos principais pontos:

*   NecessÃ¡rio usar todas as partes da referÃªncia para gerenciar suas prÃ³prias imagens em registros.
*   Nomes locais sÃ£o flexÃ­veis, mas para compartilhar, precisam de mais detalhes (identificador Ãºnico no registro).
*   **Valores PadrÃ£o:**
    *   Registro padrÃ£o: Docker Hub (`docker.io`).
    *   Tag padrÃ£o: `latest`.
*   Exemplo: `diamol/golang` Ã© versÃ£o curta de `docker.io/diamol/golang:latest`.
*   `diamol`: organizaÃ§Ã£o no Docker Hub. `golang`: repositÃ³rio pÃºblico. SÃ³ membros podem enviar (push).
*   **Registros Privados/PrÃ³prios:**
    *   Grandes empresas geralmente tÃªm seus prÃ³prios registros (nuvem ou datacenter).
    *   Para usar um registro prÃ³prio, inclua o domÃ­nio no inÃ­cio da referÃªncia (ex: `r.sixeyed.com/diamol/golang`).
*   **ImportÃ¢ncia da Tag:**
    *   Identifica diferentes *versÃµes* da mesma aplicaÃ§Ã£o.
    *   Deve-se *sempre* usar tags explÃ­citas ao construir/compartilhar suas imagens.
    *   Exemplo: Imagem oficial `openjdk` tem centenas de tags para diferentes versÃµes (openjdk:13, openjdk:8u212-jdk, etc.) e variantes (Linux/Windows).
    *   Tag `latest` Ã© enganosa; pode *nÃ£o* ser a versÃ£o mais recente. Evite depender dela para suas imagens ou bases.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a a estrutura da referÃªncia da imagem e explica como ela Ã© usada na prÃ¡tica.
*   Destaca a existÃªncia de **registros privados**, comuns em ambientes corporativos para controle e seguranÃ§a, e como acessÃ¡-los prefixando o nome da imagem com o domÃ­nio do registro privado.
*   Enfatiza a importÃ¢ncia crucial das **tags**. Elas sÃ£o usadas para versionamento. Em vez de ter imagens chamadas `meuapp-v1`, `meuapp-v2`, usa-se o mesmo nome de repositÃ³rio (`meuapp`) com tags diferentes (`:v1`, `:v2`).
*   **Alerta:** A tag `:latest` Ã© apenas um ponteiro padrÃ£o, frequentemente apontando para a Ãºltima imagem enviada *sem* uma tag especÃ­fica. NÃ£o hÃ¡ garantia de que ela seja realmente a versÃ£o mais recente ou estÃ¡vel. Ã‰ uma **mÃ¡ prÃ¡tica** depender de `:latest` para imagens de produÃ§Ã£o ou como base (`FROM`) em seus Dockerfiles. Sempre use tags explÃ­citas e significativas (como nÃºmeros de versÃ£o semÃ¢ntica: `:1.0.0`, `:2.1.5`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Explore tags no Docker Hub: VÃ¡ para `https://hub.docker.com/_/openjdk` e clique na aba "Tags". Veja a enorme variedade de tags disponÃ­veis para diferentes versÃµes, JRE/JDK, e sistemas operacionais.
*   Ao construir suas imagens (CapÃ­tulos anteriores), vocÃª usou `-t web-ping`, que implicitamente criou a tag `:latest`. No futuro, vocÃª usarÃ¡ tags explÃ­citas como `-t meuapp:1.0`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   ReferÃªncia da Imagem = EndereÃ§o completo de um livro numa biblioteca gigante.
    *   Registro = Nome da Biblioteca (Docker Hub, Biblioteca Municipal).
    *   UsuÃ¡rio/Org = SeÃ§Ã£o da Biblioteca (FicÃ§Ã£o, NÃ£o-FicÃ§Ã£o, SeÃ§Ã£o Infantil 'diamol').
    *   RepositÃ³rio = TÃ­tulo do Livro ('golang').
    *   Tag = EdiÃ§Ã£o/VersÃ£o do Livro (1Âª EdiÃ§Ã£o, EdiÃ§Ã£o Revisada ':v2', EdiÃ§Ã£o de Bolso ':slim').
*   `:latest` Ã© como pedir "o Ãºltimo livro que chegou nessa seÃ§Ã£o" - pode nÃ£o ser o melhor ou o que vocÃª esperava.

---

ğŸ“„ PÃ¡gina 89

ğŸ” Resumo dos principais pontos:

*   **5.2 Pushing your own images to Docker Hub** (Enviando suas prÃ³prias imagens para o Docker Hub)
*   NecessÃ¡rio: Conta no Docker Hub (gratuita, criar em https://hub.docker.com).
*   **Passos para enviar (push) uma imagem:**
    1.  **Login:** Autenticar-se no registro via linha de comando Docker (`docker login`).
    2.  **Tag:** Dar Ã  imagem local uma referÃªncia que inclua seu nome de usuÃ¡rio/organizaÃ§Ã£o do registro.
*   **ExercÃ­cio Try-it-now:** Capturar seu Docker ID (username) em uma variÃ¡vel de ambiente.
    *   Comando Windows (PowerShell): `$dockerId="<seu-docker-id>"`.
    *   Comando Linux/Mac (Bash): `export dockerId="<seu-docker-id>"`.
    *   Usar username, nÃ£o email. Exemplo do autor: `$dockerId="sixeyed"`.
*   Verificar variÃ¡vel: `echo $dockerId` (funciona em ambos).
*   **ExercÃ­cio Try-it-now:** Fazer login no Docker Hub.
    *   Comando: `docker login --username $dockerId`.
    *   PedirÃ¡ a senha (do Docker Hub).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o inicia o processo prÃ¡tico de compartilhar uma imagem que vocÃª construiu localmente, enviando-a para o Docker Hub.
*   Os dois prÃ©-requisitos sÃ£o: ter uma conta no Docker Hub e estar logado nela atravÃ©s da ferramenta de linha de comando do Docker.
*   O primeiro exercÃ­cio facilita os comandos subsequentes, pedindo para armazenar seu nome de usuÃ¡rio do Docker Hub (Docker ID) em uma variÃ¡vel de ambiente (`$dockerId` no PowerShell, `dockerId` no Bash). Isso permite copiar e colar os prÃ³ximos comandos do livro, que usarÃ£o essa variÃ¡vel.
*   O segundo exercÃ­cio executa o `docker login`. Este comando interage com o Docker Hub (ou outro registro, se especificado) para autenticar suas credenciais (usuÃ¡rio e senha). Uma vez logado com sucesso, o Docker Engine estarÃ¡ autorizado a enviar imagens para os repositÃ³rios associados Ã  sua conta.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Se ainda nÃ£o tem, crie uma conta em `https://hub.docker.com`. Lembre-se do seu **username** (Docker ID).
    2.  Abra o PowerShell. Defina a variÃ¡vel de ambiente (substitua `<seu-docker-id>`):
        ```powershell
        $dockerId="<seu-docker-id>"
        ```
    3.  Verifique se a variÃ¡vel foi definida:
        ```powershell
        echo $dockerId
        # DeverÃ¡ imprimir seu Docker ID
        ```
    4.  FaÃ§a o login:
        ```powershell
        docker login --username $dockerId
        # Ou docker login -u $dockerId
        ```
        Digite sua senha do Docker Hub quando solicitado (ela nÃ£o aparecerÃ¡ na tela). VocÃª deve receber uma mensagem "Login Succeeded".

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `docker login`: Ã‰ como fazer login no site de uma transportadora antes de poder enviar um pacote (imagem).
*   VariÃ¡vel `$dockerId`: Ã‰ como escrever seu nome de remetente em um adesivo para usar em todos os pacotes que vocÃª vai enviar.

---

ğŸ“„ PÃ¡gina 90

ğŸ” Resumo dos principais pontos:

*   Figura 5.2: Mostra a saÃ­da do `docker login` (senha nÃ£o Ã© exibida).
*   ApÃ³s login, pode-se fazer push para sua conta ou organizaÃ§Ãµes onde tem permissÃ£o.
*   Sem ser membro de organizaÃ§Ã£o, sÃ³ pode fazer push para repositÃ³rios na sua conta (ex: `<seu-docker-id>/<nome-repo>`).
*   Imagem `image-gallery` construÃ­da no Cap. 4 tem nome local, nÃ£o pode ser enviada diretamente.
*   **Retagging:** NÃ£o Ã© preciso reconstruir a imagem para dar nova referÃªncia. Imagens podem ter mÃºltiplas referÃªncias (tags) apontando para o mesmo conteÃºdo (mesmo ID).
*   **ExercÃ­cio Try-it-now:** Criar uma nova tag para a imagem `image-gallery` existente, incluindo seu Docker ID e uma versÃ£o `v1`.
    *   Comando: `docker image tag image-gallery $dockerId/image-gallery:v1`.
*   Agora a imagem tem duas referÃªncias (local e para o Hub) apontando para o mesmo ID.
*   **ExercÃ­cio Try-it-now:** Listar as referÃªncias da imagem `image-gallery`.
    *   Comando: `docker image ls --filter reference=image-gallery --filter reference='*/image-gallery'`.
    *   Usa dois filtros: um para o nome local, outro para qualquer nome que termine com `/image-gallery`.
*   SaÃ­da esperada (Figura 5.3): Duas linhas com o *mesmo* IMAGE ID, mas REPOSITORY/TAG diferentes (uma local `image-gallery:latest`, outra com seu Docker ID e tag `v1`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Uma vez logado, vocÃª estÃ¡ pronto para preparar a imagem para o envio. O problema Ã© que a imagem `image-gallery` que vocÃª construiu tem apenas um nome local. Para enviÃ¡-la ao Docker Hub, ela precisa ser nomeada de acordo com o padrÃ£o `<seu-docker-id>/<nome-repositorio>:<tag>`.
*   A boa notÃ­cia Ã© que vocÃª nÃ£o precisa reconstruir a imagem. O comando `docker image tag` cria um **novo nome (referÃªncia)** que aponta para uma imagem *existente*. Ã‰ como criar um atalho ou um apelido para a mesma imagem.
*   O primeiro exercÃ­cio cria a tag `alexandre/image-gallery:v1` (substituindo `alexandre` pelo seu `$dockerId`), que agora aponta para o mesmo conteÃºdo da imagem que antes era sÃ³ `image-gallery:latest`.
*   O segundo exercÃ­cio usa `docker image ls` com filtros para mostrar que agora existem duas referÃªncias (`image-gallery:latest` e `<seu-docker-id>/image-gallery:v1`) mas ambas compartilham o mesmo `IMAGE ID`, confirmando que sÃ£o apenas nomes diferentes para a mesma imagem.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Certifique-se de ter a imagem `image-gallery:latest` construÃ­da (do Cap. 4, p. 83).
    2.  Crie a nova tag (o PowerShell substituirÃ¡ `$dockerId`):
        ```powershell
        docker image tag image-gallery $dockerId/image-gallery:v1
        ```
    3.  Liste as imagens para ver ambas as referÃªncias:
        ```powershell
        docker image ls --filter reference=image-gallery --filter reference="*/image-gallery"
        # Ou liste todas e procure por 'image-gallery':
        # docker image ls | Select-String "image-gallery"
        ```
        Confirme que as duas linhas tÃªm o mesmo IMAGE ID.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `docker image tag`: Ã‰ como colocar uma nova etiqueta de endereÃ§o de destinatÃ¡rio (`$dockerId/image-gallery:v1`) em um pacote (`image-gallery:latest`) que vocÃª jÃ¡ tinha pronto, sem precisar refazer o pacote. O conteÃºdo Ã© o mesmo, sÃ³ o rÃ³tulo mudou/foi adicionado.

---

ğŸ“„ PÃ¡gina 91

ğŸ” Resumo dos principais pontos:

*   Agora com referÃªncia correta (`<user>/<repo>:<tag>`) e logado, a imagem estÃ¡ pronta para ser compartilhada.
*   Comando `docker image push`: Envia camadas da imagem local para o registro.
*   **ExercÃ­cio Try-it-now:** Enviar a imagem tagueada para o Docker Hub.
    *   Comando: `docker image push $dockerId/image-gallery:v1`.
*   AnÃ¡lise da saÃ­da do `push`:
    *   Registros trabalham com camadas, assim como o Docker Engine local.
    *   `push` faz upload das camadas. SaÃ­da mostra progresso por ID de camada.
    *   Exemplo: `c8c60e5dbe37: Pushed`.
    *   Se uma camada jÃ¡ existe no registro (enviada antes por vocÃª ou outra pessoa), ela *nÃ£o* Ã© enviada novamente (mostra "Layer already exists").
*   OtimizaÃ§Ã£o de Dockerfiles (Cap. 3) Ã© importante aqui: se 90% das camadas vÃªm do cache no build, 90% provavelmente jÃ¡ existirÃ£o no registro no push. Reduz tempo de build, espaÃ§o em disco *e* banda de rede.
*   **VerificaÃ§Ã£o no Docker Hub:**
    *   Pode-se navegar atÃ© a pÃ¡gina da imagem no Docker Hub para verificar.
    *   URL Ã© baseado no nome de usuÃ¡rio e repositÃ³rio.
*   **ExercÃ­cio Try-it-now:** Script simples para gerar o URL da sua imagem no Docker Hub.
    *   Comando: `echo "https://hub.docker.com/r/$dockerId/image-gallery/tags"`.
*   Resultado esperado no Hub (Figura 5.4): PÃ¡gina mostrando a tag `v1`, quando foi atualizada, etc.
*   RepositÃ³rio Ã© criado automaticamente no Hub se nÃ£o existir; por padrÃ£o, Ã© pÃºblico (qualquer um pode baixar).
*   Pode-se adicionar documentaÃ§Ã£o no Docker Hub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com a imagem corretamente nomeada (`<seu-docker-id>/image-gallery:v1`) e vocÃª logado, o comando `docker image push` envia sua imagem para o Docker Hub.
*   Assim como no `pull`, o `push` opera no nÃ­vel das **camadas**. O Docker verifica quais camadas da sua imagem jÃ¡ existem no registro de destino. Apenas as camadas que o registro *nÃ£o* possui sÃ£o efetivamente transferidas pela rede. Isso torna o `push` (especialmente de atualizaÃ§Ãµes) muito eficiente se as imagens compartilharem muitas camadas base ou se apenas pequenas partes foram alteradas.
*   Isso reforÃ§a a importÃ¢ncia de otimizar os Dockerfiles (como visto no Cap. 3 e que serÃ¡ aprofundado no Cap. 17): imagens bem otimizadas com bom reuso de cache nÃ£o sÃ³ constroem mais rÃ¡pido, mas tambÃ©m sÃ£o enviadas (`push`) e baixadas (`pull`) mais rapidamente.
*   ApÃ³s o `push`, a imagem fica disponÃ­vel publicamente no Docker Hub. O exercÃ­cio mostra como construir o URL para visualizar sua imagem recÃ©m-enviada no site do Hub.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Execute o comando push (PowerShell substituirÃ¡ `$dockerId`):
        ```powershell
        docker image push $dockerId/image-gallery:v1
        ```
        Observe a saÃ­da mostrando o envio das camadas. Se vocÃª jÃ¡ tiver enviado imagens baseadas nas mesmas camadas antes, verÃ¡ muitas mensagens "Layer already exists".
    2.  Execute o comando para obter o URL:
        ```powershell
        echo "https://hub.docker.com/r/$dockerId/image-gallery/tags"
        ```
    3.  Copie o URL gerado e cole no seu navegador. VocÃª deverÃ¡ ver sua imagem `image-gallery` com a tag `v1` listada na sua conta do Docker Hub.

---

ğŸ“„ PÃ¡gina 92

ğŸ” Resumo dos principais pontos:

*   Figura 5.4: Mostra a listagem da imagem no Docker Hub.
*   Docker Hub Ã© o registro mais fÃ¡cil, gratuito, com funcionalidades extras pagas (repositÃ³rios privados).
*   Existem **registros alternativos**:
    *   API do registro Ã© especificaÃ§Ã£o aberta.
    *   Servidor de registro core Ã© open source (`docker/distribution`).
    *   Provedores de nuvem tÃªm seus prÃ³prios serviÃ§os de registro (ex: ACR no Azure, ECR na AWS, GCR no Google).
    *   Pode-se rodar registro prÃ³prio no datacenter (comercial: Docker Trusted Registry; simples: rodando o registro open source em um contÃªiner).
*   **5.3 Running and using your own Docker registry** (Rodando e usando seu prÃ³prio registro Docker)
    *   Utilidade de rodar um registro na rede local:
        *   Reduz uso de banda e tempo de transferÃªncia.
        *   Permite manter controle dos dados/imagens internamente.
        *   OpÃ§Ã£o de backup se registro principal estiver offline.
    *   Registro open source da Docker (`docker/distribution`):
        *   Funcionalidade bÃ¡sica de push/pull.
        *   Usa mesmo sistema de cache de camadas.
        *   NÃ£o tem interface web como o Hub.
        *   Leve, pode rodar em um contÃªiner.
    *   Imagem prÃ©-empacotada pelo autor: `diamol/registry`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a parte do Docker Hub e introduz a ideia de usar **outros registros Docker**. AlÃ©m do Hub, existem muitas opÃ§Ãµes: serviÃ§os gerenciados nas nuvens pÃºblicas (Azure Container Registry, AWS Elastic Container Registry, Google Container Registry) e a possibilidade de vocÃª hospedar seu prÃ³prio registro, seja usando soluÃ§Ãµes comerciais robustas (como Docker Trusted Registry, agora parte do Mirantis Kubernetes Engine) ou simplesmente rodando o servidor de registro open source bÃ¡sico em um contÃªiner.
*   A seÃ§Ã£o 5.3 foca nesta Ãºltima opÃ§Ã£o: rodar seu prÃ³prio registro localmente, usando uma imagem (`diamol/registry`) que empacota o servidor de registro open source. Isso Ã© Ãºtil para desenvolvimento, testes locais rÃ¡pidos, ou para ter um cache/backup local de imagens.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando para executar ainda, mas prepara para rodar o contÃªiner do registro local na prÃ³xima pÃ¡gina.
*   Se vocÃª usa Azure, AWS ou GCP, pode explorar os serviÃ§os de registro de contÃªiner deles como alternativas ao Docker Hub para seus projetos.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Docker Hub Ã© a biblioteca pÃºblica gigante. Registros da nuvem sÃ£o bibliotecas grandes gerenciadas por empresas (Amazon, Google, Microsoft). Rodar seu prÃ³prio registro Ã© como ter sua prÃ³pria estante de livros em casa ou no escritÃ³rio.

---

ğŸ“„ PÃ¡gina 93

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar o registro Docker em um contÃªiner.
    *   Comando: `docker container run -d -p 5000:5000 --restart always --name registry diamol/registry`.
    *   `-p 5000:5000`: Publica a porta padrÃ£o do registro (5000).
    *   `--restart always`: Garante que o contÃªiner do registro reinicie automaticamente se o Docker for reiniciado.
    *   `--name registry`: Nomeia o contÃªiner.
*   Registro rodando em `localhost:5000`. Ãštil apenas na mÃ¡quina local.
*   Para uso na rede: melhor dar um nome de domÃ­nio alias Ã  mÃ¡quina.
*   **Criando alias `registry.local`:**
    *   Edita o arquivo `hosts` do computador para mapear `registry.local` para `127.0.0.1` (localhost).
    *   Arquivo `hosts` existe em Windows, Linux e Mac, mas em locais diferentes.
*   **ExercÃ­cio Try-it-now:** Adicionar a entrada no arquivo `hosts`.
    *   Comando Windows (PowerShell como Admin): `Add-Content -Value "127.0.0.1 registry.local" -Path C:\Windows\System32\drivers\etc\hosts`.
    *   Comando Linux/Mac (com `sudo`): `echo $'\n127.0.0.1 registry.local' | sudo tee -a /etc/hosts`.
    *   Pode dar erro de permissÃ£o; requer privilÃ©gios de administrador.
*   **Teste:** Rodar `ping registry.local`. Deve responder de `127.0.0.1` (Figura 5.5).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O primeiro exercÃ­cio inicia o contÃªiner do servidor de registro Docker bÃ¡sico (`diamol/registry`). Ele fica ouvindo na porta 5000. A opÃ§Ã£o `--restart always` Ã© Ãºtil para serviÃ§os de infraestrutura como este, garantindo que ele volte a funcionar apÃ³s reinicializaÃ§Ãµes do Docker ou da mÃ¡quina.
*   O problema: Acessar o registro como `localhost:5000` sÃ³ funciona na prÃ³pria mÃ¡quina. Para que outras mÃ¡quinas na rede (ou atÃ© mesmo outros contÃªineres na mesma mÃ¡quina de forma mais elegante) possam usÃ¡-lo, e para que os nomes das imagens fiquem mais limpos (ex: `registry.local:5000/meuapp` em vez de `<ip_da_maquina>:5000/meuapp`), Ã© uma boa prÃ¡tica criar um nome de domÃ­nio local.
*   O arquivo `hosts` do sistema operacional permite mapear nomes de domÃ­nio para endereÃ§os IP localmente, antes de consultar um servidor DNS externo. O segundo exercÃ­cio adiciona uma linha a esse arquivo para que o nome `registry.local` seja resolvido para o endereÃ§o IP de loopback (`127.0.0.1`), ou seja, para a prÃ³pria mÃ¡quina. **Importante:** Editar o arquivo `hosts` requer permissÃµes de administrador.
*   O teste com `ping registry.local` confirma que o sistema operacional agora reconhece esse nome e o direciona para `127.0.0.1`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  Inicie o contÃªiner do registro:
        ```powershell
        docker container run -d -p 5000:5000 --restart always --name registry diamol/registry
        ```
    2.  **Abra o PowerShell COMO ADMINISTRADOR.** (Clique com botÃ£o direito no Ã­cone do PowerShell -> Executar como administrador).
    3.  Execute o comando para editar o arquivo hosts:
        ```powershell
        Add-Content -Value "127.0.0.1 registry.local" -Path C:\Windows\System32\drivers\etc\hosts
        ```
        *(Se der erro de acesso mesmo como admin, pode ser necessÃ¡rio ajustar permissÃµes do arquivo ou usar um editor de texto como admin para adicionar a linha manualmente)*.
    4.  Teste o alias (em qualquer PowerShell, nÃ£o precisa ser admin):
        ```powershell
        ping registry.local
        ```
        VocÃª deve ver respostas de `127.0.0.1`.

---

ğŸ“„ PÃ¡gina 94

ğŸ” Resumo dos principais pontos:

*   Agora pode-se usar `registry.local:5000` nas referÃªncias de imagem para usar o registro local.
*   Processo de taguear Ã© o mesmo (`docker image tag`).
*   **ExercÃ­cio Try-it-now:** Taguear a imagem `image-gallery` com o domÃ­nio do registro local.
    *   Comando: `docker image tag image-gallery registry.local:5000/gallery/ui:v

**You**

ğŸ§  Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), pÃ¡gina por pÃ¡gina, a partir da pÃ¡gina 93.

Para cada pÃ¡gina, siga este modelo de resposta:

ğŸ“„ PÃ¡gina X

ğŸ” Resumo dos principais pontos:

\[bullet points com os tÃ³picos abordados na pÃ¡gina\]

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

\[explique os conceitos em portuguÃªs claro, com boa didÃ¡tica, como um professor\]

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

\[comandos ou trechos de cÃ³digo aplicÃ¡veis no Windows, usando Docker Desktop e Visual Studio Code\]

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

\[comparaÃ§Ãµes simples ou observaÃ§Ãµes para reforÃ§ar o entendimento\]

âœ… Importante: percorra automaticamente todas as pÃ¡ginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "prÃ³xima".

Quando terminar todas as pÃ¡ginas, diga:

"âœ… Fim da leitura. Todas as pÃ¡ginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Okay, Alexandre, continuando a partir da pÃ¡gina 93.

---

ğŸ“„ PÃ¡gina 93

ğŸ” Resumo dos principais pontos:

*   Agora pode-se usar `registry.local:5000` nas referÃªncias de imagem para usar o registro local.
*   Processo de taguear Ã© o mesmo (`docker image tag`).
*   **ExercÃ­cio Try-it-now:** Taguear a imagem `image-gallery` com o domÃ­nio do registro local e um nome/tag estruturado (`gallery/ui:v1`).
    *   Comando: `docker image tag image-gallery registry.local:5000/gallery/ui:v1`.
*   Explica que o registro local nÃ£o tem autenticaÃ§Ã£o/autorizaÃ§Ã£o (nÃ£o pronto para produÃ§Ã£o, mas OK para times pequenos).
*   Sugere um esquema de nomenclatura para agrupar imagens relacionadas (ex: `gallery/ui`, `gallery/api`, `gallery/logs`).
*   Necessidade de configurar o Docker Engine para confiar em registros "inseguros" (HTTP vs HTTPS padrÃ£o).
    *   O registro rodando no contÃªiner usa HTTP simples. Docker nÃ£o confia por padrÃ£o.
*   ConfiguraÃ§Ã£o via arquivo `daemon.json` (C:\ProgramData\docker\config no Windows Server, /etc/docker no Linux) ou via UI do Docker Desktop (Daemon tab - *Nota: em versÃµes mais recentes pode ser na seÃ§Ã£o "Docker Engine"*).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com o alias `registry.local` funcionando, agora podemos referenciar nosso registro Docker local usando o nome `registry.local:5000`. Para enviar uma imagem para lÃ¡, precisamos primeiro "etiquetÃ¡-la" (tag) com esse endereÃ§o, seguido pelo nome que queremos dar Ã  imagem dentro do registro.
*   O comando `docker image tag <imagem_existente> <nova_referencia>` faz isso. A `<nova_referencia>` deve seguir o formato `<dominio_registro>/<usuario_ou_projeto>/<nome_imagem>:<tag>`. No exemplo, usamos `registry.local:5000/gallery/ui:v1`.
*   Ã‰ importante notar que este registro bÃ¡sico que subimos nÃ£o tem seguranÃ§a (autenticaÃ§Ã£o). Qualquer um na rede poderia acessÃ¡-lo. Isso Ã© aceitÃ¡vel para testes locais, mas nÃ£o para produÃ§Ã£o.
*   Outro ponto crucial: por padrÃ£o, o Docker se comunica com registros usando HTTPS para seguranÃ§a. Como nosso registro local estÃ¡ rodando sobre HTTP simples (porta 5000), o Docker Engine irÃ¡ recusar a conexÃ£o por considerÃ¡-la insegura. Precisamos configurar explicitamente o Docker para *permitir* a comunicaÃ§Ã£o com este registro especÃ­fico via HTTP. Isso Ã© feito adicionando `registry.local:5000` Ã  lista de "insecure-registries" na configuraÃ§Ã£o do Docker Engine.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Taguear a imagem para o registro local:** (Certifique-se de ter a imagem `image-gallery:latest` do Cap. 4)
    ```powershell
    docker image tag image-gallery registry.local:5000/gallery/ui:v1
    ```
2.  **Configurar Registro Inseguro no Docker Desktop:**
    *   Clique com o botÃ£o direito no Ã­cone da baleia Docker na Ã¡rea de notificaÃ§Ã£o e selecione "Settings".
    *   VÃ¡ para a seÃ§Ã£o "Docker Engine".
    *   No editor JSON que aparece, adicione a chave `insecure-registries`. Se jÃ¡ houver outras chaves, adicione uma vÃ­rgula no final da linha anterior. O JSON deve ficar parecido com isto (mantenha outras configuraÃ§Ãµes que jÃ¡ existirem):
      ```json
      {
        "builder": {
          "gc": {
            "defaultKeepStorage": "20GB",
            "enabled": true
          }
        },
        "experimental": false,
        "insecure-registries": [
          "registry.local:5000"
        ]
      }
      ```
    *   Clique em "Apply & Restart". O Docker Desktop reiniciarÃ¡ o Engine.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Configurar "insecure-registries" Ã© como ajustar as configuraÃ§Ãµes de seguranÃ§a do seu navegador para permitir o acesso a um site especÃ­fico que usa HTTP antigo, mesmo sabendo que HTTPS Ã© mais seguro. VocÃª sÃ³ faz isso para sites que confia (neste caso, seu prÃ³prio registro local).

---

ğŸ“„ PÃ¡gina 94

ğŸ” Resumo dos principais pontos:

*   Figura 5.6: Mostra a interface de configuraÃ§Ã£o do Docker Desktop para adicionar "insecure registries".
*   ReinÃ­cio do Docker Engine Ã© necessÃ¡rio apÃ³s alterar `daemon.json` (manualmente em servidores, automÃ¡tico no Docker Desktop).
    *   Comandos de reinÃ­cio: `Restart-Service docker` (Windows Server), `service docker restart` (Linux).
*   VerificaÃ§Ã£o da configuraÃ§Ã£o: Comando `docker info`.
*   **ExercÃ­cio Try-it-now:** Rodar `docker info` e verificar a lista de "Insecure Registries".
*   Aviso sobre riscos de seguranÃ§a de registros inseguros (conexÃ£o pode ser comprometida, dados lidos ou injetados).
*   Registros comerciais e o registro open source podem ser configurados para usar HTTPS.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina foca na conclusÃ£o da configuraÃ§Ã£o para permitir registros inseguros (HTTP). Mostra a interface grÃ¡fica do Docker Desktop (Figura 5.6) como uma forma fÃ¡cil de fazer isso.
*   Explica que, se a configuraÃ§Ã£o fosse feita editando o arquivo `daemon.json` diretamente (mais comum em servidores), seria necessÃ¡rio reiniciar o serviÃ§o Docker manualmente. O Docker Desktop simplifica isso reiniciando automaticamente apÃ³s a aplicaÃ§Ã£o das mudanÃ§as na interface.
*   O comando `docker info` Ã© apresentado como forma de verificar se a configuraÃ§Ã£o foi aplicada corretamente. Ele exibe uma vasta quantidade de informaÃ§Ãµes sobre o Docker Engine, incluindo a lista de registros inseguros permitidos.
*   Reitera fortemente o **risco de seguranÃ§a** ao usar HTTP para registros. A comunicaÃ§Ã£o nÃ£o Ã© criptografada, tornando-a vulnerÃ¡vel a espionagem (alguÃ©m ver suas imagens) ou adulteraÃ§Ã£o (alguÃ©m injetar cÃ³digo malicioso em imagens que vocÃª baixa). Para ambientes reais, especialmente produÃ§Ã£o, deve-se SEMPRE usar HTTPS.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar ConfiguraÃ§Ã£o:** ApÃ³s o Docker Desktop reiniciar (depois de aplicar a configuraÃ§Ã£o na pÃ¡gina anterior), execute no PowerShell:
    ```powershell
    docker info
    ```
2.  Procure pela seÃ§Ã£o "Insecure Registries" na saÃ­da. VocÃª deve ver `registry.local:5000` listado lÃ¡ (como na Figura 5.7 da prÃ³xima pÃ¡gina).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `docker info`: Ã‰ como pedir um relatÃ³rio completo do estado atual da "fÃ¡brica" Docker, incluindo suas configuraÃ§Ãµes de seguranÃ§a.

---

ğŸ“„ PÃ¡gina 95

ğŸ” Resumo dos principais pontos:

*   Figura 5.7: Mostra a saÃ­da do `docker info` destacando a seÃ§Ã£o "Insecure Registries" com `registry.local:5000`.
*   Confirma que o risco de HTTP Ã© aceitÃ¡vel para demonstraÃ§Ã£o local.
*   Agora estÃ¡ tudo pronto para enviar a imagem para o registro local.
*   **ExercÃ­cio Try-it-now:** Enviar (push) a imagem tagueada para o registro local.
    *   Comando: `docker image push registry.local:5000/gallery/ui:v1`.
*   AnÃ¡lise da saÃ­da do `push`:
    *   Se for o primeiro push, todas as camadas serÃ£o enviadas ("Pushed").
    *   Se repetir o comando, mostrarÃ¡ "Layer already exists" para todas as camadas.
*   ConclusÃ£o sobre rodar registro prÃ³prio: Pode-se compartilhar na rede usando IP ou nome real da mÃ¡quina.
*   **5.4 Using image tags effectively** (Usando tags de imagem eficazmente)
    *   Tags podem ser qualquer string; mÃºltiplas tags podem apontar para a mesma imagem.
    *   Uso principal: Versionamento de software.
    *   Esquema comum: Versionamento SemÃ¢ntico (`[major].[minor].[patch]`).
        *   Patch: CorreÃ§Ãµes de bugs, sem quebra de compatibilidade.
        *   Minor: Novas funcionalidades, sem quebra de compatibilidade.
        *   Major: MudanÃ§as que podem quebrar compatibilidade.
    *   Permite aos usuÃ¡rios escolherem nÃ­vel de atualizaÃ§Ã£o/risco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com a configuraÃ§Ã£o de registro inseguro confirmada pelo `docker info`, finalmente podemos enviar nossa imagem para o registro local.
*   O comando `docker image push registry.local:5000/gallery/ui:v1` faz exatamente isso. Como o nome da imagem comeÃ§a com `registry.local:5000`, o Docker sabe que deve enviÃ¡-la para o registro nesse endereÃ§o, usando o protocolo HTTP (porque configuramos como inseguro). A saÃ­da mostrarÃ¡ o progresso do envio das camadas. Se vocÃª rodar o comando novamente, ele serÃ¡ instantÃ¢neo, pois o registro jÃ¡ terÃ¡ todas as camadas.
*   A seÃ§Ã£o 5.4 inicia uma discussÃ£o importante sobre como usar **tags** de forma eficaz, principalmente para **versionamento**. A ideia Ã© usar tags para comunicar o significado das diferentes versÃµes de uma imagem. O **Versionamento SemÃ¢ntico (SemVer)** Ã© um padrÃ£o amplamente adotado:
    *   `MAJOR.MINOR.PATCH` (ex: `2.1.106`)
    *   Incrementar `PATCH`: CorreÃ§Ã£o de bug, totalmente compatÃ­vel.
    *   Incrementar `MINOR`: AdiÃ§Ã£o de funcionalidade, ainda compatÃ­vel com versÃµes anteriores.
    *   Incrementar `MAJOR`: MudanÃ§a incompatÃ­vel com versÃµes anteriores.
*   Usar esse padrÃ£o nas tags ajuda os consumidores da sua imagem a decidirem o quÃ£o atualizados (ou estÃ¡veis) eles querem estar.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Enviar para Registro Local:**
    ```powershell
    docker image push registry.local:5000/gallery/ui:v1
    ```
    Observe a saÃ­da.
2.  **(Opcional) Enviar Novamente:**
    ```powershell
    docker image push registry.local:5000/gallery/ui:v1
    ```
    Observe as mensagens "Layer already exists".
3.  **Exemplos de Tags SemVer:**
    *   `my-app:1.0.0`
    *   `my-app:1.0.1` (apenas patch)
    *   `my-app:1.1.0` (nova funcionalidade menor)
    *   `my-app:2.0.0` (grande mudanÃ§a, pode quebrar compatibilidade)

---

ğŸ“„ PÃ¡gina 96

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Criar mÃºltiplas tags (aliases) para a mesma imagem `image-gallery`, apontando para o registro local, simulando tags de versÃ£o major, minor, patch e latest.
    *   Comandos:
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:latest`
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:2`
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:2.1`
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:2.1.106`
*   Diagrama (Figura 5.8): Ilustra a evoluÃ§Ã£o das tags ao longo de vÃ¡rios releases (Julho a Outubro).
    *   Mostra como tags mais genÃ©ricas (`:2`, `:2.1`, `:latest`) mudam para apontar para diferentes versÃµes especÃ­ficas (`:2.1.106`, `:2.1.114`, `:2.2.11`, `:3.0.42`) a cada novo release.
    *   Tags especÃ­ficas (`:2.1.106`) permanecem apontando para a mesma imagem imutÃ¡vel.
*   Explica a estratÃ©gia de escolha do consumidor:
    *   Usar tag de patch especÃ­fica (`:2.1.106`): Garante que a imagem nunca mudarÃ¡ (estabilidade mÃ¡xima), mas nÃ£o recebe correÃ§Ãµes futuras.
    *   Usar tag minor (`:2.1`): Recebe correÃ§Ãµes de patch (`2.1.x`), mas nÃ£o novas funcionalidades `2.y.z`. Risco baixo de quebra.
    *   Usar tag major (`:2`): Recebe patches e novas funcionalidades minor (`2.x.y`), mas nÃ£o mudanÃ§as major `3.y.z`. Risco moderado.
    *   Usar `latest`: Recebe tudo, incluindo mudanÃ§as major. Maior risco de quebra.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina demonstra como aplicar mÃºltiplas tags Ã  mesma imagem e ilustra um esquema de versionamento comum usando SemVer.
*   O exercÃ­cio cria vÃ¡rios "apelidos" (tags) para a imagem `image-gallery` original, todos apontando para o mesmo conteÃºdo (mesmo IMAGE ID), mas com nomes diferentes que representam nÃ­veis distintos de versionamento (`:latest`, `:2`, `:2.1`, `:2.1.106`).
*   A Figura 5.8 Ã© fundamental: ela mostra que, a cada novo lanÃ§amento de software, as tags mais genÃ©ricas (`latest`, `:2`, `:2.1`) sÃ£o *movidas* para apontar para a nova versÃ£o especÃ­fica, enquanto as tags especÃ­ficas (`:2.1.106`) continuam apontando para a versÃ£o antiga e imutÃ¡vel.
*   Isso permite que quem usa a imagem escolha seu nÃ­vel de risco/atualizaÃ§Ã£o:
    *   Quer estabilidade total? Use a tag mais especÃ­fica (`:2.1.106`).
    *   Quer correÃ§Ãµes de bugs mas sem novas funcionalidades arriscadas? Use a tag minor (`:2.1`).
    *   Quer novas funcionalidades compatÃ­veis e correÃ§Ãµes? Use a tag major (`:2`).
    *   Quer sempre a Ãºltima versÃ£o, custe o que custar? Use `latest` (geralmente nÃ£o recomendado para produÃ§Ã£o).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar MÃºltiplas Tags:** (Assumindo que `image-gallery:latest` ainda existe localmente)
    ```powershell
    docker image tag image-gallery registry.local:5000/gallery/ui:latest
    docker image tag image-gallery registry.local:5000/gallery/ui:2
    docker image tag image-gallery registry.local:5000/gallery/ui:2.1
    docker image tag image-gallery registry.local:5000/gallery/ui:2.1.106
    ```
2.  **Verificar as Tags:**
    ```powershell
    docker image ls registry.local:5000/gallery/ui*
    ```
    VocÃª verÃ¡ vÃ¡rias linhas com o *mesmo* IMAGE ID, mas tags diferentes.
3.  **(Opcional) Enviar Todas as Tags:**
    ```powershell
    docker image push --all-tags registry.local:5000/gallery/ui
    # Ou enviar uma por uma:
    # docker image push registry.local:5000/gallery/ui:latest
    # docker image push registry.local:5000/gallery/ui:2
    # ... etc
    ```

---

ğŸ“„ PÃ¡gina 97

ğŸ” Resumo dos principais pontos:

*   Qualquer escolha de tag (patch, minor, major) Ã© vÃ¡lida, depende do equilÃ­brio de risco.
*   **Importante:** Usar tags *especÃ­ficas* para imagens base em seus Dockerfiles (`FROM <imagem>:<tag_especifica>`).
    *   Evita que builds futuros quebrem inesperadamente devido a atualizaÃ§Ãµes automÃ¡ticas da imagem base se vocÃª usar `latest` ou tags muito genÃ©ricas.
*   **5.5 Turning official images into golden images** (Transformando imagens oficiais em "golden images")
    *   QuestÃ£o de confianÃ§a: Pode-se confiar nas imagens do Docker Hub?
    *   Qualquer um pode enviar imagens; risco de malware.
    *   SoluÃ§Ã£o do Docker Hub: **Verified Publishers** e **Official Images**.
        *   **Verified Publishers:** Empresas (Microsoft, Oracle, IBM) cujas imagens passam por aprovaÃ§Ã£o e varredura de seguranÃ§a. Podem ser "Certified" (endossadas pelo Docker e publisher). Boa opÃ§Ã£o para software COTS.
        *   **Official Images:** Geralmente projetos open source, mantidos conjuntamente pelo projeto e Docker. Passam por varredura, atualizados, seguem boas prÃ¡ticas. Dockerfiles geralmente no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre tags enfatizando a prÃ¡tica crucial de usar tags **especÃ­ficas** na instruÃ§Ã£o `FROM` de seus Dockerfiles. Se vocÃª usar `FROM ubuntu:latest`, seu build pode funcionar hoje, mas quebrar amanhÃ£ se uma nova versÃ£o incompatÃ­vel do Ubuntu for lanÃ§ada e marcada como `latest`. Usar `FROM ubuntu:22.04` garante que vocÃª sempre construirÃ¡ sobre a mesma base testada.
*   A seÃ§Ã£o 5.5 aborda a questÃ£o da **confianÃ§a e seguranÃ§a** das imagens que vocÃª usa, especialmente as do Docker Hub.
*   O Docker Hub combate o risco de imagens maliciosas atravÃ©s de dois mecanismos principais:
    *   **Verified Publisher:** Imagens publicadas por empresas conhecidas que passaram por um processo de verificaÃ§Ã£o e varredura de seguranÃ§a. Algumas sÃ£o atÃ© "Certified", indicando um nÃ­vel extra de confianÃ§a e suporte. Ideal para softwares comerciais ou de grandes fornecedores.
    *   **Official Image:** Imagens de projetos (geralmente open source populares como `nginx`, `python`, `node`, `postgres`) que sÃ£o curadas e mantidas em colaboraÃ§Ã£o entre a equipe do projeto e a Docker Inc. Elas tambÃ©m passam por varreduras de seguranÃ§a, sÃ£o atualizadas regularmente e seguem boas prÃ¡ticas. SÃ£o a escolha mais comum e recomendada para imagens base.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Boas prÃ¡ticas `FROM`:**
    *   Ruim: `FROM python` (usa `latest` implÃ­cito)
    *   Melhor: `FROM python:3.10` (usa a Ãºltima patch da 3.10)
    *   Ainda melhor: `FROM python:3.10.4` (fixa na patch especÃ­fica)
    *   Considerar variantes: `FROM python:3.10-slim` ou `FROM python:3.10-alpine` (imagens menores)
*   Navegue no Docker Hub (hub.docker.com) e procure por imagens como `nginx`, `microsoft/dotnet/sdk`, `oracle/database`. Observe os selos "Official Image" ou "Verified Publisher".

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Verified Publisher: Ã‰ como comprar um produto de uma marca famosa e estabelecida na loja oficial.
*   Official Image: Ã‰ como pegar um ingrediente "selo de qualidade" recomendado pelo prÃ³prio chef famoso (Docker).

---

ğŸ“„ PÃ¡gina 98

ğŸ” Resumo dos principais pontos:

*   A maioria usa "Official Images" como base.
*   Eventualmente, surge a necessidade de mais controle -> CriaÃ§Ã£o de **Golden Images**.
*   **Golden Image:** Imagem base *preferida/customizada* pela sua organizaÃ§Ã£o.
*   Diagrama (Figura 5.9): Fluxo Golden Image.
    *   Official Image (ex: `openjdk:11-jre-slim`) -> Sua Golden Image (ex: `my-org/openjdk:11.0.1`) -> Imagem da sua AplicaÃ§Ã£o (ex: `my-org/my-app:1.0`).
*   **Como funciona:**
    *   Golden Image usa uma Official Image como `FROM`.
    *   Adiciona customizaÃ§Ãµes necessÃ¡rias (ex: certificados de seguranÃ§a internos, configuraÃ§Ãµes padrÃ£o, ferramentas comuns da empresa).
    *   Golden Image Ã© armazenada no registro da empresa (privado ou no Hub).
    *   *Todas* as imagens de aplicaÃ§Ã£o da empresa usam a Golden Image como `FROM`.
*   **BenefÃ­cios das Golden Images:**
    *   Combina boas prÃ¡ticas da imagem oficial com customizaÃ§Ãµes da empresa.
    *   Centraliza configuraÃ§Ãµes/ferramentas comuns.
    *   Controla a versÃ£o base usada pelas aplicaÃ§Ãµes (atualiza a golden image, depois as apps).
*   **ExercÃ­cio Try-it-now:** Construir duas golden images para .NET Core (SDK e runtime).
    *   Navegar para `ch05/exercises/dotnet-sdk`, construir `golden/dotnetcore-sdk:3.0`.
    *   Navegar para `ch05/exercises/aspnet-runtime`, construir `golden/aspnet-core:3.0`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Mesmo usando imagens oficiais confiÃ¡veis, muitas organizaÃ§Ãµes precisam adicionar suas prÃ³prias configuraÃ§Ãµes, ferramentas ou padrÃµes de seguranÃ§a. Em vez de repetir essas adiÃ§Ãµes em cada Dockerfile de aplicaÃ§Ã£o, cria-se uma **Golden Image**.
*   A Golden Image Ã© uma imagem intermediÃ¡ria: ela comeÃ§a (`FROM`) com uma imagem oficial cuidadosamente escolhida (ex: uma versÃ£o especÃ­fica e mÃ­nima do OpenJDK ou .NET Runtime) e adiciona tudo que Ã© padrÃ£o ou necessÃ¡rio para *todas* as aplicaÃ§Ãµes daquela tecnologia na sua empresa (ex: certificados raiz da empresa, agentes de monitoramento, configuraÃ§Ãµes de proxy).
*   Essa Golden Image Ã© entÃ£o publicada no registro interno da empresa. A partir daÃ­, **todos os Dockerfiles de aplicaÃ§Ã£o** devem usar `FROM sua-golden-image` em vez de `FROM imagem-oficial`.
*   **Vantagens:** CentralizaÃ§Ã£o de padrÃµes, controle sobre as dependÃªncias base (vocÃª decide quando atualizar a golden image), simplificaÃ§Ã£o dos Dockerfiles de aplicaÃ§Ã£o (nÃ£o precisam repetir instalaÃ§Ãµes comuns).
*   O exercÃ­cio instrui a construir duas golden images de exemplo para .NET Core, uma com o SDK (para builds) e outra com o runtime (para execuÃ§Ã£o).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    1.  No PowerShell/terminal, navegue e construa a golden image do SDK:
        ```powershell
        cd C:\DockerLabs\diamol\ch05\exercises\dotnet-sdk
        docker image build -t golden/dotnetcore-sdk:3.0 .
        ```
    2.  Navegue e construa a golden image do runtime:
        ```powershell
        cd ..\aspnet-runtime
        docker image build -t golden/aspnet-core:3.0 .
        ```
    3.  Verifique as novas imagens locais:
        ```powershell
        docker image ls golden/*
        ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Golden Image Ã© como criar o "Kit de Ferramentas PadrÃ£o da Empresa" ou o "Modelo de Documento Oficial". Em vez de cada funcionÃ¡rio montar seu kit do zero ou comeÃ§ar um documento em branco, eles partem do kit/modelo padrÃ£o, que jÃ¡ contÃ©m o essencial e segue as normas da empresa.

---

ğŸ“„ PÃ¡gina 99

ğŸ” Resumo dos principais pontos:

*   Figura 5.9: Diagrama ilustrando o conceito de Golden Image.
*   Golden images nÃ£o sÃ£o especiais tecnicamente (construÃ­das com Dockerfile), apenas conceitualmente (padronizaÃ§Ã£o).
*   Dockerfile da golden image construÃ­da adiciona metadados (`LABEL`) e configura defaults.
*   **Uso em Dockerfile de AplicaÃ§Ã£o:**
    *   Pode-se usar essas golden images em um Dockerfile multi-estÃ¡gio para uma aplicaÃ§Ã£o .NET Core.
*   **Listagem 5.1:** Exemplo de Dockerfile multi-estÃ¡gio usando as golden images .NET Core.
    ```dockerfile
    FROM golden/dotnetcore-sdk:3.0 AS builder # Usa SDK golden image
    COPY . .
    RUN dotnet publish -o /out/app app.csproj # Build .NET

    FROM golden/aspnet-core:3.0 # Usa runtime golden image
    COPY --from=builder /out /app # Copia app compilado
    CMD ["dotnet", "/app/app.dll"] # Roda app
    ```
*   Dockerfile da aplicaÃ§Ã£o tem formato multi-estÃ¡gio padrÃ£o, mas agora baseado nas imagens da empresa.
*   **GovernanÃ§a:**
    *   Pode-se *forÃ§ar* o uso de golden images em pipelines de CI/CD.
    *   Ferramentas podem escanear Dockerfiles e falhar o build se nÃ£o usarem `FROM golden/...`.
    *   Boa forma de controlar as imagens base permitidas.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra (Figura 5.9) o fluxo: a imagem oficial Ã© a base da golden image, que por sua vez Ã© a base da imagem da aplicaÃ§Ã£o.
*   Explica que as golden images que acabaram de ser construÃ­das sÃ£o simples, apenas adicionando alguns metadados (`LABEL`) e configuraÃ§Ãµes.
*   A **Listagem 5.1** demonstra como um Dockerfile de aplicaÃ§Ã£o .NET Core usaria essas golden images em um build multi-estÃ¡gio:
    *   O estÃ¡gio `builder` usa `FROM golden/dotnetcore-sdk:3.0` para obter o ambiente de build .NET Core jÃ¡ customizado pela empresa.
    *   O estÃ¡gio final usa `FROM golden/aspnet-core:3.0` para obter o ambiente de runtime .NET Core customizado.
*   O benefÃ­cio final Ã© a **governanÃ§a**. Ao padronizar o uso de golden images, a empresa pode controlar melhor as dependÃªncias e a seguranÃ§a. Ã‰ possÃ­vel integrar verificaÃ§Ãµes no pipeline de CI/CD para garantir que todos os projetos estejam usando as golden images aprovadas, bloqueando builds que tentem usar imagens base nÃ£o autorizadas diretamente do Docker Hub, por exemplo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Abra os arquivos `Dockerfile` nas pastas `dotnet-sdk` e `aspnet-runtime` (em `ch05\exercises`) no VS Code para ver o que as golden images realmente fazem (provavelmente adicionam `LABEL`s e talvez algumas configuraÃ§Ãµes padrÃ£o).
*   Analise a Listagem 5.1. Se vocÃª tivesse um projeto .NET Core simples, poderia usar um Dockerfile como este, substituindo `app.csproj` e `app.dll` pelos nomes corretos, para construÃ­-lo usando as golden images.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   ForÃ§ar o uso de Golden Images Ã© como a empresa dizer: "Para construir qualquer produto nosso (aplicaÃ§Ã£o), vocÃª *deve* usar o nosso kit de ferramentas oficial (golden image SDK) e ele *deve* ser montado no nosso chassi padrÃ£o (golden image runtime)".

---

ğŸ“„ PÃ¡gina 100

ğŸ” Resumo dos principais pontos:

*   **5.6 Lab** (LaboratÃ³rio)
    *   Tarefa de "detetive", requer pesquisa na especificaÃ§Ã£o da API v2 do Docker Registry (link fornecido).
    *   Motivo: CLI Docker padrÃ£o nÃ£o permite buscar ou deletar imagens de um registro (ainda). InteraÃ§Ã£o via API REST Ã© necessÃ¡ria para o registro local bÃ¡sico.
*   **Objetivo do Lab:**
    1.  Enviar (push) *todas* as tags da imagem `gallery/ui` (criadas na p. 97: `:latest`, `:2`, `:2.1`, `:2.1.106`, `:v1`) para o registro local (`registry.local:5000`).
    2.  Verificar (via API REST) que todas as tags estÃ£o lÃ¡.
    3.  Deletar (via API REST) *todas* as tags/manifestos dessa imagem do registro local.
    4.  Verificar (via API REST) que foram removidas.
*   **Dicas:**
    *   Pode usar um Ãºnico comando `push` com flag `--all-tags`.
    *   URL base da API do registro local: `http://registry.local:5000/v2`.
    *   ComeÃ§ar listando tags do repositÃ³rio (`.../tags/list`).
    *   PrecisarÃ¡ obter o "manifest" da imagem/tag para deletar.
    *   DeleÃ§Ã£o requer obter o *digest* do manifesto (um hash sha256).
    *   DeleÃ§Ã£o via API usa mÃ©todo HTTP `DELETE` em um endpoint especÃ­fico com o digest.
    *   DocumentaÃ§Ã£o da API menciona um header HTTP especÃ­fico necessÃ¡rio para obter o digest (requisiÃ§Ã£o `HEAD` com `Accept: application/vnd.docker.distribution.manifest.v2+json`).
*   SoluÃ§Ã£o no GitHub. Menciona que pode ser complicado e permite "colar".

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este lab Ã© mais avanÃ§ado e foca na interaÃ§Ã£o direta com a **API do Docker Registry**, especificamente o registro bÃ¡sico rodando localmente no contÃªiner `registry`. Como a linha de comando `docker` tem limitaÃ§Ãµes para gerenciar conteÃºdos de um registro (ela Ã© Ã³tima para push/pull, mas nÃ£o para listar tudo ou deletar remotamente de forma granular), precisamos usar ferramentas como `curl` para fazer requisiÃ§Ãµes HTTP diretamente Ã  API do registro.
*   A tarefa envolve:
    1.  Enviar todas as tags criadas para `gallery/ui` para o registro local. A dica `--all-tags` no `docker push` facilita isso.
    2.  Usar `curl` para acessar o endpoint `_catalog` (listar repositÃ³rios) e `/<repo>/tags/list` (listar tags de um repo) da API do registro local para confirmar o envio.
    3.  **Parte difÃ­cil:** Para deletar, a API exige o `digest` (um hash SHA256 que identifica unicamente o conteÃºdo do manifesto da imagem/tag). Para obter o digest, Ã© preciso fazer uma requisiÃ§Ã£o HTTP `HEAD` para o manifesto da tag, incluindo um header `Accept` especÃ­fico (conforme a dica).
    4.  Com o digest em mÃ£os, fazer uma requisiÃ§Ã£o HTTP `DELETE` para o endpoint de manifestos do registro, incluindo o digest.
    5.  Verificar novamente com `curl .../tags/list` se a tag/manifesto foi removido. (Nota: O registro bÃ¡sico pode precisar de configuraÃ§Ã£o extra ou reinÃ­cio para realmente liberar o espaÃ§o em disco apÃ³s a deleÃ§Ã£o via API - "garbage collection").
*   Este lab ensina sobre o funcionamento interno dos registros e como interagir programaticamente com eles.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Passos gerais (requer `curl` instalado no Windows ou usar `Invoke-WebRequest` do PowerShell):**
    1.  **Push All Tags:**
        ```powershell
        docker image push --all-tags registry.local:5000/gallery/ui
        ```
    2.  **Listar Repos e Tags (VerificaÃ§Ã£o):**
        ```powershell
        curl http://registry.local:5000/v2/_catalog
        curl http://registry.local:5000/v2/gallery/ui/tags/list
        # Ou com PowerShell:
        # Invoke-WebRequest http://registry.local:5000/v2/_catalog | ConvertFrom-Json
        # Invoke-WebRequest http://registry.local:5000/v2/gallery/ui/tags/list | ConvertFrom-Json
        ```
    3.  **Obter Digest (Exemplo para tag 'v1'):**
        ```powershell
        # Com curl (pode precisar instalar):
        # curl --head -H "Accept: application/vnd.docker.distribution.manifest.v2+json" http://registry.local:5000/v2/gallery/ui/manifests/v1
        # Procure pelo header 'Docker-Content-Digest:' na saÃ­da.

        # Com PowerShell:
        $response = Invoke-WebRequest -Method Head -Uri http://registry.local:5000/v2/gallery/ui/manifests/v1 -Headers @{"Accept"="application/vnd.docker.distribution.manifest.v2+json"}
        $digest = $response.Headers.'Docker-Content-Digest'
        echo $digest # Anote este valor (ex: sha256:...)
        ```
    4.  **Deletar Manifesto (usando o digest obtido):**
        ```powershell
        # Com curl:
        # curl -X DELETE http://registry.local:5000/v2/gallery/ui/manifests/<digest_completo>

        # Com PowerShell:
        # Invoke-WebRequest -Method Delete -Uri http://registry.local:5000/v2/gallery/ui/manifests/$digest
        # (Nota: O registro bÃ¡sico pode retornar um erro 405 Method Not Allowed se a deleÃ§Ã£o nÃ£o estiver habilitada na configuraÃ§Ã£o dele)
        ```
    5.  **Verificar DeleÃ§Ã£o:** Repita o passo 2 para listar as tags. A tag `v1` (ou a que vocÃª deletou) nÃ£o deve mais aparecer.

---

ğŸ“„ PÃ¡gina 101

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 6: Using Docker volumes for persistent storage** (Usando volumes Docker para armazenamento persistente)
*   ContÃªineres sÃ£o perfeitos para aplicaÃ§Ãµes *stateless* (sem estado).
    *   FÃ¡cil escalar rodando mÃºltiplos contÃªineres idÃªnticos.
    *   AtualizaÃ§Ãµes via rolling upgrade mantÃªm o app online.
*   Mas nem todo componente Ã© stateless (ex: bancos de dados, caches em disco).
*   Pode-se rodar componentes *stateful* (com estado) em contÃªineres tambÃ©m.
*   Armazenamento adiciona complexidade; precisa entender como Dockerizar apps stateful.
*   Foco do Cap. 6: **Docker volumes** e **mounts**, como funciona o filesystem do contÃªiner.
*   **6.1 Why data in containers is not permanent** (Por que dados em contÃªineres nÃ£o sÃ£o permanentes)
    *   ContÃªiner tem filesystem (disco virtual) populado com arquivos da imagem (`COPY`).
    *   Filesystem Ã© construÃ­do pela uniÃ£o das camadas da imagem (read-only).
    *   Cada contÃªiner tem seu prÃ³prio filesystem, independente de outros (mesmo da mesma imagem).
    *   ModificaÃ§Ãµes em um contÃªiner nÃ£o afetam outros ou a imagem.
    *   DemonstraÃ§Ã£o com app que escreve nÃºmero aleatÃ³rio em arquivo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 6, que aborda um tÃ³pico essencial: como lidar com dados que precisam **persistir** (durar mais que a vida de um contÃªiner).
*   ContÃªineres sÃ£o ideais para aplicaÃ§Ãµes *stateless* (que nÃ£o armazenam dados internamente entre requisiÃ§Ãµes), pois podem ser criados, destruÃ­dos e substituÃ­dos facilmente.
*   No entanto, muitas aplicaÃ§Ãµes *precisam* armazenar dados (sÃ£o *stateful*), como bancos de dados ou aplicaÃ§Ãµes que mantÃªm sessÃµes de usuÃ¡rio em arquivos. Rodar esses apps em contÃªineres exige uma soluÃ§Ã£o para que os dados nÃ£o sejam perdidos quando o contÃªiner for atualizado ou recriado.
*   A seÃ§Ã£o 6.1 comeÃ§a explicando *por que* o armazenamento padrÃ£o dentro de um contÃªiner *nÃ£o* Ã© persistente. O filesystem que o contÃªiner "vÃª" Ã© uma construÃ§Ã£o virtual feita a partir das camadas da imagem (que sÃ£o somente leitura) mais uma camada gravÃ¡vel especÃ­fica para aquele contÃªiner.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ainda, mas prepara para um exercÃ­cio que demonstrarÃ¡ a natureza nÃ£o persistente e isolada do filesystem padrÃ£o do contÃªiner.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Stateless vs. Stateful: Um caixa eletrÃ´nico (ATM) Ã© stateless - nÃ£o importa qual ATM vocÃª usa, a transaÃ§Ã£o Ã© a mesma. Sua conta bancÃ¡ria Ã© stateful - o saldo precisa ser armazenado e persistir.
*   Filesystem do ContÃªiner (inicial): Pense em um bloco de anotaÃ§Ãµes (camada gravÃ¡vel) colocado sobre um livro impresso (camadas da imagem). VocÃª pode escrever no bloco, e atÃ© mesmo riscar ou cobrir partes do livro (escondÃª-las), mas vocÃª nÃ£o altera o livro original. Se vocÃª jogar o bloco fora (remover o contÃªiner), suas anotaÃ§Ãµes se perdem.

---

ğŸ“„ PÃ¡gina 102

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar dois contÃªineres da imagem `diamol/ch06-random-number`.
    *   Comandos: `docker container run --name rn1 ...`, `docker container run --name rn2 ...`.
    *   App escreve nÃºmero aleatÃ³rio em `/random/number.txt` e sai (`Exited`).
*   ContÃªineres parados mantÃªm seus filesystems.
*   Comando `docker container cp`: Copia arquivos entre contÃªiner (parado ou rodando) e mÃ¡quina host.
    *   Sintaxe: `docker cp <container>:<caminho_no_container> <caminho_local>` ou vice-versa.
*   **ExercÃ­cio Try-it-now:** Copiar os arquivos `number.txt` de cada contÃªiner para o host e verificar o conteÃºdo.
    *   Comandos: `docker cp rn1:/random/number.txt number1.txt`, `docker cp rn2:/random/number.txt number2.txt`, `cat number1.txt`, `cat number2.txt` (ou `type` no Windows).
*   Resultado esperado (Figura 6.1): ConteÃºdos dos arquivos `number1.txt` e `number2.txt` sÃ£o diferentes, mostrando independÃªncia dos filesystems.
*   **ExplicaÃ§Ã£o do Filesystem do ContÃªiner:**
    *   Parece um disco Ãºnico (`/dev/sda1` Linux, `C:\` Windows).
    *   Ã‰ virtual, construÃ­do pelo Docker a partir de vÃ¡rias fontes.
    *   Fontes base: **Camadas da Imagem (Image Layers)** (compartilhadas, read-only) + **Camada GravÃ¡vel do ContÃªiner (Container's Writable Layer)** (Ãºnica por contÃªiner).
*   Diagrama (Figura 6.2): Ilustra a construÃ§Ã£o do filesystem a partir das camadas da imagem (compartilhadas) e da camada gravÃ¡vel (Ãºnica por contÃªiner).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O primeiro exercÃ­cio demonstra o isolamento do filesystem: dois contÃªineres (`rn1`, `rn2`), criados da *mesma* imagem, executam o mesmo script que cria um arquivo no mesmo caminho (`/random/number.txt`), mas o conteÃºdo do arquivo Ã© diferente em cada um (um nÃºmero aleatÃ³rio).
*   O comando `docker container cp` Ã© introduzido como forma de extrair arquivos de dentro de um contÃªiner para a sua mÃ¡quina local (host), permitindo inspecionar o resultado do exercÃ­cio anterior.
*   A explicaÃ§Ã£o tÃ©cnica do filesystem do contÃªiner Ã© aprofundada:
    *   Ele Ã© uma **uniÃ£o** de camadas. A base vem das camadas da imagem (que sÃ£o somente leitura e podem ser compartilhadas).
    *   Por cima delas, cada contÃªiner recebe sua prÃ³pria **camada gravÃ¡vel**, onde todas as *modificaÃ§Ãµes* (arquivos novos ou alterados) sÃ£o escritas.
    *   A Figura 6.2 mostra isso: `rn1` e `rn2` compartilham as camadas da imagem `random-number`, mas cada um tem sua prÃ³pria camada gravÃ¡vel onde o `number.txt` (com conteÃºdo diferente) Ã© armazenado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # Rodar os dois contÃªineres
    docker container run --name rn1 diamol/ch06-random-number
    docker container run --name rn2 diamol/ch06-random-number

    # Verificar que estÃ£o parados (Exited)
    docker ps -a

    # Copiar os arquivos para o diretÃ³rio atual no host
    docker container cp rn1:/random/number.txt number1.txt
    docker container cp rn2:/random/number.txt number2.txt

    # Exibir o conteÃºdo dos arquivos (no Windows, 'type' Ã© similar a 'cat')
    type number1.txt
    type number2.txt
    # Ou use Get-Content no PowerShell
    # Get-Content number1.txt
    # Get-Content number2.txt
    ```
    Confirme que os nÃºmeros exibidos sÃ£o diferentes.

---

ğŸ“„ PÃ¡gina 103

ğŸ” Resumo dos principais pontos:

*   Dois pontos importantes da Figura 6.2:
    1.  Camadas da imagem sÃ£o compartilhadas -> devem ser read-only.
    2.  Camada gravÃ¡vel Ã© Ãºnica por contÃªiner e tem o *mesmo ciclo de vida* do contÃªiner.
*   Ciclo de vida:
    *   Camadas de imagem: Persistem no cache atÃ© serem removidas (`docker image rm`).
    *   Camada gravÃ¡vel: Criada com `docker run`, deletada com `docker rm`. (ContÃªiner parado ainda tem sua camada gravÃ¡vel).
*   **EdiÃ§Ã£o de arquivos das camadas da imagem:**
    *   Como editar se sÃ£o read-only? Docker usa **Copy-on-Write (CoW)**.
    *   Quando contÃªiner tenta editar arquivo de uma camada da imagem:
        1.  Docker *copia* o arquivo da camada read-only para a camada *gravÃ¡vel*.
        2.  As ediÃ§Ãµes ocorrem na *cÃ³pia* dentro da camada gravÃ¡vel.
    *   Processo Ã© transparente para o app no contÃªiner.
    *   CoW Ã© a base da eficiÃªncia de armazenamento do Docker.
*   **ExercÃ­cio Try-it-now:** Demonstrar CoW e ciclo de vida da camada gravÃ¡vel.
    *   Imagem `diamol/ch06-file-display` exibe conteÃºdo de `/input.txt`.
    *   1. Rodar contÃªiner `f1` para ver conteÃºdo original (`docker run --name f1 ...`).
    *   2. Criar `url.txt` local com novo conteÃºdo (`echo ... > url.txt`).
    *   3. Copiar `url.txt` para *dentro* de `f1` como `/input.txt` (`docker cp url.txt f1:/input.txt`). Isso usa CoW.
    *   4. Reiniciar `f1` para ver conteÃºdo modificado (`docker start --attach f1`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica a consequÃªncia mais importante do compartilhamento de camadas de imagem: elas **tÃªm que ser somente leitura (read-only)**. Se pudessem ser modificadas, uma mudanÃ§a em uma imagem afetaria todas as outras que a compartilham, o que seria caÃ³tico.
*   EntÃ£o, como um contÃªiner pode modificar um arquivo que veio da imagem base (ex: um arquivo de configuraÃ§Ã£o padrÃ£o)? A resposta Ã© a tÃ©cnica **Copy-on-Write (CoW)**:
    *   Quando o contÃªiner tenta *escrever* (modificar) um arquivo que estÃ¡ em uma camada read-only, o Docker primeiro *copia* esse arquivo para a camada *gravÃ¡vel* do contÃªiner.
    *   A partir daÃ­, todas as modificaÃ§Ãµes acontecem nessa cÃ³pia na camada gravÃ¡vel. O arquivo original na camada read-only permanece intocado.
    *   Para o aplicativo dentro do contÃªiner, parece que ele simplesmente editou o arquivo; o CoW Ã© invisÃ­vel para ele.
*   A pÃ¡gina tambÃ©m reforÃ§a o **ciclo de vida da camada gravÃ¡vel**: ela estÃ¡ intrinsecamente ligada ao contÃªiner. Quando o contÃªiner Ã© removido (`docker rm`), a camada gravÃ¡vel e todas as modificaÃ§Ãµes feitas nela sÃ£o **perdidas permanentemente**.
*   O exercÃ­cio prepara a demonstraÃ§Ã£o: rodar um contÃªiner, modificar um arquivo dentro dele (o `docker cp` para dentro do contÃªiner acionarÃ¡ o CoW se `/input.txt` vier da imagem) e depois reiniciar o *mesmo* contÃªiner para ver a modificaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Rodar container f1 (ele imprime o conteÃºdo e sai)
    docker container run --name f1 diamol/ch06-file-display
    # (SaÃ­da deve ser o conteÃºdo padrÃ£o de input.txt)

    # 2. Criar arquivo local com novo conteÃºdo
    echo "http://eltonstoneman.com" > url.txt
    # Verifique o conteÃºdo: type url.txt

    # 3. Copiar o arquivo local para dentro do container f1 (que estÃ¡ parado)
    docker container cp url.txt f1:/input.txt

    # 4. Reiniciar f1 e ver a saÃ­da (--attach anexa ao output do container)
    docker container start --attach f1
    # (SaÃ­da AGORA deve ser http://eltonstoneman.com)
    ```

---

ğŸ“„ PÃ¡gina 104

ğŸ” Resumo dos principais pontos:

*   Figura 6.3: Mostra a saÃ­da do exercÃ­cio anterior, confirmando que o arquivo dentro do contÃªiner `f1` foi modificado.
*   Modificar arquivo em um contÃªiner *nÃ£o* afeta a imagem original nem outros contÃªineres.
*   Arquivo modificado sÃ³ existe na camada gravÃ¡vel daquele contÃªiner especÃ­fico (`f1`).
*   **ExercÃ­cio Try-it-now:** Verificar que a imagem nÃ£o mudou e que os dados se perdem com `rm`.
    *   1. Rodar *novo* contÃªiner `f2` da *mesma imagem* para ver conteÃºdo original (`docker run --name f2 ...`).
    *   2. Remover contÃªiner `f1` (`docker rm -f f1`).
    *   3. Tentar copiar o arquivo de `f1` (que nÃ£o existe mais) para o host (`docker cp f1:/input.txt .`). Deve falhar.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o resultado do exercÃ­cio anterior (Figura 6.3): o reinÃ­cio do contÃªiner `f1` mostrou o conteÃºdo modificado, provando que a alteraÃ§Ã£o feita via `docker cp` foi persistida *naquele contÃªiner especÃ­fico*, mesmo ele estando parado entre a cÃ³pia e o reinÃ­cio.
*   O ponto crucial Ã© reiterado: essa modificaÃ§Ã£o vive *apenas* na camada gravÃ¡vel de `f1`. A imagem `diamol/ch06-file-display` original nÃ£o foi alterada. Qualquer *novo* contÃªiner criado a partir dessa imagem comeÃ§arÃ¡ com o conteÃºdo original do arquivo `/input.txt`.
*   O segundo exercÃ­cio demonstra isso e a natureza efÃªmera da camada gravÃ¡vel:
    *   Um novo contÃªiner `f2` Ã© criado da mesma imagem e mostra o conteÃºdo original, provando que a imagem estÃ¡ intacta.
    *   O contÃªiner `f1` Ã© removido com `docker rm -f f1`. Isso **destrÃ³i permanentemente** sua camada gravÃ¡vel.
    *   A tentativa de copiar o arquivo `/input.txt` de `f1` *apÃ³s* a remoÃ§Ã£o falha, pois o contÃªiner (e seus dados modificados) nÃ£o existe mais.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Rodar novo container f2 (deve mostrar conteÃºdo original)
    docker container run --name f2 diamol/ch06-file-display

    # 2. Remover f1 forÃ§adamente
    docker container rm -f f1

    # 3. Tentar copiar de f1 (deve dar erro "No such container")
    docker container cp f1:/input.txt .
    ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Remover o contÃªiner (`docker rm`) Ã© como jogar fora o bloco de anotaÃ§Ãµes onde vocÃª escreveu sobre o livro. Suas anotaÃ§Ãµes (camada gravÃ¡vel) se foram, mas o livro original (imagem) continua na estante, inalterado, pronto para outra pessoa ler (criar novo contÃªiner).

---

ğŸ“„ PÃ¡gina 105

ğŸ” Resumo dos principais pontos:

*   Figura 6.4: Mostra a saÃ­da do exercÃ­cio anterior, confirmando que `f2` tem o conteÃºdo original e `f1` nÃ£o existe mais.
*   **ConclusÃ£o sobre filesystem padrÃ£o:**
    *   Tem o mesmo ciclo de vida do contÃªiner.
    *   Camada gravÃ¡vel Ã© removida com `docker rm`.
    *   Qualquer dado modificado no contÃªiner Ã© perdido.
*   **ImplicaÃ§Ã£o:** Remover contÃªineres Ã© comum (ex: atualizaÃ§Ãµes de app onde imagem nova substitui a antiga). Se dados importantes foram escritos na camada gravÃ¡vel, eles sÃ£o perdidos na atualizaÃ§Ã£o.
*   **CenÃ¡rios:**
    *   OK para dados transitÃ³rios (ex: cache local). O novo contÃªiner comeÃ§a com cache vazio.
    *   Desastre para dados persistentes (ex: banco de dados). NÃ£o se espera perder tudo ao atualizar o SGBD.
*   **SoluÃ§Ã£o Docker:** Fontes adicionais para o filesystem do contÃªiner alÃ©m das camadas: **Volumes Docker** e **Mounts**.
    *   TÃªm ciclo de vida *separado* dos contÃªineres.
    *   Usados para armazenar dados que precisam persistir entre recriaÃ§Ãµes/atualizaÃ§Ãµes de contÃªineres.
*   IntroduÃ§Ã£o Ã  SeÃ§Ã£o 6.2 sobre Volumes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina consolida a liÃ§Ã£o da seÃ§Ã£o 6.1: a camada gravÃ¡vel padrÃ£o de um contÃªiner Ã© **efÃªmera**. Ela desaparece junto com o contÃªiner quando ele Ã© removido (`docker rm`).
*   Isso Ã© um problema sÃ©rio para aplicaÃ§Ãµes **stateful** (que precisam guardar dados), porque o processo normal de atualizaÃ§Ã£o de uma aplicaÃ§Ã£o em contÃªineres envolve justamente remover os contÃªineres antigos e substituÃ­-los por novos baseados na imagem atualizada. Se os dados estivessem apenas na camada gravÃ¡vel, seriam perdidos a cada atualizaÃ§Ã£o.
*   A soluÃ§Ã£o do Docker para persistÃªncia de dados Ã© usar mecanismos de armazenamento que existem *fora* do ciclo de vida normal do contÃªiner:
    1.  **Volumes Docker:** Uma Ã¡rea de armazenamento gerenciada *pelo Docker*, mas que existe independentemente dos contÃªineres.
    2.  **Bind Mounts:** Um mapeamento direto de um diretÃ³rio ou arquivo da mÃ¡quina *host* para dentro do contÃªiner.
*   Ambos permitem que um novo contÃªiner acesse os mesmos dados que um contÃªiner anterior (removido) estava usando. O capÃ­tulo explorarÃ¡ essas duas opÃ§Ãµes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. Prepara para os conceitos e comandos de `docker volume` e `docker run -v` (ou `--mount`) nas prÃ³ximas seÃ§Ãµes.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Volumes e Bind Mounts sÃ£o como conectar um **HD externo** (Volume Docker) ou compartilhar uma **pasta de rede** (Bind Mount) com o seu contÃªiner (que antes sÃ³ tinha o "disco interno" efÃªmero). Mesmo que vocÃª troque de contÃªiner (como trocar de computador), vocÃª pode reconectar o mesmo HD externo ou pasta de rede e seus arquivos estarÃ£o lÃ¡.

---

ğŸ“„ PÃ¡gina 106

ğŸ” Resumo dos principais pontos:

*   **6.2 Running containers with Docker volumes** (Rodando contÃªineres com volumes Docker)
*   **Volume Docker:** Unidade de armazenamento gerenciada pelo Docker, como um "pendrive para contÃªineres".
    *   Existe independentemente dos contÃªineres.
    *   Tem seu prÃ³prio ciclo de vida.
    *   Pode ser anexado a contÃªineres.
*   SoluÃ§Ã£o para dados persistentes em apps stateful.
*   **Fluxo de uso:**
    1.  Criar volume.
    2.  Anexar ao contÃªiner da aplicaÃ§Ã£o (aparece como um diretÃ³rio).
    3.  ContÃªiner escreve dados no diretÃ³rio (que vÃ£o para o volume).
    4.  Ao atualizar o app, anexa-se o *mesmo* volume ao novo contÃªiner.
    5.  Novo contÃªiner tem acesso aos dados originais.
*   **Duas formas de usar volumes:**
    1.  **Manualmente:** Criar volumes (`docker volume create`) e anexÃ¡-los com `docker run -v <nome_volume>:<caminho_container>`.
    2.  **Via Dockerfile:** Usar instruÃ§Ã£o `VOLUME <caminho_destino>`. Imagem cria um volume *anÃ´nimo* automaticamente quando contÃªiner inicia (se nenhum volume for especificado no `run`).
*   **Listagem 6.1:** Trecho de Dockerfile usando `VOLUME /data`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    WORKDIR /app
    ENTRYPOINT ["dotnet", "ToDoList.dll"]
    VOLUME /data # Declara um ponto de montagem para volume
    COPY --from=builder /out/ .
    ```
*   Quando contÃªiner roda dessa imagem (sem `-v` explÃ­cito), Docker cria volume anÃ´nimo e monta em `/data` (ou `C:\data` Windows). Dados escritos lÃ¡ persistem no volume.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o introduz os **Volumes Docker** como a principal forma gerenciada pelo Docker para persistir dados.
*   A analogia do "pendrive para contÃªineres" Ã© boa: um volume Ã© uma unidade de armazenamento separada que vocÃª pode "plug" em um contÃªiner. O Docker cuida de onde esse "pendrive" Ã© armazenado fisicamente no host, mas vocÃª interage com ele atravÃ©s de nomes.
*   O fluxo de uso para atualizaÃ§Ãµes Ã© explicado: o volume sobrevive Ã  remoÃ§Ã£o do contÃªiner antigo e Ã© re-anexado ao novo, preservando os dados.
*   HÃ¡ duas maneiras principais de associar um volume a um contÃªiner:
    1.  **ExplÃ­cita (`docker run -v`):** VocÃª cria um volume com nome (`docker volume create meu-volume`) e o monta em um caminho especÃ­fico dentro do contÃªiner (`docker run -v meu-volume:/app/data ...`). Esta Ã© a forma **recomendada** para dados importantes, pois vocÃª controla o nome e o ciclo de vida do volume.
    2.  **ImplÃ­cita (`VOLUME` no Dockerfile):** A instruÃ§Ã£o `VOLUME /data` no Dockerfile diz ao Docker: "Se alguÃ©m rodar um contÃªiner desta imagem *sem* montar explicitamente algo em `/data`, por favor, crie um volume *anÃ´nimo* (com um nome aleatÃ³rio longo) e monte-o aqui". Isso garante que os dados escritos em `/data` sempre vÃ£o para *algum* volume, mas gerenciar volumes anÃ´nimos Ã© mais difÃ­cil.
*   A Listagem 6.1 mostra um Dockerfile que usa a instruÃ§Ã£o `VOLUME`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   A instruÃ§Ã£o `VOLUME /data` no Dockerfile funciona para imagens baseadas em Linux ou Windows. O Docker mapearÃ¡ para `C:\data` automaticamente se a imagem base for Windows.
*   VocÃª executarÃ¡ um contÃªiner baseado na imagem com `VOLUME` no prÃ³ximo passo para ver o volume anÃ´nimo sendo criado.

---

ğŸ“„ PÃ¡gina 107

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar contÃªiner da imagem `diamol/ch06-todo-list` (que tem `VOLUME /data`) e verificar o volume criado.
    *   Comandos:
        *   `docker container run --name todo1 -d -p 8010:80 diamol/ch06-todo-list`
        *   `docker container inspect --format '{{.Mounts}}' todo1` (mostra detalhes da montagem do volume)
        *   `docker volume ls` (lista volumes existentes)
*   SaÃ­da esperada (Figura 6.5): `inspect` mostra um volume montado em `/data` (ou `C:\data`); `volume ls` mostra um volume com nome longo e aleatÃ³rio (anÃ´nimo).
*   Volumes sÃ£o transparentes para o app (vÃª apenas o diretÃ³rio `/data`).
*   **Teste da AplicaÃ§Ã£o To-Do:**
    *   Acessar `http://localhost:8010` no navegador.
    *   App armazena dados em um arquivo no diretÃ³rio `/data`.
    *   Itens adicionados via web sÃ£o salvos no volume Docker.
*   Figura 6.6: Mostra a interface web do app To-Do. App permite adicionar itens, mas nÃ£o remover (piada do autor).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra a criaÃ§Ã£o automÃ¡tica de um volume **anÃ´nimo** devido Ã  instruÃ§Ã£o `VOLUME /data` no Dockerfile da imagem `diamol/ch06-todo-list`.
*   Ao rodar o contÃªiner `todo1` sem especificar um volume com `-v`, o Docker cria um volume com um nome longo e aleatÃ³rio (ex: `f8a...`) e o monta no caminho `/data` dentro do contÃªiner.
*   O comando `docker container inspect --format '{{.Mounts}}'` Ã© usado para confirmar que um volume estÃ¡ montado e qual o seu nome/ID.
*   O comando `docker volume ls` lista todos os volumes gerenciados pelo Docker, incluindo o anÃ´nimo recÃ©m-criado.
*   Para a aplicaÃ§Ã£o To-Do dentro do contÃªiner, o volume Ã© invisÃ­vel; ela simplesmente lÃª e escreve arquivos no diretÃ³rio `/data` como faria em qualquer sistema de arquivos. O Docker garante que essas operaÃ§Ãµes aconteÃ§am no volume associado.
*   Acessar a aplicaÃ§Ã£o no navegador e adicionar itens demonstra que os dados estÃ£o sendo salvos (no volume).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # Rodar o container (Docker cria volume anÃ´nimo)
    docker container run --name todo1 -d -p 8010:80 diamol/ch06-todo-list

    # Inspecionar as montagens do container
    docker container inspect --format '{{.Mounts}}' todo1
    # Observe a saÃ­da, deve mostrar um 'Type=volume', um 'Name' longo (ID), e 'Destination=/data' (ou C:\data)

    # Listar os volumes (procurar pelo volume com o ID longo)
    docker volume ls

    # Testar o App
    # Abra o navegador e acesse http://localhost:8010
    # Adicione alguns itens Ã  lista (ex: "Aprender Volumes", "Fazer Lab 6")
    ```

---

ğŸ“„ PÃ¡gina 108

ğŸ” Resumo dos principais pontos:

*   Volumes de imagem (anÃ´nimos) sÃ£o criados *por contÃªiner*. Novo contÃªiner = novo volume anÃ´nimo.
*   Pode-se **compartilhar** volumes entre contÃªineres usando `docker run --volumes-from <outro_container>`.
    *   Anexa os volumes do `<outro_container>` ao novo contÃªiner.
*   **ExercÃ­cio Try-it-now:** Demonstrar volumes dedicados vs. compartilhados.
    *   1. Rodar `todo2` (terÃ¡ seu prÃ³prio volume anÃ´nimo, lista vazia).
        *   `docker container run --name todo2 -d diamol/ch06-todo-list`
        *   Verificar conteÃºdo de `/data` em `todo2` (usando `docker exec ... ls /data` ou `dir C:\data`). Deve estar vazio.
    *   2. Rodar `t3` usando volumes de `todo1` (terÃ¡ os itens adicionados anteriormente).
        *   `docker container run -d --name t3 --volumes-from todo1 diamol/ch06-todo-list`
        *   Verificar conteÃºdo de `/data` em `t3`. Deve mostrar o arquivo de dados (ex: `todolist.db`).
*   SaÃ­da esperada (Figura 6.7): Mostra `todo2` vazio e `t3` com dados de `todo1`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que a criaÃ§Ã£o automÃ¡tica de volumes via instruÃ§Ã£o `VOLUME` resulta em um volume *novo e separado* para cada contÃªiner iniciado (a menos que se use `-v`).
*   Introduz a flag `--volumes-from`. Ela permite que um novo contÃªiner (`t3` no exemplo) "pegue emprestado" e use exatamente os mesmos volumes que outro contÃªiner (`todo1`) estÃ¡ usando (ou usou, mesmo que `todo1` esteja parado). Isso permite que mÃºltiplos contÃªineres acessem e modifiquem os mesmos dados armazenados em um volume.
*   O exercÃ­cio demonstra isso:
    *   `todo2` Ã© iniciado normalmente e recebe seu prÃ³prio volume anÃ´nimo vazio.
    *   `t3` Ã© iniciado com `--volumes-from todo1`, entÃ£o ele nÃ£o cria um novo volume, mas monta o volume *de todo1* em `/data`. Como `todo1` jÃ¡ tinha dados (do exercÃ­cio anterior), `t3` agora vÃª esses mesmos dados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Rodar todo2
    docker container run --name todo2 -d diamol/ch06-todo-list

    # Verificar /data em todo2 (assumindo imagem Linux base)
    docker container exec todo2 ls /data
    # (SaÃ­da deve ser vazia)
    # Se for imagem Windows base:
    # docker container exec todo2 cmd /C "dir C:\data"

    # 2. Rodar t3 usando volumes de todo1
    docker container run -d --name t3 --volumes-from todo1 diamol/ch06-todo-list

    # Verificar /data em t3 (assumindo imagem Linux base)
    docker container exec t3 ls /data
    # (SaÃ­da deve mostrar o arquivo de dados, ex: 'todolist.db')
    # Se for imagem Windows base:
    # docker container exec t3 cmd /C "dir C:\data"
    ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `--volumes-from`: Ã‰ como dizer ao novo funcionÃ¡rio (`t3`) para usar exatamente a mesma gaveta de arquivos (`volume`) que o funcionÃ¡rio antigo (`todo1`) usava, em vez de pegar uma gaveta nova e vazia.

---

ğŸ“„ PÃ¡gina 109

ğŸ” Resumo dos principais pontos:

*   Figura 6.7: Mostra a saÃ­da do exercÃ­cio anterior (diretÃ³rios em `todo2` vs `t3`).
*   Compartilhar volumes (`--volumes-from`) pode nÃ£o ser ideal.
    *   Apps podem esperar acesso exclusivo aos arquivos e falhar se outro contÃªiner modificar ao mesmo tempo.
*   Melhor uso de volumes: preservar estado entre atualizaÃ§Ãµes de app.
*   Abordagem recomendada: **Volumes Nomeados (Named Volumes)**.
    *   Criar explicitamente um volume com um nome (`docker volume create <nome>`).
    *   Anexar o volume nomeado ao contÃªiner (`docker run -v <nome>:<caminho>`).
*   **ExercÃ­cio Try-it-now:** Demonstrar atualizaÃ§Ã£o de app usando volume nomeado.
    *   1. Definir variÃ¡vel `target` para caminho no contÃªiner (`/data` ou `c:\data`).
    *   2. Criar volume nomeado `todo-list`: `docker volume create todo-list`.
    *   3. Rodar app v1 (`todo-v1`) usando o volume nomeado: `docker run ... -v todo-list:$target ... diamol/ch06-todo-list`.
    *   4. Adicionar dados via UI (`http://localhost:8011`).
    *   5. Remover contÃªiner v1: `docker rm -f todo-v1`.
    *   6. Rodar app v2 (`todo-v2`) usando o *mesmo* volume nomeado: `docker run ... -v todo-list:$target ... diamol/ch06-todo-list:v2`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina adverte sobre os riscos de compartilhar o mesmo volume entre mÃºltiplos contÃªineres simultaneamente (`--volumes-from`), pois muitas aplicaÃ§Ãµes nÃ£o sÃ£o projetadas para lidar com escrita concorrente nos mesmos arquivos.
*   Introduz a abordagem preferida para persistÃªncia de dados entre atualizaÃ§Ãµes: **Volumes Nomeados**. Em vez de deixar o Docker criar volumes anÃ´nimos (via `VOLUME` no Dockerfile ou sem `-v`), vocÃª cria um volume com um nome significativo (`docker volume create todo-list`) e o gerencia explicitamente.
*   Ao iniciar um contÃªiner, vocÃª usa a sintaxe `-v <nome_do_volume>:<caminho_no_container>` para montar esse volume especÃ­fico.
*   O exercÃ­cio demonstra o ciclo de atualizaÃ§Ã£o:
    1.  Cria-se o volume `todo-list`.
    2.  Inicia-se a versÃ£o 1 do app (`todo-v1`), montando `todo-list` em `/data`.
    3.  Adicionam-se dados (eles vÃ£o para o volume `todo-list`).
    4.  Remove-se o contÃªiner `todo-v1` (o volume `todo-list` permanece intacto).
    5.  Inicia-se a versÃ£o 2 do app (`todo-v2`), montando o *mesmo* volume `todo-list` em `/data`.
    6.  O `todo-v2` agora tem acesso aos dados criados pelo `todo-v1`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Definir variÃ¡vel de destino (ajuste C: se necessÃ¡rio para imagem Windows)
    $target='/data' # Para imagem Linux
    # $target='c:\data' # Para imagem Windows

    # 2. Criar volume nomeado
    docker volume create todo-list

    # 3. Rodar v1 com volume nomeado (nova porta 8011)
    docker container run -d -p 8011:80 -v todo-list:$target --name todo-v1 diamol/ch06-todo-list

    # 4. Adicionar dados
    # Abra o navegador em http://localhost:8011 e adicione itens.

    # 5. Remover v1
    docker container rm -f todo-v1

    # 6. Rodar v2 com o mesmo volume nomeado
    docker container run -d -p 8011:80 -v todo-list:$target --name todo-v2 diamol/ch06-todo-list:v2
    ```
*   O teste final serÃ¡ na prÃ³xima pÃ¡gina (acessar a UI do v2).

---

ğŸ“„ PÃ¡gina 110

ğŸ” Resumo dos principais pontos:

*   Figura 6.8: Mostra a saÃ­da do exercÃ­cio anterior, confirmando que o volume nomeado persiste apÃ³s remoÃ§Ã£o do contÃªiner v1 e Ã© usado pelo v2.
*   **Teste:** Acessar `http://localhost:8011` (agora servido pelo v2).
    *   Deve-se ver a UI da versÃ£o 2 (Figura 6.9), mas com os dados adicionados na v1.
*   **Esclarecimento sobre `VOLUME` vs. `-v`:**
    *   InstruÃ§Ã£o `VOLUME` no Dockerfile e flag `-v` (ou `--volume`) no `docker run` sÃ£o features separadas.
    *   Se imagem tem `VOLUME /data` e `docker run` *nÃ£o* usa `-v ...:/data`: Docker cria volume *anÃ´nimo*.
    *   Se imagem tem `VOLUME /data` e `docker run` *usa* `-v meu-volume:/data`: Docker usa o volume *nomeado* `meu-volume`, *nÃ£o* cria um anÃ´nimo.
    *   Se imagem *nÃ£o* tem `VOLUME` e `docker run` usa `-v meu-volume:/data`: Docker usa o volume nomeado `meu-volume`.
*   **RecomendaÃ§Ã£o:**
    *   Autor da imagem: Usar `VOLUME` como "fail-safe" para apps stateful (garante que dados vÃ£o para *algum* volume).
    *   UsuÃ¡rio da imagem: Ã‰ melhor *nÃ£o* depender do `VOLUME` implÃ­cito e usar sempre volumes *nomeados* (`docker volume create ...` e `docker run -v ...`) para clareza e controle.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o resultado esperado do exercÃ­cio anterior: ao acessar a aplicaÃ§Ã£o na porta 8011 (agora servida pelo contÃªiner `todo-v2`), vocÃª verÃ¡ a nova interface (Figura 6.9), mas a lista de tarefas conterÃ¡ os itens que vocÃª adicionou quando a v1 estava rodando. Isso prova que o volume nomeado persistiu os dados atravÃ©s da atualizaÃ§Ã£o.
*   Um ponto importante de esclarecimento Ã© a interaÃ§Ã£o entre a instruÃ§Ã£o `VOLUME` no Dockerfile e a flag `-v` no comando `docker run`:
    *   Se vocÃª *especifica* um volume com `-v` (seja nomeado ou anÃ´nimo criado na hora), ele tem **prioridade** sobre a instruÃ§Ã£o `VOLUME` no Dockerfile para aquele caminho especÃ­fico. O Docker usarÃ¡ o que vocÃª mandou no `-v`.
    *   A instruÃ§Ã£o `VOLUME` sÃ³ entra em aÃ§Ã£o (criando um volume anÃ´nimo) se o `docker run` *nÃ£o* montar nada naquele caminho especÃ­fico.
*   A recomendaÃ§Ã£o final Ã© clara: embora `VOLUME` no Dockerfile possa ser um backup, a **melhor prÃ¡tica** para gerenciar dados persistentes Ã© usar **volumes nomeados** criados e montados explicitamente com `docker volume create` e `docker run -v <nome>:<caminho>`. Isso dÃ¡ controle total sobre o ciclo de vida e a identificaÃ§Ã£o dos seus dados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Teste:** Abra o navegador em `http://localhost:8011`. VocÃª deve ver a nova interface (Figura 6.9) e os itens que adicionou anteriormente.
2.  **Listar Volumes:** Execute `docker volume ls`. VocÃª verÃ¡ o volume `todo-list` na lista, junto com quaisquer volumes anÃ´nimos de execuÃ§Ãµes anteriores (que podem ser limpos com `docker volume prune`).

---

ğŸ“„ PÃ¡gina 111

ğŸ” Resumo dos principais pontos:

*   Figura 6.9: Mostra a nova interface (v2) do app To-Do.
*   **6.3 Running containers with filesystem mounts** (Rodando contÃªineres com montagens de sistema de arquivos)
*   Volumes sÃ£o gerenciados pelo Docker; localizaÃ§Ã£o fÃ­sica no host Ã© abstraÃ­da.
*   Alternativa: **Bind Mounts**.
    *   Mapeia um diretÃ³rio ou arquivo *diretamente* da mÃ¡quina **host** para dentro do contÃªiner.
    *   ContÃªiner vÃª como parte normal do seu filesystem.
    *   Permite acesso a arquivos do host pelo contÃªiner e vice-versa.
*   **Casos de uso para Bind Mounts:**
    *   Usar armazenamento especÃ­fico do host (SSD rÃ¡pido, RAID, storage de rede). Se o host acessa, o contÃªiner pode usar via bind mount.
    *   Exemplo: Rodar BD em contÃªiner, mas armazenar dados em array RAID do host para confiabilidade.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o introduz a segunda forma principal de fornecer armazenamento externo a contÃªineres: **Bind Mounts**.
*   A diferenÃ§a fundamental em relaÃ§Ã£o aos Volumes Docker Ã©:
    *   **Volumes Docker:** SÃ£o gerenciados *pelo Docker*. VocÃª cria (`docker volume create`), lista (`docker volume ls`), remove (`docker volume rm`). O Docker decide onde armazenÃ¡-los fisicamente no sistema de arquivos do host (geralmente em um local especÃ­fico dentro da Ã¡rea de dados do Docker, como `/var/lib/docker/volumes` no Linux). VocÃª geralmente nÃ£o interage com o local fÃ­sico.
    *   **Bind Mounts:** SÃ£o um mapeamento *direto* de um caminho (diretÃ³rio ou arquivo) que jÃ¡ existe na sua mÃ¡quina **host** para um caminho dentro do contÃªiner. VocÃª especifica o caminho exato no host.
*   Bind mounts sÃ£o Ãºteis quando vocÃª quer que o contÃªiner acesse ou modifique arquivos que estÃ£o em um local especÃ­fico do seu sistema host, ou quando quer usar um tipo especÃ­fico de armazenamento (como um disco SSD rÃ¡pido ou um compartilhamento de rede montado no host) que o Docker nÃ£o gerenciaria diretamente como um volume.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O comando para bind mount usa a flag `-v` com uma sintaxe diferente dos volumes nomeados, ou a flag mais moderna e explÃ­cita `--mount`.
    *   Sintaxe `-v`: `docker run -v <caminho_ABSULUTO_no_host>:<caminho_no_container> ...`
    *   Sintaxe `--mount`: `docker run --mount type=bind,source=<caminho_ABSOLUTO_no_host>,target=<caminho_no_container> ...`
*   **Importante para Windows:** Caminhos do host devem ser absolutos (ex: `C:\Users\Alexandre\MeuApp\data`) ou usar variÃ¡veis como `$(pwd)` (PowerShell) ou `%cd%` (CMD) para obter o diretÃ³rio atual de forma absoluta. O Docker Desktop pode exigir configuraÃ§Ãµes de compartilhamento de arquivos (File Sharing) nas Settings para permitir que os contÃªineres acessem diretÃ³rios fora do perfil do usuÃ¡rio.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Volume Docker: Docker te dÃ¡ um armÃ¡rio trancado (volume) para guardar suas coisas; ele cuida de onde o armÃ¡rio fica.
*   Bind Mount: VocÃª diz ao Docker para usar uma gaveta especÃ­fica *da sua prÃ³pria mesa* (diretÃ³rio no host) para guardar as coisas do contÃªiner. VocÃª sabe exatamente onde estÃ¡, e pode acessar diretamente pela sua mesa ou pelo contÃªiner.

---

ğŸ“„ PÃ¡gina 112

ğŸ” Resumo dos principais pontos:

*   ExercÃ­cio usa bind mount para armazenar dados do app To-Do em um diretÃ³rio local do host.
*   **ExercÃ­cio Try-it-now:** Usar bind mount com o app To-Do.
    *   1. Definir variÃ¡veis `source` (caminho no host, ex: `$(pwd)\databases`) e `target` (caminho no contÃªiner, ex: `c:\data`). Ajustar para Windows/Linux.
    *   2. Criar diretÃ³rio `databases` no host (`mkdir`).
    *   3. Rodar contÃªiner usando `--mount type=bind...`: `docker run --mount type=bind,source=$source,target=$target -d -p 8012:80 diamol/ch06-todo-list`.
    *   4. Fazer requisiÃ§Ã£o `curl` para o app (para ele criar o arquivo de dados).
    *   5. Listar conteÃºdo do diretÃ³rio `databases` *no host* (`ls ./databases`).
*   Resultado esperado (Figura 6.10): Arquivo de dados do app (ex: `todolist.db`) aparece no diretÃ³rio `databases` da mÃ¡quina *host*.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra o bind mount na prÃ¡tica.
*   Primeiro, definem-se variÃ¡veis para os caminhos no host (`source`) e no contÃªiner (`target`) para facilitar a portabilidade do comando entre Windows e Linux. O caminho `source` usa `$(pwd)` (ou similar) para referenciar um subdiretÃ³rio (`databases`) dentro do diretÃ³rio atual onde o comando estÃ¡ sendo executado no host.
*   Cria-se o diretÃ³rio `databases` no host.
*   Inicia-se o contÃªiner `diamol/ch06-todo-list` usando a sintaxe `--mount type=bind,source=$source,target=$target`. Isso diz ao Docker: "pegue o diretÃ³rio `$source` do host e faÃ§a ele aparecer como `$target` dentro do contÃªiner".
*   Uma requisiÃ§Ã£o `curl` Ã© feita apenas para garantir que a aplicaÃ§Ã£o dentro do contÃªiner seja inicializada e crie seu arquivo de dados dentro do diretÃ³rio `$target` (que, na verdade, Ã© o diretÃ³rio `$source` do host).
*   O passo final Ã© listar o conteÃºdo do diretÃ³rio `$source` (`databases`) *no host*. A presenÃ§a do arquivo de dados da aplicaÃ§Ã£o confirma que o bind mount funcionou e os dados do contÃªiner estÃ£o sendo escritos diretamente no sistema de arquivos do host.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Definir variÃ¡veis (estando no diretÃ³rio C:\DockerLabs\diamol\ch06\exercises)
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "databases"
    $TargetPath = 'c:\data' # Assumindo imagem base Windows para consistÃªncia no exemplo
                            # Se a imagem for Linux, use $TargetPath = '/data'
                            # A imagem diamol/ch06-todo-list Ã© multi-arch,
                            # no Windows ele puxarÃ¡ a variante Windows se disponÃ­vel.

    # 2. Criar diretÃ³rio no host
    mkdir $SourcePath -ErrorAction SilentlyContinue

    # 3. Rodar container com bind mount (porta 8012)
    docker container run --mount type=bind,source=$SourcePath,target=$TargetPath `
      -d -p 8012:80 --name todo-bind diamol/ch06-todo-list

    # 4. Fazer requisiÃ§Ã£o (pode demorar um pouco para o app iniciar)
    Start-Sleep -Seconds 5 # Dar tempo para iniciar
    Invoke-WebRequest http://localhost:8012

    # 5. Listar conteÃºdo do diretÃ³rio no HOST
    Get-ChildItem -Path $SourcePath
    # VocÃª deve ver o arquivo de dados (ex: todolist.db) aqui
    ```
*   **ObservaÃ§Ã£o sobre compartilhamento:** Se `$SourcePath` estiver fora de `C:\Users`, o Docker Desktop pode pedir permissÃ£o para compartilhar o drive/diretÃ³rio na primeira vez que vocÃª usar um bind mount nele. Verifique as configuraÃ§Ãµes em Settings > Resources > File Sharing.

---

ğŸ“„ PÃ¡gina 113

ğŸ” Resumo dos principais pontos:

*   Bind mount Ã© bidirecional: mudanÃ§as no host refletem no contÃªiner e vice-versa.
*   **ImplicaÃ§Ãµes de SeguranÃ§a:**
    *   ContÃªineres geralmente devem rodar com privilÃ©gios mÃ­nimos.
    *   Bind mounts podem exigir privilÃ©gios elevados no contÃªiner para ler/escrever no host.
    *   Imagem de exemplo usa `USER root` (Linux) ou `ContainerAdministrator` (Windows) para ter direitos administrativos.
*   OpÃ§Ã£o `readonly`: Montar diretÃ³rio do host como somente leitura dentro do contÃªiner.
    *   Ãštil para injetar arquivos de configuraÃ§Ã£o do host sem permitir que o contÃªiner os modifique.
*   **ExercÃ­cio Try-it-now:** Usar bind mount `readonly` para injetar configuraÃ§Ã£o.
    *   App To-Do procura por arquivo de config extra em `/app/config` (ou `C:\app\config`).
    *   1. Navegar para `ch06/exercises/todo-list`.
    *   2. Definir variÃ¡veis `source` (pasta `config` local) e `target` (`/app/config` ou `C:\app\config`).
    *   3. Rodar contÃªiner com mount `readonly`: `docker run --name todo-configured ... --mount type=bind,source=$source,target=$target,readonly diamol/ch06-todo-list`.
    *   4. Fazer requisiÃ§Ã£o `curl`.
    *   5. Verificar logs do contÃªiner (`docker logs todo-configured`).
*   Arquivo de config no host (`config/appsettings.json` ou similar) ativa logging detalhado (Debug).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina destaca que a natureza bidirecional dos bind mounts traz consideraÃ§Ãµes de **seguranÃ§a**. Se o contÃªiner puder escrever em qualquer lugar do host mapeado, um contÃªiner comprometido poderia causar danos ao sistema host. Por isso, Ã© comum que imagens que precisam de bind mounts rodem com usuÃ¡rios com mais privilÃ©gios (como `root` ou `Administrator`), o que por si sÃ³ jÃ¡ Ã© um risco de seguranÃ§a.
*   Uma forma de mitigar o risco Ã© usar a opÃ§Ã£o `readonly` no bind mount. Isso permite que o contÃªiner *leia* arquivos do host (Ã³timo para fornecer arquivos de configuraÃ§Ã£o), mas impede que ele os *modifique*.
*   O exercÃ­cio demonstra o uso de um bind mount `readonly` para injetar um arquivo de configuraÃ§Ã£o que altera o nÃ­vel de logging da aplicaÃ§Ã£o To-Do. A aplicaÃ§Ã£o Ã© projetada para procurar por configuraÃ§Ãµes em um local especÃ­fico (`/app/config`). Montando uma pasta local do host nesse local *dentro* do contÃªiner (como `readonly`), a aplicaÃ§Ã£o lerÃ¡ o arquivo de configuraÃ§Ã£o do host, ativando o logging detalhado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Navegar para o diretÃ³rio
    cd C:\DockerLabs\diamol\ch06\exercises\todo-list

    # 2. Definir variÃ¡veis (ajuste target se necessÃ¡rio)
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config"
    $TargetPath = 'C:\app\config' # Para imagem Windows
    # $TargetPath = '/app/config' # Para imagem Linux

    # 3. Rodar container com mount readonly (porta 8013)
    docker container run --name todo-configured -d -p 8013:80 `
      --mount type=bind,source=$SourcePath,target=$TargetPath,readonly `
      diamol/ch06-todo-list

    # 4. Fazer requisiÃ§Ã£o
    Start-Sleep -Seconds 5
    Invoke-WebRequest http://localhost:8013

    # 5. Verificar logs (procurar por mensagens de nÃ­vel 'Debug' ou mais detalhadas)
    docker container logs todo-configured
    ```
*   Abra a pasta `config` local e veja o arquivo `appsettings.json` (ou similar) que define o nÃ­vel de logging como "Debug".

---

ğŸ“„ PÃ¡gina 114

ğŸ” Resumo dos principais pontos:

*   SaÃ­da esperada do exercÃ­cio anterior (Figura 6.11): Logs detalhados (nÃ­vel Debug) no output do `docker logs`.
*   Pode-se usar bind mount com qualquer fonte acessÃ­vel pelo host (ex: drive de rede montado como `/mnt/nfs` no Linux ou `X:` no Windows).
*   Ãštil para storage confiÃ¡vel ou distribuÃ­do, mas hÃ¡ limitaÃ§Ãµes.
*   **6.4 Limitations of filesystem mounts** (LimitaÃ§Ãµes das montagens de sistema de arquivos)
*   CenÃ¡rios e limitaÃ§Ãµes importantes:
    1.  **OcultaÃ§Ã£o (Shadowing):** O que acontece se montar algo em um diretÃ³rio (`target`) que *jÃ¡ existe* e tem arquivos na imagem base?
        *   IntuiÃ§Ã£o: talvez mesclasse os conteÃºdos? **NÃ£o.**
        *   Comportamento: O diretÃ³rio montado (`source`) **substitui/oculta** completamente o diretÃ³rio original da imagem no `target`. Arquivos da imagem naquele `target` ficam inacessÃ­veis.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o sucesso do exercÃ­cio anterior: o bind mount `readonly` injetou a configuraÃ§Ã£o do host, e os logs do contÃªiner refletem isso.
*   Menciona a flexibilidade de usar bind mounts com armazenamento em rede mapeado no host.
*   Inicia a seÃ§Ã£o 6.4, que aborda **limitaÃ§Ãµes e comportamentos especÃ­ficos** de montagens que podem causar surpresas.
*   **Primeira limitaÃ§Ã£o: OcultaÃ§Ã£o (Shadowing) de DiretÃ³rios.** Se a imagem Docker jÃ¡ contÃ©m um diretÃ³rio com arquivos (ex: `/app/config/default.conf`) e vocÃª faz um bind mount de um diretÃ³rio do host para `/app/config`, o conteÃºdo original de `/app/config` da imagem *desaparece* dentro do contÃªiner. O contÃªiner sÃ³ verÃ¡ o conteÃºdo vindo do host. Isso Ã© diferente de uma mesclagem; Ã© uma substituiÃ§Ã£o completa daquele ponto de montagem. Isso pode ser problemÃ¡tico se vocÃª esperava que o contÃªiner usasse uma combinaÃ§Ã£o de arquivos da imagem e do host naquele diretÃ³rio.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio demonstrarÃ¡ o efeito de ocultaÃ§Ã£o.
*   Pense no exemplo anterior: montamos a pasta `config` do host em `/app/config`. Se a imagem `diamol/ch06-todo-list` jÃ¡ tivesse algum arquivo dentro de `/app/config`, ele teria ficado inacessÃ­vel para o contÃªiner `todo-configured`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   OcultaÃ§Ã£o com Bind Mount: Ã‰ como colocar um tapete (diretÃ³rio do host) sobre uma Ã¡rea do piso (diretÃ³rio da imagem). VocÃª sÃ³ vÃª o tapete; o que estava no piso embaixo dele fica escondido.

---

ğŸ“„ PÃ¡gina 115

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Demonstrar ocultaÃ§Ã£o de diretÃ³rio com bind mount.
    *   Imagem `diamol/ch06-bind-mount` lista conteÃºdo de `/init` (Linux) ou `C:\init` (Windows).
    *   1. Rodar contÃªiner sem mount: mostra arquivos da imagem (`abc.txt`, `def.txt`).
    *   2. Definir variÃ¡veis `source` (pasta `new` local) e `target` (`/init` ou `C:\init`).
    *   3. Rodar contÃªiner *com* bind mount da pasta `new`: mostra arquivos do host (`123.txt`, `456.txt`), *nÃ£o* os da imagem.
*   Figura 6.12: Mostra a saÃ­da, confirmando a ocultaÃ§Ã£o.
*   **Segunda limitaÃ§Ã£o: Montagem de Arquivo Ãšnico.** O que acontece se montar um *arquivo* do host em um diretÃ³rio (`target`) que existe na imagem?
    *   Comportamento Linux: ConteÃºdo Ã© mesclado. VÃª-se arquivos originais da imagem + o arquivo montado do host.
    *   Comportamento Windows Containers: **NÃ£o suportado.** Feature nÃ£o implementada.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra o efeito de ocultaÃ§Ã£o (shadowing). A imagem `diamol/ch06-bind-mount` foi construÃ­da para ter arquivos (`abc.txt`, `def.txt`) no diretÃ³rio `/init`.
    *   Quando rodada sem mount, ela lista esses arquivos.
    *   Quando rodada com um bind mount que mapeia a pasta `new` do host (contendo `123.txt`, `456.txt`) para `/init`, o contÃªiner lista *apenas* `123.txt` e `456.txt`. Os arquivos `abc.txt` e `def.txt` originais da imagem foram ocultados.
*   A segunda limitaÃ§Ã£o discutida Ã© a montagem de **arquivos individuais** (em vez de diretÃ³rios) do host para dentro de um diretÃ³rio no contÃªiner.
    *   No **Linux**, isso funciona como uma "mesclagem": o contÃªiner vÃª os arquivos que jÃ¡ estavam no diretÃ³rio da imagem *mais* o arquivo que foi montado do host.
    *   No **Windows Containers**, tentar montar um arquivo individual do host para dentro de um diretÃ³rio do contÃªiner **resulta em erro**. Ã‰ uma limitaÃ§Ã£o conhecida da plataforma.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais (Demonstrar OcultaÃ§Ã£o):**
    ```powershell
    # Navegar para o diretÃ³rio
    cd C:\DockerLabs\diamol\ch06\exercises\bind-mount

    # 1. Rodar sem mount (ver abc.txt, def.txt)
    docker container run diamol/ch06-bind-mount

    # 2. Definir variÃ¡veis (ajuste target se imagem for Linux)
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "new"
    $TargetPath = 'C:\init' # Para imagem Windows
    # $TargetPath = '/init' # Para imagem Linux (Verifique a base da imagem diamol/ch06-bind-mount)

    # 3. Rodar com bind mount (ver 123.txt, 456.txt)
    docker container run --mount type=bind,source=$SourcePath,target=$TargetPath `
      diamol/ch06-bind-mount
    ```
*   **Sobre Montagem de Arquivo Ãšnico:** O prÃ³ximo exercÃ­cio na p.116 mostrarÃ¡ a diferenÃ§a de comportamento. No Windows, vocÃª esperaria um erro.

---

ğŸ“„ PÃ¡gina 116

ğŸ” Resumo dos principais pontos:

*   **ContinuaÃ§Ã£o: LimitaÃ§Ãµes de Filesystem Mounts**
*   Filesystem do contÃªiner Ã© Ã¡rea com diferenÃ§as entre Linux e Windows containers.
    *   Uso de paths estilo Linux (`/data`) funciona em Dockerfiles para ambos (vira `C:\data` no Windows).
    *   Mas *nÃ£o* funciona para source/target em volume mounts/bind mounts (precisa usar paths nativos do OS - `/data` Linux, `C:\data` Windows).
    *   LimitaÃ§Ã£o de montagem de arquivo Ãºnico no Windows.
*   **ExercÃ­cio Try-it-now:** Demonstrar diferenÃ§a de montagem de arquivo Ãºnico.
    *   Comandos separados para Linux e Windows, montando `new/123.txt` em `/init/123.txt` ou `C:\init\123.txt`.
*   Resultado esperado (Figura 6.13): Linux funciona (mostra `abc.txt`, `def.txt`, `123.txt`), Windows dÃ¡ erro.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a que, embora o Docker tente abstrair diferenÃ§as, o **sistema de arquivos** Ã© uma Ã¡rea onde o comportamento entre contÃªineres Linux e Windows pode divergir.
    *   Especificamente, ao definir `source` e `target` para volumes ou bind mounts no comando `docker run` (ou Docker Compose), vocÃª *deve* usar a sintaxe de caminho correta para o sistema operacional *do contÃªiner* (e do host, para o `source` do bind mount). Usar `/` universalmente nÃ£o funciona aqui como funciona em algumas instruÃ§Ãµes do Dockerfile.
    *   Reafirma a limitaÃ§Ã£o de montar arquivos individuais no Windows.
*   O exercÃ­cio visa comprovar experimentalmente a diferenÃ§a na montagem de arquivos Ãºnicos:
    *   O comando para Linux monta `new/123.txt` em `/init/123.txt`. Como `/init` jÃ¡ continha `abc.txt` e `def.txt` na imagem, e montagem de arquivo Ãºnico mescla no Linux, a saÃ­da esperada Ã© a listagem dos trÃªs arquivos.
    *   O comando para Windows tenta montar `new/123.txt` em `C:\init\123.txt`. Como isso nÃ£o Ã© suportado, a expectativa Ã© que o comando `docker run` falhe com um erro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais (Executar no Windows):**
    ```powershell
    # (Estando em C:\DockerLabs\diamol\ch06\exercises\bind-mount)

    # Tentar montar arquivo Ãºnico (assumindo imagem base Windows)
    $SourceFile = Join-Path -Path (Get-Location).Path -ChildPath "new\123.txt"
    $TargetFile = 'C:\init\123.txt'
    docker container run --mount type=bind,source=$SourceFile,target=$TargetFile `
      diamol/ch06-bind-mount
    ```
    *VocÃª deve receber um erro do Docker, confirmando a limitaÃ§Ã£o.*

*   **Se vocÃª estiver usando Docker Desktop no modo Linux:**
    ```powershell
    # Tentar montar arquivo Ãºnico (imagem base Linux)
    $SourceFile = "$(pwd)/new/123.txt" # Docker Desktop converte o path
    $TargetFile = '/init/123.txt'
    docker container run --mount type=bind,source=$SourceFile,target=$TargetFile `
      diamol/ch06-bind-mount
    ```
    *Neste caso (modo Linux), deve funcionar e listar `abc.txt`, `def.txt`, `123.txt`.*

---

ğŸ“„ PÃ¡gina 117

ğŸ” Resumo dos principais pontos:

*   Figura 6.13: Mostra a saÃ­da do exercÃ­cio anterior (Linux funciona, Windows falha).
*   **Terceira limitaÃ§Ã£o: Bind Mounts com Filesystems DistribuÃ­dos.**
    *   CenÃ¡rio menos comum, difÃ­cil de reproduzir facilmente.
    *   Pergunta: O que acontece ao montar um filesystem distribuÃ­do (compartilhamento SMB, Azure Files, AWS S3) como bind mount? App no contÃªiner funcionarÃ¡?
    *   Filesystems distribuÃ­dos podem *nÃ£o suportar todas as operaÃ§Ãµes* de um filesystem local padrÃ£o (ex: criar links simbÃ³licos, bloqueios especÃ­ficos).
    *   Aplicativos podem falhar se usarem operaÃ§Ãµes nÃ£o suportadas pelo storage distribuÃ­do subjacente.
*   Diagrama (Figura 6.14): Exemplo concreto - rodar banco de dados Postgres em contÃªiner na nuvem, usando Azure Files para os dados via bind mount.
    *   Postgres tenta criar um "file link" (operaÃ§Ã£o de filesystem).
    *   Azure Files nÃ£o suporta essa operaÃ§Ã£o especÃ­fica.
    *   Resultado: App (Postgres) trava/falha.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina discute uma terceira limitaÃ§Ã£o, mais sutil, dos bind mounts: quando a origem (`source`) no host nÃ£o Ã© um disco local comum, mas sim um **sistema de arquivos distribuÃ­do** (como um compartilhamento de rede Windows/SMB, um mount NFS de um servidor Linux, ou um serviÃ§o de arquivos na nuvem como Azure Files ou AWS EFS).
*   Embora vocÃª possa montar esses armazenamentos distribuÃ­dos no seu host e depois fazer um bind mount deles para dentro de um contÃªiner, existe um risco: esses sistemas de arquivos distribuÃ­dos podem **nÃ£o implementar todas as funcionalidades** que um sistema de arquivos local padrÃ£o oferece.
*   AplicaÃ§Ãµes complexas, especialmente bancos de dados como o Postgres (exemplo da Figura 6.14), podem depender de operaÃ§Ãµes de filesystem especÃ­ficas (como links simbÃ³licos, tipos especÃ­ficos de bloqueio de arquivo, etc.). Se o storage distribuÃ­do usado no bind mount nÃ£o suportar exatamente a operaÃ§Ã£o que a aplicaÃ§Ã£o tenta realizar, a aplicaÃ§Ã£o dentro do contÃªiner **falharÃ¡**, muitas vezes com erros obscuros.
*   A liÃ§Ã£o Ã©: tenha cuidado ao usar bind mounts com storage distribuÃ­do para aplicaÃ§Ãµes stateful complexas. Teste exaustivamente e esteja ciente das limitaÃ§Ãµes do sistema de armazenamento subjacente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª pode mapear um compartilhamento de rede no seu Windows (ex: `\\server\share` para `Z:`) e depois tentar usar `Z:\data` como `source` em um bind mount:
    ```powershell
    # Exemplo conceitual (requer compartilhamento SMB acessÃ­vel)
    # New-PSDrive -Name "Z" -PSProvider "FileSystem" -Root "\\server\share"
    # $SourcePath = "Z:\MinhaAppDB"
    # $TargetPath = "C:\data"
    # mkdir $SourcePath -ErrorAction SilentlyContinue
    # docker run --mount type=bind,source=$SourcePath,target=$TargetPath ... <imagem_bd>
    ```
    O sucesso ou falha dependerÃ¡ da aplicaÃ§Ã£o (`<imagem_bd>`) e das funcionalidades suportadas pelo compartilhamento SMB (`\\server\share`).

---

ğŸ“„ PÃ¡gina 118

ğŸ” Resumo dos principais pontos:

*   CenÃ¡rio de falha com storage distribuÃ­do Ã© exceÃ§Ã£o, mas importante saber. Storage de origem pode nÃ£o ter todas as features esperadas pelo app. NÃ£o dÃ¡ para planejar, sÃ³ testando.
*   Storage distribuÃ­do tambÃ©m terÃ¡ caracterÃ­sticas de **performance** diferentes (mais lento que disco local devido Ã  rede). Apps com I/O intenso podem ficar lentos.
*   **6.5 Understanding how the container filesystem is built** (Entendendo como o sistema de arquivos do contÃªiner Ã© construÃ­do)
*   RecapitulaÃ§Ã£o: storage em contÃªineres Ã© diferente de mÃ¡quinas fÃ­sicas/VMs.
*   ConsolidaÃ§Ã£o dos conceitos do capÃ­tulo.
*   ContÃªiner tem disco Ãºnico *virtual* (union filesystem).
*   Docker monta esse disco a partir de vÃ¡rias fontes.
*   ImplementaÃ§Ã£o do union filesystem varia (Docker escolhe a melhor para o SO).
*   App no contÃªiner vÃª disco Ãºnico, mas partes podem vir de locais diferentes.
*   Diagrama (Figura 6.15): Mostra o union filesystem combinando mÃºltiplas fontes (camadas da imagem, camada gravÃ¡vel, volumes, bind mounts).
*   **Diretrizes/Boas PrÃ¡ticas para OpÃ§Ãµes de Storage:**
    *   **Camada GravÃ¡vel (Writable Layer):** Boa para dados de curta duraÃ§Ã£o/transitÃ³rios (ex: cache). Ãšnica por contÃªiner, perdida com `docker rm`.
    *   **Bind Mounts Locais:** Compartilhar dados host <-> contÃªiner. Ãštil para desenvolvimento (montar cÃ³digo fonte local no contÃªiner para ver mudanÃ§as na hora).
    *   **Bind Mounts DistribuÃ­dos:** Compartilhar storage de rede com contÃªineres. Usar com cautela (performance, features). Bom para config read-only ou cache compartilhado.
    *   **Volume Mounts (Volumes Docker):** Compartilhar dados entre contÃªiner e storage gerenciado pelo Docker. **Ideal para dados persistentes** que precisam sobreviver a atualizaÃ§Ãµes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre limitaÃ§Ãµes, mencionando tambÃ©m o impacto na **performance** ao usar bind mounts com storage distribuÃ­do (rede Ã© mais lenta que disco local).
*   A seÃ§Ã£o 6.5 serve como um **resumo e consolidaÃ§Ã£o** de tudo que foi visto no CapÃ­tulo 6 sobre o filesystem do contÃªiner.
*   O conceito chave Ã© o **Union Filesystem**: o Docker cria a ilusÃ£o de um disco Ãºnico para o contÃªiner, "unindo" diferentes fontes de dados de forma transparente para a aplicaÃ§Ã£o.
*   A Figura 6.15 Ã© uma Ã³tima representaÃ§Ã£o visual disso, mostrando que o filesystem final pode ser uma combinaÃ§Ã£o de:
    *   Camadas somente leitura da imagem base.
    *   A camada gravÃ¡vel Ãºnica do contÃªiner.
    *   Um ou mais Volumes Docker montados.
    *   Um ou mais Bind Mounts (locais ou distribuÃ­dos) montados.
*   A pÃ¡gina finaliza com um guia prÃ¡tico sobre **quando usar cada tipo de armazenamento**:
    *   **Camada GravÃ¡vel:** SÃ³ para dados temporÃ¡rios que podem ser perdidos.
    *   **Bind Mount Local:** Principalmente para desenvolvimento (acesso ao cÃ³digo fonte) ou acesso a arquivos especÃ­ficos do host.
    *   **Bind Mount DistribuÃ­do:** Para acesso a storage de rede, com cuidado quanto a performance e compatibilidade.
    *   **Volume Docker:** A soluÃ§Ã£o **preferida** para dados que a aplicaÃ§Ã£o no contÃªiner precisa que **persistam** (bancos de dados, uploads de usuÃ¡rios, etc.).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revise os comandos usados no capÃ­tulo:
    *   Volumes: `docker volume create`, `docker run -v <volume>:<target>`
    *   Bind Mounts: `docker run --mount type=bind,source=<host_path>,target=<target>[,readonly]` ou `docker run -v <host_path>:<target>[:ro]`
*   Pense em um cenÃ¡rio seu:
    *   Precisa salvar dados de um banco de dados rodando em contÃªiner? Use **Volume Docker**.
    *   Quer editar cÃ³digo no VS Code e ver o resultado na hora em um servidor web rodando em contÃªiner? Use **Bind Mount Local** do seu diretÃ³rio de cÃ³digo.
    *   Precisa ler um arquivo de configuraÃ§Ã£o central que estÃ¡ em um compartilhamento de rede? Use **Bind Mount DistribuÃ­do** (read-only).

---

ğŸ“„ PÃ¡gina 119

ğŸ” Resumo dos principais pontos:

*   **Diretrizes de Storage (continuaÃ§Ã£o):**
    *   **Camadas da Imagem (Image Layers):** Apresentam o filesystem inicial. SÃ£o empilhadas; camadas posteriores podem ocultar/sobrescrever arquivos de camadas anteriores. Read-only, compartilhadas.
*   **6.6 Lab** (LaboratÃ³rio)
    *   ReÃºne os conceitos do capÃ­tulo.
    *   App: To-Do list novamente.
    *   Tarefa: Rodar o app (`diamol/ch06-lab`) que *jÃ¡ vem* com tarefas prÃ©-criadas (na imagem). Modificar a forma como ele roda para:
        1.  Fazer a lista comeÃ§ar *vazia*.
        2.  Garantir que itens salvos *persistam* em um **Volume Docker**.
    *   Dica: ExercÃ­cios do capÃ­tulo devem ajudar.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina finaliza as diretrizes de storage, lembrando que as **camadas da imagem** formam a base inicial do filesystem do contÃªiner.
*   Introduz o **Lab do CapÃ­tulo 6**. O desafio envolve usar os conceitos de volumes/mounts para alterar o comportamento de armazenamento de uma imagem existente (`diamol/ch06-lab`).
*   A imagem base do lab jÃ¡ contÃ©m dados (tarefas prÃ©-existentes). O objetivo Ã© duplo:
    1.  **Impedir que os dados da imagem sejam usados:** Fazer a aplicaÃ§Ã£o iniciar com uma lista vazia. Isso provavelmente envolverÃ¡ montar um volume ou bind mount *vazio* no local onde a aplicaÃ§Ã£o armazena seus dados, efetivamente *ocultando* os dados que vieram da imagem (lembra do "shadowing"?).
    2.  **Garantir persistÃªncia:** Certificar-se de que os *novos* itens adicionados sejam salvos em um **Volume Docker** nomeado, para que sobrevivam se o contÃªiner for recriado.
*   A chave Ã© descobrir *onde* a aplicaÃ§Ã£o `diamol/ch06-lab` armazena seus dados (provavelmente em `/data` ou similar, como nos exemplos anteriores) e montar um volume nomeado nesse local.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 6:**
    1.  **Investigar:** Rode `docker run diamol/ch06-lab` para ver as tarefas iniciais. Tente descobrir onde os dados sÃ£o armazenados (pode ser `/data` ou talvez inspecionando o Dockerfile se disponÃ­vel, ou por tentativa e erro). Assumiremos que Ã© `/data` (ou `C:\data`).
    2.  **Criar Volume:** Crie um volume nomeado: `docker volume create lab6-data`.
    3.  **Rodar com OcultaÃ§Ã£o e PersistÃªncia:** Rode o contÃªiner montando o volume nomeado no local dos dados. Isso ocultarÃ¡ os dados da imagem e garantirÃ¡ que os novos dados vÃ£o para o volume.
        ```powershell
        # Assumindo target /data para imagem Linux
        $target='/data'
        docker container run -d -p 8099:80 --name lab6-app -v lab6-data:$target diamol/ch06-lab
        ```
    4.  **Verificar:** Acesse `http://localhost:8099`. A lista deve estar *vazia*. Adicione alguns itens.
    5.  **Testar PersistÃªncia:** Remova o contÃªiner (`docker rm -f lab6-app`) e rode-o novamente com o *mesmo* comando do passo 3. Acesse `http://localhost:8099` novamente. Os itens que vocÃª adicionou devem estar lÃ¡.

---

ğŸ“„ PÃ¡gina 120

ğŸ” Resumo dos principais pontos:

*   **Dicas do Lab 6 (continuaÃ§Ã£o):**
    *   Lembrete: `docker rm -f $(docker ps -aq)` remove todos os contÃªineres.
    *   Rodar `diamol/ch06-lab` primeiro para ver as tarefas existentes.
    *   PrecisarÃ¡ rodar novamente com opÃ§Ãµes de montagem (`-v` ou `--mount`).
    *   App usa arquivo de configuraÃ§Ã£o; pode haver mais do que apenas settings de log nele (dica sutil sobre onde os dados podem ser configurados/localizados).
*   Link para soluÃ§Ã£o no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina oferece as dicas finais para o Lab 6. Relembra o comando de limpeza geral. Sugere rodar a imagem original primeiro para entender o estado inicial (com tarefas). Indica que a soluÃ§Ã£o envolverÃ¡ rodar o contÃªiner novamente com flags de montagem (`-v` ou `--mount`). A Ãºltima dica Ã© interessante: sugere que o *local* onde a aplicaÃ§Ã£o salva os dados pode ser configurÃ¡vel atravÃ©s de um arquivo de configuraÃ§Ã£o, o que poderia influenciar qual caminho (`target`) usar na montagem do volume.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   A estratÃ©gia da pÃ¡gina anterior (montar um volume nomeado em `/data` ou `C:\data`) Ã© a mais provÃ¡vel, baseada nos exemplos do capÃ­tulo. A dica sobre o arquivo de configuraÃ§Ã£o pode ser uma pista secundÃ¡ria ou uma forma alternativa de descobrir o caminho dos dados, caso `/data` nÃ£o seja o correto para a imagem `diamol/ch06-lab`.

---

ğŸ“„ PÃ¡gina 121

ğŸ” Resumo dos principais pontos:

*   **InÃ­cio da Parte 2: Running distributed applications in containers**
*   RevisÃ£o: Poucas aplicaÃ§Ãµes sÃ£o um componente Ãºnico; geralmente sÃ£o distribuÃ­das.
*   Foco da Parte 2: Usar Docker e **Docker Compose** para definir, rodar e gerenciar aplicaÃ§Ãµes com mÃºltiplos contÃªineres.
*   TÃ³picos:
    *   Definir apps multi-contÃªiner (Compose).
    *   Gerenciar apps multi-contÃªiner (Compose).
    *   Isolar workloads com redes Docker.
    *   Construir pipelines de IntegraÃ§Ã£o ContÃ­nua (CI) com Docker.
    *   Configurar mÃºltiplos ambientes.
    *   Preparar para produÃ§Ã£o: health checks e observabilidade (mÃ©tricas/logging).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina introduz a Parte 2 do livro, que move o foco de contÃªineres individuais para **aplicaÃ§Ãµes distribuÃ­das**, compostas por mÃºltiplos contÃªineres que precisam trabalhar juntos (ex: um frontend web, uma API backend, um banco de dados).
*   A ferramenta central para gerenciar essas aplicaÃ§Ãµes em um Ãºnico host Docker serÃ¡ o **Docker Compose**.
*   Os capÃ­tulos desta parte ensinarÃ£o a usar o Compose para definir a estrutura da aplicaÃ§Ã£o, configurar a comunicaÃ§Ã£o entre contÃªineres (redes), gerenciar diferentes ambientes (dev, teste), automatizar builds e testes (CI), e adicionar robustez com verificaÃ§Ãµes de saÃºde e monitoramento.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Antecipa o uso extensivo de arquivos `docker-compose.yml` e comandos `docker-compose ...` (como `up`, `down`, `build`, `ps`, `logs`, `exec`) no seu terminal PowerShell/VS Code.

---

ğŸ“„ PÃ¡gina 122

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, separando a introduÃ§Ã£o da Parte 2 do inÃ­cio do CapÃ­tulo 7.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 123

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 7: Running multi-container apps with Docker Compose** (Rodando apps multi-contÃªiner com Docker Compose)
*   RevisÃ£o: A maioria dos apps nÃ£o Ã© um componente Ãºnico. Docker Ã© ideal para apps distribuÃ­dos (n-tier, microsserviÃ§os).
*   Docker Compose: ferramenta para definir e gerenciar apps multi-contÃªiner.
*   Foco do Cap. 7: IntroduÃ§Ã£o ao Compose, revisitando apps anteriores para ver como Compose facilita o uso.
*   **7.1 The anatomy of a Docker Compose file** (A anatomia de um arquivo Docker Compose)
    *   Dockerfile empacota *um* componente. App distribuÃ­do pode ter vÃ¡rios Dockerfiles (web, API, BD).
    *   Como rodar? `docker run` para cada um Ã© manual e propenso a erros (configuraÃ§Ã£o, comunicaÃ§Ã£o).
    *   SoluÃ§Ã£o: Arquivo Docker Compose descreve a *estrutura* da aplicaÃ§Ã£o.
    *   Arquivo Compose define o *estado desejado* do app.
    *   Coloca opÃ§Ãµes do `docker run` no arquivo Compose.
    *   Ferramenta `docker-compose` lÃª o arquivo e cria/gerencia os recursos Docker (contÃªineres, redes, volumes).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 7, focado no **Docker Compose**. Ele Ã© apresentado como a soluÃ§Ã£o para gerenciar aplicaÃ§Ãµes que consistem em mÃºltiplos contÃªineres interdependentes.
*   A seÃ§Ã£o 7.1 explica a necessidade do Compose: enquanto um `Dockerfile` define como construir a imagem de *um* serviÃ§o, o arquivo `docker-compose.yml` define como *mÃºltiplos* serviÃ§os (cada um com sua imagem) devem ser executados juntos para formar a aplicaÃ§Ã£o completa.
*   Em vez de executar vÃ¡rios comandos `docker run` complexos e garantir manualmente que os contÃªineres se conectem corretamente, vocÃª descreve toda a aplicaÃ§Ã£o (serviÃ§os, redes, volumes, portas, variÃ¡veis de ambiente, etc.) em um Ãºnico arquivo `docker-compose.yml`.
*   A ferramenta de linha de comando `docker-compose` entÃ£o lÃª esse arquivo e interage com o Docker Engine para criar ou atualizar os recursos necessÃ¡rios para atingir o "estado desejado" descrito no arquivo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ o VS Code para criar e editar arquivos `docker-compose.yml`.
*   VocÃª usarÃ¡ comandos como `docker-compose up` (para iniciar a aplicaÃ§Ã£o definida no arquivo) e `docker-compose down` (para parar e remover os recursos) no terminal PowerShell/VS Code.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Dockerfile: Receita para fazer um tipo de bolo (ex: bolo de chocolate).
*   docker-compose.yml: O plano completo de uma festa de aniversÃ¡rio, especificando quantos bolos de chocolate, quantos de baunilha (outros serviÃ§os), quais bebidas (volumes), onde colocar cada coisa (redes, portas).
*   `docker-compose up`: O comando para o organizador da festa (ferramenta Compose) ler o plano e montar tudo.

---

ğŸ“„ PÃ¡gina 124

ğŸ” Resumo dos principais pontos:

*   **Listagem 7.1:** Exemplo de arquivo `docker-compose.yml` simples para o app To-Do.
    ```yaml
    version: '3.7' # VersÃ£o da especificaÃ§Ã£o Compose
    services: # Define os componentes/contÃªineres
      todo-web: # Nome do serviÃ§o (serÃ¡ o hostname na rede)
        image: diamol/ch06-todo-list # Imagem a usar
        ports: # Mapeamento de portas host:container
          - "8020:80"
        networks: # Redes a que o serviÃ§o se conecta
          - app-net
    networks: # Define as redes usadas no arquivo
      app-net: # Nome lÃ³gico da rede dentro do Compose
        external: # Indica que a rede jÃ¡ existe fora do Compose
          name: nat # Nome real da rede externa a ser usada
    ```
*   **Formato YAML:** LegÃ­vel por humanos, traduz fÃ¡cil para JSON (usado por APIs). **EspaÃ§os/IndentaÃ§Ã£o sÃ£o importantes!**
*   **Estrutura do arquivo:**
    *   `version`: Especifica a versÃ£o do formato Compose (influencia features disponÃ­veis).
    *   `services`: Lista os componentes (serviÃ§os) da aplicaÃ§Ã£o. Compose pensa em "serviÃ§os" (podem ter mÃºltiplas rÃ©plicas/contÃªineres) em vez de contÃªineres individuais.
    *   `networks`: Lista as redes Docker usadas pelos serviÃ§os.
*   **AnÃ¡lise do Exemplo:**
    *   Define 1 serviÃ§o (`todo-web`) e 1 rede (`app-net`).
    *   ServiÃ§o `todo-web`: usa imagem `diamol/ch06-todo-list`, publica porta `8020` do host para `80` do contÃªiner, conecta Ã  rede `app-net`.
    *   Rede `app-net`: Ã© definida como `external` com nome real `nat`. Isso significa que o Compose *espera* que uma rede chamada `nat` jÃ¡ exista no Docker e a usarÃ¡; ele *nÃ£o* tentarÃ¡ criar a rede `app-net`.
*   Resultado final Ã© similar a rodar `docker run -p 8020:80 --name todo-web --network nat diamol/ch06-todo-list`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina apresenta o primeiro exemplo concreto de um arquivo `docker-compose.yml` e explica sua estrutura e sintaxe.
*   **YAML:** Ã‰ um formato de dados legÃ­vel que usa indentaÃ§Ã£o (espaÃ§os, nÃ£o tabs!) para definir a estrutura hierÃ¡rquica. Ã‰ crucial prestar atenÃ§Ã£o Ã  indentaÃ§Ã£o correta.
*   **SeÃ§Ãµes Principais:**
    *   `version`: Informa ao Docker qual versÃ£o da especificaÃ§Ã£o do Compose estÃ¡ sendo usada. Diferentes versÃµes suportam diferentes funcionalidades. `'3.7'` Ã© uma versÃ£o comum e moderna.
    *   `services`: O coraÃ§Ã£o do arquivo. Define cada componente da sua aplicaÃ§Ã£o como um "serviÃ§o". Cada serviÃ§o terÃ¡ um nome (ex: `todo-web`) e suas configuraÃ§Ãµes (imagem, portas, redes, volumes, variÃ¡veis de ambiente, etc.), que sÃ£o muito parecidas com as opÃ§Ãµes do `docker run`. O nome do serviÃ§o tambÃ©m funciona como nome DNS dentro da rede Compose.
    *   `networks`, `volumes`, `configs`, `secrets`: SeÃ§Ãµes opcionais para definir ou referenciar outros recursos Docker que os serviÃ§os utilizarÃ£o.
*   **Exemplo `todo-web`:** Define um Ãºnico serviÃ§o que mapeia diretamente para um contÃªiner. A configuraÃ§Ã£o `networks` especifica que ele deve se conectar Ã  rede `app-net`.
*   **Exemplo `app-net`:** A seÃ§Ã£o `networks` define `app-net`. A opÃ§Ã£o `external: name: nat` Ã© importante: ela diz ao Compose para *nÃ£o criar* uma rede chamada `app-net`, mas sim para usar uma rede *jÃ¡ existente* no Docker chamada `nat`. Isso foi feito para garantir compatibilidade com o exemplo anterior (onde criamos a rede `nat` manualmente) e tambÃ©m funciona bem no Windows onde `nat` Ã© frequentemente a rede padrÃ£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar o arquivo:** No diretÃ³rio `C:\DockerLabs\diamol\ch07\exercises\todo-list`, crie um arquivo chamado `docker-compose.yml` usando o VS Code.
2.  **Copiar ConteÃºdo:** Cole o conteÃºdo da Listagem 7.1 no arquivo. Preste muita atenÃ§Ã£o Ã  indentaÃ§Ã£o (geralmente 2 espaÃ§os).
3.  **PrÃ©-requisito:** Certifique-se de que a rede `nat` existe (comando `docker network create nat` da p. 77, ignore erro se jÃ¡ existir).

---

ğŸ“„ PÃ¡gina 125

ğŸ” Resumo dos principais pontos:

*   Propriedades do serviÃ§o (`image`, `ports`, `networks`) mapeiam para opÃ§Ãµes do `docker run`.
*   Nome do serviÃ§o (`todo-web`) vira nome do contÃªiner e nome DNS na rede Docker.
*   DefiniÃ§Ã£o de rede `external` significa que Compose espera que a rede `nat` jÃ¡ exista.
*   Gerenciamento com `docker-compose`: ferramenta separada da CLI `docker`.
*   Comando `docker-compose up`: Inicia a aplicaÃ§Ã£o.
    *   Inspeciona arquivo Compose.
    *   Cria o que for necessÃ¡rio para atingir o estado desejado.
*   **ExercÃ­cio Try-it-now:** Criar rede `nat` (se necessÃ¡rio) e rodar o app com Compose.
    *   Comandos:
        *   `docker network create nat` (ignorar erro se existir).
        *   `cd ./ch07/exercises/todo-list`.
        *   `docker-compose up`.
*   `docker-compose` procura por `docker-compose.yml` no diretÃ³rio atual por padrÃ£o.
*   Como nenhum contÃªiner correspondente existe, Compose iniciarÃ¡ um.
*   `docker-compose up` (sem `-d`) roda em primeiro plano e agrega logs de todos os contÃªineres do app, prefixados pelo nome do serviÃ§o. Ãštil para dev/teste.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina continua explicando o arquivo Compose e introduz o comando principal para usÃ¡-lo: `docker-compose up`.
*   Este comando Ã© o equivalente Compose ao `docker run`, mas aplicado a *toda* a aplicaÃ§Ã£o definida no arquivo `docker-compose.yml`.
*   Ele lÃª o arquivo, compara o estado desejado com o que jÃ¡ existe rodando no Docker (para aquele projeto), e faz as alteraÃ§Ãµes necessÃ¡rias (criar contÃªineres, redes, volumes, etc.).
*   O exercÃ­cio instrui a executar `docker-compose up` no diretÃ³rio que contÃ©m o `docker-compose.yml` da Listagem 7.1.
*   **Importante:** Executar `docker-compose up` sem a flag `-d` (detach) faz com que a aplicaÃ§Ã£o rode em **primeiro plano (foreground)**. Isso significa que:
    1.  Seu terminal ficarÃ¡ "preso" exibindo os logs da aplicaÃ§Ã£o.
    2.  Os logs de *todos* os contÃªineres definidos no arquivo serÃ£o exibidos juntos (intercalados), com um prefixo indicando de qual serviÃ§o/contÃªiner veio cada linha. Isso Ã© extremamente Ãºtil durante o desenvolvimento para ver a interaÃ§Ã£o entre os serviÃ§os.
    3.  Para parar a aplicaÃ§Ã£o e liberar o terminal, vocÃª precisarÃ¡ pressionar `Ctrl+C`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar/Criar Rede:**
    ```powershell
    docker network create nat # Ignore erro se jÃ¡ existir
    ```
2.  **Navegar para o DiretÃ³rio:**
    ```powershell
    cd C:\DockerLabs\diamol\ch07\exercises\todo-list
    ```
3.  **Iniciar a AplicaÃ§Ã£o:**
    ```powershell
    docker-compose up
    ```
4.  **Observar SaÃ­da:** VocÃª verÃ¡ o Docker (possivelmente) baixar a imagem, depois criar a rede (ou usar a existente) e criar/iniciar o contÃªiner `todo-list_todo-web_1`. Em seguida, comeÃ§arÃ¡ a ver os logs da aplicaÃ§Ã£o To-Do sendo exibidos no terminal. O terminal ficarÃ¡ ativo mostrando esses logs.

---

ğŸ“„ PÃ¡gina 126

ğŸ” Resumo dos principais pontos:

*   Figura 7.2: Mostra a saÃ­da do `docker-compose up`, incluindo criaÃ§Ã£o de rede e contÃªiner, e logs da aplicaÃ§Ã£o.
*   ApÃ³s `docker-compose up` rodar (em foreground), pode-se acessar o app no navegador em `http://localhost:8020`.
*   Funciona igual ao Cap. 6, mas Compose oferece forma mais robusta de iniciar/definir o app.
*   Arquivo Compose serve como:
    *   DefiniÃ§Ã£o executÃ¡vel da aplicaÃ§Ã£o.
    *   DocumentaÃ§Ã£o da configuraÃ§Ã£o (imagem, porta, etc., nÃ£o precisa de README separado para isso).
*   Compose pode registrar outros recursos (volumes, secrets).
*   Exemplo atual Ã© simples (1 serviÃ§o), mas Compose brilha com apps multi-contÃªiner.
*   **7.2 Running a multi-container application with Compose** (Rodando uma aplicaÃ§Ã£o multi-contÃªiner com Compose)
    *   Relembra app "image gallery" do Cap. 4 (Go web frontend, Java API, Node.js log API).
    *   Antes, rodamos cada contÃªiner separadamente, cuidando da rede/nomes. Compose simplifica isso.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que, enquanto `docker-compose up` estÃ¡ rodando em primeiro plano, a aplicaÃ§Ã£o To-Do estÃ¡ acessÃ­vel em `http://localhost:8020`.
*   Destaca os benefÃ­cios de usar o Docker Compose mesmo para um app de um sÃ³ contÃªiner:
    *   **Robustez:** Define a configuraÃ§Ã£o de forma declarativa e reprodutÃ­vel.
    *   **DocumentaÃ§Ã£o:** O prÃ³prio arquivo `docker-compose.yml` documenta como a aplicaÃ§Ã£o deve ser configurada e executada.
*   A seÃ§Ã£o 7.2 introduz o cenÃ¡rio onde o Compose realmente se destaca: gerenciar aplicaÃ§Ãµes compostas por **mÃºltiplos serviÃ§os** interdependentes, usando como exemplo a aplicaÃ§Ã£o "Image Gallery" que foi construÃ­da no CapÃ­tulo 4. O Compose eliminarÃ¡ a necessidade de iniciar manualmente cada um dos trÃªs contÃªineres (Go, Java, Node.js) e configurar suas conexÃµes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Testar o App:** Com o `docker-compose up` ainda rodando no terminal, abra o navegador e acesse `http://localhost:8020`. Adicione alguns itens Ã  lista.
2.  **Parar o App:** Volte ao terminal onde `docker-compose up` estÃ¡ rodando e pressione `Ctrl+C`. Isso interromperÃ¡ e removerÃ¡ o contÃªiner (comportamento padrÃ£o de `up` sem `-d` seguido de `Ctrl+C`).
3.  **Preparar para PrÃ³ximo Exemplo:** VocÃª precisarÃ¡ do `docker-compose.yml` para o app Image Gallery (provavelmente em `ch07\exercises\image-of-the-day`).

---

ğŸ“„ PÃ¡gina 127

ğŸ” Resumo dos principais pontos:

*   **Listagem 7.2:** SeÃ§Ã£o `services` de um Compose file para o app Image Gallery (sem a config de rede, que Ã© igual ao exemplo anterior - usa `nat`).
    ```yaml
    services:
      accesslog: # ServiÃ§o Node.js (log API)
        image: diamol/ch04-access-log
      iotd: # ServiÃ§o Java (NASA API)
        image: diamol/ch04-image-of-the-day
        ports:
          - "80" # Publica porta 80 do container em porta ALEATÃ“RIA do host
      image-gallery: # ServiÃ§o Go (Web UI)
        image: diamol/ch04-image-gallery
        ports:
          - "8010:80" # Publica porta 80 do container na 8010 do host
        depends_on: # Define dependÃªncias de inicializaÃ§Ã£o
          - accesslog
          - iotd
    ```
*   **AnÃ¡lise da ConfiguraÃ§Ã£o dos ServiÃ§os:**
    *   `accesslog`: Mais simples, sÃ³ define a imagem. NÃ£o expÃµe portas ao host (serviÃ§o interno).
    *   `iotd`: Define imagem. `ports: - "80"` (sem porta do host) significa: publique a porta 80 do contÃªiner em uma porta *aleatÃ³ria* disponÃ­vel no host. Ãštil quando nÃ£o se importa com a porta externa ou quer evitar conflitos.
    *   `image-gallery`: Define imagem, publica porta 80 interna na porta *fixa* 8010 do host. ContÃ©m seÃ§Ã£o `depends_on`.
*   **`depends_on`:** Indica que o serviÃ§o `image-gallery` depende dos serviÃ§os `accesslog` e `iotd`. O Compose tentarÃ¡ iniciar `accesslog` e `iotd` *antes* de iniciar `image-gallery`. (Nota: Isso sÃ³ controla a *ordem de inÃ­cio* dos contÃªineres, *nÃ£o* garante que os serviÃ§os dependentes estejam *prontos* para receber conexÃµes - Cap. 8 abordarÃ¡ isso).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Listagem 7.2 mostra a definiÃ§Ã£o dos trÃªs serviÃ§os para a aplicaÃ§Ã£o Image Gallery dentro de um arquivo `docker-compose.yml`.
*   Cada serviÃ§o (`accesslog`, `iotd`, `image-gallery`) especifica a imagem Docker a ser usada.
*   **ConfiguraÃ§Ã£o de Portas:**
    *   `accesslog`: NÃ£o tem `ports`, indicando que Ã© um serviÃ§o puramente interno, nÃ£o acessÃ­vel diretamente do host.
    *   `iotd`: Usa `ports: - "80"`. Esta sintaxe (sem especificar a porta do host) instrui o Docker a mapear a porta 80 do contÃªiner para uma porta *aleatÃ³ria* livre no host. Isso Ã© Ãºtil para serviÃ§os internos que talvez precisem ser acessados ocasionalmente para depuraÃ§Ã£o, sem se preocupar em escolher uma porta fixa que possa conflitar.
    *   `image-gallery`: Usa `ports: - "8010:80"`. Mapeia a porta 80 do contÃªiner para a porta *fixa* 8010 no host. Ã‰ o ponto de entrada principal para o usuÃ¡rio acessar a aplicaÃ§Ã£o web.
*   **`depends_on`:** Esta seÃ§Ã£o no serviÃ§o `image-gallery` informa ao Compose que ele nÃ£o deve iniciar o contÃªiner `image-gallery` atÃ© que os contÃªineres para `accesslog` e `iotd` tenham sido iniciados. Ã‰ um controle bÃ¡sico de ordem de inicializaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar/Editar Arquivo:** No diretÃ³rio `C:\DockerLabs\diamol\ch07\exercises\image-of-the-day`, crie/edite `docker-compose.yml`.
2.  **Adicionar ConteÃºdo:** Combine a seÃ§Ã£o `services` da Listagem 7.2 com as seÃ§Ãµes `version` e `networks` (usando a rede externa `nat`) da Listagem 7.1. O arquivo completo deve parecer com:
    ```yaml
    version: '3.7'
    services:
      accesslog:
        image: diamol/ch04-access-log
        networks:
          - app-net
      iotd:
        image: diamol/ch04-image-of-the-day
        ports:
          - "80"
        networks:
          - app-net
      image-gallery:
        image: diamol/ch04-image-gallery
        ports:
          - "8010:80"
        depends_on:
          - accesslog
          - iotd
        networks:
          - app-net
    networks:
      app-net:
        external:
          name: nat
    ```

---

ğŸ“„ PÃ¡gina 128

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 7.3): Mostra a arquitetura da aplicaÃ§Ã£o com 3 serviÃ§os conectados Ã  mesma rede. Gerado por ferramenta `docker-compose-viz`.
*   Rodar app com `docker-compose`, mas agora em modo **detached** (`-d`).
    *   Logs ainda sÃ£o coletados, mas contÃªineres rodam em background.
    *   Terminal fica livre para usar outros comandos Compose.
*   **ExercÃ­cio Try-it-now:** Navegar para a pasta `image-of-the-day` e rodar o app em modo detached.
    *   Comandos: `cd ./ch07/exercises/image-of-the-day`, `docker-compose up --detach`. (A flag `-d` Ã© abreviaÃ§Ã£o de `--detach`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 7.3 visualiza a aplicaÃ§Ã£o que serÃ¡ iniciada: trÃªs serviÃ§os (`accesslog`, `iotd`, `image-gallery`) todos conectados Ã  rede `nat` (representada como `app-net` no Compose). O diagrama foi feito com uma ferramenta que lÃª o arquivo Compose, mostrando como ele pode servir de base para documentaÃ§Ã£o automÃ¡tica.
*   Introduz a execuÃ§Ã£o do `docker-compose up` com a flag `-d` (ou `--detach`). Diferente da execuÃ§Ã£o anterior (foreground), o modo detached inicia os contÃªineres em **segundo plano** e retorna o controle do terminal imediatamente. Os logs nÃ£o sÃ£o exibidos automaticamente, mas o Docker continua a coletÃ¡-los (podemos vÃª-los depois com `docker-compose logs`). Este Ã© o modo mais comum para rodar aplicaÃ§Ãµes que nÃ£o exigem interaÃ§Ã£o direta constante.
*   O exercÃ­cio instrui a iniciar a aplicaÃ§Ã£o Image Gallery neste modo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar (se necessÃ¡rio):**
    ```powershell
    cd C:\DockerLabs\diamol\ch07\exercises\image-of-the-day
    ```
2.  **Iniciar em Background:**
    ```powershell
    docker-compose up -d
    # ou docker-compose up --detach
    ```
3.  **Observar SaÃ­da:** O comando deve retornar rapidamente apÃ³s criar/iniciar os contÃªineres (Figura 7.4 na prÃ³xima pÃ¡gina), e o prompt do PowerShell estarÃ¡ disponÃ­vel novamente. VocÃª verÃ¡ mensagens indicando a criaÃ§Ã£o dos contÃªineres (ex: `Creating image-of-the-day_accesslog_1 ... done`).
4.  **Verificar:**
    ```powershell
    docker-compose ps
    # ou docker ps
    ```
    Isso mostrarÃ¡ os trÃªs contÃªineres (`accesslog`, `iotd`, `image-gallery`) em execuÃ§Ã£o (estado `Up`).

---

ğŸ“„ PÃ¡gina 129

ğŸ” Resumo dos principais pontos:

*   Figura 7.4: Mostra a saÃ­da do `docker-compose up -d`, indicando a ordem de criaÃ§Ã£o respeitando `depends_on`.
*   App agora rodando; acessar web UI em `http://localhost:8010`. Funciona como no Cap. 4.
*   BenefÃ­cio Compose: DefiniÃ§Ã£o clara da configuraÃ§Ã£o e comunicaÃ§Ã£o.
*   Gerenciamento da aplicaÃ§Ã£o como um todo com Compose.
*   **Escalando ServiÃ§os:** Pode-se escalar serviÃ§os stateless (como a API `iotd`) para rodar mÃºltiplos contÃªineres. Docker distribui requisiÃ§Ãµes entre eles.
*   **ExercÃ­cio Try-it-now:** Escalar o serviÃ§o `iotd` para 3 instÃ¢ncias e verificar logs.
    *   Comando para escalar: `docker-compose up -d --scale iotd=3`.
    *   Acessar/refrescar `http://localhost:8010` algumas vezes para gerar trÃ¡fego para a API.
    *   Comando para ver o Ãºltimo log de cada contÃªiner `iotd`: `docker-compose logs --tail=1 iotd`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que a aplicaÃ§Ã£o estÃ¡ rodando e acessÃ­vel.
*   Introduz o conceito de **escalar serviÃ§os** usando Docker Compose. Se um serviÃ§o Ã© stateless (nÃ£o guarda estado entre requisiÃ§Ãµes), como a API `iotd`, podemos facilmente rodar mÃºltiplas instÃ¢ncias (rÃ©plicas/contÃªineres) dele para lidar com mais carga. O Docker (atravÃ©s da rede Compose) farÃ¡ um balanceamento de carga bÃ¡sico (round-robin via DNS) entre as instÃ¢ncias.
*   O comando `docker-compose up -d --scale <nome_servico>=<numero_instancias>` Ã© usado para ajustar o nÃºmero de contÃªineres para um serviÃ§o especÃ­fico. Se o nÃºmero for maior que o atual, o Compose cria novos contÃªineres. Se for menor, ele remove os excedentes.
*   O exercÃ­cio escala o serviÃ§o `iotd` para 3 contÃªineres. Depois de gerar trÃ¡fego, o comando `docker-compose logs --tail=1 iotd` Ã© usado para ver a Ãºltima linha de log de *cada um* dos 3 contÃªineres `iotd`, permitindo verificar quais deles receberam as requisiÃ§Ãµes da UI `image-gallery`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Escalar o ServiÃ§o `iotd`:** (No mesmo diretÃ³rio do `docker-compose.yml`)
    ```powershell
    docker-compose up -d --scale iotd=3
    ```
    Observe a saÃ­da indicando a criaÃ§Ã£o de `image-of-the-day_iotd_2` e `image-of-the-day_iotd_3`.
2.  **Verificar ContÃªineres:**
    ```powershell
    docker-compose ps
    ```
    VocÃª deve ver 5 contÃªineres agora (1 accesslog, 3 iotd, 1 image-gallery).
3.  **Gerar TrÃ¡fego:** Abra `http://localhost:8010` no navegador e pressione F5 algumas vezes.
4.  **Verificar Logs:**
    ```powershell
    docker-compose logs --tail=1 iotd
    ```
    VocÃª verÃ¡ 3 linhas de log, uma de cada contÃªiner `iotd`, mostrando quais processaram as Ãºltimas requisiÃ§Ãµes (a distribuiÃ§Ã£o pode nÃ£o ser perfeitamente igual em poucos requests).

---

ğŸ“„ PÃ¡gina 130

ğŸ” Resumo dos principais pontos:

*   Figura 7.5: Mostra a saÃ­da do scaling e dos logs distribuÃ­dos.
*   Compose agora gerencia 5 contÃªineres.
*   Pode-se usar Compose para parar (`stop`) e iniciar (`start`) toda a aplicaÃ§Ã£o.
*   ContÃªineres criados pelo Compose sÃ£o contÃªineres Docker normais, gerenciÃ¡veis tambÃ©m via CLI `docker`.
*   Compose e CLI `docker` usam a mesma API Docker.
*   **ExercÃ­cio Try-it-now:** Parar e iniciar o app com Compose, depois listar contÃªineres com CLI `docker`.
    *   Comandos:
        *   `docker-compose stop`
        *   `docker-compose start`
        *   `docker container ls` (ou `docker ps`)
*   AnÃ¡lise da saÃ­da (Figura 7.6):
    *   `stop` lista contÃªineres individuais sendo parados.
    *   `start` lista serviÃ§os sendo iniciados (respeitando `depends_on`).
    *   `docker container ls` mostra que os contÃªineres *existentes* foram reiniciados (CREATED time Ã© antigo, STATUS Up Ã© recente), nÃ£o recriados.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz os comandos `docker-compose stop` e `docker-compose start` para gerenciar o ciclo de vida da aplicaÃ§Ã£o definida no arquivo Compose como um todo.
    *   `docker-compose stop`: Para todos os contÃªineres associados ao projeto, mas *nÃ£o* os remove. Eles ficam no estado `Exited`.
    *   `docker-compose start`: Reinicia os contÃªineres que foram previamente parados com `stop`.
*   Um ponto importante Ã© que os contÃªineres gerenciados pelo Compose sÃ£o contÃªineres Docker comuns. VocÃª ainda pode usar os comandos `docker ps`, `docker logs <nome_container>`, `docker exec`, etc., diretamente neles. O Compose Ã© apenas uma ferramenta de orquestraÃ§Ã£o que facilita o gerenciamento do *conjunto*.
*   O exercÃ­cio demonstra o ciclo `stop`/`start` e confirma com `docker ps` que os mesmos contÃªineres foram reutilizados (nÃ£o foram destruÃ­dos e recriados, preservando seus estados internos, se houver - embora estes serviÃ§os sejam stateless).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Parar a AplicaÃ§Ã£o:**
    ```powershell
    docker-compose stop
    ```
    Observe a saÃ­da. Verifique com `docker-compose ps` (mostrarÃ¡ estado `Exit`) ou `docker ps` (nÃ£o mostrarÃ¡ nada).
2.  **Iniciar a AplicaÃ§Ã£o:**
    ```powershell
    docker-compose start
    ```
    Observe a saÃ­da.
3.  **Listar com CLI Docker:**
    ```powershell
    docker container ls
    # ou docker ps
    ```
    Confirme que os 5 contÃªineres estÃ£o rodando (estado `Up`). Compare o tempo em `CREATED` com o tempo em `STATUS`. `CREATED` deve ser mais antigo que `STATUS`.

---

ğŸ“„ PÃ¡gina 131

ğŸ” Resumo dos principais pontos:

*   Figura 7.6: Mostra a saÃ­da dos comandos `stop`, `start`, `docker ps`.
*   Compose tem muitos outros comandos (`docker-compose --help`).
*   **ConsideraÃ§Ã£o Importante:** Docker Compose Ã© uma ferramenta **client-side**.
    *   LÃª o arquivo `.yml` local e envia instruÃ§Ãµes para a API Docker.
    *   O Docker Engine *nÃ£o* sabe que os contÃªineres formam uma "aplicaÃ§Ã£o"; sÃ³ o Compose sabe, baseado no arquivo `.yml`.
    *   Ã‰ preciso ter o arquivo `.yml` para gerenciar a aplicaÃ§Ã£o com Compose.
*   Risco de dessincronizaÃ§Ã£o: Se o estado real (contÃªineres rodando) mudar (ex: escalar manualmente com `docker run`, ou arquivo `.yml` for alterado) sem usar o Compose, o prÃ³ximo comando Compose pode ter comportamento inesperado.
*   Exemplo: Escalamos `iotd` para 3 rÃ©plicas (p. 129), mas isso *nÃ£o* foi salvo no `.yml`. Se derrubarmos e subirmos a aplicaÃ§Ã£o, ela voltarÃ¡ ao estado original do `.yml` (1 rÃ©plica para `iotd`).
*   **ExercÃ­cio Try-it-now:** Demonstrar a perda do estado de escala.
    *   Comandos:
        *   `docker-compose down` (para e *remove* contÃªineres, redes, etc., definidos no Compose).
        *   `docker-compose up -d` (recria tudo do zero, baseado no `.yml`).
        *   `docker container ls` (ou `docker-compose ps`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina destaca uma caracterÃ­stica fundamental e limitaÃ§Ã£o do Docker Compose: ele Ã© uma ferramenta que opera do **lado do cliente (client-side)**. Isso significa que:
    *   Toda a inteligÃªncia sobre a estrutura da sua aplicaÃ§Ã£o reside no arquivo `docker-compose.yml` que vocÃª tem na sua mÃ¡quina (ou no servidor de CI).
    *   Quando vocÃª executa `docker-compose up/down/stop/start/ps`, a ferramenta lÃª esse arquivo e envia os comandos apropriados para a API do Docker Engine.
    *   O Docker Engine em si apenas executa as ordens (criar contÃªiner X, parar contÃªiner Y), ele nÃ£o tem uma noÃ§Ã£o persistente de que X e Y pertencem Ã  mesma "aplicaÃ§Ã£o Compose".
*   **ImplicaÃ§Ãµes:**
    *   VocÃª precisa do arquivo `docker-compose.yml` correto para gerenciar a aplicaÃ§Ã£o consistentemente.
    *   MudanÃ§as feitas manualmente via `docker run/rm/scale` ou alteraÃ§Ãµes no arquivo `.yml` que nÃ£o sÃ£o aplicadas com `docker-compose up` podem levar a um estado onde o Compose nÃ£o sabe mais exatamente qual Ã© o estado real da aplicaÃ§Ã£o, podendo causar resultados inesperados em comandos futuros.
*   O exemplo prÃ¡tico disso Ã© a escala do serviÃ§o `iotd`. O comando `--scale iotd=3` foi uma instruÃ§Ã£o *imperativa* dada ao Compose naquele momento, mas ela nÃ£o alterou o arquivo `docker-compose.yml` *declarativo*. O arquivo ainda descreve `iotd` com 1 rÃ©plica (implicitamente).
*   O comando `docker-compose down` destrÃ³i os recursos gerenciados pelo Compose. Quando `docker-compose up -d` Ã© executado novamente, ele lÃª o arquivo `.yml` original e recria a aplicaÃ§Ã£o com apenas 1 rÃ©plica de `iotd`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Derrubar e Remover:**
    ```powershell
    docker-compose down
    ```
    Observe a saÃ­da (removing network, removing containers...). Verifique com `docker-compose ps` (deve dar erro ou nÃ£o mostrar nada).
2.  **Subir Novamente:**
    ```powershell
    docker-compose up -d
    ```
3.  **Verificar ContÃªineres:**
    ```powershell
    docker container ls
    # ou docker-compose ps
    ```
    Confirme que agora sÃ³ hÃ¡ 3 contÃªineres no total (1 `accesslog`, 1 `iotd`, 1 `image-gallery`), pois a escala de `iotd` foi perdida e voltou ao padrÃ£o do arquivo `.yml`.

---

ğŸ“„ PÃ¡gina 132

ğŸ” Resumo dos principais pontos:

*   Figura 7.7: Mostra a saÃ­da do `down`/`up`, confirmando que `iotd` voltou a ter 1 rÃ©plica.
*   Docker Compose Ã© simples e poderoso, mas requer gerenciamento cuidadoso dos arquivos `.yml`.
*   Recursos criados pelo Compose sÃ£o recursos Docker normais, mas a relaÃ§Ã£o entre eles ("aplicaÃ§Ã£o") sÃ³ existe enquanto se tem o arquivo Compose para gerenciÃ¡-los.
*   **7.3 How Docker plugs containers together** (Como Docker conecta os contÃªineres)
    *   Como os componentes (contÃªineres) se comunicam?
    *   ContÃªineres tÃªm IP virtual prÃ³prio na rede Docker.
    *   ContÃªineres na *mesma* rede Docker podem se alcanÃ§ar pelo IP.
    *   Problema: IPs mudam quando contÃªineres sÃ£o recriados.
    *   SoluÃ§Ã£o Docker: **Descoberta de ServiÃ§o (Service Discovery) via DNS interno**.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre a natureza client-side do Compose e a importÃ¢ncia de manter os arquivos `.yml` como a fonte da verdade para a definiÃ§Ã£o da aplicaÃ§Ã£o.
*   A seÃ§Ã£o 7.3 aprofunda como funciona a **comunicaÃ§Ã£o entre contÃªineres** gerenciados pelo Docker (e Compose).
*   A base Ã© a **rede virtual Docker**. ContÃªineres conectados Ã  mesma rede podem, em teoria, comunicar-se usando seus endereÃ§os IP internos (ex: `172.18.0.3`).
*   No entanto, usar IPs diretamente Ã© frÃ¡gil, pois um contÃªiner pode ser removido e recriado (durante uma atualizaÃ§Ã£o ou falha) e receber um IP *diferente*.
*   A soluÃ§Ã£o robusta que o Docker oferece Ã© um **servidor DNS embutido**. Quando um contÃªiner (`app-a`) tenta se conectar a outro usando seu **nome de serviÃ§o** (definido no `docker-compose.yml`, ex: `http://iotd/...`), a requisiÃ§Ã£o DNS Ã© interceptada pelo Docker. O DNS interno do Docker resolve o nome do serviÃ§o (`iotd`) para o endereÃ§o IP *atual* do(s) contÃªiner(es) que rodam aquele serviÃ§o naquela rede. Isso permite comunicaÃ§Ã£o estÃ¡vel mesmo que os IPs dos contÃªineres mudem.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   No Dockerfile do `image-gallery` (Listagem 4.4, p. 82), as variÃ¡veis de ambiente definem os URLs das APIs como `http://iotd/image` e `http://accesslog/access-log`. O app Go usa esses nomes. Como os trÃªs contÃªineres (`image-gallery`, `iotd`, `accesslog`) estÃ£o na mesma rede `nat` (via Compose), o DNS interno do Docker resolverÃ¡ `iotd` e `accesslog` para os IPs corretos quando o app Go tentar acessÃ¡-los.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   DNS Interno Docker: Ã‰ como ter uma lista telefÃ´nica atualizada automaticamente dentro do condomÃ­nio (rede Docker). Em vez de decorar o ramal (IP) de cada apartamento (contÃªiner), que pode mudar se o morador trocar de apartamento, vocÃª simplesmente liga para o "Apartamento do Fulano" (nome do serviÃ§o), e a central telefÃ´nica do condomÃ­nio (DNS Docker) te conecta ao ramal certo naquele momento.

---

ğŸ“„ PÃ¡gina 133

ğŸ” Resumo dos principais pontos:

*   **Funcionamento do DNS Docker:**
    *   App no contÃªiner faz lookup DNS (ex: para `blog.sixeyed.com` ou `iotd`).
    *   DNS interno Docker intercepta.
    *   Se nome Ã© de outro contÃªiner/serviÃ§o na mesma rede Docker, retorna IP(s) do(s) contÃªiner(es).
    *   Se nome nÃ£o Ã© de contÃªiner, repassa consulta para DNS do host (rede da empresa ou internet).
*   **DNS e Escala:**
    *   Se serviÃ§o roda em 1 contÃªiner, DNS retorna 1 IP.
    *   Se serviÃ§o roda em mÃºltiplos contÃªineres (escalado), DNS retorna **mÃºltiplos IPs** (um para cada contÃªiner).
*   **ExercÃ­cio Try-it-now:** Ver DNS em aÃ§Ã£o com `nslookup`.
    *   1. Escalar `iotd` para 3 rÃ©plicas: `docker-compose up -d --scale iotd=3`.
    *   2. Conectar a um terminal dentro do contÃªiner `image-gallery`.
        *   Comando Linux: `docker container exec -it image-of-the-day_image-gallery_1 sh`.
        *   Comando Windows: `docker container exec -it image-of-the-day_image-gallery_1 cmd`.
    *   3. Dentro do contÃªiner, usar `nslookup <nome_servico>` para consultar o DNS Docker.
        *   `nslookup accesslog` (deve retornar 1 IP).
    *   4. Sair do contÃªiner (`exit`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina detalha o funcionamento do DNS interno do Docker, explicando como ele resolve nomes tanto para outros contÃªineres na mesma rede quanto para endereÃ§os externos (repassando para o DNS configurado no host).
*   Um ponto importante Ã© como o DNS lida com serviÃ§os **escalados**: se um serviÃ§o (`iotd`) estÃ¡ rodando com 3 rÃ©plicas (contÃªineres), uma consulta DNS para `iotd` retornarÃ¡ os endereÃ§os IP de **todos os trÃªs** contÃªineres. Isso permite que o cliente (o contÃªiner `image-gallery`) escolha um dos IPs (ou use algum mecanismo de balanceamento) para se conectar.
*   O exercÃ­cio usa o comando `docker exec -it <container> <comando>` para executar um comando (neste caso, `sh` ou `cmd` para obter um shell) *dentro de um contÃªiner que jÃ¡ estÃ¡ rodando*. Isso Ã© diferente de `docker run -it`, que inicia um novo contÃªiner.
*   Dentro do contÃªiner `image-gallery`, a ferramenta `nslookup` (um utilitÃ¡rio padrÃ£o de consulta DNS) Ã© usada para consultar o nome do serviÃ§o `accesslog`. Como `accesslog` estÃ¡ rodando com apenas 1 rÃ©plica, a expectativa Ã© que `nslookup` retorne um Ãºnico endereÃ§o IP.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Escalar `iotd`:** (No diretÃ³rio do `docker-compose.yml`)
    ```powershell
    docker-compose up -d --scale iotd=3
    ```
2.  **Obter Nome Completo do ContÃªiner:**
    ```powershell
    docker ps # Encontre o nome completo na coluna NAMES, ex: image-of-the-day_image-gallery_1
    ```
3.  **Executar Shell Interativo no ContÃªiner:** (Substitua pelo nome real)
    ```powershell
    # Se a imagem base for Linux (provÃ¡vel):
    docker container exec -it image-of-the-day_image-gallery_1 sh
    # Se a imagem base for Windows:
    # docker container exec -it image-of-the-day_image-gallery_1 cmd
    ```
4.  **Dentro do ContÃªiner (prompt /app # ou C:\app>):**
    ```
    nslookup accesslog
    ```
    Observe a saÃ­da. Ignore a primeira mensagem de erro sobre o servidor DNS; procure pela(s) linha(s) `Address:` que mostram o IP resolvido (deve ser um da faixa 172.x.x.x). Anote o IP retornado.
5.  **Sair do ContÃªiner:**
    ```
    exit
    ```

---

ğŸ“„ PÃ¡gina 134

ğŸ” Resumo dos principais pontos:

*   Figura 7.8: Mostra a saÃ­da do `nslookup accesslog` dentro do contÃªiner, retornando 1 IP.
*   IPs sÃ£o da rede Docker (ex: 172.24.0.2).
*   DNS permite que app funcione mesmo se IP mudar apÃ³s recriaÃ§Ã£o do contÃªiner.
*   **ExercÃ­cio Try-it-now:** Verificar mudanÃ§a de IP (ou nÃ£o) e resoluÃ§Ã£o para serviÃ§o escalado.
    *   1. Remover manualmente o contÃªiner `accesslog`: `docker container rm -f image-of-the-day_accesslog_1`.
    *   2. Rodar `docker-compose up -d --scale iotd=3` novamente. Compose recriarÃ¡ `accesslog`.
    *   3. Conectar novamente ao `image-gallery` (`docker exec -it ... sh/cmd`).
    *   4. Rodar `nslookup accesslog` (verificar se IP mudou).
    *   5. Rodar `nslookup iotd` (verificar se retorna 3 IPs, um para cada rÃ©plica).
    *   6. Sair (`exit`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que o uso de nomes DNS garante a resiliÃªncia da comunicaÃ§Ã£o: mesmo que um contÃªiner seja destruÃ­do e recriado (possivelmente com um novo IP), o DNS interno do Docker sempre fornecerÃ¡ o IP *atual* quando outro contÃªiner consultar pelo nome do serviÃ§o.
*   O exercÃ­cio demonstra isso e tambÃ©m como o DNS funciona para serviÃ§os escalados:
    1.  ForÃ§a-se a recriaÃ§Ã£o do contÃªiner `accesslog` removendo-o manualmente e rodando `docker-compose up` (que detecta a falta e o recria).
    2.  Ao consultar `nslookup accesslog` novamente de dentro do `image-gallery`, o IP retornado *pode* ser diferente do anterior (depende de como o Docker aloca IPs), mas a resoluÃ§Ã£o funciona.
    3.  Ao consultar `nslookup iotd` (que estÃ¡ escalado para 3 rÃ©plicas), o DNS Docker retorna os **trÃªs** endereÃ§os IP correspondentes Ã s rÃ©plicas.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Remover `accesslog`:** (Precisa do nome completo do contÃªiner)
    ```powershell
    docker container rm -f image-of-the-day_accesslog_1
    ```
2.  **Recriar/Garantir Estado:**
    ```powershell
    docker-compose up -d --scale iotd=3
    ```
3.  **Executar Shell no `image-gallery`:** (Use o nome completo)
    ```powershell
    docker container exec -it image-of-the-day_image-gallery_1 sh
    ```
4.  **Dentro do ContÃªiner:**
    ```
    nslookup accesslog
    # Anote o IP. Mudou em relaÃ§Ã£o Ã  p.133? (Pode nÃ£o mudar em ambiente simples)

    nslookup iotd
    # Verifique se 3 IPs diferentes sÃ£o retornados na seÃ§Ã£o 'Non-authoritative answer'.
    ```
5.  **Sair:**
    ```
    exit
    ```

---

ğŸ“„ PÃ¡gina 135

ğŸ” Resumo dos principais pontos:

*   Figura 7.9: Mostra a saÃ­da do `nslookup` para `accesslog` (1 IP, talvez o mesmo de antes) e `iotd` (3 IPs).
*   Servidores DNS podem retornar mÃºltiplos IPs para um nome.
*   Docker Compose usa isso para **balanceamento de carga simples**: retorna todos os IPs das rÃ©plicas.
*   Cliente decide como usar mÃºltiplos IPs (alguns usam sÃ³ o primeiro).
*   DNS Docker retorna a lista em **ordem diferente** a cada consulta (tentativa bÃ¡sica de distribuir carga - round-robin DNS). Pode-se verificar repetindo `nslookup iotd`.
*   **ConclusÃ£o:** Compose grava opÃ§Ãµes de inicializaÃ§Ã£o, cuida da comunicaÃ§Ã£o via DNS. PrÃ³ximo: configuraÃ§Ã£o de ambiente com Compose.
*   **7.4 Application configuration in Docker Compose** (ConfiguraÃ§Ã£o de aplicaÃ§Ã£o no Docker Compose)
    *   Revisita app To-Do (Cap. 6). Pode usar SQLite (arquivo local) ou BD remoto (Postgres).
    *   Rodar BD Postgres em outro contÃªiner.
    *   Imagem To-Do pode ser configurada para usar Postgres em vez de SQLite.
    *   ConfiguraÃ§Ã£o aplicada via Docker Compose.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica o mecanismo de balanceamento de carga padrÃ£o do Docker (e Compose) quando um serviÃ§o Ã© escalado: o DNS interno retorna *todos* os IPs das rÃ©plicas, geralmente embaralhando a ordem a cada consulta (Round-Robin DNS). Cabe Ã  aplicaÃ§Ã£o cliente decidir como usar essa lista (usar o primeiro, tentar outro se falhar, etc.). Ã‰ um balanceamento simples, mas funcional para muitos casos.
*   A seÃ§Ã£o 7.4 introduz como usar o Docker Compose para gerenciar a **configuraÃ§Ã£o das aplicaÃ§Ãµes** nos contÃªineres, permitindo que a mesma imagem se comporte de maneira diferente dependendo de como Ã© iniciada.
*   O exemplo serÃ¡ novamente o aplicativo To-Do, que pode ser configurado para usar ou um banco de dados de arquivo simples (SQLite) ou um servidor de banco de dados mais robusto (PostgreSQL), que tambÃ©m rodarÃ¡ em um contÃªiner separado. O Compose serÃ¡ usado para "dizer" ao contÃªiner do To-Do qual banco de dados usar e como se conectar a ele.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo, mas prepara para usar as diretivas `environment` e `secrets` no `docker-compose.yml` para passar configuraÃ§Ãµes para os contÃªineres.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Balanceamento via DNS: Imagine pedir o telefone da pizzaria "Pizza Boa" (serviÃ§o `iotd`). A telefonista (DNS Docker) te dÃ¡ trÃªs nÃºmeros diferentes (IPs das 3 rÃ©plicas), talvez em ordem diferente a cada vez que vocÃª liga. VocÃª (contÃªiner `image-gallery`) escolhe um deles para fazer o pedido.

---

ğŸ“„ PÃ¡gina 136

ğŸ” Resumo dos principais pontos:

*   Postgres: BD relacional open source popular, roda bem em Docker.
*   Imagem To-Do segue boas prÃ¡ticas: configuraÃ§Ã£o padrÃ£o para dev (SQLite), mas permite overrides para outros ambientes (Postgres).
*   **Listagem 7.3:** SeÃ§Ã£o `services` de Compose file para rodar To-Do com Postgres.
    ```yaml
    services:
      todo-db: # ServiÃ§o do Banco de Dados Postgres
        image: diamol/postgres:11.5
        ports:
          - "5433:5432" # Publica porta do BD (opcional, para acesso externo/debug)
        networks:
          - app-net
      todo-web: # ServiÃ§o da AplicaÃ§Ã£o Web To-Do
        image: diamol/ch06-todo-list
        ports:
          - "8020:80" # Porta da aplicaÃ§Ã£o web
        environment: # Define VariÃ¡vel de Ambiente
          - Database:Provider=Postgres # Configura app para usar Postgres
        depends_on: # Depende do BD para iniciar antes
          - todo-db
        networks:
          - app-net
        secrets: # Define uso de um Secret
          - source: postgres-connection # Nome lÃ³gico do secret
            target: /app/config/secrets.json # Onde o secret aparecerÃ¡ como arquivo no container
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina apresenta a configuraÃ§Ã£o no `docker-compose.yml` (Listagem 7.3) para rodar a aplicaÃ§Ã£o To-Do conectada a um banco de dados PostgreSQL, ambos em contÃªineres separados.
*   **ServiÃ§o `todo-db`:** Simplesmente inicia um contÃªiner usando uma imagem PostgreSQL (`diamol/postgres:11.5`) e o conecta Ã  rede `app-net`. A porta 5432 (padrÃ£o do Postgres) Ã© opcionalmente publicada na porta 5433 do host para permitir acesso direto ao banco de dados de fora (ex: com uma ferramenta de cliente SQL), mas a aplicaÃ§Ã£o web se conectarÃ¡ a ele usando o nome `todo-db` atravÃ©s da rede interna `app-net`.
*   **ServiÃ§o `todo-web`:**
    *   Usa a imagem `diamol/ch06-todo-list`.
    *   **`environment:`**: Define a variÃ¡vel de ambiente `Database:Provider` com o valor `Postgres`. A aplicaÃ§Ã£o To-Do lerÃ¡ essa variÃ¡vel e saberÃ¡ que deve usar o modo PostgreSQL em vez do modo SQLite padrÃ£o.
    *   **`depends_on:`**: Garante que o contÃªiner `todo-db` seja iniciado antes do `todo-web`.
    *   **`secrets:`**: Esta Ã© uma nova diretiva. Ela instrui o Docker a pegar um "segredo" chamado `postgres-connection` (que conterÃ¡ a string de conexÃ£o do banco de dados) e tornÃ¡-lo disponÃ­vel *dentro* do contÃªiner `todo-web` como um arquivo localizado em `/app/config/secrets.json`. A aplicaÃ§Ã£o To-Do estÃ¡ programada para ler este arquivo e usar seu conteÃºdo para se conectar ao banco de dados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª precisarÃ¡ de um arquivo `docker-compose.yml` (ex: em `ch07\exercises\todo-list-postgres`) contendo a Listagem 7.3, mais as seÃ§Ãµes `version`, `networks` (usando `nat` externo) e a definiÃ§Ã£o do `secrets` (prÃ³xima pÃ¡gina).
*   O uso de `secrets` Ã© a forma recomendada para passar informaÃ§Ãµes sensÃ­veis (como senhas de banco de dados) para contÃªineres, em vez de usar variÃ¡veis de ambiente que podem ser inspecionadas mais facilmente.

---

ğŸ“„ PÃ¡gina 137

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o da anÃ¡lise da Listagem 7.3.
*   **Secrets em Cluster vs. Compose:**
    *   `secrets` sÃ£o ideais para dados sensÃ­veis (strings de conexÃ£o, chaves API, certificados).
    *   Em clusters (Swarm, Kubernetes), secrets sÃ£o armazenados de forma segura (criptografados) no banco de dados do cluster e entregues aos contÃªineres.
    *   No Docker Compose (single-host), nÃ£o hÃ¡ cluster DB. Secrets sÃ£o carregados de **arquivos locais** no host.
*   **Listagem 7.4:** SeÃ§Ã£o `secrets` no final do Compose file para carregar de arquivo.
    ```yaml
    secrets:
      postgres-connection: # Nome lÃ³gico do secret (referenciado no serviÃ§o)
        file: ./config/secrets.json # Caminho do arquivo NO HOST que contÃ©m o segredo
    ```
*   **Como funciona:** `docker-compose` lÃª o conteÃºdo do arquivo `./config/secrets.json` do host e o injeta no contÃªiner `todo-web` no caminho `/app/config/secrets.json` (definido no serviÃ§o).
*   Ã‰ similar a bind mount, mas usar a diretiva `secrets` permite migraÃ§Ã£o futura para segredos reais de cluster.
*   **BenefÃ­cio:** Permite usar a mesma imagem em diferentes ambientes, mudando a configuraÃ§Ã£o (BD, logging, etc.) via Compose (variÃ¡veis de ambiente, secrets/arquivos).
*   AplicaÃ§Ã£o se comporta igual, mas dados agora vÃ£o para contÃªiner Postgres.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como os `secrets` funcionam especificamente no contexto do **Docker Compose**. Enquanto em um cluster como Docker Swarm ou Kubernetes os segredos sÃ£o objetos gerenciados e armazenados de forma segura pelo orquestrador, no Docker Compose (que roda em uma Ãºnica mÃ¡quina), a diretiva `secrets` serve como um mecanismo para ler o conteÃºdo de um **arquivo local** na mÃ¡quina host e montÃ¡-lo como um arquivo dentro do contÃªiner, de forma similar a um bind mount, mas usando a semÃ¢ntica de "secret".
*   A **Listagem 7.4** mostra como definir a *origem* do secret `postgres-connection` referenciado no serviÃ§o `todo-web`. A chave `file:` indica o caminho relativo (`./config/secrets.json`) no host onde o Compose encontrarÃ¡ o arquivo contendo a string de conexÃ£o do Postgres.
*   **Vantagem:** Embora no Compose local isso seja apenas uma montagem de arquivo, usar a sintaxe `secrets:` torna o arquivo `docker-compose.yml` mais preparado para ser usado (com modificaÃ§Ãµes mÃ­nimas ou via overrides) em um ambiente de cluster que suporte segredos nativos.
*   O resultado final Ã© que a aplicaÃ§Ã£o To-Do (`todo-web`), ao iniciar, serÃ¡ configurada via variÃ¡vel de ambiente para usar `Postgres` e lerÃ¡ o arquivo `/app/config/secrets.json` (injetado pelo Compose a partir do arquivo local) para obter os detalhes de conexÃ£o para o contÃªiner `todo-db`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Completar o `docker-compose.yml`:** No diretÃ³rio `ch07\exercises\todo-list-postgres`, adicione a seÃ§Ã£o `secrets:` da Listagem 7.4 ao arquivo que contÃ©m a Listagem 7.3 e as seÃ§Ãµes `version`/`networks`.
2.  **Criar o Arquivo do Secret:** No mesmo diretÃ³rio, crie uma subpasta `config`. Dentro dela, crie um arquivo `secrets.json` com o seguinte conteÃºdo (string de conexÃ£o para o contÃªiner `todo-db`):
    ```json
    {
      "ConnectionStrings": {
        "ToDoDb": "Host=todo-db; Port=5432; Database=todo; Username=postgres; Include Error Detail=true"
      }
    }
    ```
    *(Nota: A imagem `diamol/postgres` provavelmente estÃ¡ configurada para aceitar conexÃµes sem senha do usuÃ¡rio `postgres` para o banco `todo` a partir da mesma rede Docker).*

---

ğŸ“„ PÃ¡gina 138

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar o app To-Do + Postgres com Compose.
    *   Navegar para `ch07/exercises/todo-list-postgres`.
    *   Arquivo `secrets.json` estÃ¡ lÃ¡.
    *   Comando Linux: `docker-compose up -d`.
    *   Comando Windows (usa `-f docker-compose-windows.yml` que ajusta paths): `docker-compose -f docker-compose-windows.yml up -d`.
    *   Verificar contÃªineres rodando: `docker-compose ps`.
*   Figura 7.10: Mostra saÃ­da do `up` e `ps`, com contÃªineres `todo-db` e `todo-web`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio instrui a iniciar a aplicaÃ§Ã£o completa (web + banco de dados Postgres) usando o arquivo `docker-compose.yml` que define ambos os serviÃ§os e a configuraÃ§Ã£o via `environment` e `secrets`.
*   Uma observaÃ§Ã£o importante Ã© a diferenÃ§a no comando para usuÃ¡rios Windows. Isso ocorre porque montar arquivos como `secrets` (ou em bind mounts/volumes) pode exigir sintaxe de caminho ligeiramente diferente ou tratamento especial no Windows. O autor forneceu um arquivo `docker-compose-windows.yml` que provavelmente ajusta o caminho na definiÃ§Ã£o do `secret` (ou talvez em volumes, se houvesse) para funcionar corretamente no Windows. A flag `-f` permite especificar um arquivo Compose alternativo ou adicional.
*   ApÃ³s rodar `docker-compose up -d`, os dois contÃªineres (`todo-db` e `todo-web`) devem estar em execuÃ§Ã£o. O comando `docker-compose ps` Ã© o equivalente a `docker ps` mas focado nos serviÃ§os definidos no arquivo Compose do projeto atual.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar para o DiretÃ³rio:**
    ```powershell
    cd C:\DockerLabs\diamol\ch07\exercises\todo-list-postgres
    ```
2.  **Verificar Arquivos:** Confirme que `docker-compose.yml`, `docker-compose-windows.yml` e a pasta `config` com `secrets.json` existem.
3.  **Iniciar a AplicaÃ§Ã£o (usando o arquivo Windows):**
    ```powershell
    docker-compose -f docker-compose-windows.yml up -d
    ```
    *Nota: Se o arquivo principal jÃ¡ estiver ajustado para Windows, talvez `docker-compose up -d` funcione diretamente.*
4.  **Verificar ServiÃ§os:**
    ```powershell
    docker-compose ps
    ```
    VocÃª deve ver dois serviÃ§os (`todo-db`, `todo-web`) no estado `Up`.

---

ğŸ“„ PÃ¡gina 139

ğŸ” Resumo dos principais pontos:

*   Figura 7.10: Mostra a saÃ­da do `up` e `ps`.
*   Acessar a aplicaÃ§Ã£o web em `http://localhost:8030`. (Porta 8030 - **CorreÃ§Ã£o**: A Listagem 7.3 na p.136 mostra porta 8020. Assumirei 8020 conforme a listagem original).
*   Funcionalidade Ã© a mesma, mas dados agora salvos no contÃªiner Postgres.
*   **VerificaÃ§Ã£o:** Usar cliente de BD (ex: Sqlectron - open source, cross-platform) para conectar ao Postgres.
    *   EndereÃ§o: `localhost:5433` (porta do host mapeada para `todo-db`).
    *   DB: `todo`, User: `postgres`, Senha: (nenhuma).
*   Figura 7.11: Mostra dados adicionados via web sendo consultados no Postgres via Sqlectron.
*   **BenefÃ­cio Chave:** SeparaÃ§Ã£o do pacote da aplicaÃ§Ã£o (imagem Docker) da configuraÃ§Ã£o de runtime (Compose file).
    *   Mesma imagem progride pelos ambientes (teste -> prod).
    *   Cada ambiente aplica sua config (via env vars, bind mounts, secrets no Compose).
    *   Garante que o binÃ¡rio/dependÃªncias testados sÃ£o os mesmos em produÃ§Ã£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s iniciar os contÃªineres, a pÃ¡gina instrui a testar a aplicaÃ§Ã£o acessando a interface web (usarei a porta 8020 conforme Listagem 7.3). A aplicaÃ§Ã£o deve funcionar como antes, mas agora, os dados inseridos estÃ£o sendo armazenados no banco de dados PostgreSQL que roda no contÃªiner `todo-db`.
*   Para comprovar isso, sugere-se conectar ao banco de dados diretamente usando uma ferramenta cliente SQL (como DBeaver, pgAdmin, ou o Sqlectron mencionado). A conexÃ£o Ã© feita para `localhost:5433` porque essa Ã© a porta do *host* que foi mapeada para a porta 5432 *dentro* do contÃªiner `todo-db`. As credenciais padrÃ£o (usuÃ¡rio `postgres`, banco `todo`, sem senha) sÃ£o usadas. A Figura 7.11 mostra que Ã© possÃ­vel ver os dados inseridos pela aplicaÃ§Ã£o web diretamente no banco de dados.
*   A pÃ¡gina conclui reforÃ§ando um dos maiores benefÃ­cios dessa abordagem: a **separaÃ§Ã£o entre a imagem da aplicaÃ§Ã£o e sua configuraÃ§Ã£o**. VocÃª constrÃ³i a imagem Docker *uma vez*. Essa mesma imagem imutÃ¡vel Ã© usada em todos os ambientes (desenvolvimento, teste, homologaÃ§Ã£o, produÃ§Ã£o). O que muda em cada ambiente Ã© apenas a *configuraÃ§Ã£o* aplicada no momento da execuÃ§Ã£o, geralmente definida nos arquivos Docker Compose (ou manifestos de orquestraÃ§Ã£o), garantindo consistÃªncia e reduzindo erros de implantaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Testar App:** Abra o navegador em `http://localhost:8020`. Adicione alguns itens.
2.  **(Opcional) Conectar ao BD:**
    *   Baixe e instale um cliente Postgres como DBeaver (https://dbeaver.io/) ou use a extensÃ£o PostgreSQL no VS Code.
    *   Crie uma nova conexÃ£o:
        *   Host: `localhost`
        *   Port: `5433`
        *   Database: `todo`
        *   Username: `postgres`
        *   Password: (deixe em branco)
    *   Conecte-se e execute uma query SQL: `SELECT * FROM public."ToDos";` VocÃª deverÃ¡ ver os itens que adicionou via web.

---

ğŸ“„ PÃ¡gina 140

ğŸ” Resumo dos principais pontos:

*   Figura 7.11: Mostra consulta ao BD Postgres.
*   **7.5 Understanding the problem Docker Compose solves** (Entendendo o problema que Docker Compose resolve)
*   Compose Ã© forma elegante de descrever apps complexos em arquivo simples (YAML).
*   Arquivo Compose = guia de deploy executÃ¡vel (melhor que docs manuais desatualizados).
*   Ãštil na toolkit Docker, mas entender suas limitaÃ§Ãµes Ã© importante.
*   **O que Compose faz:** Define app, aplica definiÃ§Ã£o a *um Ãºnico* host Docker. Compara estado desejado (YAML) com estado real (host), envia comandos API para criar/atualizar recursos.
*   **O que Compose NÃƒO faz (LimitaÃ§Ãµes):**
    *   NÃ£o Ã© plataforma completa como Docker Swarm ou Kubernetes.
    *   NÃ£o monitora continuamente o estado desejado.
    *   Se contÃªineres falham/sÃ£o removidos manualmente, Compose *nÃ£o* os reinicia/substitui automaticamente (sÃ³ se rodar `docker-compose up` de novo).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A seÃ§Ã£o 7.5 recapitula o valor do Docker Compose: ele fornece uma maneira **declarativa**, **legÃ­vel** e **executÃ¡vel** de definir aplicaÃ§Ãµes multi-contÃªiner, servindo tanto como ferramenta de implantaÃ§Ã£o quanto como documentaÃ§Ã£o viva.
*   No entanto, Ã© crucial entender suas **limitaÃ§Ãµes**, principalmente ao comparÃ¡-lo com orquestradores de cluster como Docker Swarm ou Kubernetes:
    *   **Escopo:** Compose Ã© projetado para gerenciar contÃªineres em um **Ãºnico host Docker**. Orquestradores gerenciam contÃªineres em **mÃºltiplos hosts (cluster)**.
    *   **Natureza:** Compose Ã© uma ferramenta **client-side** que aplica um estado desejado *uma vez* (quando vocÃª roda `up`). Ele **nÃ£o Ã© um controlador ativo** que fica monitorando e corrigindo desvios continuamente.
    *   **Auto-recuperaÃ§Ã£o (Self-healing):** Se um contÃªiner gerenciado pelo Compose falhar ou for parado por outra razÃ£o, o Compose *nÃ£o farÃ¡ nada* para reiniciÃ¡-lo ou substituÃ­-lo automaticamente. VocÃª precisaria intervir e rodar `docker-compose up` novamente. Orquestradores, por outro lado, monitoram ativamente e tentam manter o estado desejado, substituindo contÃªineres falhos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Demonstrar LimitaÃ§Ã£o:**
    ```powershell
    # Certifique-se que o app todo-list-postgres estÃ¡ rodando (via docker-compose ps)
    # Pare manualmente um dos containers
    docker stop todo-list-postgres_todo-web_1
    # Verifique o estado com Compose
    docker-compose ps
    # VocÃª verÃ¡ que o todo-web estÃ¡ 'Exit' ou 'Stopped'. O Compose nÃ£o o reiniciou.
    # Para trazÃª-lo de volta:
    docker-compose up -d # Ou docker-compose start todo-web
    ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Docker Compose: Ã‰ como um arquiteto que entrega a planta da casa (YAML) e supervisiona a construÃ§Ã£o inicial (`up`). Depois que a casa estÃ¡ pronta, ele vai embora. Se uma parede cair (contÃªiner falhar), ele nÃ£o volta para consertar sozinho.
*   Orquestrador (Swarm/K8s): Ã‰ como um sÃ­ndico que tem a planta (YAML), supervisiona a construÃ§Ã£o, e *continua morando no prÃ©dio*, monitorando tudo. Se uma lÃ¢mpada queima (contÃªiner falha), o sÃ­ndico automaticamente troca a lÃ¢mpada para manter tudo funcionando conforme a planta.

---

ğŸ“„ PÃ¡gina 141

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 7.12): Mostra onde Compose se encaixa no ciclo de vida: principalmente Desenvolvimento e Teste/CI, mas *pode* ser usado em ProduÃ§Ã£o simples.
*   **Compose em ProduÃ§Ã£o:**
    *   NÃ£o Ã© ideal, mas pode ser ponto de partida se migrando de VMs individuais para contÃªineres.
    *   NÃ£o oferece alta disponibilidade, balanceamento real, failover (como Swarm/K8s).
    *   Mas *oferece* consistÃªncia de artefatos (Dockerfiles, Compose files) e ferramentas entre ambientes.
    *   Pode ser suficiente antes de adotar um cluster.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 7.12 posiciona o Docker Compose no ciclo de vida de uma aplicaÃ§Ã£o. Seu ponto forte Ã© nos ambientes de **Desenvolvimento** e **Teste/IntegraÃ§Ã£o ContÃ­nua (CI)**, onde a facilidade de definir e rodar rapidamente aplicaÃ§Ãµes multi-contÃªiner em uma Ãºnica mÃ¡quina Ã© uma grande vantagem.
*   A pÃ¡gina discute a viabilidade de usar Compose em **ProduÃ§Ã£o**. Embora *nÃ£o* seja a soluÃ§Ã£o ideal para aplicaÃ§Ãµes que exigem alta disponibilidade, escalabilidade real e recuperaÃ§Ã£o automÃ¡tica de falhas (para isso servem Swarm e Kubernetes), ele *pode* ser um passo inicial vÃ¡lido em certos cenÃ¡rios:
    *   Se vocÃª estÃ¡ apenas comeÃ§ando com contÃªineres e migrando aplicaÃ§Ãµes que antes rodavam em VMs isoladas (que tambÃ©m nÃ£o tinham alta disponibilidade intrÃ­nseca).
    *   O benefÃ­cio principal seria padronizar o processo de deploy e gerenciamento usando Docker e Compose, mesmo que rodando em um Ãºnico servidor de produÃ§Ã£o por aplicaÃ§Ã£o.
*   A mensagem principal Ã©: Compose Ã© Ã³timo para dev/teste, e um ponto de partida *possÃ­vel* (mas limitado) para produÃ§Ã£o, antes de avanÃ§ar para orquestradores de cluster.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª estÃ¡ usando o Compose exatamente como descrito para dev/teste no seu Docker Desktop. Todos os `docker-compose up/down` que vocÃª rodou simulam esse fluxo.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Usar Compose em ProduÃ§Ã£o (simples): Ã‰ como usar a planta da casa para construir uma Ãºnica casa prÃ©-fabricada em um terreno. Funciona, mas se a casa pegar fogo, nÃ£o hÃ¡ outra.
*   Usar Orquestrador em ProduÃ§Ã£o: Ã‰ como usar a planta para construir um condomÃ­nio com vÃ¡rias casas idÃªnticas, seguranÃ§a e equipe de manutenÃ§Ã£o. Se uma casa tem problema, os moradores podem usar outra, e a equipe conserta a que falhou.

---

ğŸ“„ PÃ¡gina 142

ğŸ” Resumo dos principais pontos:

*   **7.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Adicionar **confiabilidade** Ã  execuÃ§Ã£o do app To-Do web usando features do Compose.
    *   CenÃ¡rio: Ambiente de teste.
*   **Requisitos:**
    1.  ContÃªineres da aplicaÃ§Ã£o (`todo-db`, `todo-web`) devem reiniciar automaticamente se mÃ¡quina/Docker reiniciar.
    2.  ContÃªiner do banco de dados (`todo-db`) deve usar **bind mount** para armazenar arquivos, permitindo `down`/`up` sem perder dados.
    3.  AplicaÃ§Ã£o web (`todo-web`) deve ouvir na porta **80 padrÃ£o** do host para o ambiente de teste.
*   **Dica:** Consultar a especificaÃ§Ã£o do arquivo Docker Compose na documentaÃ§Ã£o oficial (link fornecido) para encontrar as configuraÃ§Ãµes necessÃ¡rias.
*   SoluÃ§Ã£o de exemplo no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O **Lab do CapÃ­tulo 7** foca em usar configuraÃ§Ãµes do Docker Compose para tornar a execuÃ§Ã£o da aplicaÃ§Ã£o To-Do (com banco de dados Postgres) mais robusta para um ambiente de teste.
*   Os requisitos sÃ£o:
    1.  **ReinÃ­cio AutomÃ¡tico:** Garantir que os contÃªineres voltem a rodar se o Docker ou a mÃ¡quina forem reiniciados. Isso geralmente Ã© feito com a diretiva `restart:` (ex: `restart: always` ou `restart: unless-stopped`) na definiÃ§Ã£o do serviÃ§o no `docker-compose.yml`.
    2.  **PersistÃªncia com Bind Mount:** Em vez de usar um Volume Docker (gerenciado pelo Docker) para os dados do Postgres, o lab pede para usar um **Bind Mount**, mapeando um diretÃ³rio do *host* para o local onde o Postgres armazena seus dados dentro do contÃªiner (geralmente `/var/lib/postgresql/data`). Isso garante que os dados fiquem diretamente no host e persistam entre `docker-compose down` e `up`. (Precisa encontrar o caminho de dados do Postgres e usar a sintaxe `volumes:` com caminho do host).
    3.  **Porta PadrÃ£o:** Configurar o serviÃ§o `todo-web` para mapear sua porta interna 80 para a porta **80** do host (`ports: - "80:80"`), em vez de portas altas como 8020.
*   A dica aponta para a documentaÃ§Ã£o oficial, que Ã© a fonte definitiva para todas as opÃ§Ãµes de configuraÃ§Ã£o do Compose.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 7:**
    1.  Comece com o arquivo `docker-compose-windows.yml` (ou `.yml` principal) de `ch07\exercises\todo-list-postgres`.
    2.  **ReinÃ­cio:** Adicione `restart: unless-stopped` a ambos os serviÃ§os (`todo-db` e `todo-web`).
    3.  **Bind Mount para BD:**
        *   Encontre o caminho de dados do Postgres dentro do contÃªiner (geralmente `/var/lib/postgresql/data`).
        *   Crie uma pasta no host (ex: `C:\DockerLabs\diamol\ch07\lab\pgdata`).
        *   No serviÃ§o `todo-db`, substitua a possÃ­vel seÃ§Ã£o `volumes:` existente (se houver) ou adicione uma nova, usando a sintaxe de bind mount:
          ```yaml
          services:
            todo-db:
              # ... outras configs ...
              restart: unless-stopped
              volumes:
                - C:\DockerLabs\diamol\ch07\lab\pgdata:/var/lib/postgresql/data # Bind mount
          ```
          *(Nota: Use `\` ou `/` consistentes no caminho do host, ou use variÃ¡veis como no exercÃ­cio da p.112)*.
    4.  **Porta Web:** No serviÃ§o `todo-web`, mude a seÃ§Ã£o `ports:` para:
        ```yaml
          ports:
            - "80:80"
        ```
    5.  Salve o arquivo como `docker-compose.yml` (ou outro nome) na pasta `ch07\lab`.
    6.  **Teste:** Navegue para a pasta `ch07\lab` e rode `docker-compose up -d`. Acesse `http://localhost` (porta 80). Adicione dados. Rode `docker-compose down`, depois `docker-compose up -d` novamente. Verifique se os dados ainda estÃ£o lÃ¡ (acessando `http://localhost`). Verifique se a pasta `pgdata` no host contÃ©m arquivos.

---

ğŸ“„ PÃ¡gina 143

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 8: Supporting reliability with health checks and dependency checks** (Suportando confiabilidade com verificaÃ§Ãµes de saÃºde e dependÃªncia)
*   Jornada para produÃ§Ã£o: jÃ¡ vimos empacotamento, execuÃ§Ã£o e Compose.
*   ProduÃ§Ã£o usa orquestradores (Swarm, K8s) que tÃªm features para **auto-recuperaÃ§Ã£o (self-healing)**.
*   Plataforma usa informaÃ§Ãµes fornecidas pela imagem para checar se app estÃ¡ saudÃ¡vel.
*   Se app falha, plataforma pode remover contÃªiner doente e substituÃ­-lo.
*   Foco do Cap. 8: Como empacotar essas **verificaÃ§Ãµes (checks)** nas imagens para habilitar auto-recuperaÃ§Ã£o.
*   **8.1 Building health checks into Docker images** (Construindo verificaÃ§Ãµes de saÃºde nas imagens Docker)
    *   Docker jÃ¡ monitora o bÃ¡sico: o processo principal do contÃªiner.
    *   Se processo principal morre, contÃªiner entra em `Exited`.
    *   Plataforma pode reiniciar/substituir contÃªiner `Exited`.
    *   **LimitaÃ§Ã£o:** Checagem bÃ¡sica sÃ³ garante que o *processo* estÃ¡ rodando, nÃ£o que o *app* estÃ¡ respondendo corretamente.
    *   Exemplo: Web app travado retornando erro 503, mas processo rodando -> Docker acha que estÃ¡ saudÃ¡vel.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 8, focado em aumentar a **confiabilidade** das aplicaÃ§Ãµes em contÃªineres, especialmente em ambientes orquestrados (produÃ§Ã£o).
*   A ideia central Ã© que orquestradores como Docker Swarm e Kubernetes podem automaticamente detectar e corrigir problemas (ex: substituir um contÃªiner que nÃ£o estÃ¡ respondendo), criando aplicaÃ§Ãµes "auto-reparÃ¡veis" (self-healing). Mas, para isso, o orquestrador precisa saber *como* verificar se a aplicaÃ§Ã£o dentro do contÃªiner estÃ¡ realmente funcionando bem.
*   A seÃ§Ã£o 8.1 comeÃ§a explicando o monitoramento padrÃ£o e sua limitaÃ§Ã£o: o Docker, por si sÃ³, apenas verifica se o processo principal que ele iniciou dentro do contÃªiner ainda estÃ¡ ativo. Se o processo morrer, o contÃªiner para. Isso Ã© Ãºtil, mas insuficiente. Uma aplicaÃ§Ã£o pode estar com o processo rodando, mas completamente travada ou respondendo com erros para todas as requisiÃ§Ãµes. Para o Docker, ela pareceria "saudÃ¡vel".

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Pense em um contÃªiner rodando um servidor web. Se o processo do servidor web (`nginx.exe`, `httpd.exe`, `dotnet run`, etc.) travar e fechar, `docker ps` mostrarÃ¡ o contÃªiner como `Exited`.
*   No entanto, se o servidor web continuar rodando mas, devido a um bug ou sobrecarga, comeÃ§ar a retornar apenas pÃ¡ginas de erro (HTTP 500 ou 503), `docker ps` ainda mostrarÃ¡ o contÃªiner como `Up`, pois o processo principal estÃ¡ ativo. O prÃ³ximo passo Ã© ensinar o Docker a detectar essa segunda situaÃ§Ã£o.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Monitoramento bÃ¡sico do Docker: Ã‰ como verificar se o funcionÃ¡rio (processo) bateu o ponto e estÃ¡ na empresa. NÃ£o garante que ele esteja realmente trabalhando ou atendendo bem os clientes (aplicaÃ§Ã£o saudÃ¡vel).

---

ğŸ“„ PÃ¡gina 144

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Demonstrar a limitaÃ§Ã£o do monitoramento padrÃ£o.
    *   App: API REST simples (`diamol/ch08-numbers-api`) que retorna nÃºmero aleatÃ³rio.
    *   Bug: ApÃ³s 3 chamadas, API entra em estado de falha e sempre retorna erro 500.
    *   1. Rodar contÃªiner da API: `docker run -d -p 8080:80 diamol/ch08-numbers-api`.
    *   2. Chamar API 3 vezes via `curl http://localhost:8080/rng` (funciona).
    *   3. Chamar 4Âª vez (falha, retorna 500).
    *   4. Verificar status do contÃªiner: `docker container ls`.
*   Resultado esperado (Figura 8.1): ApÃ³s a 4Âª chamada, API falha, mas `docker ls` ainda mostra o contÃªiner como `Up`. Docker nÃ£o sabe que o app interno falhou.
*   **SoluÃ§Ã£o: InstruÃ§Ã£o `HEALTHCHECK` no Dockerfile.**
    *   Permite definir um comando que o Docker executa *dentro* do contÃªiner periodicamente para verificar a saÃºde *real* do app.
    *   Comando deve retornar cÃ³digo de status: 0 para sucesso (saudÃ¡vel), diferente de 0 para falha.
    *   Docker roda o comando em intervalos. Se falhar mÃºltiplas vezes seguidas, marca contÃªiner como `unhealthy`.
*   **Listagem 8.1:** Trecho de Dockerfile com `HEALTHCHECK`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENTRYPOINT ["dotnet", "/app/Numbers.Api.dll"]
    # Define o comando de verificaÃ§Ã£o de saÃºde:
    HEALTHCHECK CMD curl --fail http://localhost/health
    WORKDIR /app
    COPY --from=builder /out/ .
    ```
*   ExplicaÃ§Ã£o do `HEALTHCHECK`:
    *   Usa `curl --fail` para chamar um endpoint `/health` dentro do prÃ³prio contÃªiner.
    *   Endpoint `/health` (implementado no app) retorna status 200 (OK) se app estÃ¡ bom, 500 (Erro) se bug foi ativado.
    *   `--fail` faz `curl` retornar cÃ³digo de saÃ­da 0 se HTTP status for 2xx, e diferente de 0 caso contrÃ¡rio.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio prÃ¡tico comprova a limitaÃ§Ã£o do monitoramento padrÃ£o: a API falha internamente, mas o contÃªiner continua `Up` para o Docker.
*   A soluÃ§Ã£o Ã© a instruÃ§Ã£o `HEALTHCHECK` no Dockerfile. Ela permite que *vocÃª* defina um teste mais inteligente para verificar a saÃºde da *sua aplicaÃ§Ã£o*.
*   **Como funciona:**
    1.  VocÃª adiciona `HEALTHCHECK CMD <comando_de_teste>` ao seu Dockerfile.
    2.  O `<comando_de_teste>` Ã© algo que roda *dentro* do contÃªiner e verifica se a aplicaÃ§Ã£o estÃ¡ respondendo corretamente. Pode ser chamar um endpoint HTTP especial (`/health`, `/ping`), executar uma query no banco de dados, verificar um arquivo, etc.
    3.  O comando deve terminar com um **cÃ³digo de saÃ­da**: `0` significa "saudÃ¡vel", qualquer outro valor significa "nÃ£o saudÃ¡vel".
    4.  O Docker Engine executa esse comando periodicamente (configurÃ¡vel).
    5.  Se o comando falha por um nÃºmero configurÃ¡vel de vezes consecutivas, o Docker muda o status do contÃªiner para `unhealthy`.
*   A Listagem 8.1 mostra um `HEALTHCHECK` que usa `curl` para verificar um endpoint `/health` da prÃ³pria aplicaÃ§Ã£o. Se o endpoint responder OK (HTTP 200), `curl --fail` retorna 0. Se responder com erro (HTTP 500), `curl --fail` retorna um cÃ³digo diferente de 0.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais (Demonstrar problema):**
    ```powershell
    # 1. Rodar API v1 (sem healthcheck)
    docker container run -d -p 8080:80 --name api-v1 diamol/ch08-numbers-api

    # 2. Chamar 3 vezes (OK)
    Invoke-WebRequest http://localhost:8080/rng
    Invoke-WebRequest http://localhost:8080/rng
    Invoke-WebRequest http://localhost:8080/rng

    # 3. Chamar 4Âª vez (Erro 500)
    Invoke-WebRequest http://localhost:8080/rng -UseBasicParsing # -UseBasicParsing evita erro no PowerShell em caso de HTTP 500

    # 4. Verificar status (ainda 'Up')
    docker ps
    ```
*   **Analisar `HEALTHCHECK`:** Veja a Listagem 8.1. O comando `curl --fail http://localhost/health` serÃ¡ executado *pelo Docker Engine*, dentro do namespace do contÃªiner, em intervalos regulares.

---

ğŸ“„ PÃ¡gina 145

ğŸ” Resumo dos principais pontos:

*   Resto do Dockerfile (Listagem 8.1) Ã© padrÃ£o (.NET Core app). `ENTRYPOINT` roda `dotnet`, que Ã© o processo monitorado pelo Docker para status `Up`/`Exited`.
*   `curl --fail` passa o status HTTP como cÃ³digo de saÃ­da para o Docker (0=sucesso, !=0=falha).
*   **Construindo imagem com `HEALTHCHECK`:**
    *   Exemplo usa estrutura diferente: Dockerfile (`Dockerfile.v2`) em pasta separada do cÃ³digo fonte.
    *   NecessÃ¡rio usar flag `-f` no `docker build` para especificar caminho do Dockerfile.
*   **ExercÃ­cio Try-it-now:** Construir a imagem v2 com health check.
    *   Navegar para `ch08/exercises/numbers`.
    *   Comando: `docker image build -t diamol/ch08-numbers-api:v2 -f ./numbers-api/Dockerfile.v2 .`
    *   `.` no final ainda Ã© o contexto (pasta `numbers`), `-f` especifica o Dockerfile a usar dentro desse contexto ou subpastas.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a anÃ¡lise do Dockerfile com `HEALTHCHECK`, explicando como o `curl --fail` traduz o status HTTP da aplicaÃ§Ã£o em um cÃ³digo de saÃ­da que o Docker entende.
*   Introduz uma variaÃ§Ã£o no comando `docker build`: a flag `-f` (ou `--file`). Ela Ã© usada quando o arquivo Dockerfile nÃ£o se chama `Dockerfile` ou nÃ£o estÃ¡ na raiz do diretÃ³rio de contexto. No exemplo, o arquivo se chama `Dockerfile.v2` e estÃ¡ em uma subpasta (`numbers-api`), enquanto o contexto (`.`) Ã© a pasta pai (`numbers`, que contÃ©m o cÃ³digo fonte a ser copiado).
*   O exercÃ­cio instrui a construir a imagem `v2` da API, que inclui a instruÃ§Ã£o `HEALTHCHECK`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal:**
    1.  No PowerShell/terminal, navegue para:
        ```powershell
        cd C:\DockerLabs\diamol\ch08\exercises\numbers
        ```
    2.  Execute o comando de build especificando o Dockerfile v2:
        ```powershell
        docker image build -t diamol/ch08-numbers-api:v2 -f ./numbers-api/Dockerfile.v2 .
        ```
    3.  Observe a saÃ­da do build. SerÃ¡ similar aos builds anteriores, mas a imagem resultante `diamol/ch08-numbers-api:v2` agora tem a capacidade de health check embutida.

---

ğŸ“„ PÃ¡gina 146

ğŸ” Resumo dos principais pontos:

*   Imagem v2 construÃ­da, pronta para rodar com health check.
*   ConfiguraÃ§Ã£o do `HEALTHCHECK` (intervalo, tentativas) pode ser ajustada.
    *   PadrÃµes: checa a cada 30s, marca como `unhealthy` apÃ³s 3 falhas seguidas.
*   **ExercÃ­cio Try-it-now:** Rodar e testar o contÃªiner v2 com health check.
    *   1. Rodar contÃªiner v2: `docker run -d -p 8081:80 diamol/ch08-numbers-api:v2`.
    *   2. Esperar ~30s e listar contÃªineres: `docker container ls`. Status deve incluir `(healthy)`.
    *   3. Chamar API 4 vezes via `curl` (3 OK, 1 Falha).
    *   4. Esperar ~90s (3x 30s) e listar contÃªineres de novo: `docker container ls`. Status deve mudar para `(unhealthy)`.
*   Figura 8.2: Mostra a saÃ­da com status mudando de `healthy` para `unhealthy`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Agora que a imagem `v2` com `HEALTHCHECK` foi construÃ­da, esta pÃ¡gina mostra como ela se comporta em tempo de execuÃ§Ã£o.
*   O Docker Engine automaticamente detecta a instruÃ§Ã£o `HEALTHCHECK` na imagem e comeÃ§a a executar o comando de teste nos intervalos padrÃ£o (a cada 30 segundos).
*   O comando `docker ps` (ou `docker container ls`) agora exibe o estado de saÃºde entre parÃªnteses no final da coluna `STATUS`:
    *   `(starting)`: O contÃªiner iniciou, mas o primeiro health check ainda nÃ£o rodou ou ainda nÃ£o passou no `start_period` (ver p. 157).
    *   `(healthy)`: O Ãºltimo health check foi bem-sucedido.
    *   `(unhealthy)`: O health check falhou o nÃºmero configurado de vezes consecutivas.
*   O exercÃ­cio demonstra exatamente isso:
    *   Ao iniciar o contÃªiner `v2`, ele comeÃ§a como `(healthy)`.
    *   ApÃ³s causar o bug na API (chamando-a 4 vezes), os health checks subsequentes (que rodam a cada 30s) comeÃ§arÃ£o a falhar.
    *   ApÃ³s 3 falhas consecutivas (aproximadamente 90 segundos depois de causar o bug), o status do contÃªiner em `docker ps` mudarÃ¡ para `(unhealthy)`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Remover ContÃªiner v1 (se ainda existir):**
    ```powershell
    docker rm -f api-v1
    ```
2.  **Rodar API v2:**
    ```powershell
    docker container run -d -p 8081:80 --name api-v2 diamol/ch08-numbers-api:v2
    ```
3.  **Verificar Status Inicial:** Espere uns 30 segundos.
    ```powershell
    docker ps
    # Procure por api-v2. STATUS deve terminar com '(healthy)'
    ```
4.  **Causar o Bug:**
    ```powershell
    Invoke-WebRequest http://localhost:8081/rng # Repetir 4 vezes
    ```
5.  **Verificar Status Final:** Espere cerca de 90 segundos (ou um pouco mais).
    ```powershell
    docker ps
    # Procure por api-v2. STATUS agora deve terminar com '(unhealthy)'
    ```

---

ğŸ“„ PÃ¡gina 147

ğŸ” Resumo dos principais pontos:

*   Figura 8.2: Mostra a mudanÃ§a de status para `unhealthy`.
*   Status `unhealthy` Ã© publicado como evento pela API Docker -> plataforma orquestradora Ã© notificada e pode agir.
*   Docker tambÃ©m grava resultado dos Ãºltimos health checks. Pode ser visto com `docker inspect`.
*   **ExercÃ­cio Try-it-now:** Inspecionar o contÃªiner `unhealthy` para ver detalhes do health check.
    *   Comando usa `docker ls --last 1 ...` para pegar ID do Ãºltimo contÃªiner (o `v2`).
    *   Comando: `docker container inspect $(docker container ls --last 1 --format '{{.ID}}')`.
*   SaÃ­da do `inspect`: Procurar pela seÃ§Ã£o `State.Health`.
    *   ContÃ©m `Status` (healthy/unhealthy), `FailingStreak` (nÃºmero de falhas seguidas), e `Log` (histÃ³rico das Ãºltimas verificaÃ§Ãµes com saÃ­da/erro).
*   Figura 8.3: Mostra trecho do `inspect` com detalhes do health check falhando (HTTP 500).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica o que acontece quando um contÃªiner Ã© marcado como `unhealthy`. O Docker Engine nÃ£o apenas muda o status visÃ­vel em `docker ps`, mas tambÃ©m dispara um evento interno e armazena informaÃ§Ãµes detalhadas sobre as falhas.
*   Essas informaÃ§Ãµes podem ser acessadas usando `docker container inspect <container>`. Dentro do grande JSON de saÃ­da, a seÃ§Ã£o `State.Health` contÃ©m o histÃ³rico recente das execuÃ§Ãµes do comando `HEALTHCHECK`, incluindo o cÃ³digo de saÃ­da e a saÃ­da de texto (stdout/stderr) do comando de teste. Isso Ã© muito Ãºtil para diagnosticar por que um health check estÃ¡ falhando.
*   O exercÃ­cio mostra como usar `docker inspect` para ver esses detalhes no contÃªiner `api-v2` que ficou `unhealthy`. A sintaxe `$(docker container ls --last 1 --format '{{.ID}}')` Ã© um truque para obter automaticamente o ID do Ãºltimo contÃªiner iniciado, evitando ter que copiÃ¡-lo manualmente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Inspecionar ContÃªiner:** (Certifique-se que `api-v2` foi o Ãºltimo iniciado e estÃ¡ `unhealthy`)
    ```powershell
    docker container inspect $(docker container ls --last 1 --format '{{.ID}}')
    ```
2.  **Analisar SaÃ­da:** Role a longa saÃ­da JSON atÃ© encontrar a chave `"State"` e dentro dela, a chave `"Health"`. Examine os campos:
    *   `"Status"`: Deve ser `"unhealthy"`.
    *   `"FailingStreak"`: Deve ser 3 ou mais.
    *   `"Log"`: SerÃ¡ um array mostrando as Ãºltimas execuÃ§Ãµes do `curl`. As mais recentes devem indicar um cÃ³digo de saÃ­da diferente de 0 e talvez uma mensagem de erro HTTP 500 na saÃ­da (Output).

---

ğŸ“„ PÃ¡gina 148

ğŸ” Resumo dos principais pontos:

*   Figura 8.3: Mostra detalhes do health check falho no `inspect`.
*   **Pergunta:** Por que o Docker *nÃ£o* reinicia ou substitui o contÃªiner `unhealthy` automaticamente?
*   **Resposta:** Em um *Ãºnico* servidor, Docker Engine nÃ£o pode fazer isso com seguranÃ§a.
    *   Reiniciar contÃªiner = downtime do app.
    *   Recriar contÃªiner = downtime + **perda de dados** (se app escreveu na camada gravÃ¡vel).
    *   Docker nÃ£o sabe se a aÃ§Ã£o corretiva pioraria a situaÃ§Ã£o.
*   **Comportamento PadrÃ£o:** Docker apenas *notifica* (via status e eventos) que estÃ¡ `unhealthy`, mas o deixa rodando.
*   Health check continua rodando; se falha for temporÃ¡ria e check passar depois, status volta para `healthy`.
*   **BenefÃ­cio Real em Cluster (Swarm/Kubernetes):**
    *   Orquestrador Ã© notificado sobre `unhealthy`.
    *   Pode agir com seguranÃ§a: iniciar contÃªiner de substituiÃ§Ã£o *antes* de parar o doente (se houver capacidade no cluster).
    *   Evita downtime da aplicaÃ§Ã£o.
*   **8.2 Starting containers with dependency checks** (Iniciando contÃªineres com verificaÃ§Ãµes de dependÃªncia)
    *   Health check: teste *contÃ­nuo* para manter app rodando.
    *   Clusters lidam com falhas temporÃ¡rias iniciando novos contÃªineres.
    *   **Novo Desafio em Cluster:** Ordem de inicializaÃ§Ã£o nÃ£o Ã© garantida. ContÃªiner pode iniciar *antes* de suas dependÃªncias (outros contÃªineres/serviÃ§os) estarem prontas.
    *   Exemplo: App Web (`numbers-web`) depende da API (`numbers-api`). Web pode iniciar antes da API.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina responde a uma pergunta importante: se o Docker sabe que o contÃªiner estÃ¡ `unhealthy`, por que ele nÃ£o o conserta (reinicia/substitui)? A razÃ£o Ã© que, rodando em uma **Ãºnica mÃ¡quina**, o Docker Engine nÃ£o tem contexto suficiente para fazer isso sem potencialmente causar mais problemas (interrupÃ§Ã£o do serviÃ§o ou perda de dados). Ele adota uma abordagem conservadora: sinaliza o problema, mas nÃ£o age.
*   O verdadeiro poder do `HEALTHCHECK` se manifesta em um **ambiente orquestrado (cluster)**. O orquestrador (Swarm ou Kubernetes) *pode* tomar aÃ§Ãµes corretivas com seguranÃ§a, como iniciar uma rÃ©plica substituta em outro nÃ³ do cluster antes de remover a rÃ©plica doente, garantindo a continuidade do serviÃ§o.
*   A seÃ§Ã£o 8.2 introduz um problema relacionado, tambÃ©m mais pronunciado em clusters: **dependÃªncias de inicializaÃ§Ã£o**. Em um cluster, vocÃª nÃ£o pode garantir que o contÃªiner da sua API estarÃ¡ pronto antes do contÃªiner da sua aplicaÃ§Ã£o web que a consome. O que acontece se a web iniciar primeiro?

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo, mas a explicaÃ§Ã£o sobre a limitaÃ§Ã£o do Docker Engine vs. Orquestrador Ã© fundamental. O `HEALTHCHECK` que vocÃª configurou Ã© Ãºtil mesmo no Docker Desktop (para saber o estado), mas a *aÃ§Ã£o automÃ¡tica* sÃ³ ocorreria em um Swarm ou Kubernetes.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Docker Engine (sozinho) vendo `unhealthy`: Ã‰ como um mÃ©dico de plantÃ£o que diagnostica um paciente gravemente doente em casa, mas nÃ£o tem recursos para operar ali mesmo. Ele apenas informa a condiÃ§Ã£o.
*   Orquestrador (cluster) vendo `unhealthy`: Ã‰ como um hospital. O mÃ©dico diagnostica, e o hospital tem recursos (outras salas, mÃ©dicos, enfermeiros) para mover o paciente, operar, e garantir que o atendimento continue sem interrupÃ§Ã£o.

---

ğŸ“„ PÃ¡gina 149

ğŸ” Resumo dos principais pontos:

*   O que acontece se app iniciar antes de suas dependÃªncias depende do app. App `numbers-web` nÃ£o lida bem.
*   **ExercÃ­cio Try-it-now:** Demonstrar falha por dependÃªncia ausente.
    *   1. Remover todos os contÃªineres rodando: `docker container rm -f $(docker container ls -aq)`.
    *   2. Rodar *apenas* o contÃªiner web (`numbers-web`): `docker run -d -p 8082:80 diamol/ch08-numbers-web`.
    *   3. Verificar status: `docker container ls` (estarÃ¡ `Up`).
    *   4. Acessar `http://localhost:8082` no navegador. App carrega, mas botÃ£o "random number" darÃ¡ erro (Figura 8.4).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que o comportamento de uma aplicaÃ§Ã£o quando suas dependÃªncias nÃ£o estÃ£o disponÃ­veis varia. Algumas podem ter lÃ³gica interna para esperar ou tentar reconectar, mas muitas (como o exemplo `numbers-web`) simplesmente assumem que as dependÃªncias estarÃ£o lÃ¡ e falham quando tentam usÃ¡-las.
*   O exercÃ­cio demonstra isso:
    1.  Garante-se que nenhum contÃªiner (especialmente a API `numbers-api`) esteja rodando.
    2.  Inicia-se *apenas* o contÃªiner da aplicaÃ§Ã£o web (`numbers-web`).
    3.  O contÃªiner inicia normalmente (o processo web server estÃ¡ rodando), entÃ£o `docker ps` o mostra como `Up`.
    4.  No entanto, ao tentar usar a funcionalidade principal da aplicaÃ§Ã£o (clicar no botÃ£o que depende da API), ocorre um erro porque a API nÃ£o estÃ¡ disponÃ­vel para ser contatada.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Limpar containers anteriores (incluindo api-v2)
    docker container rm -f $(docker container ls -aq)

    # 2. Rodar SÃ“ o web app (sem a API)
    docker container run -d -p 8082:80 --name web-no-api diamol/ch08-numbers-web

    # 3. Verificar status (estarÃ¡ 'Up')
    docker ps

    # 4. Testar no Navegador
    # Abra http://localhost:8082. A pÃ¡gina carrega.
    # Clique no botÃ£o "Get Random Number >>". VocÃª verÃ¡ uma mensagem de erro.
    ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Ã‰ como tentar usar seu telefone (`web-no-api`) para ligar para um amigo (`numbers-api`) quando a torre de celular do amigo estÃ¡ desligada. Seu telefone funciona (`Up`), mas a chamada (`clicar no botÃ£o`) falha.

---

ğŸ“„ PÃ¡gina 150

ğŸ” Resumo dos principais pontos:

*   Figura 8.4: Mostra o erro na UI web quando a API estÃ¡ indisponÃ­vel.
*   SituaÃ§Ã£o indesejada: ContÃªiner `Up`, mas app inutilizÃ¡vel.
*   Alguns apps verificam dependÃªncias na inicializaÃ§Ã£o, mas muitos nÃ£o (como `numbers-web`).
*   **SoluÃ§Ã£o: Adicionar verificaÃ§Ã£o de dependÃªncia *na inicializaÃ§Ã£o* do contÃªiner.**
    *   Diferente de health check (contÃ­nuo), dependency check roda *antes* do app iniciar.
    *   Garante que tudo que o app precisa estÃ¡ disponÃ­vel.
    *   Se OK -> app inicia.
    *   Se Falha -> check falha -> contÃªiner *sai* (`Exited`).
*   Docker nÃ£o tem instruÃ§Ã£o como `HEALTHCHECK` para isso, mas pode-se colocar a lÃ³gica no **comando de inicializaÃ§Ã£o** (`CMD` ou `ENTRYPOINT`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina critica a situaÃ§Ã£o onde um contÃªiner parece saudÃ¡vel (`Up`) mas a aplicaÃ§Ã£o estÃ¡ quebrada devido a uma dependÃªncia ausente.
*   A soluÃ§Ã£o proposta Ã© implementar uma **VerificaÃ§Ã£o de DependÃªncia (Dependency Check)** que rode *antes* da aplicaÃ§Ã£o principal ser iniciada dentro do contÃªiner.
*   **Fluxo do Dependency Check:**
    1.  O comando de inicializaÃ§Ã£o do contÃªiner (`CMD` ou `ENTRYPOINT`) primeiro executa o script/comando de verificaÃ§Ã£o.
    2.  Esse script tenta contatar/verificar todas as dependÃªncias essenciais (outros serviÃ§os, bancos de dados, etc.).
    3.  Se todas as dependÃªncias responderem OK, o script termina com sucesso (cÃ³digo 0).
    4.  Se alguma dependÃªncia falhar, o script termina com erro (cÃ³digo diferente de 0).
    5.  O comando de inicializaÃ§Ã£o Ã© encadeado (ex: com `&&`) para que a aplicaÃ§Ã£o principal *sÃ³* seja iniciada se o script de verificaÃ§Ã£o terminar com sucesso (cÃ³digo 0). Se o script falhar, a aplicaÃ§Ã£o principal nunca inicia, e o contÃªiner termina (entra em `Exited`).
*   Isso implementa um padrÃ£o "fail-fast" (falhar rapidamente).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   A prÃ³xima pÃ¡gina mostrarÃ¡ como implementar isso no `CMD` do Dockerfile usando `curl` e `&&`.
*   Em PowerShell, o encadeamento `&&` nÃ£o existe nativamente da mesma forma que no `cmd` ou `sh`. Dentro de um `CMD` de Dockerfile Windows, pode-se usar `&&`. Se fosse um script PowerShell, seria mais comum usar tratamento de erro explÃ­cito (`try/catch` ou checar `$?` ou `$LASTEXITCODE`).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Dependency Check: Ã‰ como um piloto fazendo o checklist prÃ©-voo *antes* de ligar os motores principais. Se algum item crÃ­tico falhar no checklist (ex: combustÃ­vel baixo, asa faltando), o piloto nÃ£o decola (contÃªiner sai). Se tudo estiver OK, ele prossegue para a decolagem (inicia o app).

---

ğŸ“„ PÃ¡gina 151

ğŸ” Resumo dos principais pontos:

*   **Listagem 8.2:** EstÃ¡gio final de um Dockerfile (`Dockerfile.v2`) para `numbers-web` com dependency check no `CMD`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENV RngApi:Url=http://numbers-api/rng # App usa esta ENV para saber URL da API
    # CMD agora tem duas partes, ligadas por &&
    CMD curl --fail http://numbers-api/rng && \ # 1. Verifica se API responde
        dotnet Numbers.Web.dll # 2. SÃ“ SE curl OK, roda o app .NET
    WORKDIR /app
    COPY --from=builder /out/ .
    ```
*   **AnÃ¡lise do `CMD`:**
    *   Usa `curl --fail` para fazer uma requisiÃ§Ã£o Ã  API no nome DNS `numbers-api`.
    *   `&&` (operador E lÃ³gico de shell): Executa o segundo comando *somente se* o primeiro (`curl`) for bem-sucedido (retornar cÃ³digo 0).
    *   Se `curl` falhar (API indisponÃ­vel -> HTTP erro -> `curl --fail` retorna != 0), o comando `dotnet` *nÃ£o Ã© executado*, `CMD` termina, contÃªiner sai.
    *   Se `curl` sucesso (API disponÃ­vel -> HTTP 200 -> `curl --fail` retorna 0), o comando `dotnet` Ã© executado, iniciando a aplicaÃ§Ã£o web. Docker entÃ£o monitora o processo `dotnet`.
*   **ExercÃ­cio Try-it-now:** Rodar contÃªiner da imagem v2 (`diamol/ch08-numbers-web:v2`). API ainda nÃ£o estÃ¡ rodando.
    *   Comandos:
        *   `docker container run -d -p 8084:80 diamol/ch08-numbers-web:v2`
        *   `docker container ls --all` (verificar que saiu).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Listagem 8.2 mostra a implementaÃ§Ã£o prÃ¡tica do dependency check diretamente na instruÃ§Ã£o `CMD` do Dockerfile para a aplicaÃ§Ã£o web.
*   O `CMD` agora consiste em dois comandos unidos pelo operador `&&`:
    1.  `curl --fail http://numbers-api/rng`: Tenta acessar a API. Se a API estiver rodando e responder OK, este comando termina com sucesso (cÃ³digo 0). Se a API nÃ£o estiver acessÃ­vel ou retornar erro, o comando falha (cÃ³digo != 0).
    2.  `dotnet Numbers.Web.dll`: O comando para iniciar a aplicaÃ§Ã£o web .NET.
*   Devido ao `&&`, o `dotnet` sÃ³ serÃ¡ executado se o `curl` for bem-sucedido. Isso garante que a aplicaÃ§Ã£o web sÃ³ inicie se sua dependÃªncia (a API) estiver respondendo.
*   O exercÃ­cio pede para rodar um contÃªiner desta nova imagem `v2` *sem* ter a API rodando. A expectativa Ã© que o `curl` dentro do `CMD` falhe, o `dotnet` nÃ£o seja executado, e o contÃªiner termine quase imediatamente, aparecendo como `Exited` no `docker ps -a`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **PrÃ©-requisito:** Construir a imagem v2 do web app (similar Ã  p.145 para a API):
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch08\exercises\numbers
    docker image build -t diamol/ch08-numbers-web:v2 -f ./numbers-web/Dockerfile.v2 .
    ```
*   **AÃ§Ã£o principal (Testar Dependency Check):**
    ```powershell
    # Garantir que API NÃƒO estÃ¡ rodando (se api-v2 ainda existe, pare/remova)
    # docker stop api-v2 && docker rm api-v2

    # Rodar web app v2
    docker container run -d -p 8084:80 --name web-v2-fail diamol/ch08-numbers-web:v2

    # Esperar alguns segundos e verificar TODOS os containers
    Start-Sleep -Seconds 5
    docker container ls --all
    ```
    Procure pelo contÃªiner `web-v2-fail`. O STATUS deve ser `Exited (...)` com um cÃ³digo de erro (nÃ£o 0). O tempo em `STATUS` (ex: `Exited (1) 4 seconds ago`) serÃ¡ muito curto.

---

ğŸ“„ PÃ¡gina 152

ğŸ” Resumo dos principais pontos:

*   Figura 8.5: Mostra a saÃ­da do exercÃ­cio anterior, com contÃªiner v2 `Exited` rapidamente, enquanto v1 (sem check) continua `Up` mas quebrado.
*   **Fail-fast Ã© melhor:** ContÃªiner `Exited` Ã© preferÃ­vel a `Up` mas nÃ£o funcional.
*   Em ambientes escalados (cluster), contÃªiner `Exited` pode ser automaticamente substituÃ­do/reagendado pela plataforma.
    *   Ex: API demora a iniciar. Web inicia -> check falha -> web sai -> web Ã© reagendado -> na segunda tentativa, API estÃ¡ pronta -> web inicia com sucesso.
*   Com health e dependency checks, app torna-se "bom cidadÃ£o" na plataforma.
*   Checks atÃ© agora foram bÃ¡sicos (HTTP com `curl`).
    *   Simplista, depende de ferramenta externa (`curl`).
*   **8.3 Writing custom utilities for application check logic** (Escrevendo utilitÃ¡rios customizados para lÃ³gica de verificaÃ§Ã£o da aplicaÃ§Ã£o)
    *   `curl` Ã© Ãºtil, mas adiciona dependÃªncia extra na imagem que o app nÃ£o precisa para rodar.
    *   Viola princÃ­pio de imagem mÃ­nima (Cap. 4). Aumenta tamanho, superfÃ­cie de ataque, frequÃªncia de updates da imagem base.
    *   **Melhor abordagem:** Escrever utilitÃ¡rio de verificaÃ§Ã£o na *mesma linguagem* do app (Java para app Java, etc.).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina argumenta que o comportamento "fail-fast" (falhar e sair rapidamente) do contÃªiner `v2` Ã©, na verdade, mais desejÃ¡vel do que o contÃªiner `v1` que fica rodando (`Up`) mas estÃ¡ funcionalmente quebrado. Em um ambiente orquestrado, a plataforma pode detectar o contÃªiner `Exited` e tentar reiniciÃ¡-lo ou criar um substituto, potencialmente resolvendo o problema se a dependÃªncia estiver disponÃ­vel na segunda tentativa.
*   No entanto, depender de uma ferramenta externa como `curl` para os checks nÃ£o Ã© ideal por razÃµes de otimizaÃ§Ã£o e seguranÃ§a (princÃ­pio da imagem mÃ­nima).
*   A seÃ§Ã£o 8.3 propÃµe uma soluÃ§Ã£o mais robusta: criar pequenos **utilitÃ¡rios customizados**, escritos na mesma linguagem da aplicaÃ§Ã£o principal, para realizar as verificaÃ§Ãµes de saÃºde e dependÃªncia.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. A ideia Ã© que, em vez de `HEALTHCHECK CMD curl ...` ou `CMD curl ... && dotnet ...`, vocÃª teria algo como:
    *   `HEALTHCHECK CMD ["dotnet", "MeuHealthCheckUtil.dll", "--url", "http://localhost/health"]`
    *   `CMD dotnet MeuDependencyCheckUtil.dll --url http://numbers-api/rng && dotnet MinhaAppPrincipal.dll`
    *   Onde `MeuHealthCheckUtil.dll` e `MeuDependencyCheckUtil.dll` seriam pequenos programas .NET compilados e incluÃ­dos na imagem junto com a aplicaÃ§Ã£o principal.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Usar `curl`: Ã‰ como pedir ao carteiro (ferramenta externa) para verificar se a loja (dependÃªncia) estÃ¡ aberta antes de vocÃª ir lÃ¡.
*   Usar utilitÃ¡rio customizado: Ã‰ como vocÃª mesmo ter um pequeno app no seu telefone (utilitÃ¡rio na mesma linguagem) que verifica o status da loja online antes de vocÃª sair de casa. VocÃª nÃ£o depende do carteiro e usa a mesma tecnologia que jÃ¡ domina.

---

ğŸ“„ PÃ¡gina 153

ğŸ” Resumo dos principais pontos:

*   **Vantagens de UtilitÃ¡rios Customizados:**
    1.  **Menos DependÃªncias na Imagem:** NÃ£o precisa instalar `curl` ou outras ferramentas; utilitÃ¡rio usa runtime jÃ¡ presente para o app.
    2.  **LÃ³gica Complexa:** Permite verificaÃ§Ãµes mais sofisticadas (retentativas, condicionais) difÃ­ceis em scripts shell, especialmente cross-platform (Linux/Windows).
    3.  **Reuso de Config:** UtilitÃ¡rio pode ler a *mesma* configuraÃ§Ã£o do app (ex: URLs de dependÃªncias), evitando duplicaÃ§Ã£o e dessincronizaÃ§Ã£o.
    4.  **Testes Abrangentes:** Pode verificar conexÃµes de BD, existÃªncia de arquivos (certificados, etc.) usando as mesmas bibliotecas do app.
*   UtilitÃ¡rios podem ser genÃ©ricos (ex: um checador HTTP reutilizÃ¡vel).
*   Autor criou utilitÃ¡rio HTTP em .NET Core para usar nos exemplos.
*   **Build com UtilitÃ¡rios:** Dockerfile multi-estÃ¡gio pode ter:
    *   EstÃ¡gio para compilar app.
    *   EstÃ¡gio para compilar utilitÃ¡rio.
    *   EstÃ¡gio final copia ambos (app + utilitÃ¡rio).
*   Diagrama (Figura 8.6): Ilustra fluxo multi-estÃ¡gio com compilaÃ§Ã£o separada do app e do utilitÃ¡rio.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina lista as vantagens significativas de escrever seus prÃ³prios utilitÃ¡rios para health/dependency checks:
    *   **Imagens Menores/Mais Seguras:** VocÃª nÃ£o precisa adicionar ferramentas extras como `curl` Ã  sua imagem final. O utilitÃ¡rio usa o mesmo runtime (.NET, Java, Node.js) que sua aplicaÃ§Ã£o jÃ¡ requer.
    *   **Checks Mais Inteligentes:** Ã‰ mais fÃ¡cil escrever lÃ³gica complexa (ex: "tente conectar Ã  API 3 vezes com intervalo de 5 segundos antes de falhar") em C#, Java, Python, etc., do que em um script shell, especialmente se precisar funcionar em Windows e Linux.
    *   **ConfiguraÃ§Ã£o Consistente:** O utilitÃ¡rio pode ler as mesmas variÃ¡veis de ambiente ou arquivos de configuraÃ§Ã£o que a aplicaÃ§Ã£o principal usa para descobrir os endereÃ§os das dependÃªncias. Isso evita ter que definir a mesma URL em dois lugares (no check e no app).
    *   **VerificaÃ§Ãµes Profundas:** O utilitÃ¡rio pode ir alÃ©m de um simples ping HTTP e usar as bibliotecas da linguagem para, por exemplo, tentar autenticar no banco de dados ou verificar a validade de um certificado.
*   O autor menciona que criou um utilitÃ¡rio genÃ©rico de checagem HTTP em .NET Core para os prÃ³ximos exemplos.
*   A Figura 8.6 mostra como integrar a compilaÃ§Ã£o desse utilitÃ¡rio no build multi-estÃ¡gio: um estÃ¡gio compila o app principal, outro compila o utilitÃ¡rio, e o estÃ¡gio final copia ambos para a imagem de runtime.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª nÃ£o precisarÃ¡ escrever o utilitÃ¡rio, ele jÃ¡ estarÃ¡ compilado e serÃ¡ copiado nos Dockerfiles `v3` dos exemplos.
*   O Dockerfile `v3` (prÃ³xima pÃ¡gina) seguirÃ¡ o padrÃ£o da Figura 8.6.

---

ğŸ“„ PÃ¡gina 154

ğŸ” Resumo dos principais pontos:

*   **Listagem 8.3:** EstÃ¡gio final do Dockerfile `v3` para a API (`Dockerfile.v3`), usando utilitÃ¡rio customizado (`Utilities.HttpCheck.dll`) no `HEALTHCHECK`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENTRYPOINT ["dotnet", "Numbers.Api.dll"]
    # HEALTHCHECK agora chama o utilitÃ¡rio .NET
    HEALTHCHECK CMD ["dotnet", "Utilities.HttpCheck.dll", "-u", "http://localhost/health"]
    WORKDIR /app
    COPY --from=http-check-builder /out/ . # Copia utilitÃ¡rio compilado
    COPY --from=builder /out/ . # Copia app compilado
    ```
*   Comportamento similar ao `curl`, mas sem precisar dele na imagem. Logs de health check no `inspect` serÃ£o menos verbosos.
*   App ainda deve reportar `healthy` inicialmente, depois `unhealthy` apÃ³s bug ser ativado.
*   **ExercÃ­cio Try-it-now:** Rodar e testar API v3 com utilitÃ¡rio customizado.
    *   1. Remover contÃªineres existentes.
    *   2. Rodar contÃªiner v3, especificando intervalo de health check menor (`--health-interval 5s`).
        *   `docker run -d -p 8080:80 --health-interval 5s diamol/ch08-numbers-api:v3`.
    *   3. Esperar ~5s, verificar `docker ps` (deve estar `healthy`).
    *   4. Chamar API 4x via `curl`.
    *   5. Esperar ~15s (3x 5s), verificar `docker ps` de novo (deve estar `unhealthy`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Listagem 8.3 mostra como a instruÃ§Ã£o `HEALTHCHECK` Ã© modificada para usar o utilitÃ¡rio .NET customizado (`Utilities.HttpCheck.dll`) em vez do `curl`. O comando agora executa o runtime `dotnet` com o DLL do utilitÃ¡rio e passa a URL a ser verificada (`-u ...`). Note tambÃ©m as duas instruÃ§Ãµes `COPY --from=...` no final, trazendo os resultados dos estÃ¡gios de build do app e do utilitÃ¡rio para a imagem final.
*   O comportamento esperado Ã© o mesmo da versÃ£o com `curl`, mas a imagem final nÃ£o contÃ©m mais a dependÃªncia do `curl`.
*   O exercÃ­cio instrui a rodar a imagem `v3` da API. A flag `--health-interval 5s` Ã© adicionada ao `docker run` para *sobrescrever* o intervalo padrÃ£o (30s) e fazer os health checks rodarem a cada 5 segundos, acelerando o teste. (Outras opÃ§Ãµes como `--health-retries`, `--health-timeout`, `--health-start-period` tambÃ©m podem ser usadas no `docker run` para customizar o health check em tempo de execuÃ§Ã£o). O teste segue a mesma lÃ³gica: verificar se fica `healthy`, causar o bug, verificar se fica `unhealthy`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **PrÃ©-requisito:** Construir a imagem v3 da API (similar Ã  p.145, mas usando `Dockerfile.v3`):
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch08\exercises\numbers
    docker image build -t diamol/ch08-numbers-api:v3 -f ./numbers-api/Dockerfile.v3 .
    ```
*   **AÃ§Ãµes principais:**
    ```powershell
    # 1. Limpar containers
    docker container rm -f $(docker container ls -aq)

    # 2. Rodar API v3 com intervalo customizado
    docker container run -d -p 8080:80 --health-interval 5s --name api-v3 diamol/ch08-numbers-api:v3

    # 3. Verificar status inicial (apÃ³s ~5s)
    Start-Sleep -Seconds 5
    docker ps # Deve mostrar api-v3 (healthy)

    # 4. Causar bug (chamar 4x)
    Invoke-WebRequest http://localhost:8080/rng # Repetir 4 vezes

    # 5. Verificar status final (apÃ³s ~15s)
    Start-Sleep -Seconds 15
    docker ps # Deve mostrar api-v3 (unhealthy)
    ```

---

ğŸ“„ PÃ¡gina 155

ğŸ” Resumo dos principais pontos:

*   Figura 8.7: Mostra a saÃ­da do exercÃ­cio anterior, com v3 mudando para `unhealthy`.
*   UtilitÃ¡rio HTTP Ã© flexÃ­vel (muitas opÃ§Ãµes). Usado tambÃ©m para dependency check no `Dockerfile.v3` do web app.
*   **Listagem 8.4:** EstÃ¡gio final do Dockerfile v3 para `numbers-web`, usando utilitÃ¡rio no `CMD`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENV RngApi:Url=http://numbers-api/rng
    # CMD usa utilitÃ¡rio para checar API antes de rodar app
    CMD dotnet Utilities.HttpCheck.dll -c RngApi:Url -t 900 && \
        dotnet Numbers.Web.dll
    WORKDIR /app
    COPY --from=http-check-builder /out/ . # Copia utilitÃ¡rio
    COPY --from=builder /out/ . # Copia app
    ```
*   **AnÃ¡lise do `CMD`:**
    *   Chama `Utilities.HttpCheck.dll`.
    *   `-c RngApi:Url`: Diz ao utilitÃ¡rio para ler o URL da API (`http://numbers-api/rng`) da *mesma configuraÃ§Ã£o* que o app usa (via `ENV RngApi:Url`).
    *   `-t 900`: Define um timeout (em segundos? milissegundos? - *Nota: precisa verificar a doc do utilitÃ¡rio, mas provavelmente ms*) para a checagem.
    *   `&& dotnet ...`: Inicia app principal sÃ³ se check passar.
*   Remove necessidade de `curl` na imagem web tambÃ©m. Comportamento similar (sai se API nÃ£o disponÃ­vel).
*   **ExercÃ­cio Try-it-now:** Rodar web app v3 (sem a API rodando). Esperado: sair rapidamente.
    *   Comandos:
        *   `docker run -d -p 8081:80 diamol/ch08-numbers-web:v3`
        *   `docker container ls --all`

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra como o mesmo utilitÃ¡rio customizado (`Utilities.HttpCheck.dll`) pode ser usado tambÃ©m para a **verificaÃ§Ã£o de dependÃªncia** na inicializaÃ§Ã£o do contÃªiner web (`numbers-web`).
*   A Listagem 8.4 apresenta o `CMD` modificado no `Dockerfile.v3` do web app. Em vez de `curl`, ele agora chama `dotnet Utilities.HttpCheck.dll`.
*   **Vantagens demonstradas:**
    *   NÃ£o precisa mais do `curl` na imagem.
    *   Reuso de configuraÃ§Ã£o: A flag `-c RngApi:Url` instrui o utilitÃ¡rio a obter o endereÃ§o da API (`http://numbers-api/rng`) lendo a mesma variÃ¡vel de ambiente (`ENV RngApi:Url`) que a aplicaÃ§Ã£o principal usa. Isso evita definir a URL em dois lugares.
    *   Flexibilidade: A flag `-t 900` (provavelmente 900ms) adiciona um timeout Ã  verificaÃ§Ã£o.
*   O comportamento "fail-fast" Ã© mantido: se o check falhar, o `&&` impede a execuÃ§Ã£o do `dotnet Numbers.Web.dll`, e o contÃªiner sai.
*   O exercÃ­cio pede para rodar esta imagem `v3` do web app (que vocÃª precisarÃ¡ construir primeiro) sem a API, para confirmar que ela ainda sai rapidamente devido Ã  falha no dependency check.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **PrÃ©-requisito:** Construir a imagem v3 do web app:
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch08\exercises\numbers
    docker image build -t diamol/ch08-numbers-web:v3 -f ./numbers-web/Dockerfile.v3 .
    ```
*   **AÃ§Ã£o principal (Testar Dependency Check v3):**
    ```powershell
    # Garantir que API NÃƒO estÃ¡ rodando
    # docker stop api-v3 && docker rm api-v3 (se ainda existir)

    # Rodar web app v3
    docker container run -d -p 8081:80 --name web-v3-fail diamol/ch08-numbers-web:v3

    # Esperar alguns segundos e verificar TODOS os containers
    Start-Sleep -Seconds 5
    docker container ls --all
    ```
    Procure por `web-v3-fail`. O STATUS deve ser `Exited (...)` com um cÃ³digo de erro, e o tempo em `STATUS` deve ser curto.

---

ğŸ“„ PÃ¡gina 156

ğŸ” Resumo dos principais pontos:

*   Figura 8.8: Mostra a saÃ­da do exercÃ­cio anterior (web v3 `Exited`). API v3 (de antes) ainda rodando mas `unhealthy`.
*   Web v3 falhou ao procurar `numbers-api` via DNS, pois contÃªiner `api-v3` nÃ£o foi nomeado assim. Mesmo se tivesse achado, API estava `unhealthy`.
*   **BenefÃ­cio do utilitÃ¡rio customizado:** Torna imagem portÃ¡vel. LÃ³gica de check estÃ¡ na imagem, funciona igual em Compose, Swarm, Kubernetes (que tÃªm formas diferentes de *declarar* checks).
*   **8.4 Defining health checks and dependency checks in Docker Compose** (Definindo health checks e dependency checks no Docker Compose)
    *   Relembra: ContÃªiner sair quando dependÃªncia falha Ã© bom para auto-recuperaÃ§Ã£o em cluster.
    *   Docker Compose *pode* ajudar com confiabilidade, mas limitado (single host).
    *   Compose *nÃ£o* substitui contÃªineres `unhealthy` automaticamente (mesma razÃ£o do Engine: risco de downtime/perda de dados).
    *   Mas Compose pode:
        *   Configurar contÃªineres para reiniciar se saÃ­rem (`restart: on-failure`).
        *   *Adicionar* um health check a um serviÃ§o cuja imagem *nÃ£o* tenha `HEALTHCHECK` embutido.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica por que o `web-v3` falhou (nÃ£o encontrou a API pelo nome DNS esperado) e reforÃ§a um benefÃ­cio chave dos utilitÃ¡rios customizados: eles encapsulam a lÃ³gica de verificaÃ§Ã£o *dentro* da imagem, tornando-a mais independente da plataforma de execuÃ§Ã£o. Diferentes orquestradores (Compose, Swarm, K8s) tÃªm sintaxes diferentes para *definir* health checks, mas se a *lÃ³gica* do check jÃ¡ estÃ¡ na sua imagem (via `HEALTHCHECK` ou `CMD` chamando seu utilitÃ¡rio), ela funcionarÃ¡ consistentemente em todos eles.
*   A seÃ§Ã£o 8.4 volta ao **Docker Compose** e como ele pode interagir com health/dependency checks.
*   **LimitaÃ§Ã£o Reiterada:** Compose em um host Ãºnico nÃ£o faz substituiÃ§Ã£o automÃ¡tica de contÃªineres `unhealthy`.
*   **O que Compose Pode Fazer:**
    1.  **PolÃ­tica de ReinÃ­cio (`restart:`):** VocÃª pode instruir o Compose a reiniciar automaticamente um contÃªiner se ele parar (sair). `restart: on-failure` reinicia apenas se o contÃªiner saiu com um cÃ³digo de erro (nÃ£o 0). `restart: always` reinicia sempre que parar (a menos que parado manualmente via `docker stop` ou `docker-compose stop`). `restart: unless-stopped` Ã© similar a `always`, mas nÃ£o reinicia se foi parado manualmente. Isso ajuda a recuperar contÃªineres que falharam no dependency check.
    2.  **Definir/Sobrescrever Healthcheck:** O Compose permite definir um `healthcheck` na configuraÃ§Ã£o do serviÃ§o no `.yml`. Se a imagem jÃ¡ tem `HEALTHCHECK`, a definiÃ§Ã£o no Compose pode *sobrescrever* parÃ¢metros como intervalo, retries, etc. Se a imagem *nÃ£o* tem `HEALTHCHECK`, a definiÃ§Ã£o no Compose *adiciona* um health check ao contÃªiner em tempo de execuÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   A prÃ³xima pÃ¡gina mostrarÃ¡ como usar `healthcheck:` e `restart:` no `docker-compose.yml`.
*   Exemplo de polÃ­tica de reinÃ­cio:
    ```yaml
    services:
      my-service:
        image: my-image
        restart: on-failure # Reinicia se container sair com erro
    ```

---

ğŸ“„ PÃ¡gina 157

ğŸ” Resumo dos principais pontos:

*   **Listagem 8.5:** ServiÃ§o API (`numbers-api`) no `docker-compose.yml` (em `ch08/exercises/numbers/`), usando imagem `v3` (com `HEALTHCHECK` embutido) e *customizando* parÃ¢metros do health check via Compose.
    ```yaml
    numbers-api:
      image: diamol/ch08-numbers-api:v3
      ports:
        - "8087:80"
      healthcheck: # Sobrescreve/define parÃ¢metros do healthcheck
        interval: 5s # Intervalo entre checks
        timeout: 1s # Tempo mÃ¡ximo para check rodar
        retries: 2 # NÂº de falhas seguidas para ficar unhealthy
        start_period: 5s # Espera inicial antes do 1Âº check (dÃ¡ tempo p/ app iniciar)
      networks:
        - app-net
    ```
*   Explica os parÃ¢metros de `healthcheck` no Compose: `interval`, `timeout`, `retries`, `start_period`.
*   Balancear frequÃªncia vs. overhead (checks consomem recursos).
*   Pode-se *adicionar* health check no Compose para imagens sem `HEALTHCHECK`.
*   **Listagem 8.6:** ServiÃ§o Web (`numbers-web`) no mesmo Compose file, usando imagem `v3` (com check de dependÃªncia no `CMD`), adicionando `restart: on-failure` e definindo um `healthcheck` via Compose.
    ```yaml
    numbers-web:
      image: diamol/ch08-numbers-web:v3
      restart: on-failure # Reinicia se sair com erro (ex: falha no dependency check)
      ports:
        - "8088:80"
      healthcheck: # Adiciona healthcheck via Compose
        test: ["CMD", "dotnet", "Utilities.HttpCheck.dll", "-t", "150"] # Comando do check
        interval: 5s
        timeout: 1s
        retries: 2
        start_period: 10s
      networks:
        - app-net
    ```
*   Campo `test:` no `healthcheck` do Compose define o comando a ser executado (se imagem nÃ£o tiver `HEALTHCHECK` ou se quiser sobrescrever o comando). Formato `["CMD", "comando", "arg1"...]` Ã© um dos aceitos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   As listagens mostram como configurar health checks e reinÃ­cios no `docker-compose.yml`.
*   **Listagem 8.5 (API):** A imagem `v3` jÃ¡ tem um `HEALTHCHECK CMD [...]`. A seÃ§Ã£o `healthcheck:` no Compose aqui *nÃ£o* define o comando (`test:` estÃ¡ ausente), entÃ£o ela usa o comando da imagem, mas *sobrescreve* os parÃ¢metros de temporizaÃ§Ã£o (`interval`, `timeout`, `retries`, `start_period`). O `start_period` Ã© Ãºtil para evitar que o health check falhe logo no inÃ­cio, enquanto a aplicaÃ§Ã£o ainda estÃ¡ carregando.
*   **Listagem 8.6 (Web):**
    *   `restart: on-failure`: Instrui o Docker/Compose a reiniciar este contÃªiner automaticamente *se* ele sair com um cÃ³digo de erro (o que acontecerÃ¡ se o dependency check no `CMD` falhar).
    *   `healthcheck:`: Como a imagem `web:v3` nÃ£o tinha `HEALTHCHECK` no Dockerfile, esta seÃ§Ã£o *adiciona* um health check em tempo de execuÃ§Ã£o. O campo `test:` especifica o comando a ser usado (neste caso, o mesmo utilitÃ¡rio .NET, mas sem a flag `-c` e com timeout diferente). Os outros campos (`interval`, etc.) definem a temporizaÃ§Ã£o.
*   **CombinaÃ§Ã£o:** Agora o web app tem:
    1.  Um dependency check no `CMD` que o faz sair se a API nÃ£o estiver pronta.
    2.  Uma polÃ­tica `restart: on-failure` que o reinicia se ele sair por causa do check.
    3.  Um health check adicionado pelo Compose para monitoramento contÃ­nuo apÃ³s iniciar.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar Arquivo Compose:** No diretÃ³rio `C:\DockerLabs\diamol\ch08\exercises\numbers`, crie `docker-compose.yml` combinando Listagens 8.5 e 8.6 (mais `version:` e `networks:`).
    ```yaml
    version: '3.7'
    services:
      numbers-api:
        image: diamol/ch08-numbers-api:v3
        # ... (resto da Listagem 8.5) ...
        networks:
          - app-net
      numbers-web:
        image: diamol/ch08-numbers-web:v3
        # ... (resto da Listagem 8.6) ...
        networks:
          - app-net
    networks:
      app-net: # Cria rede gerenciada pelo Compose (nÃ£o externa)
        name: numbers-app
    ```
    *(Nota: Usei `name: numbers-app` para a rede, em vez de `external: name: nat`)*

---

ğŸ“„ PÃ¡gina 158

ğŸ” Resumo dos principais pontos:

*   Com `restart: on-failure` e dependency check, o sistema pode se auto-corrigir na inicializaÃ§Ã£o.
    *   Compose pode iniciar em qualquer ordem (sem `depends_on`).
    *   Se Web iniciar antes, check falha, Web sai. API inicia. Web reinicia (devido a `restart:`), check passa, app funciona.
*   **ExercÃ­cio Try-it-now:** Rodar o app completo com Compose e observar a auto-correÃ§Ã£o.
    *   Navegar para `ch08/exercises/numbers`.
    *   Remover contÃªineres existentes.
    *   Iniciar app: `docker-compose up -d`.
    *   Esperar ~5s, listar contÃªineres: `docker container ls`.
    *   Verificar logs do web: `docker container logs numbers_numbers-web_1`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica o mecanismo de auto-correÃ§Ã£o que emerge da combinaÃ§Ã£o do dependency check (que faz o contÃªiner sair se a dependÃªncia nÃ£o estÃ¡ pronta) e da polÃ­tica `restart: on-failure` (que reinicia o contÃªiner que saiu com erro).
*   Como o `docker-compose.yml` neste exemplo *nÃ£o* usa `depends_on`, o Compose pode iniciar os contÃªineres `numbers-api` e `numbers-web` em qualquer ordem, ou atÃ© simultaneamente.
*   **CenÃ¡rio ProvÃ¡vel:**
    1.  `numbers-web` inicia.
    2.  Seu dependency check (`dotnet Utilities.HttpCheck...`) roda.
    3.  `numbers-api` ainda nÃ£o estÃ¡ pronto/respondendo.
    4.  O check falha, `numbers-web` sai com erro.
    5.  `numbers-api` termina de iniciar e fica pronto.
    6.  Docker/Compose detecta que `numbers-web` saiu com erro e o reinicia (devido a `restart: on-failure`).
    7.  `numbers-web` (reiniciado) roda seu dependency check novamente.
    8.  Desta vez, `numbers-api` estÃ¡ pronto, o check passa.
    9.  A aplicaÃ§Ã£o web (`dotnet Numbers.Web.dll`) inicia.
*   O exercÃ­cio instrui a rodar a aplicaÃ§Ã£o com `docker-compose up -d` e verificar os logs do contÃªiner web para observar essa sequÃªncia de falha inicial e sucesso na retentativa.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Limpeza:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch08\exercises\numbers
    # Certifique-se que 'docker-compose.yml' existe aqui (criado na p.157)
    ```
3.  **Iniciar App:**
    ```powershell
    docker-compose up -d
    ```
4.  **Esperar e Listar:**
    ```powershell
    Start-Sleep -Seconds 10 # Dar tempo para possÃ­vel reinÃ­cio
    docker container ls
    # Ou docker-compose ps
    # Ambos os containers devem estar 'Up' e 'healthy' (apÃ³s o start_period)
    ```
5.  **Verificar Logs Web:** (O nome do container serÃ¡ `numbers_numbers-web_1` por padrÃ£o)
    ```powershell
    docker container logs numbers_numbers-web_1
    ```
    Procure por mensagens indicando a execuÃ§Ã£o do `HttpCheck`. VocÃª *pode* ver uma falha inicial seguida por uma execuÃ§Ã£o bem-sucedida (depende do timing exato da inicializaÃ§Ã£o dos contÃªineres).

---

ğŸ“„ PÃ¡gina 159

ğŸ” Resumo dos principais pontos:

*   Figura 8.9: Mostra a saÃ­da dos logs do web container, indicando falha inicial do check (timeout > 150ms) e sucesso na segunda tentativa (115ms) apÃ³s reinÃ­cio.
*   App agora funcional: Acessar `http://localhost:8088`.
*   BotÃ£o funciona (3 vezes), depois falha (bug da API). Figura 8.10 mostra sucesso.
*   **Por que dependency check no CMD vs. `depends_on` no Compose?**
    *   `depends_on`: Controla *ordem de inÃ­cio* dos contÃªineres, sÃ³ funciona no Compose (single host).
    *   Dependency Check no CMD/ENTRYPOINT: Verifica se dependÃªncia estÃ¡ *realmente pronta* para responder. Funciona em qualquer plataforma (Compose, Swarm, K8s). Essencial para clusters onde ordem de inÃ­cio nÃ£o Ã© garantida.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a Figura 8.9, que confirma a sequÃªncia de auto-correÃ§Ã£o: o web container falhou na primeira tentativa de verificar a API (porque a API ainda estava iniciando), foi reiniciado automaticamente, e na segunda tentativa o check passou, permitindo que a aplicaÃ§Ã£o web iniciasse corretamente.
*   Confirma que a aplicaÃ§Ã£o estÃ¡ agora acessÃ­vel em `http://localhost:8088` e funciona (pelas primeiras 3 requisiÃ§Ãµes, atÃ© o bug da API ser ativado).
*   Finalmente, aborda uma questÃ£o importante: por que se dar ao trabalho de colocar o dependency check dentro do `CMD` da imagem, se o Docker Compose tem a opÃ§Ã£o `depends_on`?
    *   `depends_on`: Garante apenas que o contÃªiner B comece *depois* do contÃªiner A ter iniciado. **NÃ£o garante** que a *aplicaÃ§Ã£o dentro* do contÃªiner A esteja pronta para receber conexÃµes. Pode haver um tempo de inicializaÃ§Ã£o interno do app.
    *   Dependency Check (no CMD/ENTRYPOINT): Verifica ativamente se a dependÃªncia estÃ¡ *respondendo*. Ã‰ uma garantia mais forte.
    *   **Portabilidade:** `depends_on` Ã© uma feature especÃ­fica do Docker Compose. Orquestradores de cluster como Swarm e Kubernetes tÃªm seus prÃ³prios mecanismos, mas nÃ£o garantem ordem de inÃ­cio entre serviÃ§os diferentes da mesma forma. Colocar o check *dentro* da imagem (no `CMD` ou `ENTRYPOINT`) garante que essa lÃ³gica de verificaÃ§Ã£o funcione consistentemente em *qualquer* ambiente onde o contÃªiner for executado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Testar App:** Abra `http://localhost:8088`. Clique no botÃ£o. Deve funcionar (3 vezes). Na 4Âª, deve dar erro.
2.  **(Opcional) Observar RecuperaÃ§Ã£o da API:** Se vocÃª esperar o suficiente (dependendo do `health-interval` da API, ex: 15s+), a API `unhealthy` serÃ¡ eventualmente reiniciada pelo Docker Engine (se configurado com `restart:`) ou substituÃ­da em um cluster. ApÃ³s a recuperaÃ§Ã£o da API, o botÃ£o no `http://localhost:8088` voltaria a funcionar (por mais 3 vezes). *Nota: No Compose simples sem Swarm, a substituiÃ§Ã£o de `unhealthy` nÃ£o Ã© automÃ¡tica.*

---

ğŸ“„ PÃ¡gina 160

ğŸ” Resumo dos principais pontos:

*   Figura 8.10: Mostra a UI web funcionando (antes de ativar o bug).
*   **8.5 Understanding how checks power self-healing apps** (Entendendo como as verificaÃ§Ãµes potencializam apps auto-reparÃ¡veis)
*   Apps distribuÃ­dos sÃ£o complexos; tentador querer controlar ordem de inÃ­cio, mas nÃ£o Ã© ideal para escala.
*   Exemplo cluster: App com 20 APIs, 50 Webs. ForÃ§ar inÃ­cio de todas APIs antes das Webs Ã© ineficiente e frÃ¡gil (e se 1 API demorar?).
*   **Abordagem com Checks:**
    *   Plataforma inicia contÃªineres o mais rÃ¡pido possÃ­vel, sem ordem garantida entre serviÃ§os.
    *   Se contÃªiner nÃ£o encontra dependÃªncia (dependency check falha) -> falha rÃ¡pido (`Exited`).
    *   Se contÃªiner fica doente durante execuÃ§Ã£o (health check falha) -> marcado como `unhealthy`.
    *   Plataforma (Swarm/K8s) detecta `Exited` ou `unhealthy` e reinicia/substitui o contÃªiner.
    *   App pode levar alguns minutos para estabilizar 100% em deploy grande, mas fica parcialmente online durante o processo.
*   Diagrama (Figura 8.11): Ciclo de vida de contÃªiner em cluster prod (Running -> Unhealthy -> Shutdown -> Starting -> Running).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o resume a filosofia por trÃ¡s do uso de health checks e dependency checks em ambientes orquestrados para alcanÃ§ar a **auto-recuperaÃ§Ã£o (self-healing)**.
*   A ideia principal Ã© **abraÃ§ar a natureza dinÃ¢mica e potencialmente caÃ³tica** de um cluster. Em vez de tentar impor uma ordem de inicializaÃ§Ã£o rÃ­gida (que Ã© difÃ­cil e ineficiente em escala), vocÃª projeta seus contÃªineres para serem resilientes:
    *   Eles verificam suas prÃ³prias dependÃªncias na inicializaÃ§Ã£o e **falham rapidamente** se algo estiver faltando.
    *   Eles expÃµem um health check que permite Ã  plataforma monitorar sua saÃºde *real* durante a execuÃ§Ã£o.
*   O **orquestrador** (Swarm/Kubernetes) entÃ£o assume o papel de "curador":
    *   Ele lanÃ§a todos os contÃªineres necessÃ¡rios.
    *   Se um contÃªiner sai (falha no dependency check), o orquestrador tenta iniciÃ¡-lo novamente (ou um substituto).
    *   Se um contÃªiner fica `unhealthy` (falha no health check), o orquestrador o remove e o substitui por um novo.
*   O resultado Ã© um sistema que tende a se corrigir automaticamente de falhas transitÃ³rias (ex: dependÃªncia demorou a iniciar, bug temporÃ¡rio causou travamento). A Figura 8.11 ilustra esse ciclo de monitoramento e substituiÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Os conceitos aqui sÃ£o mais relevantes para Swarm (Caps 12-14) e Kubernetes. No Docker Compose simples, vocÃª viu a parte do reinÃ­cio com `restart: on-failure`, mas a substituiÃ§Ã£o de `unhealthy` nÃ£o acontece automaticamente.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   OrquestraÃ§Ã£o com Checks: Ã‰ como um time de futebol onde cada jogador (contÃªiner) sabe sua funÃ§Ã£o e a dos seus companheiros prÃ³ximos (dependÃªncias). O tÃ©cnico (orquestrador) coloca todos em campo. Se um jogador se machuca (unhealthy) ou nÃ£o encontra a bola para comeÃ§ar (dependency fail), o tÃ©cnico o substitui rapidamente, mantendo o time jogando. Ele nÃ£o espera todos estarem perfeitamente posicionados antes de comeÃ§ar o jogo.

---

ğŸ“„ PÃ¡gina 161

ğŸ” Resumo dos principais pontos:

*   **Cuidado com os Checks:**
    *   Health Checks (periÃ³dicos): NÃ£o devem ser muito pesados (CPU/recursos). Balancear frequÃªncia vs. overhead. Testar partes chave sem demorar muito.
    *   Dependency Checks (inicializaÃ§Ã£o): Menos preocupaÃ§Ã£o com recursos (sÃ³ rodam uma vez), mas cuidado com o que verificar. Se dependÃªncia estiver fora do controle da plataforma (ex: serviÃ§o externo), falhar o contÃªiner pode nÃ£o ajudar se a plataforma nÃ£o puder consertar a dependÃªncia.
*   LÃ³gica dos checks Ã© a parte difÃ­cil; Docker facilita a execuÃ§Ã£o. Checks corretos -> plataforma mantÃ©m app rodando.
*   **8.6 Lab** (LaboratÃ³rio)
    *   App: Simulador de "Memory Hog" (consumidor de memÃ³ria) em Node.js.
    *   **Tarefa:** Adicionar checks ao Dockerfile (`ch08/lab`).
        1.  **Dependency Check (InicializaÃ§Ã£o):** Verificar se hÃ¡ memÃ³ria *suficiente* para rodar. Se nÃ£o, sair.
        2.  **Health Check (Runtime):** A cada 5s, verificar se memÃ³ria *alocada* (simulada) excedeu limite. Se sim, ficar `unhealthy`.
    *   LÃ³gica dos testes jÃ¡ existe no script `memory-check.js`. Precisa conectar ao Dockerfile.
    *   Scripts e Dockerfile inicial na pasta `ch08/lab`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina oferece conselhos importantes sobre como projetar boas verificaÃ§Ãµes:
    *   **Health Checks:** Devem ser leves e rÃ¡pidos, pois rodam repetidamente. Teste apenas o essencial para saber se o app estÃ¡ funcional.
    *   **Dependency Checks:** Podem ser mais completos, mas pense nas consequÃªncias. Se vocÃª verificar uma dependÃªncia externa (ex: uma API de terceiros) e ela estiver fora do ar, fazer seu contÃªiner sair e ser reiniciado continuamente nÃ£o resolverÃ¡ o problema. Verifique principalmente dependÃªncias *internas* que a plataforma *pode* gerenciar/reiniciar.
*   O **Lab do CapÃ­tulo 8** foca em implementar ambos os tipos de check em um Dockerfile para uma aplicaÃ§Ã£o Node.js que simula consumir memÃ³ria.
*   A lÃ³gica de verificaÃ§Ã£o jÃ¡ estÃ¡ pronta em um script (`memory-check.js`). Sua tarefa Ã© modificar o `Dockerfile` para:
    1.  Executar o `memory-check.js` com um parÃ¢metro (provavelmente) para fazer a verificaÃ§Ã£o inicial de memÃ³ria disponÃ­vel como parte do `CMD` ou `ENTRYPOINT`, antes de iniciar o app principal, e encadear com `&&`.
    2.  Adicionar uma instruÃ§Ã£o `HEALTHCHECK` que execute o mesmo `memory-check.js` com outro parÃ¢metro (provavelmente) para verificar o uso atual de memÃ³ria.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 8:**
    1.  Navegue para `C:\DockerLabs\diamol\ch08\lab`.
    2.  Examine `Dockerfile` e `memory-check.js`. O script provavelmente aceita argumentos para distinguir entre check inicial e health check.
    3.  **Modificar `Dockerfile`:**
        *   Encontre o `CMD` ou `ENTRYPOINT` atual que inicia o app Node.js (ex: `node server.js`).
        *   Altere-o para primeiro chamar o check inicial e depois o app, usando `&&`. Exemplo: `CMD ["node", "memory-check.js", "startup-check", "500"] && ["node", "server.js"]` (Argumentos `startup-check`, `500` sÃ£o hipotÃ©ticos, veja o script real).
        *   Adicione uma instruÃ§Ã£o `HEALTHCHECK`. Configure o `interval` para 5s (ex: `--interval=5s`). O comando (`test:`) deve chamar o check de runtime. Exemplo: `HEALTHCHECK --interval=5s --retries=3 CMD ["node", "memory-check.js", "runtime-check", "1000"]` (Argumentos `runtime-check`, `1000` sÃ£o hipotÃ©ticos).
    4.  **Construir e Testar:**
        *   `docker build -t lab8-memhog .`
        *   Rodar com limite de memÃ³ria baixo (para falhar startup): `docker run --memory=10m lab8-memhog`. Verificar se sai (`docker ps -a`).
        *   Rodar com memÃ³ria suficiente mas limite para health check: `docker run -d --memory=100m --name memhog-test lab8-memhog`. Verificar se fica `healthy` e depois `unhealthy` (`docker ps`). (Os limites de memÃ³ria simulados no script podem nÃ£o corresponder aos limites reais do Docker).

---

ğŸ“„ PÃ¡gina 162

ğŸ” Resumo dos principais pontos:

*   **Nota sobre SimulaÃ§Ã£o de MemÃ³ria:** App do lab *simula* uso de memÃ³ria, nÃ£o aloca de verdade, pois gerenciamento real varia entre ambientes Docker (Desktop vs. Linux CE).
*   SoluÃ§Ã£o de exemplo no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina contÃ©m uma nota importante esclarecendo que a aplicaÃ§Ã£o do Lab 8 apenas *finge* usar memÃ³ria, para simplificar o exercÃ­cio e tornÃ¡-lo consistente entre diferentes setups Docker. O gerenciamento real de limites de memÃ³ria em contÃªineres (`docker run --memory=...`) pode ter comportamentos diferentes dependendo de como e onde o Docker estÃ¡ rodando.
*   Fornece o link para a soluÃ§Ã£o do lab no repositÃ³rio do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Ao fazer o Lab 8, lembre-se que os limites de memÃ³ria mencionados nos comandos (`startup-check 500`, `runtime-check 1000`) referem-se a valores *internos* da simulaÃ§Ã£o do app, nÃ£o necessariamente a MB reais que vocÃª define com `docker run --memory`.

---

ğŸ“„ PÃ¡gina 163

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 9: Adding observability with containerized monitoring** (Adicionando observabilidade com monitoramento conteinerizado)
*   Apps autÃ´nomos (escalam/reparam sozinhos) parecem bons demais, e sÃ£o. Plataforma ajuda, mas monitoramento humano e alertas ainda sÃ£o necessÃ¡rios para problemas sÃ©rios.
*   Falta de insight (observabilidade) impede ir para produÃ§Ã£o.
*   **Observabilidade:** CrÃ­tica para apps em contÃªineres. Diz o que apps estÃ£o fazendo, como estÃ£o performando, ajuda a achar problemas.
*   Foco do Cap. 9: Abordagem estabelecida com **Prometheus** e **Grafana**.
    *   Expor **mÃ©tricas** dos contÃªineres da aplicaÃ§Ã£o.
    *   Usar **Prometheus** para coletar mÃ©tricas.
    *   Usar **Grafana** para visualizar mÃ©tricas em dashboards.
*   Ferramentas sÃ£o open source, multiplataforma, rodam em contÃªineres.
*   BenefÃ­cio: Mesmo insight em todos os ambientes (dev a prod).
*   **9.1 The monitoring stack for containerized applications** (A pilha de monitoramento para aplicaÃ§Ãµes conteinerizadas)
    *   Monitoramento tradicional (lista de servidores, uso de CPU/disco) nÃ£o funciona bem para contÃªineres (dinÃ¢micos, curta duraÃ§Ã£o, muitos).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 9, focado em **observabilidade** - a capacidade de entender o que estÃ¡ acontecendo dentro da sua aplicaÃ§Ã£o e infraestrutura.
*   Argumenta que, apesar da automaÃ§Ã£o e auto-recuperaÃ§Ã£o oferecidas por contÃªineres e orquestradores, o monitoramento ativo ainda Ã© essencial, especialmente para diagnosticar problemas complexos e garantir a prontidÃ£o para produÃ§Ã£o.
*   Apresenta a pilha de ferramentas open source popular para isso:
    1.  **MÃ©tricas:** As aplicaÃ§Ãµes sÃ£o instrumentadas para expor dados numÃ©ricos sobre seu desempenho e atividade (ex: nÃºmero de requisiÃ§Ãµes, tempo de resposta, uso de memÃ³ria interna) em um formato padronizado (formato Prometheus).
    2.  **Prometheus:** Um sistema de monitoramento e alerta que *coleta* (scraping) essas mÃ©tricas periodicamente de vÃ¡rias fontes (aplicaÃ§Ãµes, infraestrutura), as armazena em um banco de dados de sÃ©ries temporais e permite fazer consultas sobre elas.
    3.  **Grafana:** Uma plataforma de visualizaÃ§Ã£o que se conecta a fontes de dados como Prometheus e permite criar dashboards grÃ¡ficos interativos para exibir as mÃ©tricas de forma compreensÃ­vel.
*   A grande vantagem Ã© que Prometheus e Grafana rodam em contÃªineres, permitindo que vocÃª tenha o mesmo sistema de monitoramento robusto desde o ambiente de desenvolvimento local atÃ© a produÃ§Ã£o.
*   A seÃ§Ã£o 9.1 comeÃ§a contrastando o monitoramento moderno (focado em serviÃ§os e mÃ©tricas de aplicaÃ§Ã£o) com o tradicional (focado em mÃ¡quinas individuais), que Ã© inadequado para a natureza dinÃ¢mica dos contÃªineres.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª rodarÃ¡ contÃªineres para Prometheus e Grafana usando `docker run` ou `docker-compose`.
*   VocÃª modificarÃ¡ (ou usarÃ¡ versÃµes modificadas) das aplicaÃ§Ãµes de exemplo para expor um endpoint `/metrics` no formato Prometheus.
*   VocÃª configurarÃ¡ o Prometheus para encontrar e coletar mÃ©tricas desses endpoints.
*   VocÃª acessarÃ¡ a interface web do Grafana para visualizar os dados coletados pelo Prometheus.

---

ğŸ“„ PÃ¡gina 164

ğŸ” Resumo dos principais pontos:

*   Monitoramento de contÃªineres precisa ser "container-aware" (consciente de contÃªineres).
*   **Prometheus:** Open source, CNCF, descobre e coleta mÃ©tricas de contÃªineres dinamicamente. Roda em contÃªiner.
*   Diagrama (Figura 9.1): Mostra Prometheus em contÃªiner coletando mÃ©tricas de outros contÃªineres de aplicaÃ§Ã£o e do prÃ³prio Docker Engine.
*   **BenefÃ­cio Prometheus: ConsistÃªncia.**
    *   Mesmo formato de mÃ©tricas para todos os apps (ex: .NET/Windows, Node.js/Linux).
    *   Mesma linguagem de consulta (PromQL).
*   **Docker Engine tambÃ©m pode exportar mÃ©tricas** no formato Prometheus.
    *   DÃ¡ insight sobre a plataforma/infraestrutura Docker.
    *   Precisa ser habilitado explicitamente na config do Docker Engine (`daemon.json` ou UI Docker Desktop).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz formalmente o **Prometheus** como a ferramenta central para coleta de mÃ©tricas no mundo dos contÃªineres. Sua capacidade de descobrir dinamicamente os contÃªineres que precisa monitorar (em vez de depender de uma lista estÃ¡tica de IPs) Ã© crucial.
*   A Figura 9.1 ilustra o cenÃ¡rio: um contÃªiner Prometheus "observando" tanto os contÃªineres das aplicaÃ§Ãµes quanto o prÃ³prio motor Docker (Docker Engine) que os estÃ¡ executando.
*   A **consistÃªncia** Ã© um grande trunfo: Prometheus usa um formato de texto simples para mÃ©tricas, que pode ser gerado por qualquer aplicaÃ§Ã£o, independentemente da linguagem ou SO. Isso unifica o monitoramento.
*   Uma fonte importante de mÃ©tricas Ã© o prÃ³prio **Docker Engine**. Ele pode ser configurado para expor informaÃ§Ãµes sobre seu prÃ³prio desempenho, uso de recursos e o estado dos contÃªineres que gerencia. Habilitar isso fornece uma visÃ£o valiosa da saÃºde da infraestrutura subjacente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo passo serÃ¡ habilitar a exportaÃ§Ã£o de mÃ©tricas no seu Docker Desktop.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Prometheus: Ã‰ como um agente de censo que visita regularmente cada "casa" (contÃªiner, Docker Engine) que foi instruÃ­do a monitorar, pergunta "quantas pessoas moram aÃ­?", "quanta energia estÃ£o usando?" (coleta mÃ©tricas), anota as respostas com a data e hora, e guarda tudo em um arquivo central.
*   MÃ©tricas do Docker Engine: SÃ£o os dados do prÃ³prio "sÃ­ndico" do prÃ©dio, informando quantos apartamentos estÃ£o ocupados, se o elevador estÃ¡ funcionando, etc.

---

ğŸ“„ PÃ¡gina 165

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Habilitar mÃ©tricas do Prometheus no Docker Engine.
    *   Editar `daemon.json` (Linux/Win Server) ou usar UI Docker Desktop (Settings > Daemon/Docker Engine).
    *   Adicionar/Garantir as seguintes configuraÃ§Ãµes JSON:
        ```json
        "metrics-addr" : "0.0.0.0:9323",
        "experimental": true
        ```
    *   `metrics-addr`: Diz ao Engine para expor mÃ©tricas na porta 9323, acessÃ­vel de qualquer endereÃ§o IP (`0.0.0.0`).
    *   `experimental`: NecessÃ¡rio porque a feature de mÃ©tricas ainda Ã© considerada experimental (embora estÃ¡vel).
*   Figura 9.2: Mostra o arquivo de configuraÃ§Ã£o JSON completo no editor do Docker Desktop.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este exercÃ­cio ativa a funcionalidade do Docker Engine para expor suas prÃ³prias mÃ©tricas internas no formato Prometheus.
*   Isso Ã© feito adicionando duas linhas Ã  configuraÃ§Ã£o do Docker Engine (o `daemon.json`):
    *   `"metrics-addr" : "0.0.0.0:9323"`: Configura um pequeno servidor web dentro do Docker Engine para responder na porta `9323`. O endereÃ§o `0.0.0.0` significa que ele aceitarÃ¡ conexÃµes de qualquer interface de rede da mÃ¡quina (incluindo conexÃµes de contÃªineres rodando na mesma mÃ¡quina). Se usasse `127.0.0.1`, sÃ³ seria acessÃ­vel da prÃ³pria mÃ¡quina host.
    *   `"experimental": true`: Algumas funcionalidades do Docker, como as mÃ©tricas do Prometheus, sÃ£o liberadas primeiro sob a flag "experimental". Ã‰ preciso habilitÃ¡-la para usar essas features.
*   Para usuÃ¡rios do Docker Desktop, a maneira mais fÃ¡cil Ã© editar o JSON diretamente na interface de ConfiguraÃ§Ãµes (na seÃ§Ã£o "Docker Engine"). ApÃ³s clicar em "Apply & Restart", o Docker Engine reiniciarÃ¡ com as novas configuraÃ§Ãµes. Para usuÃ¡rios de servidor, seria preciso editar o arquivo `daemon.json` manualmente e reiniciar o serviÃ§o Docker.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Abrir ConfiguraÃ§Ãµes:** Clique direito no Ã­cone Docker > Settings.
2.  **Editar JSON:** VÃ¡ para a seÃ§Ã£o "Docker Engine".
3.  **Adicionar Linhas:** No editor JSON, adicione as duas linhas, garantindo que a sintaxe JSON permaneÃ§a vÃ¡lida (vÃ­rgulas entre itens, chaves `{}` corretas). Exemplo mÃ­nimo:
    ```json
    {
      "experimental": true,
      "metrics-addr": "0.0.0.0:9323"
    }
    ```
    *(Mantenha outras configuraÃ§Ãµes que jÃ¡ existirem, como `builder`)*.
4.  **Aplicar:** Clique em "Apply & Restart". Aguarde o Docker Engine reiniciar (o Ã­cone da baleia ficarÃ¡ amarelo e depois verde novamente).

---

ğŸ“„ PÃ¡gina 166

ğŸ” Resumo dos principais pontos:

*   MÃ©tricas do Engine sÃ£o experimentais, mas estÃ¡veis e Ãºteis para dashboards (saÃºde do sistema).
*   **Teste:** Acessar `http://localhost:9323/metrics` no navegador.
    *   Deve mostrar saÃ­da em texto no formato Prometheus (Figura 9.3).
*   **Formato Prometheus:**
    *   Texto simples.
    *   Cada mÃ©trica: nome e valor.
    *   Precedida por linhas `# HELP` (descriÃ§Ã£o) e `# TYPE` (tipo de dado - gauge, counter, etc.).
*   Essas linhas de texto sÃ£o a base da soluÃ§Ã£o de monitoramento. Prometheus coleta, adiciona timestamp e armazena para consulta.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s habilitar as mÃ©tricas e reiniciar o Engine, a pÃ¡gina instrui a verificar se estÃ¡ funcionando acessando o endpoint `http://localhost:9323/metrics` diretamente no navegador.
*   O resultado esperado Ã© uma pÃ¡gina de texto (nÃ£o HTML) contendo uma longa lista de mÃ©tricas no formato especÃ­fico do Prometheus.
*   O formato Ã© explicado:
    *   Linhas comeÃ§ando com `# HELP` descrevem o que a mÃ©trica significa.
    *   Linhas comeÃ§ando com `# TYPE` indicam o tipo da mÃ©trica (os mais comuns sÃ£o `gauge` - valor pode subir/descer, como uso de memÃ³ria; e `counter` - valor sÃ³ sobe ou reseta, como nÃºmero total de requisiÃ§Ãµes).
    *   A linha da mÃ©trica em si contÃ©m o `nome_da_metrica`, opcionalmente {`labels="valores"`} (pares chave-valor para diferenciar instÃ¢ncias da mesma mÃ©trica), e o `valor` numÃ©rico atual.
*   Esses dados brutos expostos pelo Docker Engine (e pelas aplicaÃ§Ãµes) serÃ£o a fonte para o Prometheus construir seu histÃ³rico e para o Grafana gerar os grÃ¡ficos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Acessar MÃ©tricas:** Abra seu navegador e vÃ¡ para `http://localhost:9323/metrics`.
2.  **Observar SaÃ­da:** VocÃª verÃ¡ uma grande quantidade de texto similar Ã  Figura 9.3. Procure por mÃ©tricas como `engine_daemon_engine_version`, `engine_daemon_container_states_containers`, `process_resident_memory_bytes`. Observe as linhas `# HELP` e `# TYPE` acima delas.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Endpoint `/metrics`: Ã‰ como um painel de instrumentos cheio de mostradores e contadores, mas em formato de texto, que o Prometheus sabe ler.

---

ğŸ“„ PÃ¡gina 167

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar Prometheus em contÃªiner para coletar mÃ©tricas do Docker host.
    *   NecessÃ¡rio obter IP da mÃ¡quina host para passar ao contÃªiner Prometheus.
    *   Comandos para obter IP do host (Windows PowerShell, Linux, Mac).
    *   Rodar contÃªiner Prometheus (`diamol/prometheus:2.13.1`), passando IP do host via variÃ¡vel de ambiente `DOCKER_HOST`.
        *   `docker run -e DOCKER_HOST=$hostIP -d -p 9090:9090 diamol/prometheus:2.13.1`.
*   ConfiguraÃ§Ã£o na imagem `diamol/prometheus` usa `DOCKER_HOST` para encontrar e coletar (`scrape`) mÃ©tricas do Engine na porta 9323.
*   Acessar serviÃ§o no host a partir de contÃªiner Ã© raro; geralmente usa-se nome DNS, mas IP funciona para dev local.
*   **Prometheus em execuÃ§Ã£o:**
    *   Coleta mÃ©tricas do host em intervalos.
    *   Armazena valores com timestamp em seu BD interno.
    *   ExpÃµe UI web bÃ¡sica na porta 9090.
*   UI Prometheus permite ver mÃ©tricas coletadas, filtrar

**You**

ğŸ§  Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), pÃ¡gina por pÃ¡gina, a apartir da pÃ¡gina 167.

Para cada pÃ¡gina, siga este modelo de resposta:

ğŸ“„ PÃ¡gina X

ğŸ” Resumo dos principais pontos:

\[bullet points com os tÃ³picos abordados na pÃ¡gina\]

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

\[explique os conceitos em portuguÃªs claro, com boa didÃ¡tica, como um professor\]

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

\[comandos ou trechos de cÃ³digo aplicÃ¡veis no Windows, usando Docker Desktop e Visual Studio Code\]

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

\[comparaÃ§Ãµes simples ou observaÃ§Ãµes para reforÃ§ar o entendimento\]

âœ… Importante: percorra automaticamente todas as pÃ¡ginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "prÃ³xima".

Quando terminar todas as pÃ¡ginas, diga:

"âœ… Fim da leitura. Todas as pÃ¡ginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Okay, Alexandre, continuando a anÃ¡lise do PDF a partir da pÃ¡gina 167.

---

ğŸ“„ PÃ¡gina 167

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar Prometheus em contÃªiner para coletar mÃ©tricas do Docker host.
    *   NecessÃ¡rio obter IP da mÃ¡quina host para passar ao contÃªiner Prometheus.
    *   Comandos fornecidos para obter IP do host (Windows PowerShell, Linux, Mac).
    *   Rodar contÃªiner Prometheus (`diamol/prometheus:2.13.1`), passando IP do host via variÃ¡vel de ambiente `DOCKER_HOST`.
        *   Comando base: `docker container run -e DOCKER_HOST=$hostIP -d -p 9090:9090 diamol/prometheus:2.13.1`.
*   ConfiguraÃ§Ã£o na imagem `diamol/prometheus` usa `DOCKER_HOST` para encontrar e coletar (`scrape`) mÃ©tricas do Engine na porta 9323.
*   Acessar serviÃ§o no host a partir de contÃªiner Ã© raro; geralmente usa-se nome DNS, mas IP funciona para dev local.
*   **Prometheus em execuÃ§Ã£o:**
    *   Coleta mÃ©tricas do host em intervalos.
    *   Armazena valores com timestamp em seu BD interno.
    *   ExpÃµe UI web bÃ¡sica na porta 9090 (`http://localhost:9090`).
*   UI Prometheus permite ver mÃ©tricas coletadas, filtrar, exibir em tabelas/grÃ¡ficos.
*   **ExercÃ­cio Try-it-now:** Verificar UI do Prometheus e consultar mÃ©trica.
    *   Acessar `http://localhost:9090`.
    *   Verificar Status > Targets (deve mostrar `DOCKER_HOST` como UP/verde).
    *   Ir para Graph, selecionar mÃ©trica `engine_daemon_container_actions_seconds_sum`, clicar Execute.
*   Figura 9.4: Mostra a UI do Prometheus com a consulta e resultado da mÃ©trica.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina mostra como iniciar o **Prometheus** em um contÃªiner para que ele comece a coletar as mÃ©tricas que o Docker Engine estÃ¡ expondo (conforme configurado nas pÃ¡ginas anteriores).
*   Um detalhe tÃ©cnico: o contÃªiner do Prometheus precisa saber o endereÃ§o IP da mÃ¡quina host onde o Docker Engine estÃ¡ rodando para poder acessar `http://<ip_do_host>:9323/metrics`. O exercÃ­cio fornece comandos para descobrir o IP do host e armazenÃ¡-lo em uma variÃ¡vel (`$hostIP` no PowerShell). Essa variÃ¡vel Ã© entÃ£o passada para dentro do contÃªiner Prometheus usando a flag `-e DOCKER_HOST=$hostIP` no comando `docker run`. A imagem `diamol/prometheus` foi prÃ©-configurada para usar essa variÃ¡vel `DOCKER_HOST` para encontrar seu alvo.
*   Uma vez que o contÃªiner Prometheus estÃ¡ rodando, ele comeÃ§a seu ciclo de "scraping": periodicamente (a cada poucos segundos ou minutos, dependendo da configuraÃ§Ã£o), ele acessa o endpoint `/metrics` do Docker Engine, coleta os dados, adiciona um timestamp e armazena em seu prÃ³prio banco de dados interno (um banco de dados de sÃ©ries temporais).
*   Prometheus tambÃ©m oferece uma interface web simples na porta 9090 para visualizaÃ§Ã£o e consulta bÃ¡sica. O exercÃ­cio guia vocÃª para:
    1.  Acessar a UI (`http://localhost:9090`).
    2.  Verificar a pÃ¡gina "Targets" para confirmar que o Prometheus estÃ¡ conseguindo se conectar ao Docker Engine (`DOCKER_HOST` deve estar verde).
    3.  Usar a pÃ¡gina "Graph" para selecionar uma mÃ©trica especÃ­fica (ex: `engine_daemon_container_actions_seconds_sum`, que mede o tempo gasto em aÃ§Ãµes de contÃªiner) e visualizar seu valor atual.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Obter IP do Host (PowerShell):**
    ```powershell
    $hostIP = (Get-NetIPConfiguration | Where-Object {$_.IPv4DefaultGateway -ne $null -and $_.NetAdapter.Status -eq 'Up' } | Select-Object -First 1).IPv4Address.IPAddress
    echo $hostIP # Anote este IP
    # Alternativa (pode funcionar melhor com WSL2): Use 'host.docker.internal' em vez do IP
    # $hostIP = 'host.docker.internal'
    ```
    *Nota: Obter o IP correto do host que Ã© visÃ­vel de dentro do contÃªiner pode ser complicado, especialmente com WSL2. Usar o nome DNS especial `host.docker.internal` (que o Docker Desktop define) pode ser mais confiÃ¡vel.* Vamos tentar com ele.
2.  **Rodar Prometheus:**
    ```powershell
    docker container run -e DOCKER_HOST='host.docker.internal' -d -p 9090:9090 --name prometheus diamol/prometheus:2.13.1
    ```
3.  **Verificar Prometheus UI:**
    *   Abra o navegador em `http://localhost:9090`.
    *   VÃ¡ para Status > Targets. Verifique se `DOCKER_HOST` (ou o nome configurado) estÃ¡ com State "UP".
    *   VÃ¡ para a aba "Graph".
    *   No campo de expressÃ£o, comece a digitar `engine_daemon_container_actions_seconds_sum`. Selecione-o na lista suspensa.
    *   Clique em "Execute". VocÃª deverÃ¡ ver os valores atuais dessa mÃ©trica na aba "Table".

---

ğŸ“„ PÃ¡gina 168

ğŸ” Resumo dos principais pontos:

*   Figura 9.4: Mostra a UI do Prometheus com a consulta.
*   UI Prometheus Ã© simples, boa para explorar mÃ©tricas disponÃ­veis e rodar queries.
*   Docker Engine expÃµe muitas mÃ©tricas:
    *   NÃ­vel alto: nÂº containers por estado, nÂº health checks falhos.
    *   NÃ­vel baixo: memÃ³ria usada pelo Engine.
    *   Info estÃ¡tica: nÂº CPUs disponÃ­veis.
*   MÃ©tricas do Engine sÃ£o nÃ­vel **infraestrutura**.
*   AplicaÃ§Ãµes expÃµem suas prÃ³prias mÃ©tricas (nÃ­vel **runtime** e **aplicaÃ§Ã£o**).
*   Objetivo: Ter endpoint `/metrics` em cada contÃªiner de app, Prometheus coletando de todos.
*   Prometheus armazena histÃ³rico, permitindo dashboards que mostram saÃºde geral do sistema.
*   **9.2 Exposing metrics from your application** (Expondo mÃ©tricas da sua aplicaÃ§Ã£o)
    *   MÃ©tricas do Engine sÃ£o comeÃ§o; mÃ©tricas das *aplicaÃ§Ãµes* exigem mais esforÃ§o.
    *   NecessÃ¡rio cÃ³digo para capturar mÃ©tricas e prover endpoint HTTP `/metrics`.
    *   **Bibliotecas Cliente Prometheus:** Facilitam isso para vÃ¡rias linguagens (Java, Go, Node.js, Python, etc.).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a introduÃ§Ã£o ao Prometheus e suas mÃ©tricas do Docker Engine, classificando-as como mÃ©tricas de **infraestrutura**.
*   A seÃ§Ã£o 9.2 introduz o prÃ³ximo passo crucial: fazer com que as **suas prÃ³prias aplicaÃ§Ãµes** exponham mÃ©tricas sobre seu funcionamento interno. Isso requer instrumentar o cÃ³digo da aplicaÃ§Ã£o.
*   A boa notÃ­cia Ã© que nÃ£o Ã© preciso fazer tudo do zero. Existem **bibliotecas cliente oficiais e comunitÃ¡rias do Prometheus** para a maioria das linguagens de programaÃ§Ã£o populares. Essas bibliotecas oferecem funcionalidades prontas para:
    1.  Coletar mÃ©tricas padrÃ£o do **runtime** da linguagem (ex: uso de memÃ³ria da JVM em Java, nÃºmero de Goroutines em Go, informaÃ§Ãµes do event loop em Node.js).
    2.  Permitir que o desenvolvedor defina e atualize mÃ©tricas **customizadas** da **aplicaÃ§Ã£o** (ex: nÃºmero de logins, tempo mÃ©dio de processamento de pedido, itens no carrinho).
    3.  Expor todas essas mÃ©tricas atravÃ©s de um endpoint HTTP (geralmente `/metrics`) no formato que o Prometheus entende.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nos prÃ³ximos exercÃ­cios, vocÃª usarÃ¡ versÃµes das aplicaÃ§Ãµes de exemplo (Image Gallery) que jÃ¡ foram modificadas para incluir essas bibliotecas cliente Prometheus e expor o endpoint `/metrics`.
*   Se vocÃª fosse fazer isso em seu prÃ³prio projeto .NET Core, por exemplo, usaria pacotes NuGet como `prometheus-net` e `prometheus-net.AspNetCore`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   MÃ©tricas do Engine: Medem a saÃºde do "prÃ©dio" (infraestrutura Docker).
*   MÃ©tricas da AplicaÃ§Ã£o: Medem a saÃºde e atividade *dentro* de cada "apartamento" (contÃªiner de aplicaÃ§Ã£o).
*   Bibliotecas Cliente Prometheus: SÃ£o como kits de "sensores + painel de leitura" prÃ©-fabricados que vocÃª instala no seu apartamento (aplicaÃ§Ã£o) para medir coisas como temperatura interna, consumo de energia, quantas vezes a porta foi aberta (mÃ©tricas de runtime e aplicaÃ§Ã£o) e exibir tudo em um formato padrÃ£o na porta (`/metrics`).

---

ğŸ“„ PÃ¡gina 169

ğŸ” Resumo dos principais pontos:

*   Autor revisitou app Image Gallery (Go, Java, Node.js) e adicionou mÃ©tricas Prometheus usando bibliotecas cliente.
*   Diagrama (Figura 9.5): Mostra cada contÃªiner da aplicaÃ§Ã£o agora com uma biblioteca cliente Prometheus expondo mÃ©tricas.
*   MÃ©tricas das bibliotecas cliente = nÃ­vel **runtime**.
    *   Info sobre o que o contÃªiner estÃ¡ fazendo, quÃ£o "quente" estÃ¡ rodando (CPU, memÃ³ria, threads, etc.).
    *   EspecÃ­ficas da linguagem (ex: Goroutines em Go, memÃ³ria JVM em Java).
    *   Geralmente obtidas "de graÃ§a" ao adicionar a biblioteca.
*   **ExercÃ­cio Try-it-now:** Rodar nova versÃ£o do app Image Gallery (com mÃ©tricas) e acessar endpoint `/metrics`.
    *   Navegar para `ch09/exercises`.
    *   Limpar contÃªineres antigos.
    *   Criar rede `nat` (ignorar erro se existir).
    *   Iniciar app: `docker-compose up -d`.
    *   Usar app: acessar `http://localhost:8010`.
    *   Ver mÃ©tricas do frontend Go: acessar `http://localhost:8010/metrics`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que as aplicaÃ§Ãµes de exemplo foram atualizadas para incluir as bibliotecas cliente Prometheus. A Figura 9.5 ilustra isso.
*   Essas bibliotecas fornecem automaticamente mÃ©tricas valiosas sobre o **runtime** da aplicaÃ§Ã£o (Go, Java, Node.js), como uso de memÃ³ria especÃ­fico da plataforma, atividade de threads/processos internos, etc. Isso jÃ¡ dÃ¡ uma boa visÃ£o de como a aplicaÃ§Ã£o estÃ¡ se comportando internamente, sem que o desenvolvedor precise escrever cÃ³digo extra para essas mÃ©tricas especÃ­ficas do runtime.
*   O exercÃ­cio instrui a iniciar a versÃ£o "instrumentada" do aplicativo Image Gallery usando `docker-compose` (o arquivo `.yml` nesta pasta jÃ¡ definirÃ¡ os 3 serviÃ§os - Go, Java, Node.js - usando as novas imagens com mÃ©tricas). ApÃ³s iniciar e usar um pouco a aplicaÃ§Ã£o web para gerar atividade, o exercÃ­cio pede para acessar diretamente o endpoint `/metrics` do serviÃ§o frontend Go (`http://localhost:8010/metrics`) para ver as mÃ©tricas que ele estÃ¡ expondo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch09\exercises
    ```
2.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Pode remover tambÃ©m o container prometheus, se quiser comeÃ§ar limpo
    # docker rm -f prometheus
    ```
3.  **Criar Rede:**
    ```powershell
    docker network create nat # Ignore erro se jÃ¡ existir
    ```
4.  **Iniciar App com MÃ©tricas:** (O `docker-compose.yml` nesta pasta define os serviÃ§os `image-gallery`, `iotd`, `accesslog` com as imagens corretas)
    ```powershell
    docker-compose up -d
    ```
5.  **Verificar:**
    ```powershell
    docker-compose ps # Deve mostrar os 3 serviÃ§os Up
    ```
6.  **Usar App:** Abra o navegador em `http://localhost:8010`. Clique algumas vezes se houver botÃµes, ou apenas carregue a pÃ¡gina.
7.  **Ver MÃ©tricas Go:** Abra uma nova aba no navegador e acesse `http://localhost:8010/metrics`. VocÃª verÃ¡ a saÃ­da de texto no formato Prometheus.

---

ğŸ“„ PÃ¡gina 170

ğŸ” Resumo dos principais pontos:

*   Figura 9.6: Mostra a saÃ­da do endpoint `/metrics` do frontend Go.
*   MÃ©tricas obtidas "de graÃ§a" (sem cÃ³digo customizado) apenas adicionando a biblioteca cliente Go.
*   Pode-se ver mÃ©tricas similares da API Java em `http://localhost:8011/actuator/prometheus`. (Nota: Porta do host para Java API pode variar se for aleatÃ³ria, mas o path `/actuator/prometheus` Ã© comum em Spring Boot).
*   Endpoints `/metrics` sÃ£o "mar de texto", mas contÃªm dados chave para dashboards (ex: container rodando "quente" - CPU, memÃ³ria, threads altos).
*   MÃ©tricas de **runtime** sÃ£o Ãºteis, mas nÃ£o contam toda a histÃ³ria.
*   NecessÃ¡rio tambÃ©m: **mÃ©tricas de aplicaÃ§Ã£o** (customizadas).
    *   Capturam info especÃ­fica sobre o app/negÃ³cio.
    *   Exemplos operacionais: nÂº eventos processados, tempo mÃ©dio de resposta.
    *   Exemplos de negÃ³cio: nÂº usuÃ¡rios ativos, nÂº cadastros novos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra o resultado de acessar o endpoint `/metrics` da aplicaÃ§Ã£o Go (Figura 9.6) e menciona o endpoint equivalente para a aplicaÃ§Ã£o Java (que usa Spring Boot Actuator, uma dependÃªncia comum que expÃµe mÃ©tricas e outros endpoints de gerenciamento).
*   ReforÃ§a que essas mÃ©tricas de **runtime** sÃ£o valiosas para entender o consumo de recursos e o estado interno da plataforma (Go, JVM), e que sÃ£o obtidas com pouco ou nenhum esforÃ§o extra do desenvolvedor, apenas incluindo a biblioteca cliente apropriada.
*   No entanto, para uma observabilidade completa, sÃ£o necessÃ¡rias tambÃ©m as **mÃ©tricas de aplicaÃ§Ã£o** (ou mÃ©tricas customizadas/de negÃ³cio). Estas sÃ£o mÃ©tricas que *o desenvolvedor* precisa definir e incrementar/decrementar explicitamente no cÃ³digo para rastrear eventos ou estados especÃ­ficos do domÃ­nio da aplicaÃ§Ã£o ou do negÃ³cio. Exemplos incluem contar quantas vezes uma funÃ§Ã£o especÃ­fica foi chamada, medir a duraÃ§Ã£o de um processo de negÃ³cio, ou rastrear o nÃºmero de itens em um carrinho de compras.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Ver MÃ©tricas Java:**
    *   Primeiro, descubra a porta externa da API Java (`iotd`):
        ```powershell
        docker-compose ps
        # Procure pela linha do serviÃ§o 'iotd'. A coluna PORTS mostrarÃ¡ algo como '0.0.0.0:32768->8080/tcp'. Anote a porta antes de '->'.
        # Ou use:
        # docker-compose port iotd 8080
        ```
    *   Acesse no navegador: `http://localhost:<porta_anotada>/actuator/prometheus`. VocÃª verÃ¡ as mÃ©tricas da JVM e do Spring Boot.
2.  **Entender MÃ©tricas Customizadas:** O prÃ³ximo passo mostrarÃ¡ como elas sÃ£o definidas no cÃ³digo e como aparecem no endpoint `/metrics`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   MÃ©tricas de Runtime: Como os indicadores no painel do seu carro (velocÃ­metro, RPM, temperatura do motor, nÃ­vel de combustÃ­vel) - informaÃ§Ãµes padrÃ£o sobre o funcionamento da mÃ¡quina.
*   MÃ©tricas de AplicaÃ§Ã£o: Como contadores ou anotaÃ§Ãµes que *vocÃª* adiciona para rastrear coisas especÃ­ficas da sua viagem (quantos pedÃ¡gios passou, tempo gasto em cada parada, nÃºmero de passageiros) - informaÃ§Ãµes especÃ­ficas do propÃ³sito da sua "viagem" (aplicaÃ§Ã£o).

---

ğŸ“„ PÃ¡gina 171

ğŸ” Resumo dos principais pontos:

*   Bibliotecas cliente Prometheus permitem gravar mÃ©tricas customizadas (aplicaÃ§Ã£o/negÃ³cio). Requer escrita de cÃ³digo explÃ­cito.
*   NÃ£o Ã© difÃ­cil. **Listagem 9.1:** Exemplo em Node.js (do app `access-log`).
    ```javascript
    // Declara mÃ©tricas customizadas:
    const accessCounter = new prom.Counter({ // Contador: sÃ³ incrementa
      name: "access_log_total",
      help: "Access Log - total log requests"
    });
    const clientIpGauge = new prom.Gauge({ // Gauge: pode incrementar/decrementar
      name: "access_client_ip_current",
      help: "Access Log - current unique IP addresses"
    });

    // Depois, no cÃ³digo que lida com requests:
    accessCounter.inc(); // Incrementa contador a cada request
    clientIpGauge.set(countOfIpAddresses); // Define valor atual do gauge
    ```
*   Menciona que exemplos Go e Java no cÃ³digo fonte tambÃ©m adicionam mÃ©tricas customizadas (usando APIs especÃ­ficas de cada biblioteca cliente).
*   **Tipos de MÃ©tricas Prometheus (simples):**
    *   **Counter:** Valor numÃ©rico que sÃ³ aumenta ou reseta para zero (ex: nÂº total de requisiÃ§Ãµes HTTP).
    *   **Gauge:** Valor numÃ©rico que pode aumentar ou diminuir (ex: uso atual de memÃ³ria, nÂº usuÃ¡rios logados).
*   Desenvolvedor escolhe tipo e atualiza valor; biblioteca/Prometheus cuidam do resto.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra como Ã© simples, conceitualmente, adicionar mÃ©tricas customizadas usando as bibliotecas cliente Prometheus. O exemplo em Node.js (Listagem 9.1) ilustra os dois tipos bÃ¡sicos de mÃ©tricas:
    *   **Counter:** Usado para contar eventos que ocorrem. VocÃª o declara uma vez (dando nome e descriÃ§Ã£o) e depois simplesmente chama um mÃ©todo como `.inc()` toda vez que o evento acontece (ex: uma requisiÃ§Ã£o HTTP chega). Prometheus armazena o valor acumulado.
    *   **Gauge:** Usado para representar um valor que pode variar para cima ou para baixo. VocÃª o declara e depois usa um mÃ©todo como `.set(valor)` para atualizar seu valor atual (ex: nÃºmero de conexÃµes ativas no momento).
*   Embora a sintaxe varie entre as linguagens (Java, Go, Node.js), a ideia Ã© a mesma: declarar a mÃ©trica e depois chamar mÃ©todos da biblioteca cliente nos pontos apropriados do cÃ³digo da aplicaÃ§Ã£o para atualizar seus valores. A biblioteca entÃ£o inclui essas mÃ©tricas customizadas no endpoint `/metrics` junto com as mÃ©tricas de runtime padrÃ£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Explorar CÃ³digo (Opcional):** Se vocÃª clonou o repositÃ³rio, pode abrir as pastas das aplicaÃ§Ãµes em `ch09/exercises` (ex: `access-log/src/server.js`, `image-gallery/src/main.go`, `image-of-the-day/src/.../ImageController.java`) no VS Code e procurar pelos exemplos de cÃ³digo que definem e atualizam contadores (`Counter`) e medidores (`Gauge`) do Prometheus.
*   O prÃ³ximo exercÃ­cio mostrarÃ¡ o resultado dessas mÃ©tricas customizadas sendo expostas.

---

ğŸ“„ PÃ¡gina 172

ğŸ” Resumo dos principais pontos:

*   App Image Gallery estÃ¡ rodando (com mÃ©tricas customizadas incluÃ­das).
*   **ExercÃ­cio Try-it-now:** Gerar carga e ver mÃ©tricas customizadas do app Node.js (`access-log`).
    *   1. Gerar 5-10 requisiÃ§Ãµes para o site (`http://localhost:8010`) usando loop `for` (comandos Windows/Linux fornecidos). Isso farÃ¡ o site chamar a API `access-log`.
    *   2. Acessar endpoint de mÃ©tricas do `access-log`: `http://localhost:8012/metrics` (Nota: porta 8012 pode precisar ser descoberta se for aleatÃ³ria).
*   Figura 9.7: Mostra saÃ­da do `/metrics` do `access-log`.
    *   Destaca as mÃ©tricas customizadas: `access_log_total` (counter) e `access_client_ip_current` (gauge).
    *   TambÃ©m inclui mÃ©tricas de runtime do Node.js padrÃ£o.
*   MÃ©tricas customizadas simples (counters/gauges) jÃ¡ permitem instrumentaÃ§Ã£o detalhada.
*   **Diretrizes para MÃ©tricas Customizadas (O que medir?):**
    *   Chamadas para sistemas externos: medir duraÃ§Ã£o e sucesso/falha.
    *   Eventos importantes (vale a pena logar?): talvez seja melhor/mais barato registrar como mÃ©trica (incrementar counter).
    *   MÃ©tricas de negÃ³cio (comportamento do usuÃ¡rio, KPIs): permitem dashboards em tempo real.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio tem como objetivo observar as mÃ©tricas *customizadas* que foram adicionadas Ã  aplicaÃ§Ã£o `access-log` (Node.js).
    1.  Primeiro, gera-se trÃ¡fego para a aplicaÃ§Ã£o web (`image-gallery`). Cada acesso Ã  web causa uma chamada para a API `access-log`.
    2.  Depois, acessa-se diretamente o endpoint `/metrics` do contÃªiner `access-log` (na porta 8012, conforme definido no `docker-compose.yml` de `ch09/exercises`).
*   A Figura 9.7 mostra que, alÃ©m das mÃ©tricas padrÃ£o do Node.js, agora aparecem as mÃ©tricas customizadas (`access_log_total`, `access_client_ip_current`) definidas na Listagem 9.1, com seus valores atuais refletindo a carga gerada.
*   A pÃ¡gina finaliza com sugestÃµes sobre quais tipos de mÃ©tricas customizadas sÃ£o geralmente Ãºteis: monitorar interaÃ§Ãµes com sistemas externos, contar eventos chave (alternativa/complemento a logs), e rastrear indicadores de negÃ³cio.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Porta `access-log`:** (No diretÃ³rio `ch09/exercises`)
    ```powershell
    docker-compose ps
    # Verifique a porta mapeada para o serviÃ§o accesslog (deve ser 8012, conforme docker-compose.yml)
    # Se nÃ£o for 8012, use a porta correta nos prÃ³ximos passos.
    ```
2.  **Gerar Carga (Windows PowerShell):**
    ```powershell
    for ($i=1; $i -le 10; $i++) { Invoke-WebRequest -UseBasicParsing http://localhost:8010 | Out-Null }
    ```
3.  **Ver MÃ©tricas `access-log`:** Abra o navegador em `http://localhost:8012/metrics`. Procure pelas linhas `access_log_total` e `access_client_ip_current`. O valor de `access_log_total` deve ser pelo menos 10.

---

ğŸ“„ PÃ¡gina 173

ğŸ” Resumo dos principais pontos:

*   **9.3 Running a Prometheus container to collect metrics** (Rodando um contÃªiner Prometheus para coletar mÃ©tricas)
*   Modelo Prometheus: **Pull** (Prometheus busca dados das fontes), nÃ£o Push (fontes enviam dados).
*   Prometheus "raspa" (scrapes) endpoints `/metrics` configurados.
*   ConfiguraÃ§Ã£o do Prometheus: Define quais endpoints raspar.
    *   Em produÃ§Ã£o (cluster): pode descobrir contÃªineres automaticamente.
    *   No Docker Compose (single host): usa lista de nomes de serviÃ§o (DNS Docker).
*   **Listagem 9.2:** ConfiguraÃ§Ã£o do Prometheus (`prometheus.yml`) para raspar apps Image Gallery.
    ```yaml
    global:
      scrape_interval: 10s # Intervalo padrÃ£o entre coletas
    scrape_configs:
      - job_name: "image-gallery" # Nome do trabalho/alvo
        metrics_path: /metrics # Path do endpoint
        static_configs: # ConfiguraÃ§Ã£o estÃ¡tica de alvos
          - targets: ["image-gallery"] # Nome DNS do serviÃ§o a raspar
      - job_name: "iotd-api"
        metrics_path: /actuator/prometheus # Path diferente para Java/Spring
        static_configs:
          - targets: ["iotd"]
      - job_name: "access-log"
        metrics_path: /metrics
        dns_sd_configs: # ConfiguraÃ§Ã£o via DNS Service Discovery
          - names:
              - accesslog # Nome DNS a procurar
            type: A # Tipo de registro DNS a esperar (endereÃ§o IPv4)
            port: 80 # Porta onde o endpoint /metrics estÃ¡ (no container)
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Agora que as aplicaÃ§Ãµes estÃ£o expondo mÃ©tricas, a seÃ§Ã£o 9.3 foca em configurar o **Prometheus** para coletÃ¡-las.
*   O modelo de coleta do Prometheus Ã© **pull**: o servidor Prometheus Ã© configurado com uma lista de "alvos" (targets) - os endpoints `/metrics` das suas aplicaÃ§Ãµes, do Docker Engine, etc. - e ele periodicamente *acessa* esses alvos para buscar (scrape) os valores atuais das mÃ©tricas.
*   A **Listagem 9.2** mostra um exemplo de arquivo de configuraÃ§Ã£o para o Prometheus (`prometheus.yml`).
    *   `global`: Define configuraÃ§Ãµes gerais, como `scrape_interval` (aqui, coletar a cada 10 segundos).
    *   `scrape_configs`: Uma lista de "trabalhos" (jobs) de coleta. Cada job define um conjunto de alvos a serem raspados.
        *   `job_name`: Um nome para o grupo de alvos (ex: "image-gallery", "iotd-api").
        *   `metrics_path`: O caminho URL onde as mÃ©tricas estÃ£o expostas (padrÃ£o `/metrics`, mas pode ser diferente, como no caso do `iotd-api` Java/Spring).
        *   `static_configs`: Define uma lista *fixa* de alvos usando seus nomes DNS (que o Docker resolverÃ¡). Usado aqui para `image-gallery` e `iotd`.
        *   `dns_sd_configs`: Define alvos usando **descoberta de serviÃ§o via DNS**. Prometheus consultarÃ¡ o DNS para o nome `accesslog`, esperando mÃºltiplos endereÃ§os IP (registros tipo `A`), e tentarÃ¡ raspar o `/metrics` na porta `80` de cada IP encontrado. Isso Ã© ideal para serviÃ§os escalados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Este arquivo `prometheus.yml` serÃ¡ usado para configurar um contÃªiner Prometheus que coletarÃ¡ mÃ©tricas dos contÃªineres da aplicaÃ§Ã£o Image Gallery. VocÃª nÃ£o precisa criar o arquivo manualmente agora; ele provavelmente serÃ¡ fornecido ou embutido em uma imagem Prometheus customizada nos prÃ³ximos exercÃ­cios.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   ConfiguraÃ§Ã£o do Prometheus: Ã‰ a lista de "endereÃ§os" que o agente do censo (Prometheus) deve visitar e o caminho da "porta" (`/metrics`) onde ele deve bater para coletar os dados. `static_configs` Ã© uma lista fixa de endereÃ§os. `dns_sd_configs` Ã© como dizer "visite todas as casas na rua 'accesslog'".

---

ğŸ“„ PÃ¡gina 174

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise da ConfiguraÃ§Ã£o Prometheus (Listagem 9.2):
    *   Poll a cada 10s.
    *   Usa DNS para achar IPs.
    *   `static_configs` para `image-gallery`/`iotd` espera 1 container; comportamento inesperado se escalar. Prometheus usa sÃ³ o 1Âº IP da lista DNS se receber vÃ¡rios.
    *   `dns_sd_configs` para `access-log` suporta mÃºltiplos IPs (escala); Prometheus raspa todos.
*   Diagrama (Figura 9.8): Ilustra Prometheus raspando mÃ©tricas dos 3 contÃªineres da aplicaÃ§Ã£o via rede Docker.
*   Imagem Prometheus customizada (`diamol/ch09-prometheus`) foi criada pelo autor.
    *   Baseada na imagem oficial Prometheus.
    *   Copia o arquivo de configuraÃ§Ã£o (Listagem 9.2) para dentro da imagem.
    *   Permite rodar Prometheus prÃ©-configurado, mas pode sobrescrever config se necessÃ¡rio.
*   MÃ©tricas ficam mais interessantes com mÃºltiplos contÃªineres (escala).
*   Pode-se escalar componente Node.js (`access-log`) e Prometheus coletarÃ¡ de todos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina detalha as implicaÃ§Ãµes da configuraÃ§Ã£o do Prometheus:
    *   Usar `static_configs` (como feito para `image-gallery` e `iotd`) Ã© simples, mas assume que haverÃ¡ apenas um contÃªiner para aquele serviÃ§o. Se vocÃª escalar o serviÃ§o, o Prometheus ainda usarÃ¡ apenas o primeiro IP retornado pelo DNS, levando a mÃ©tricas incompletas ou enganosas.
    *   Usar `dns_sd_configs` (como feito para `access-log`) Ã© a maneira correta de lidar com serviÃ§os que podem ser escalados. Prometheus consultarÃ¡ o DNS, obterÃ¡ a lista de todos os IPs das rÃ©plicas e configurarÃ¡ um alvo de scraping para *cada um* deles.
*   A Figura 9.8 mostra o Prometheus "puxando" dados dos outros contÃªineres.
*   O autor menciona que criou uma imagem Prometheus customizada (`diamol/ch09-prometheus`) que jÃ¡ embute a configuraÃ§Ã£o da Listagem 9.2. Isso simplifica os prÃ³ximos exercÃ­cios, pois nÃ£o serÃ¡ preciso montar o arquivo de configuraÃ§Ã£o manualmente.
*   Prepara para o prÃ³ximo exercÃ­cio, onde o serviÃ§o `access-log` serÃ¡ escalado para demonstrar a coleta de mÃ©tricas de mÃºltiplas instÃ¢ncias usando `dns_sd_configs`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio usarÃ¡ um `docker-compose.yml` que provavelmente inclui um serviÃ§o `prometheus` usando a imagem `diamol/ch09-prometheus`, conectado Ã  mesma rede `nat` (ou `app-net`) dos serviÃ§os da aplicaÃ§Ã£o.

---

ğŸ“„ PÃ¡gina 175

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Escalar `access-log` e verificar mÃ©tricas no Prometheus.
    *   Usar arquivo `docker-compose-scale.yml` (provavelmente define `access-log` para usar porta aleatÃ³ria, permitindo escala).
    *   1. Escalar `access-log` para 3 instÃ¢ncias: `docker-compose -f docker-compose-scale.yml up -d --scale accesslog=3`.
    *   2. Gerar carga (loop `for` com 10 requests `curl` ou `iwr` para `http://localhost:8010`).
*   Website (`image-gallery`) chama `access-log` a cada request; carga deve ser balanceada entre as 3 instÃ¢ncias `access-log`.
*   Como verificar balanceamento? Usando **labels** nas mÃ©tricas Prometheus.
    *   MÃ©tricas do `access-log` incluem label `hostname` (que serÃ¡ o ID do contÃªiner Docker).
*   **ExercÃ­cio Try-it-now:** Consultar mÃ©trica `access_log_total` no Prometheus UI (`http://localhost:9090/graph`).
    *   Selecionar `access_log_total`, clicar Execute.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio visa demonstrar como o Prometheus coleta mÃ©tricas de um serviÃ§o escalado (`access-log` com 3 rÃ©plicas).
*   Usa-se `docker-compose --scale` para criar as rÃ©plicas extras.
*   Gera-se trÃ¡fego para a aplicaÃ§Ã£o web, que por sua vez chamarÃ¡ a API `access-log`. Espera-se que o balanceamento de carga do Docker distribua essas chamadas entre as 3 rÃ©plicas de `access-log`.
*   A questÃ£o Ã©: como confirmar isso usando as mÃ©tricas? A resposta estÃ¡ nas **labels** do Prometheus. A biblioteca cliente Node.js (e muitas outras) automaticamente adiciona labels Ã s mÃ©tricas, como `instance` (o IP:porta do alvo raspado) e, neste caso, o autor parece ter configurado para incluir `hostname` (o ID do contÃªiner).
*   Ao consultar a mÃ©trica `access_log_total` (que conta o total de requests recebidas) na interface do Prometheus, como hÃ¡ 3 instÃ¢ncias sendo raspadas e cada uma tem um `hostname` diferente, o Prometheus mostrarÃ¡ **trÃªs resultados separados**, um para cada contÃªiner, cada um com seu valor atual do contador. Comparando os valores, pode-se ter uma ideia de quÃ£o bem a carga foi distribuÃ­da.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar (se necessÃ¡rio):**
    ```powershell
    cd C:\DockerLabs\diamol\ch09\exercises
    ```
2.  **Rodar/Escalar App + Prometheus:** (Assume que `docker-compose-scale.yml` existe e define `prometheus` tambÃ©m)
    ```powershell
    # Parar/Remover versÃ£o anterior se houver conflito de nome/porta
    # docker-compose down
    # Rodar com escala e (presumivelmente) Prometheus
    docker-compose -f docker-compose-scale.yml up -d --scale accesslog=3
    ```
3.  **Verificar:**
    ```powershell
    docker-compose -f docker-compose-scale.yml ps
    # Deve mostrar accesslog (x3), iotd, image-gallery, prometheus
    ```
4.  **Gerar Carga:**
    ```powershell
    for ($i=1; $i -le 20; $i++) { Invoke-WebRequest -UseBasicParsing http://localhost:8010 | Out-Null } # Aumentei para 20
    ```
5.  **Consultar Prometheus:**
    *   Abra `http://localhost:9090/graph`.
    *   Digite e selecione `access_log_total`.
    *   Clique "Execute".
    *   Na aba "Table", vocÃª deverÃ¡ ver 3 linhas, cada uma com um `{hostname="..."}` diferente e um valor para `access_log_total`. Compare os valores.

---

ğŸ“„ PÃ¡gina 176

ğŸ” Resumo dos principais pontos:

*   Figura 9.9: Mostra a saÃ­da do Prometheus com 3 resultados para `access_log_total`, um por contÃªiner, diferenciados pela label `hostname`.
*   Valores podem nÃ£o ser iguais devido a fatores de rede (DNS caching, HTTP keep-alive), especialmente em single host.
*   **Labels** sÃ£o feature poderosa do Prometheus: permitem granularidade.
*   Pode-se agregar mÃ©tricas usando **PromQL (Prometheus Query Language)**.
    *   Exemplo: `sum(access_log_total) without(hostname, instance)`.
    *   `sum()`: Agrega (soma) os valores.
    *   `without(label1, label2)`: Agrupa desconsiderando essas labels (soma todos os hostnames/instances).
*   **ExercÃ­cio Try-it-now:** Adicionar nova query no Prometheus UI para agregar `access_log_total`.
    *   Clicar "Add Graph".
    *   Colar a query `sum(...)`.
    *   Clicar Execute.
    *   Resultado: GrÃ¡fico de linha mostrando o *total* de requests ao longo do tempo (Figura 9.10).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a saÃ­da do Prometheus (Figura 9.9), confirmando que as labels permitem ver dados por instÃ¢ncia.
*   Introduz a **agregaÃ§Ã£o** usando a linguagem de consulta **PromQL**. Em vez de ver os contadores individuais de cada rÃ©plica `access-log`, podemos querer ver o **total** de requisiÃ§Ãµes processadas por todas elas juntas.
*   A query `sum(access_log_total) without(hostname, instance)` faz exatamente isso:
    *   `access_log_total`: Seleciona a mÃ©trica base.
    *   `sum(...)`: Aplica a funÃ§Ã£o de soma.
    *   `without(hostname, instance)`: Diz ao `sum` para agrupar os resultados ignorando as labels `hostname` e `instance`. Como essas sÃ£o as labels que diferenciam as rÃ©plicas, o resultado Ã© a soma de todas elas.
*   O exercÃ­cio instrui a executar essa query agregada no Prometheus. O resultado nÃ£o serÃ¡ mais uma tabela com mÃºltiplas linhas, mas um Ãºnico valor agregado (ou, na visualizaÃ§Ã£o de grÃ¡fico, uma Ãºnica linha mostrando a evoluÃ§Ã£o desse total ao longo do tempo).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Continuar no Prometheus UI:** (Com a consulta `access_log_total` ainda visÃ­vel).
2.  **Adicionar GrÃ¡fico/Query:** Clique no botÃ£o "Add graph" (ou similar, pode variar ligeiramente na UI).
3.  **Inserir Query Agregada:** No novo campo de expressÃ£o, cole:
    ```promql
    sum(access_log_total) without(hostname, instance)
    ```
4.  **Executar:** Clique "Execute".
5.  **Observar:** Agora vocÃª deve ter dois grÃ¡ficos/tabelas. O primeiro mostra os valores individuais por contÃªiner. O segundo mostra um Ãºnico valor (na tabela) ou uma Ãºnica linha (no grÃ¡fico) representando a soma total. Gere mais carga (passo 4 da p.175) e veja o grÃ¡fico agregado subir.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Labels: Como etiquetas extras na sua conta de luz (ex: consumo por andar, consumo por aparelho).
*   PromQL (agregaÃ§Ã£o): Permite calcular totais ("consumo total da casa") ou mÃ©dias ("consumo mÃ©dio por aparelho") a partir dos dados detalhados.

---

ğŸ“„ PÃ¡gina 177

ğŸ” Resumo dos principais pontos:

*   Figura 9.10: Mostra grÃ¡fico do PromQL `sum(access_log_total)`.
*   PromQL Ã© poderosa (funÃ§Ãµes estatÃ­sticas, taxas de mudanÃ§a, subqueries), mas nÃ£o precisa de complexidade para dashboards Ãºteis.
*   Estrutura Prometheus (mÃ©tricas + labels) permite visualizaÃ§Ãµes Ãºteis com queries simples (filtrar por label, agregar com `sum`).
*   Figura 9.11: Exemplo de query simples para dashboard: `sum(image_gallery_requests_total{code="200"}) without(instance)`.
    *   MÃ©trica: `image_gallery_requests_total`.
    *   Filtro por label: `{code="200"}` (sÃ³ requests OK).
    *   AgregaÃ§Ã£o: `sum(...) without(instance)` (soma total entre instÃ¢ncias).
    *   Resultado: NÂº total de respostas HTTP 200 do serviÃ§o `image-gallery`.
*   Prometheus UI Ã© bom para explorar e testar queries, mas nÃ£o Ã© dashboard final. Para isso, usa-se **Grafana**.
*   **9.4 Running a Grafana container to visualize metrics** (Rodando um contÃªiner Grafana para visualizar mÃ©tricas)
    *   Monitoramento Ã© core, mas detalhes dependem da aplicaÃ§Ã£o (o que medir, como capturar).
    *   ApÃ³s dados estarem no Prometheus, visualizaÃ§Ã£o Ã© mais padrÃ£o.
    *   Usar UI Prometheus para achar mÃ©tricas e criar queries.
    *   Rodar Grafana e plugar queries em dashboards visuais.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina enfatiza que, embora PromQL seja capaz de consultas muito complexas, vocÃª pode obter grande valor com consultas relativamente simples, principalmente combinando **filtragem por labels** e **agregaÃ§Ã£o (como `sum`)**.
*   A Figura 9.11 dÃ¡ um exemplo prÃ¡tico: contar o nÃºmero total de requisiÃ§Ãµes HTTP que o serviÃ§o `image-gallery` respondeu com sucesso (cÃ³digo 200). Isso Ã© feito selecionando a mÃ©trica `image_gallery_requests_total`, filtrando apenas as que tÃªm a label `code` igual a `"200"`, e depois somando os resultados de todas as instÃ¢ncias (`without(instance)`). Esse tipo de query Ã© a base para construir painÃ©is em um dashboard.
*   Introduz a ferramenta **Grafana** como a soluÃ§Ã£o padrÃ£o para criar dashboards bonitos e funcionais a partir dos dados armazenados no Prometheus. Enquanto o Prometheus foca na coleta e armazenamento, o Grafana foca na visualizaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Testar Query de Exemplo:** No Prometheus UI (`http://localhost:9090/graph`), execute a query:
    ```promql
    sum(image_gallery_requests_total{code="200"}) without(instance)
    ```
    VocÃª verÃ¡ o total de requests bem-sucedidas para o serviÃ§o web.
*   O prÃ³ximo passo serÃ¡ iniciar o Grafana e conectÃ¡-lo ao Prometheus.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Prometheus UI: A planilha bruta do agente do censo, com todos os dados e capacidade de fazer filtros e somas simples.
*   Grafana: Uma empresa de design grÃ¡fico que pega a planilha do censo e a transforma em grÃ¡ficos de pizza, barras, mapas de calor bonitos e fÃ¡ceis de entender para apresentar ao pÃºblico (o dashboard).

---

ğŸ“„ PÃ¡gina 178

ğŸ” Resumo dos principais pontos:

*   Figura 9.12: Mostra o dashboard final no Grafana para o app Image Gallery.
    *   VisÃ£o geral com info dos componentes e do runtime Docker.
    *   Queries suportam escala (funcionam em cluster).
*   Dashboard parece complexo, mas cada visualizaÃ§Ã£o usa query PromQL simples (filtros, agregaÃ§Ãµes).
*   Autor empacotou o dashboard em imagem Grafana customizada (`diamol/ch09-grafana`) para facilitar execuÃ§Ã£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina apresenta o objetivo final do capÃ­tulo: um dashboard completo no Grafana (Figura 9.12) que consolida informaÃ§Ãµes chave sobre a aplicaÃ§Ã£o Image Gallery e a infraestrutura Docker subjacente.
*   Embora o dashboard pareÃ§a visualmente rico e complexo, a pÃ¡gina reforÃ§a que ele Ã© construÃ­do a partir de consultas PromQL relativamente simples, como as vistas anteriormente (filtros por label + agregaÃ§Ãµes como `sum`). O poder vem da combinaÃ§Ã£o de mÃºltiplas dessas queries simples em visualizaÃ§Ãµes apropriadas (grÃ¡ficos de linha, medidores, tabelas, etc.) dentro do Grafana.
*   Para simplificar o exercÃ­cio, o autor jÃ¡ criou uma imagem Docker customizada para o Grafana (`diamol/ch09-grafana`) que vem com:
    1.  O Grafana instalado.
    2.  A conexÃ£o com o Prometheus (que tambÃ©m estarÃ¡ rodando) prÃ©-configurada.
    3.  O dashboard da Figura 9.12 jÃ¡ importado e pronto para usar.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio usarÃ¡ um `docker-compose.yml` que define nÃ£o apenas os serviÃ§os da aplicaÃ§Ã£o (`image-gallery`, `iotd`, `accesslog`) e o `prometheus`, mas tambÃ©m um serviÃ§o `grafana` usando a imagem `diamol/ch09-grafana`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Imagem Grafana customizada com dashboard: Ã‰ como receber um relatÃ³rio prÃ©-formatado e conectado Ã  planilha do censo. VocÃª sÃ³ precisa abrir o relatÃ³rio (acessar a UI do Grafana) e os grÃ¡ficos jÃ¡ estarÃ£o lÃ¡, atualizados com os dados mais recentes.

---

ğŸ“„ PÃ¡gina 179

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar app completo com Prometheus e Grafana prÃ©-configurado.
    *   1. Obter IP do host novamente (para Prometheus acessar Docker Engine) e salvar em variÃ¡vel de ambiente `HOST_IP` (diferente de `$hostIP` usado antes).
        *   Comandos PowerShell/Linux fornecidos.
    *   2. Rodar stack completa com `docker-compose -f ./docker-compose-with-grafana.yml ...`.
        *   Usa arquivo Compose que inclui Grafana.
        *   MantÃ©m escala de `accesslog=3`.
    *   3. Gerar carga (loop `for` com 20 requests).
    *   4. Acessar Grafana UI: `http://localhost:3000`.
*   Login no Grafana: user `admin`, password `admin`. PedirÃ¡ para trocar senha (pode pular - Skip).
*   Navegar no Grafana:
    *   Tela inicial -> Clicar link "Home" no topo esquerdo -> Lista de dashboards (Figura 9.13).
    *   Clicar em "Image Gallery" para abrir o dashboard da aplicaÃ§Ã£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este exercÃ­cio reÃºne tudo: inicia a aplicaÃ§Ã£o Image Gallery (com `accesslog` escalado), o Prometheus (configurado para raspar tudo, incluindo o Docker Engine usando o IP do host via `HOST_IP`) e o Grafana (prÃ©-configurado com a conexÃ£o ao Prometheus e o dashboard "Image Gallery").
*   O passo 1 repete a necessidade de obter o IP do host, mas desta vez o armazena em uma variÃ¡vel de ambiente (`HOST_IP` ou `$env:HOST_IP`) que o arquivo `docker-compose-with-grafana.yml` espera encontrar para configurar corretamente o Prometheus.
*   O passo 2 usa `docker-compose` com um arquivo `.yml` especÃ­fico que define todos os serviÃ§os (app, prometheus, grafana).
*   O passo 3 gera trÃ¡fego para popular as mÃ©tricas.
*   O passo 4 instrui a acessar a interface web do Grafana na porta 3000. Ã‰ necessÃ¡rio fazer login com as credenciais padrÃ£o (`admin`/`admin`) e entÃ£o navegar atÃ© o dashboard prÃ©-criado "Image Gallery".

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Definir HOST_IP (PowerShell):** (Use `host.docker.internal` se o IP direto nÃ£o funcionar bem)
    ```powershell
    # $env:HOST_IP = (Get-NetIPConfiguration | Where-Object {$_.IPv4DefaultGateway -ne $null -and $_.NetAdapter.Status -eq 'Up' } | Select-Object -First 1).IPv4Address.IPAddress
    $env:HOST_IP = 'host.docker.internal' # Mais confiÃ¡vel com Docker Desktop/WSL2
    echo $env:HOST_IP
    ```
2.  **Navegar e Iniciar Stack Completa:** (Estando em `ch09/exercises`)
    ```powershell
    # Parar/Remover stack anterior, se houver
    # docker-compose -f docker-compose-scale.yml down
    docker-compose -f ./docker-compose-with-grafana.yml up -d --scale accesslog=3
    ```
3.  **Verificar:**
    ```powershell
    docker-compose -f ./docker-compose-with-grafana.yml ps
    # Deve mostrar todos os serviÃ§os (app, prometheus, grafana) Up.
    ```
4.  **Gerar Carga:**
    ```powershell
    for ($i=1; $i -le 30; $i++) { Invoke-WebRequest -UseBasicParsing http://localhost:8010 | Out-Null } # Aumentei para 30
    ```
5.  **Acessar Grafana:**
    *   Abra `http://localhost:3000`.
    *   Login: `admin` / `admin`.
    *   Clique "Skip" na tela de mudanÃ§a de senha (ou mude, se preferir).
    *   Clique no tÃ­tulo "Home" no canto superior esquerdo.
    *   Na lista que aparece, clique em "Image Gallery".

---

ğŸ“„ PÃ¡gina 180

ğŸ” Resumo dos principais pontos:

*   Figura 9.13: Mostra a navegaÃ§Ã£o no Grafana para encontrar o dashboard "Image Gallery".
*   Dashboard "Image Gallery" Ã© exemplo razoÃ¡vel para produÃ§Ã£o.
*   Baseado nos "Golden Signals" do Google SRE (Site Reliability Engineering): Latency, Traffic, Errors, Saturation.
*   AnÃ¡lise detalhada da primeira linha de visualizaÃ§Ãµes (Figura 9.14) para o serviÃ§o `image-gallery` (Web UI Go).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina comeÃ§a mostrando como encontrar o dashboard no Grafana (Figura 9.13).
*   Afirma que o dashboard "Image Gallery" fornecido Ã© um bom ponto de partida, inspirado nos "Quatro Sinais de Ouro" do monitoramento definidos pelo Google SRE, que sÃ£o mÃ©tricas essenciais para entender a saÃºde de um serviÃ§o:
    *   **LatÃªncia (Latency):** Tempo para servir requisiÃ§Ãµes.
    *   **TrÃ¡fego (Traffic):** QuÃ£o demandado estÃ¡ o serviÃ§o (ex: requisiÃ§Ãµes por segundo).
    *   **Erros (Errors):** Taxa de requisiÃ§Ãµes que falham.
    *   **SaturaÃ§Ã£o (Saturation):** QuÃ£o "cheio" estÃ¡ o serviÃ§o (quÃ£o perto do limite de capacidade - ex: uso de CPU, memÃ³ria, disco).
*   ComeÃ§a a analisar a primeira linha de painÃ©is do dashboard (Figura 9.14), focada no serviÃ§o web `image-gallery`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Com o dashboard "Image Gallery" aberto no Grafana (`http://localhost:3000/...`), observe a primeira linha de grÃ¡ficos. A prÃ³xima pÃ¡gina detalharÃ¡ o que cada um representa e a query PromQL por trÃ¡s dele.

---

ğŸ“„ PÃ¡gina 181

ğŸ” Resumo dos principais pontos:

*   Figura 9.14: Detalhe da primeira linha do dashboard (serviÃ§o Web `image-gallery`).
*   **AnÃ¡lise dos 4 painÃ©is:** (Relacionados aos Golden Signals)
    1.  **HTTP 200 Responses (Respostas HTTP 200):**
        *   GrÃ¡fico de linha mostrando nÂº total de respostas "OK" ao longo do tempo. (Relacionado a Traffic/Errors - sucesso).
        *   Query PromQL: `sum(image_gallery_requests_total{code="200"}) without(instance)`. (Soma o contador `_requests_total` filtrando por `code="200"`).
        *   Poderia adicionar grÃ¡fico similar para `code="500"` (Errors).
    2.  **In-Flight Requests (RequisiÃ§Ãµes em Andamento):**
        *   GrÃ¡fico de linha mostrando nÂº de requisiÃ§Ãµes ativas no momento. (Relacionado a Saturation/Traffic).
        *   MÃ©trica Prometheus tipo `gauge`.
        *   Query PromQL: `sum(image_gallery_in_flight_requests) without(instance)`. (Soma o gauge entre instÃ¢ncias).
    3.  **Memory In Use (MemÃ³ria em Uso):**
        *   GrÃ¡fico de barras mostrando memÃ³ria usada por cada contÃªiner do serviÃ§o. (Relacionado a Saturation).
        *   Query PromQL: `go_memstats_stack_inuse_bytes{job="image-gallery"}`. (MÃ©trica padrÃ£o Go; filtro `{job=...}` necessÃ¡rio para distinguir de mÃ©tricas com mesmo nome de outras fontes, como Docker Engine).
    4.  **Active Goroutines (Goroutines Ativas):**
        *   GrÃ¡fico de linha mostrando nÂº de Goroutines (unidades de trabalho concorrente em Go). Indicador de quÃ£o "ocupado" estÃ¡ o serviÃ§o. (Relacionado a Saturation).
        *   Query PromQL: `sum(go_goroutines{job="image-gallery"}) without(instance)`. (Soma mÃ©trica padrÃ£o Go entre instÃ¢ncias).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina detalha as quatro primeiras visualizaÃ§Ãµes do dashboard do Grafana, explicando o que cada uma mede e qual consulta PromQL simples Ã© usada para obter os dados do Prometheus.
*   **Respostas 200:** Usa um `counter` (total de requests) e filtra por cÃ³digo de status 200 para mostrar o trÃ¡fego bem-sucedido.
*   **RequisiÃ§Ãµes em Andamento:** Usa um `gauge` para mostrar quantas requisiÃ§Ãµes estÃ£o sendo processadas *agora*, um indicador de carga e possÃ­vel saturaÃ§Ã£o.
*   **MemÃ³ria em Uso:** Usa uma mÃ©trica padrÃ£o do runtime Go (`go_memstats...`) para mostrar o consumo de memÃ³ria. O filtro `job="image-gallery"` Ã© importante para selecionar apenas a mÃ©trica vinda dos contÃªineres desse serviÃ§o especÃ­fico (jÃ¡ que o Prometheus pode estar coletando mÃ©tricas com nomes similares de outras fontes). O grÃ¡fico de barras Ã© Ãºtil para comparar mÃºltiplas instÃ¢ncias lado a lado se o serviÃ§o estivesse escalado.
*   **Goroutines Ativas:** Outra mÃ©trica padrÃ£o Go, indicando o nÃ­vel de concorrÃªncia interna. Um aumento sÃºbito pode indicar um problema ou sobrecarga.
*   A ideia Ã© que essas quatro mÃ©tricas juntas dÃ£o uma boa visÃ£o instantÃ¢nea da saÃºde e carga do serviÃ§o web, relacionando-se com os "Sinais de Ouro" do Google SRE.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Explorar no Grafana:** No dashboard "Image Gallery" (`http://localhost:3000/...`):
    *   Passe o mouse sobre cada um desses 4 grÃ¡ficos.
    *   Clique no tÃ­tulo de um grÃ¡fico e selecione "Edit".
    *   Na tela de ediÃ§Ã£o, vocÃª pode ver a(s) query(ies) PromQL exatas usadas na aba "Query".
    *   Explore as opÃ§Ãµes de visualizaÃ§Ã£o ("Visualization") para ver como o grÃ¡fico de linha ou barra Ã© configurado.
    *   Cancele ou volte para o dashboard (sem salvar mudanÃ§as, a menos que queira experimentar).

---

ğŸ“„ PÃ¡gina 182

ğŸ” Resumo dos principais pontos:

*   VisualizaÃ§Ãµes nas outras linhas do dashboard usam queries similares. NÃ£o precisa de PromQL complexo. Escolher mÃ©tricas e visualizaÃ§Ãµes certas Ã© chave.
*   **TendÃªncias** sÃ£o mais importantes que valores absolutos. Anomalias (picos, quedas) sÃ£o mais fÃ¡ceis de ver.
*   Conjunto de mÃ©tricas deve ajudar a ver anomalias e **correlaÃ§Ãµes** rapidamente (ex: erros subindo e Goroutines subindo -> saturaÃ§Ã£o?).
*   Grafana Ã© ferramenta poderosa, mas simples de usar para o bÃ¡sico.
*   Dashboards podem ser editados (adicionar/editar painÃ©is, redimensionar, mover) e **exportados** como JSON.
*   **ExercÃ­cio Try-it-now:** Adicionar painel de Erros HTTP ao dashboard.
    *   MÃ©trica de erro (HTTP 500) estÃ¡ faltando.
    *   1. No dashboard, clicar Ã­cone "Add Panel" (grÃ¡fico de barras com '+'). (Figura 9.15).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a que dashboards eficazes podem ser construÃ­dos com queries PromQL simples, focando em selecionar as mÃ©tricas corretas e a melhor forma de visualizÃ¡-las (linhas para sÃ©ries temporais, barras para comparaÃ§Ãµes, medidores para valores atuais, etc.).
*   Enfatiza a importÃ¢ncia de observar **tendÃªncias e anomalias** nos grÃ¡ficos, em vez de se fixar em valores absolutos. Um pico repentino no uso de memÃ³ria Ã© mais informativo do que saber que o uso mÃ©dio Ã© X MB.
*   A capacidade de **correlacionar** diferentes mÃ©tricas no mesmo dashboard Ã© vital para diagnÃ³stico (ex: ver que um aumento nos erros coincide com um aumento na latÃªncia de uma dependÃªncia externa).
*   Menciona que os dashboards do Grafana sÃ£o customizÃ¡veis (editar painÃ©is) e podem ser salvos/exportados como arquivos JSON, facilitando o versionamento e compartilhamento.
*   O exercÃ­cio instrui a adicionar um novo painel ao dashboard existente para visualizar as requisiÃ§Ãµes que resultaram em erro HTTP 500 para o serviÃ§o `image-gallery`, completando os "Sinais de Ouro" (TrÃ¡fego, Erros, SaturaÃ§Ã£o, LatÃªncia - embora latÃªncia nÃ£o esteja explicitamente mostrada nesses exemplos).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Iniciar EdiÃ§Ã£o:** No dashboard "Image Gallery" no Grafana, localize e clique no Ã­cone "Add panel" (geralmente no canto superior direito, parece um grÃ¡fico com um '+').

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Observar tendÃªncias vs. valores: Ã‰ como monitorar a febre de um paciente. Saber que a temperatura *subiu* de 37Â°C para 39Â°C rapidamente Ã© mais importante do que apenas saber que *agora* estÃ¡ 39Â°C.
*   CorrelaÃ§Ã£o: Ver no painel do carro que a temperatura do motor estÃ¡ subindo *ao mesmo tempo* que a luz de Ã³leo acendeu ajuda a diagnosticar o problema mais rapidamente.

---

ğŸ“„ PÃ¡gina 183

ğŸ” Resumo dos principais pontos:

*   Figura 9.15: Mostra a barra de ferramentas do Grafana com Ã­cone "Add Panel" destacado.
*   **ContinuaÃ§Ã£o do ExercÃ­cio (Adicionar Painel de Erros):**
    *   2. Clicar "Add Query" na janela do novo painel.
    *   3. Selecionar "Prometheus" como data source.
    *   4. No campo "Metrics" (ou Query), colar a expressÃ£o PromQL: `sum(image_gallery_requests_total{code="500"}) without(instance)`.
    *   5. Painel deve se parecer com Figura 9.16. Pode precisar gerar alguns erros (difÃ­cil no app de exemplo?) para ver dados.
    *   6. Pressionar `Esc` para voltar ao dashboard.
*   Pode-se redimensionar (arrastar canto) e mover (arrastar tÃ­tulo) painÃ©is.
*   Salvar/Exportar dashboard: Ãcone "Share Dashboard" (Figura 9.15) > Export > Save to file.
*   **Empacotando Imagem Grafana Customizada:**
    *   Imagem `diamol/ch09-grafana` jÃ¡ faz isso.
*   **Listagem 9.3:** Dockerfile para criar imagem Grafana customizada (`diamol/ch09-grafana`).
    ```dockerfile
    FROM diamol/grafana:6.4.3 # Baseia-se em imagem Grafana especÃ­fica
    # Copia arquivos de provisionamento (configuraÃ§Ã£o) para locais esperados pelo Grafana
    COPY datasource-prometheus.yaml ${GF_PATHS_PROVISIONING}/datasources/
    COPY dashboard-provider.yaml ${GF_PATHS_PROVISIONING}/dashboards/
    # Copia o arquivo JSON do dashboard exportado para pasta de dashboards
    COPY dashboard.json /var/lib/grafana/dashboards/
    ```
*   ExplicaÃ§Ã£o: Imagem comeÃ§a de uma base Grafana, depois copia arquivos de configuraÃ§Ã£o. Grafana usa **provisioning** para configurar data sources e dashboards automaticamente na inicializaÃ§Ã£o a partir de arquivos em locais especÃ­ficos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina guia atravÃ©s da adiÃ§Ã£o do novo painel no Grafana para mostrar erros HTTP 500. A query PromQL Ã© similar Ã  de sucessos (HTTP 200), mas filtra por `code="500"`.
*   Menciona como reorganizar e salvar/exportar o dashboard resultante como um arquivo JSON. Exportar Ã© Ãºtil para colocar a definiÃ§Ã£o do dashboard sob controle de versÃ£o (Git) junto com o cÃ³digo da aplicaÃ§Ã£o e os arquivos Docker/Compose.
*   Explica como a imagem `diamol/ch09-grafana` (usada no exercÃ­cio da p.179) foi criada. A Listagem 9.3 mostra um Dockerfile que:
    1.  ComeÃ§a com uma imagem base oficial do Grafana (`diamol/grafana` que por sua vez vem de `grafana/grafana`).
    2.  Usa a funcionalidade de **provisioning** do Grafana. Isso permite prÃ©-configurar o Grafana copiando arquivos YAML e JSON para diretÃ³rios especÃ­ficos dentro da imagem.
    3.  `datasource-prometheus.yaml`: Arquivo YAML que diz ao Grafana como se conectar ao Prometheus (endereÃ§o, etc.).
    4.  `dashboard-provider.yaml`: Arquivo YAML que diz ao Grafana para procurar por arquivos de dashboard em um diretÃ³rio especÃ­fico.
    5.  `dashboard.json`: O arquivo JSON do dashboard (que poderia ter sido exportado do Grafana UI) Ã© copiado para o diretÃ³rio que o provider estÃ¡ monitorando.
*   Quando um contÃªiner Ã© iniciado a partir desta imagem customizada, o Grafana automaticamente lÃª esses arquivos de provisionamento e configura a conexÃ£o com o Prometheus e carrega o dashboard "Image Gallery", tudo sem intervenÃ§Ã£o manual.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Completar AdiÃ§Ã£o do Painel:** Siga os passos no Grafana UI para adicionar a query `sum(image_gallery_requests_total{code="500"}) without(instance)`. Nomeie o painel (ex: "HTTP 500 Errors"). Pressione `Esc`.
2.  **Reorganizar/Salvar (Opcional):** Arraste o novo painel para a posiÃ§Ã£o desejada. Clique no Ã­cone de disquete (Save dashboard) no topo direito. DÃª uma mensagem de salvamento (ex: "Added 500 errors panel") e clique Save.
3.  **Exportar (Opcional):** Clique no Ã­cone de compartilhamento (ao lado do save) > Aba "Export" > Clique "Save to file". Isso baixa o `dashboard.json`.
4.  **Entender Provisioning:** Se vocÃª explorar o cÃ³digo fonte do livro em `ch09/images/grafana`, encontrarÃ¡ os arquivos `datasource-prometheus.yaml`, `dashboard-provider.yaml`, `dashboard.json` e o `Dockerfile` (similar Ã  Listagem 9.3) usados para construir `diamol/ch09-grafana`.

---

ğŸ“„ PÃ¡gina 184

ğŸ” Resumo dos principais pontos:

*   Figura 9.16: Mostra o painel de erros HTTP 500 sendo adicionado no Grafana.
*   Provisioning do Grafana pode fazer mais (criar usuÃ¡rios, playlists de dashboards).
*   Exemplo: Criar imagem com mÃºltiplos dashboards e usuÃ¡rio read-only para exibir em tela grande no escritÃ³rio, rotacionando automaticamente (playlist).
*   **9.5 Understanding the levels of observability** (Entendendo os nÃ­veis de observabilidade)
    *   Observabilidade Ã© requisito chave para produÃ§Ã£o.
    *   IntroduÃ§Ã£o a Prometheus/Grafana tambÃ©m mostra poder do ecossistema Docker e padrÃµes emergentes.
    *   Monitoramento era dor de cabeÃ§a inicial com contÃªineres; hoje hÃ¡ caminho testado.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a parte do Grafana, mencionando que o sistema de provisioning Ã© bastante flexÃ­vel, permitindo automatizar outras configuraÃ§Ãµes alÃ©m de data sources e dashboards.
*   A seÃ§Ã£o 9.5 faz uma recapitulaÃ§Ã£o da importÃ¢ncia da **observabilidade** (ter insights sobre o sistema) ao usar contÃªineres em produÃ§Ã£o.
*   Enfatiza que a pilha Prometheus + Grafana representa um padrÃ£o bem estabelecido e eficaz no ecossistema de contÃªineres para alcanÃ§ar essa observabilidade, superando os desafios iniciais que existiam quando os contÃªineres surgiram.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. Consolida a importÃ¢ncia das ferramentas apresentadas no capÃ­tulo.

---

ğŸ“„ PÃ¡gina 185

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 9.17): Sumariza a arquitetura de monitoramento com Prometheus no centro.
*   Dashboard apresentado Ã© visÃ£o geral; produÃ§Ã£o teria dashboards adicionais mais detalhados:
    *   Dashboard de Infraestrutura (uso de disco/CPU/memÃ³ria/rede dos nÃ³s do cluster).
    *   Dashboards por Componente (detalhes especÃ­ficos de cada serviÃ§o, ex: tempo por endpoint de API).
*   Dashboard sumÃ¡rio Ã© crÃ­tico: deve mostrar info chave rapidamente para identificar problemas.
*   **9.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Adicionar monitoramento (Prometheus/Grafana) ao app **To-Do List** (de capÃ­tulos anteriores).
    *   Ponto de Partida: Nova imagem `diamol/ch09-todo-list` jÃ¡ expÃµe mÃ©tricas (`/metrics`).
*   **Tarefa:**
    1.  Criar arquivo Docker Compose para rodar:
        *   App To-Do (`diamol/ch09-todo-list`).
        *   ContÃªiner Prometheus.
        *   ContÃªiner Grafana.
    2.  Configurar Prometheus para raspar mÃ©tricas do app To-Do.
    3.  Configurar Grafana com dashboard para mostrar 3 mÃ©tricas chave do To-Do: nÂº tarefas criadas, total requisiÃ§Ãµes HTTP, requisiÃ§Ãµes HTTP em andamento.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 9.17 resume a arquitetura de monitoramento discutida: aplicaÃ§Ãµes expÃµem mÃ©tricas, Prometheus coleta, Grafana visualiza.
*   A pÃ¡gina sugere que, em um cenÃ¡rio real, vocÃª provavelmente teria *mÃºltiplos* dashboards no Grafana: um de alto nÃ­vel (como o "Image Gallery"), outros focados na infraestrutura do cluster, e talvez dashboards ainda mais detalhados para cada microsserviÃ§o individual.
*   O **Lab do CapÃ­tulo 9** pede para aplicar o que foi aprendido a uma aplicaÃ§Ã£o diferente: o To-Do List.
*   VocÃª precisarÃ¡ criar um `docker-compose.yml` que orquestre o To-Do app, o Prometheus e o Grafana. O desafio principal estarÃ¡ em:
    1.  **Configurar o Prometheus:** Criar um arquivo `prometheus.yml` (ou usar uma imagem Prometheus customizada) que saiba como encontrar e raspar o endpoint `/metrics` do serviÃ§o `todo-list`.
    2.  **Configurar o Grafana:** Criar um dashboard (seja manualmente na UI ou via provisioning com JSON) que se conecte ao Prometheus e exiba as mÃ©tricas especÃ­ficas do To-Do mencionadas (nÂº tarefas, total requests, in-flight requests). VocÃª precisarÃ¡ primeiro rodar o To-Do e o Prometheus, explorar as mÃ©tricas disponÃ­veis na UI do Prometheus para encontrar os nomes corretos dessas mÃ©tricas, e entÃ£o construir as queries PromQL para o Grafana.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 9:**
    1.  **Criar `docker-compose.yml`:** Baseado no `docker-compose-with-grafana.yml` dos exercÃ­cios, mas:
        *   Remova os serviÃ§os `iotd`, `accesslog`, `image-gallery`.
        *   Adicione o serviÃ§o `todo-list` usando a imagem `diamol/ch09-todo-list`, conecte Ã  rede, publique uma porta (ex: 8090:80).
        *   Mantenha os serviÃ§os `prometheus` e `grafana`.
    2.  **Configurar Prometheus:**
        *   Crie um arquivo `prometheus.yml` (ou modifique o existente se estiver construindo imagem customizada).
        *   Adicione um `job` para o `todo-list`, especificando `job_name: "todo-list"`, `metrics_path: /metrics`, e `static_configs: - targets: ["todo-list"]`. (Assumindo que o serviÃ§o se chama `todo-list` no Compose).
        *   Se usar imagem Prometheus padrÃ£o, monte este arquivo como volume: `-v ./prometheus.yml:/etc/prometheus/prometheus.yml`.
    3.  **Configurar Grafana:**
        *   **OpÃ§Ã£o A (Manual):** Rode `docker-compose up -d`. Acesse Prometheus (`:9090`), encontre os nomes das mÃ©tricas do To-Do (ex: `todo_items_created_total`, `http_requests_total{app="todo"}`, `http_requests_in_flight{app="todo"}` - nomes hipotÃ©ticos). Acesse Grafana (`:3000`), configure Data Source Prometheus (se nÃ£o provisionado), crie novo dashboard, adicione painÃ©is com as queries PromQL corretas.
        *   **OpÃ§Ã£o B (Provisioning):** Exporte o dashboard criado manualmente como JSON. Crie um `Dockerfile` para Grafana (similar Ã  Listagem 9.3) que copie seu `datasource.yaml`, `provider.yaml` e `dashboard.json`. Atualize o `docker-compose.yml` para construir e usar esta imagem Grafana customizada.

---

ğŸ“„ PÃ¡gina 186

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 9 no GitHub, com imagem do dashboard final.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Fornece o link para a soluÃ§Ã£o completa do Lab 9 no repositÃ³rio do livro, caso o leitor precise de ajuda ou queira comparar sua soluÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 9, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch09/lab/README.md

---

ğŸ“„ PÃ¡gina 187

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 10: Running multiple environments with Docker Compose** (Rodando mÃºltiplos ambientes com Docker Compose)
*   RevisÃ£o Cap. 7: Usou Compose para definir e gerenciar apps multi-contÃªiner.
*   RevisÃ£o Caps. 8-9: Adicionou features de produÃ§Ã£o (health checks, monitoramento).
*   Foco Cap. 10: Retornar ao Compose para gerenciar **mÃºltiplos ambientes** (dev, teste, prod). Nem todas as features de produÃ§Ã£o sÃ£o necessÃ¡rias em todos os ambientes.
*   **Portabilidade Docker:** BenefÃ­cio maior Ã© rodar a *mesma* imagem em qualquer lugar, eliminando **drift** (diferenÃ§as) entre ambientes.
*   Drift: Causado por processos manuais, leva a falhas de deploy. Docker (imagem Ãºnica) resolve isso.
*   Mas ainda precisa de **flexibilidade** para comportamentos diferentes por ambiente.
*   Compose oferece isso via features avanÃ§adas (overrides, etc.).
*   **10.1 Deploying many applications with Docker Compose** (Implantando muitas aplicaÃ§Ãµes com Docker Compose)
    *   Compose Ã© Ã³timo para devs e ambientes nÃ£o-produÃ§Ã£o (teste, UAT, etc.).
    *   Permite rodar mÃºltiplas versÃµes/ambientes do mesmo app em hardware compartilhado (mÃ¡xima utilizaÃ§Ã£o).
    *   Necessidade de **diferenÃ§as** entre instÃ¢ncias (ex: portas publicadas, arquivos de dados).
    *   Como Compose identifica apps: **Nome do Projeto (Project Name)**.
        *   Usado para prefixar nomes de recursos (contÃªineres, redes, volumes) e em labels.
        *   PadrÃ£o: Nome do diretÃ³rio onde `docker-compose.yml` estÃ¡.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 10, que retorna ao Docker Compose com foco em gerenciar **mÃºltiplos ambientes** (desenvolvimento, teste, homologaÃ§Ã£o, etc.) usando a mesma base de cÃ³digo e imagens Docker.
*   Relembra o problema do "drift" de configuraÃ§Ã£o entre ambientes em setups tradicionais e como o Docker (com sua imagem Ãºnica) ajuda a eliminar isso. No entanto, ainda precisamos de maneiras de configurar *comportamentos* diferentes (ex: conectar a um banco de dados de teste vs. produÃ§Ã£o, habilitar logging detalhado em dev) para a mesma imagem em ambientes distintos. O Compose oferece mecanismos para isso.
*   A seÃ§Ã£o 10.1 aborda um prÃ©-requisito para rodar mÃºltiplos ambientes na mesma mÃ¡quina Docker: como o Compose distingue uma instÃ¢ncia de aplicaÃ§Ã£o da outra. A chave Ã© o **Nome do Projeto**. Por padrÃ£o, o Compose usa o nome da pasta onde o arquivo `docker-compose.yml` reside como o nome do projeto. Ele entÃ£o usa esse nome como prefixo ao criar contÃªineres, redes, etc. (ex: se a pasta Ã© `myapp`, o serviÃ§o `web` vira contÃªiner `myapp_web_1`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se vocÃª rodar `docker-compose up` na pasta `C:\DockerLabs\App1`, os contÃªineres serÃ£o nomeados como `app1_servicename_1`. Se rodar `docker-compose up` na pasta `C:\DockerLabs\App2`, os contÃªineres serÃ£o `app2_servicename_1`. Eles sÃ£o considerados projetos diferentes.
*   O prÃ³ximo exercÃ­cio mostrarÃ¡ o que acontece se tentarmos rodar o *mesmo* projeto duas vezes do *mesmo* diretÃ³rio.

---

ğŸ“„ PÃ¡gina 188

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Demonstrar conflito ao tentar rodar mesmo projeto duas vezes.
    *   Navegar para `ch10/exercises`.
    *   1. Rodar app "numbers": `docker-compose -f ./numbers/docker-compose.yml up -d`.
    *   2. Rodar app "todo-list": `docker-compose -f ./todo-list/docker-compose.yml up -d`. (Funciona, pois sÃ£o diretÃ³rios/projetos diferentes).
    *   3. Tentar rodar "todo-list" *novamente* do mesmo diretÃ³rio/arquivo: `docker-compose -f ./todo-list/docker-compose.yml up -d`.
*   Resultado esperado (Figura 10.1): Terceiro comando nÃ£o faz nada (ou diz que jÃ¡ estÃ¡ rodando/atualizado). Compose acha que app jÃ¡ estÃ¡ no estado desejado.
*   **NomeaÃ§Ã£o de Recursos Compose:**
    *   PadrÃ£o: `<project_name>_<service_name>_<index>`.
    *   `<project_name>` padrÃ£o = nome do diretÃ³rio.
*   Diagrama (Figura 10.2): Ilustra nome do contÃªiner `todo-list_todo-web_1`.
*   **SoluÃ§Ã£o para rodar mÃºltiplas cÃ³pias:** Sobrescrever nome do projeto padrÃ£o usando flag `-p` (ou `--project-name`).
*   **ExercÃ­cio Try-it-now:** Rodar segunda instÃ¢ncia do To-Do usando `-p`.
    *   Comandos:
        *   `docker-compose -f ./todo-list/docker-compose.yml -p todo-test up -d`.
        *   `docker container ls` (verificar ambos os contÃªineres `todo-list...` e `todo-test...`).
        *   `docker container port todo-test_todo-web_1 80` (descobrir porta aleatÃ³ria da instÃ¢ncia 'test').
    *   O `.yml` do To-Do usado aqui provavelmente publica porta 80 em porta aleatÃ³ria do host (como `- "80"`), por isso a necessidade de descobrir a porta com `docker port`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O primeiro exercÃ­cio confirma que, se vocÃª rodar `docker-compose up` para o mesmo projeto (mesmo diretÃ³rio/nome de projeto padrÃ£o) mÃºltiplas vezes, ele nÃ£o cria cÃ³pias adicionais; ele apenas garante que a aplicaÃ§Ã£o esteja rodando conforme definido no arquivo (ou a atualiza se o arquivo mudou).
*   Para rodar *instÃ¢ncias separadas* da *mesma* aplicaÃ§Ã£o (definida pelo mesmo `docker-compose.yml`) na mesma mÃ¡quina Docker, vocÃª precisa dar a cada instÃ¢ncia um **nome de projeto diferente**. Isso Ã© feito usando a flag `-p <novo_nome_projeto>` no comando `docker-compose`.
*   O segundo exercÃ­cio demonstra isso:
    *   JÃ¡ existe uma instÃ¢ncia do To-Do rodando (com nome de projeto padrÃ£o `todo-list`, derivado do nome da pasta).
    *   O comando `docker-compose -f ./todo-list/docker-compose.yml -p todo-test up -d` inicia uma *segunda* instÃ¢ncia, mas informa ao Compose para usar `todo-test` como nome do projeto.
    *   O Compose agora cria um novo contÃªiner chamado `todo-test_todo-web_1`, separado do `todo-list_todo-web_1`.
*   Como o arquivo `todo-list/docker-compose.yml` provavelmente usa publicaÃ§Ã£o de porta aleatÃ³ria (`ports: - "80"`), o exercÃ­cio usa `docker container port <nome_container> 80` para descobrir em qual porta do host a segunda instÃ¢ncia (`todo-test_...`) ficou acessÃ­vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch10\exercises
    ```
2.  **Rodar Apps (Passos 1 e 2):**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml up -d
    docker-compose -f ./todo-list/docker-compose.yml up -d
    ```
3.  **Tentar Rodar To-Do Novamente (Passo 3):**
    ```powershell
    docker-compose -f ./todo-list/docker-compose.yml up -d
    # Observe que a saÃ­da diz algo como 'network ... is up-to-date', 'container ... is up-to-date'
    ```
4.  **Rodar Segunda InstÃ¢ncia To-Do (com -p):**
    ```powershell
    docker-compose -f ./todo-list/docker-compose.yml -p todo-test up -d
    ```
5.  **Verificar ContÃªineres:**
    ```powershell
    docker container ls
    # VocÃª deve ver containers com prefixo 'numbers_', 'todo-list_', e 'todo-test_'
    ```
6.  **Descobrir Porta da InstÃ¢ncia 'test':**
    ```powershell
    docker container port todo-test_todo-web_1 80
    # Anote a porta retornada (ex: 0.0.0.0:32770)
    ```
    Acesse `http://localhost:<porta_anotada>` para ver a segunda instÃ¢ncia.

---

ğŸ“„ PÃ¡gina 189

ğŸ” Resumo dos principais pontos:

*   Figura 10.3: Mostra a saÃ­da do exercÃ­cio anterior, com novo contÃªiner `todo-test_...` criado e porta descoberta.
*   Usar `-p` permite rodar mÃºltiplas cÃ³pias de muitos apps diferentes.
*   Problema: Porta aleatÃ³ria nÃ£o Ã© ideal para ops/testes.
*   Para suportar setups diferentes (ex: portas fixas diferentes por ambiente), pode-se duplicar Compose files e editar, *mas* isso leva a duplicaÃ§Ã£o e drift.
*   **SoluÃ§Ã£o Melhor:** Arquivos de **Override** do Docker Compose.
*   **10.2 Using Docker Compose override files** (Usando arquivos de override do Docker Compose)
    *   Evita duplicaÃ§Ã£o e manutenÃ§Ã£o difÃ­cil de mÃºltiplos arquivos quase idÃªnticos.
    *   Compose permite **mesclar** mÃºltiplos arquivos `.yml`. Propriedades de arquivos posteriores na mesclagem *sobrescrevem* as de arquivos anteriores.
*   Diagrama (Figura 10.4): Ilustra estrutura com overrides.
    *   **Core `docker-compose.yml`:** ContÃ©m estrutura base e configuraÃ§Ãµes comuns a *todos* os ambientes.
    *   **Override `.yml` por ambiente (ex: `dev.yml`, `test.yml`, `prod.yml`):** ContÃ©m *apenas* as configuraÃ§Ãµes especÃ­ficas daquele ambiente (ex: portas diferentes, variÃ¡veis de ambiente diferentes, volumes diferentes). NÃ£o duplica o conteÃºdo do core.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui que usar `-p` resolve o problema de rodar mÃºltiplas instÃ¢ncias, mas a questÃ£o de ter configuraÃ§Ãµes *diferentes* para cada instÃ¢ncia (ex: portas fixas diferentes, conexÃ£o a bancos de dados diferentes) ainda persiste. A soluÃ§Ã£o Ã³bvia, mas ruim, seria copiar e colar o `docker-compose.yml` para cada ambiente (`docker-compose-dev.yml`, `docker-compose-test.yml`) e editar as partes necessÃ¡rias. Isso leva a muita duplicaÃ§Ã£o e dificulta a manutenÃ§Ã£o (uma mudanÃ§a comum precisa ser feita em vÃ¡rios arquivos).
*   A seÃ§Ã£o 10.2 introduz a soluÃ§Ã£o elegante do Docker Compose: **arquivos de override**. A ideia Ã©:
    1.  Ter um arquivo base (`docker-compose.yml`) com toda a estrutura e configuraÃ§Ãµes comuns.
    2.  Ter arquivos adicionais (`docker-compose.override.yml` por padrÃ£o, ou arquivos com nomes especÃ­ficos como `docker-compose.prod.yml`) que contÃªm *apenas* as definiÃ§Ãµes que precisam ser diferentes ou adicionadas para um ambiente especÃ­fico.
    3.  Ao executar `docker-compose`, vocÃª pode instruÃ­-lo (usando mÃºltiplas flags `-f`) a ler *ambos* os arquivos. O Compose inteligentemente **mescla** as definiÃ§Ãµes, com as configuraÃ§Ãµes dos arquivos posteriores na linha de comando tendo prioridade e sobrescrevendo as dos arquivos anteriores.
*   A Figura 10.4 ilustra essa separaÃ§Ã£o: um arquivo core + arquivos especÃ­ficos por ambiente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ a flag `-f` mÃºltiplas vezes nos comandos `docker-compose`:
    ```powershell
    # Exemplo conceitual para rodar ambiente de produÃ§Ã£o
    docker-compose -f docker-compose.yml -f docker-compose.prod.yml -p myapp-prod up -d
    ```
    Aqui, o Compose primeiro lÃª `docker-compose.yml`, depois lÃª `docker-compose.prod.yml`. Se ambos definirem, por exemplo, a porta para o serviÃ§o `web`, a definiÃ§Ã£o de `docker-compose.prod.yml` serÃ¡ a utilizada.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Arquivos Override: Ã‰ como ter um manual de instruÃ§Ãµes base (`docker-compose.yml`) e depois add-ons separados (`dev.yml`, `test.yml`) que dizem "Na pÃ¡gina X, ignore o passo Y e faÃ§a Z" ou "Adicione o passo W extra aqui". VocÃª lÃª o manual base junto com o add-on relevante para montar a versÃ£o correta.

---

ğŸ“„ PÃ¡gina 190

ğŸ” Resumo dos principais pontos:

*   Abordagem com overrides Ã© **sustentÃ¡vel**:
    *   MudanÃ§as comuns (ex: atualizar imagem base) sÃ£o feitas *uma vez* no arquivo core.
    *   MudanÃ§as especÃ­ficas de ambiente sÃ³ afetam o arquivo daquele ambiente.
    *   Arquivos override documentam claramente as diferenÃ§as entre ambientes.
*   **Listagem 10.1:** Exemplo simples de override para mudar a tag da imagem.
    *   Arquivo core (`docker-compose.yml`): define `image: diamol/ch06-todo-list` (implÃ­cito `:latest`).
    *   Arquivo override (`docker-compose-v2.yml`): define `image: diamol/ch06-todo-list:v2`.
    *   Override sÃ³ precisa especificar a propriedade a ser mudada, mantendo a estrutura (`services > todo-web > image`).
*   Comando `docker-compose config`: Valida e **mostra o resultado final da mesclagem** dos arquivos Compose, sem implantar. Ãštil para verificar o efeito do override.
*   **ExercÃ­cio Try-it-now:** Usar `config` para ver a mesclagem dos arquivos da Listagem 10.1.
    *   Comando: `docker-compose -f ./todo-list/docker-compose.yml -f ./todo-list/docker-compose-v2.yml config`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a os benefÃ­cios da abordagem de overrides para manutenÃ§Ã£o a longo prazo.
*   A Listagem 10.1 mostra um exemplo mÃ­nimo: o arquivo override (`docker-compose-v2.yml`) contÃ©m apenas o necessÃ¡rio para mudar a `image` do serviÃ§o `todo-web` para a versÃ£o `:v2`. Note que a estrutura (`services:` > `todo-web:`) precisa ser mantida para que o Compose saiba onde aplicar a mudanÃ§a.
*   Introduz o comando `docker-compose config`. Ele Ã© extremamente Ãºtil ao trabalhar com mÃºltiplos arquivos Compose. Ele executa o mesmo processo de leitura e mesclagem que o `docker-compose up` faria, mas em vez de aplicar as mudanÃ§as, ele apenas **imprime o arquivo YAML resultante final** no console (apÃ³s validaÃ§Ã£o). Isso permite que vocÃª veja exatamente qual configuraÃ§Ã£o serÃ¡ usada antes de realmente implantar a aplicaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch10\exercises
    ```
2.  **Verificar Mesclagem:**
    ```powershell
    docker-compose -f ./todo-list/docker-compose.yml -f ./todo-list/docker-compose-v2.yml config
    ```
3.  **Analisar SaÃ­da:** Procure pela seÃ§Ã£o `services:` > `todo-web:`. VocÃª verÃ¡ que todas as propriedades (`ports`, `environment`, `networks`) vieram do primeiro arquivo, mas a propriedade `image:` mostrarÃ¡ `diamol/ch06-todo-list:v2`, que veio do segundo arquivo, sobrescrevendo a original. (A ordem das seÃ§Ãµes na saÃ­da pode ser alfabÃ©tica, nÃ£o a original).

---

ğŸ“„ PÃ¡gina 191

ğŸ” Resumo dos principais pontos:

*   Figura 10.5: Mostra a saÃ­da do `docker-compose config`, destacando a tag `:v2` da imagem que foi sobrescrita.
*   Ordem dos arquivos `-f` importa: arquivos Ã  direita sobrescrevem os da esquerda. `config` ajuda a verificar.
*   SaÃ­da do `config` ordena alfabeticamente (networks, services, version), Ãºtil para comparar (`diff`) entre versÃµes.
*   Exemplo mais realista: App Random Number (`./numbers/`) com mÃºltiplos overrides:
    *   `docker-compose.yml`: Core (serviÃ§os web/api, sem portas/redes).
    *   `docker-compose-dev.yml`: Para dev (cria rede dev, adiciona portas, desabilita healthchecks).
    *   `docker-compose-test.yml`: Para teste (cria rede test, healthchecks, publica porta web, API interna).
    *   `docker-compose-uat.yml`: Para UAT (cria rede uat, porta 80 web, restart always, healthchecks rigorosos).
*   **Listagem 10.2:** ConteÃºdo do override `dev.yml`. Mostra que sÃ³ contÃ©m as diferenÃ§as/adiÃ§Ãµes (sem `image:`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma a anÃ¡lise da saÃ­da do `docker-compose config`. Enfatiza que a **ordem** em que vocÃª lista os arquivos com `-f` Ã© crucial, pois determina qual arquivo tem prioridade para sobrescrever.
*   Introduz um exemplo mais completo e realista usando a aplicaÃ§Ã£o Random Number, com arquivos Compose separados para os ambientes `dev`, `test` e `uat`, alÃ©m do arquivo `core`. Cada arquivo override define apenas as configuraÃ§Ãµes especÃ­ficas daquele ambiente (portas, redes, configuraÃ§Ãµes de healthcheck, polÃ­ticas de reinÃ­cio, etc.), aproveitando a definiÃ§Ã£o base do arquivo core.
*   A Listagem 10.2 mostra o conteÃºdo do `docker-compose-dev.yml`. Note que ele nÃ£o repete a definiÃ§Ã£o da `image`, apenas adiciona ou modifica outras propriedades como `ports`, `healthcheck` (desabilitando-o para dev) e define uma rede especÃ­fica para o ambiente de desenvolvimento.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Explorar Arquivos:** Navegue para `C:\DockerLabs\diamol\ch10\exercises\numbers` no seu explorador de arquivos ou VS Code. Abra e compare `docker-compose.yml`, `docker-compose-dev.yml`, `docker-compose-test.yml`, `docker-compose-uat.yml`. Observe como eles se complementam.
2.  **Verificar Mesclagem para Dev:**
    ```powershell
    # Estando na pasta ch10\exercises
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-dev.yml config
    # Observe como as definiÃ§Ãµes foram combinadas.
    ```
3.  **Verificar Mesclagem para UAT:**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-uat.yml config
    # Compare a saÃ­da com a do Dev. Note as diferenÃ§as nas portas, redes, restart, healthcheck.
    ```

---

ğŸ“„ PÃ¡gina 192

ğŸ” Resumo dos principais pontos:

*   Arquivos override (`test.yml`, `uat.yml`) seguem mesmo padrÃ£o (sÃ³ diferenÃ§as).
*   Cada ambiente usa portas e redes diferentes -> podem rodar simultaneamente na mesma mÃ¡quina.
*   **ExercÃ­cio Try-it-now:** Rodar o app Random Number em mÃºltiplos ambientes (dev, test, uat) simultaneamente.
    *   1. Remover contÃªineres existentes: `docker container rm -f $(docker container ls -aq)`.
    *   2. Rodar Dev: `docker-compose -f ./numbers/... -f ./numbers/dev.yml -p numbers-dev up -d`.
    *   3. Rodar Test: `docker-compose -f ./numbers/... -f ./numbers/test.yml -p numbers-test up -d`.
    *   4. Rodar UAT: `docker-compose -f ./numbers/... -f ./numbers/uat.yml -p numbers-uat up -d`.
*   Resultado: 3 cÃ³pias do app rodando, isoladas por redes Docker diferentes. Figura 10.6 mostra saÃ­da.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que os arquivos de override para os ambientes de Teste e UAT (User Acceptance Testing) seguem o mesmo princÃ­pio do `dev.yml`, apenas definindo as configuraÃ§Ãµes especÃ­ficas para cada um. A chave para permitir que todos rodem ao mesmo tempo na mesma mÃ¡quina Docker Ã© que cada override define:
    *   Um **nome de rede diferente** (ex: `numbers-dev`, `numbers-test`, `numbers-uat`).
    *   **Portas publicadas diferentes** para os serviÃ§os expostos (ex: web em 8088 no dev, 8080 no test, 80 no uat).
*   O exercÃ­cio instrui a iniciar as trÃªs instÃ¢ncias da aplicaÃ§Ã£o, uma para cada ambiente. Note que para cada comando `docker-compose up`, sÃ£o usadas:
    *   MÃºltiplas flags `-f`: para carregar o arquivo core *e* o override do ambiente especÃ­fico.
    *   A flag `-p`: para dar um nome de projeto Ãºnico a cada instÃ¢ncia (`numbers-dev`, `numbers-test`, `numbers-uat`).
*   O resultado serÃ¡ a execuÃ§Ã£o simultÃ¢nea de trÃªs conjuntos de contÃªineres (web + api), cada conjunto isolado em sua prÃ³pria rede Docker, acessÃ­veis atravÃ©s de portas diferentes no host.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Opcional: remover redes antigas se houver conflito
    # docker network rm numbers-dev numbers-test numbers-uat numbers-app (ignore erros se nÃ£o existirem)
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch10\exercises
    ```
3.  **Rodar Ambientes:**
    ```powershell
    # Dev
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-dev.yml -p numbers-dev up -d
    # Test
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-test.yml -p numbers-test up -d
    # UAT
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-uat.yml -p numbers-uat up -d
    ```
4.  **Verificar:**
    ```powershell
    docker ps
    # VocÃª deve ver 6 containers:
    # numbers-dev_numbers-web_1, numbers-dev_numbers-api_1
    # numbers-test_numbers-web_1, numbers-test_numbers-api_1
    # numbers-uat_numbers-web_1, numbers-uat_numbers-api_1
    docker network ls
    # VocÃª deve ver as redes numbers-dev, numbers-test, numbers-uat
    ```

---

ğŸ“„ PÃ¡gina 193

ğŸ” Resumo dos principais pontos:

*   Figura 10.6: Mostra a saÃ­da da criaÃ§Ã£o dos 3 ambientes.
*   Agora existem 3 deployments isolados:
    *   UAT: `http://localhost` (porta 80).
    *   Test: `http://localhost:8080`.
    *   Dev: `http://localhost:8088`.
*   Mesma aplicaÃ§Ã£o, mas cada instÃ¢ncia web sÃ³ vÃª a API na *sua prÃ³pria rede*.
*   Isolamento: Causar bug na API do Dev nÃ£o afeta Test/UAT.
*   ContÃªineres usam DNS (`numbers-api`) para comunicaÃ§Ã£o, mas Docker restringe trÃ¡fego Ã  rede do contÃªiner.
*   Diagrama (Figura 10.7): Ilustra isolamento via redes Docker separadas para cada ambiente.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que as trÃªs instÃ¢ncias da aplicaÃ§Ã£o estÃ£o rodando isoladamente, acessÃ­veis em portas diferentes no `localhost`.
*   O ponto crucial Ã© o **isolamento proporcionado pelas redes Docker separadas**. Embora a aplicaÃ§Ã£o web em todos os ambientes tente se conectar ao serviÃ§o chamado `numbers-api` (definido no `docker-compose.yml`), o DNS interno do Docker resolverÃ¡ esse nome *apenas* para o IP do contÃªiner `numbers-api` que estÃ¡ *na mesma rede* que o contÃªiner web que fez a consulta.
*   Portanto, o `numbers-dev_numbers-web_1` sÃ³ consegue falar com o `numbers-dev_numbers-api_1`, o `numbers-test_numbers-web_1` sÃ³ fala com o `numbers-test_numbers-api_1`, e assim por diante. Isso garante que os ambientes nÃ£o interfiram uns nos outros. A Figura 10.7 visualiza essa separaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Testar Acesso:**
    *   Abra `http://localhost` no navegador (UAT).
    *   Abra `http://localhost:8080` (Test).
    *   Abra `http://localhost:8088` (Dev).
    *   Todas devem mostrar a mesma UI inicial do Random Number.
2.  **Testar Isolamento:**
    *   VÃ¡ para a aba do ambiente Dev (`:8088`). Clique no botÃ£o "Get Random Number" 4 ou mais vezes para causar o bug na API *deste ambiente*. O botÃ£o deve parar de funcionar ou mostrar erro.
    *   VÃ¡ para as abas dos ambientes Test (`:8080`) e UAT (`:80`). Clique no botÃ£o. Eles devem continuar funcionando normalmente (pelas primeiras 3 vezes), pois a falha na API do Dev nÃ£o os afetou.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Redes Docker separadas por ambiente: Ã‰ como ter sistemas de interfone completamente separados para prÃ©dios diferentes (Dev, Test, UAT) que por acaso estÃ£o no mesmo terreno (mÃ¡quina host). Ligar para o "Apartamento API" do prÃ©dio Dev sÃ³ conecta com a API do Dev, nÃ£o com as dos outros prÃ©dios.

---

ğŸ“„ PÃ¡gina 194

ğŸ” Resumo dos principais pontos:

*   Relembra: Compose Ã© client-side, precisa dos arquivos `.yml` corretos e nome do projeto (`-p`) para gerenciar apps.
*   Se quiser derrubar (`down`) um ambiente especÃ­fico (ex: Test), precisa usar o mesmo comando `docker-compose` (com os mesmos `-f` e `-p`) que usou para subir (`up`).
*   **ExercÃ­cio Try-it-now:** Tentar derrubar o ambiente 'test' de formas incorretas e depois da forma correta.
    *   1. `docker-compose down` (sem `-f`, `-p`): NÃ£o remove nada (procura projeto 'numbers' padrÃ£o, nÃ£o acha).
    *   2. `docker-compose -f ... -f test.yml down` (sem `-p`): Tenta remover projeto 'numbers' com config de teste. *Pode causar erros* ao tentar remover rede (nome explÃ­cito) que tem containers de 'numbers-test' ainda ligados.
    *   3. `docker-compose -f ... -f test.yml -p numbers-test down` (Correto): Encontra e remove recursos do projeto 'numbers-test'.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina aborda como **parar e remover** os recursos de um ambiente especÃ­fico que foi iniciado com mÃºltiplos arquivos `-f` e a flag `-p`.
*   A regra Ã©: o comando `docker-compose down` (ou `stop`, `start`, `ps`, etc.) precisa receber exatamente as **mesmas informaÃ§Ãµes de contexto** (arquivos `-f` na ordem correta e nome do projeto `-p`) que foram usadas no comando `docker-compose up` para aquele ambiente especÃ­fico.
*   O exercÃ­cio demonstra isso por tentativa e erro:
    1.  Tentar `down` sem argumentos falha porque o Compose procura pelo projeto padrÃ£o do diretÃ³rio atual (`numbers`), que nÃ£o corresponde aos projetos que estÃ£o rodando (`numbers-dev`, `numbers-test`, `numbers-uat`).
    2.  Tentar `down` com os arquivos `-f` corretos mas *sem* o `-p numbers-test` tambÃ©m falha de forma confusa. O Compose usa o projeto padrÃ£o (`numbers`) mas aplica a configuraÃ§Ã£o de rede do `test.yml` (que define a rede `numbers-test` explicitamente). Ele nÃ£o encontra contÃªineres do projeto `numbers` para remover, mas tenta remover a rede `numbers-test` (achando que pertence ao projeto `numbers`). Isso falha porque os contÃªineres do projeto *real* (`numbers-test`) ainda estÃ£o conectados a ela.
    3.  Apenas o comando `down` que inclui *todos* os `-f` originais *e* o `-p numbers-test` correto consegue identificar e remover com sucesso os contÃªineres e a rede associados Ã  instÃ¢ncia de teste.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Tentativa 1 (Falha):** (Estando em `ch10/exercises`)
    ```powershell
    docker-compose down # NÃ£o deve remover nada dos 3 ambientes
    ```
2.  **Tentativa 2 (Erro/ConfusÃ£o):**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-test.yml down
    # Deve mostrar erros sobre remover a rede 'numbers-test'
    docker ps # Containers de 'numbers-test' ainda devem estar rodando
    ```
3.  **Tentativa 3 (Sucesso):**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-test.yml -p numbers-test down
    # Deve remover containers e rede 'numbers-test' com sucesso
    docker ps # Containers de 'numbers-test' nÃ£o devem mais aparecer
    # docker network ls # Rede 'numbers-test' nÃ£o deve mais aparecer
    ```

---

ğŸ“„ PÃ¡gina 195

ğŸ” Resumo dos principais pontos:

*   Figura 10.8: Mostra a saÃ­da das tentativas de `down`, incluindo o erro da segunda tentativa e o sucesso da terceira.
*   ExplicaÃ§Ã£o do erro na segunda tentativa:
    *   Sem `-p`, usou projeto padrÃ£o `numbers`.
    *   NÃ£o achou contÃªineres `numbers_...`, achou que jÃ¡ tinham sido removidos.
    *   Achou rede `numbers-test` (nome explÃ­cito no `test.yml`, sem prefixo de projeto padrÃ£o).
    *   Tentou remover a rede, mas falhou (Docker impede remover rede com contÃªineres `numbers-test_...` ainda conectados).
*   LiÃ§Ã£o: Precisa ter cuidado com Compose. Ã‰ Ã³timo para nÃ£o-produÃ§Ã£o, mas requer gerenciamento (scripts, automaÃ§Ã£o) para deploy/teardown de mÃºltiplos ambientes definidos por mÃºltiplos arquivos e `-p`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa em detalhe por que a segunda tentativa de `docker-compose down` falhou de forma estranha. O problema foi a mistura: usar os arquivos de configuraÃ§Ã£o que definiam a rede `numbers-test` explicitamente, mas nÃ£o fornecer o nome do projeto (`-p numbers-test`) fez o Compose pensar que essa rede pertencia ao projeto padrÃ£o (`numbers`). Como nÃ£o havia contÃªineres *desse* projeto (`numbers`) conectados, ele tentou remover a rede, mas o Docker Engine corretamente impediu, pois contÃªineres do projeto *real* (`numbers-test`) ainda estavam usando-a.
*   A conclusÃ£o reforÃ§a que, embora poderoso para rodar mÃºltiplos ambientes, gerenciar esses ambientes com Compose (especialmente quando se usa mÃºltiplos `-f` e `-p`) exige disciplina. Ã‰ altamente recomendÃ¡vel **automatizar** os comandos `up` e `down` para cada ambiente em scripts, para garantir que as flags `-f` e `-p` corretas sejam sempre usadas consistentemente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Exemplo de Script PowerShell para Gerenciar Ambientes:**
    ```powershell
    # Script: manage-env.ps1
    param(
        [Parameter(Mandatory=$true)]
        [ValidateSet('dev', 'test', 'uat')]
        [string]$Environment,

        [Parameter(Mandatory=$true)]
        [ValidateSet('up', 'down', 'ps', 'logs')]
        [string]$Action
    )

    $baseDir = "C:\DockerLabs\diamol\ch10\exercises\numbers" # Ajuste o caminho
    $coreFile = Join-Path -Path $baseDir -ChildPath "docker-compose.yml"
    $overrideFile = Join-Path -Path $baseDir -ChildPath "docker-compose-$Environment.yml"
    $projectName = "numbers-$Environment"

    Write-Host "Executando AÃ§Ã£o '$Action' para Ambiente '$Environment' (Projeto '$projectName')"

    docker-compose -f $coreFile -f $overrideFile -p $projectName $Action
    ```
*   **Uso:**
    ```powershell
    .\manage-env.ps1 -Environment test -Action up -d # Inicia Teste
    .\manage-env.ps1 -Environment dev -Action logs # VÃª logs do Dev
    .\manage-env.ps1 -Environment test -Action down # Derruba Teste
    ```
    *(Este script Ã© apenas um exemplo, pode precisar de ajustes)*.

---

ğŸ“„ PÃ¡gina 196

ğŸ” Resumo dos principais pontos:

*   **10.3 Injecting configuration with environment variables and secrets** (Injetando configuraÃ§Ã£o com variÃ¡veis de ambiente e secrets)
*   Isolamento (redes) e diferenÃ§as estruturais (overrides) resolvidos. PrÃ³ximo: **configuraÃ§Ã£o da aplicaÃ§Ã£o** entre ambientes.
*   Apps leem config de variÃ¡veis de ambiente ou arquivos. Compose suporta ambos.
*   OpÃ§Ãµes serÃ£o cobertas com exemplos.
*   Exemplo: App To-Do novamente. ConfiguraÃ§Ãµes que variam:
    *   NÃ­vel de Logging (verbose em dev, menos em prod).
    *   Provedor de BD (SQLite local ou BD separado/containerizado).
    *   String de ConexÃ£o do BD (se nÃ£o usar SQLite).
*   Uso de overrides para injetar config, mostrando diferentes abordagens do Compose.
*   **Listagem 10.3:** Arquivo Compose core (`docker-compose.yml`) para To-Do configurado.
    *   Define serviÃ§o `todo-web`, imagem, e uso de um `secret`.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        secrets: # AplicaÃ§Ã£o usarÃ¡ este secret
          - source: todo-db-connection # Nome lÃ³gico do secret
            target: /app/config/secrets.json # Onde aparece como arquivo no container
    ```
*   `secrets`: Suportado por Compose, Swarm, K8s.
*   `source`: De onde o runtime carrega o secret. `target`: Onde ele Ã© montado no contÃªiner.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o foca em como passar **configuraÃ§Ãµes especÃ­ficas da aplicaÃ§Ã£o** (nÃ£o apenas a estrutura dos serviÃ§os/redes) para os contÃªineres usando Docker Compose, permitindo que a mesma imagem se comporte diferentemente em cada ambiente.
*   O exemplo do To-Do App Ã© usado novamente, identificando trÃªs tipos de configuraÃ§Ã£o que tipicamente variam: nÃ­vel de log, tipo de banco de dados e detalhes de conexÃ£o.
*   A Listagem 10.3 mostra um arquivo Compose base que jÃ¡ define que o serviÃ§o `todo-web` precisarÃ¡ de um `secret` chamado `todo-db-connection`, que serÃ¡ disponibilizado como um arquivo em `/app/config/secrets.json` dentro do contÃªiner. A aplicaÃ§Ã£o lerÃ¡ este arquivo para encontrar, por exemplo, a string de conexÃ£o do banco de dados. Como vimos antes, no Compose, a *definiÃ§Ã£o* de onde esse secret vem (de um arquivo local) estarÃ¡ em outra seÃ§Ã£o do arquivo ou em um override.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Este arquivo `docker-compose.yml` (Listagem 10.3) serÃ¡ a base. Os prÃ³ximos arquivos override (dev, test) adicionarÃ£o as configuraÃ§Ãµes especÃ­ficas e a definiÃ§Ã£o da origem do `secret`.
*   O caminho `target` para o secret (`/app/config/secrets.json`) usa barras normais (`/`), que geralmente funcionam bem mesmo em contÃªineres Windows para a maioria das aplicaÃ§Ãµes modernas que lidam com paths de forma abstrata.

---

ğŸ“„ PÃ¡gina 197

ğŸ” Resumo dos principais pontos:

*   Arquivo core (Listagem 10.3) sozinho Ã© invÃ¡lido (falta seÃ§Ã£o `secrets:` e definiÃ§Ã£o da origem).
*   **Listagem 10.4:** Override para ambiente de **Desenvolvimento** (`docker-compose-dev.yml`).
    ```yaml
    services:
      todo-web:
        ports:
          - 8089:80 # Porta especÃ­fica para dev
        environment: # Via variÃ¡vel de ambiente
          - Database:Provider=Sqlite # Configura app para usar SQLite
        env_file: # Via arquivo .env no host
          - ./config/logging.debug.env # Carrega variÃ¡veis deste arquivo (para logging)
    secrets: # Define a origem do secret referenciado no serviÃ§o
      todo-db-connection:
        file: ./config/empty.json # Usa um JSON vazio, pois SQLite nÃ£o precisa de conexÃ£o
    ```
*   **TrÃªs abordagens para injetar config neste override:**
    1.  **`environment:`**: Define variÃ¡vel de ambiente diretamente no YAML (`Database:Provider=Sqlite`). FÃ¡cil, claro para valores simples nÃ£o sensÃ­veis.
    2.  **`env_file:`**: Aponta para um arquivo (`.env`) no host. Compose lÃª cada linha (`CHAVE=VALOR`) desse arquivo e as define como variÃ¡veis de ambiente no contÃªiner. Bom para compartilhar um conjunto de settings entre serviÃ§os. Aqui, usado para configurar logging detalhado (Debug).
    3.  **`secrets:` (definiÃ§Ã£o):** Define a origem do secret `todo-db-connection`. Usa `file:` para apontar para um arquivo local (`empty.json`). Como o modo Ã© SQLite, um arquivo JSON vazio Ã© suficiente (nÃ£o precisa de string de conexÃ£o).
*   **ExercÃ­cio Try-it-now:** Rodar app em config Dev usando core + override. Verificar logs.
    *   Remover contÃªineres.
    *   Comando Linux: `docker-compose -f ...core.yml -f ...dev.yml -p todo-dev up -d`.
    *   Comando Windows (precisa de 3Âº override `dev-windows.yml` para ajustar path do secret?): `docker-compose -f ...core.yml -f ...dev.yml -f ...dev-windows.yml -p todo-dev up -d`.
    *   `curl http://localhost:8089/list`.
    *   `docker logs --tail 4 todo-dev_todo-web_1`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Listagem 10.4 mostra o arquivo de override para o ambiente de desenvolvimento, demonstrando **trÃªs maneiras** diferentes de injetar configuraÃ§Ã£o usando o Compose:
    1.  **Diretamente via `environment`:** Para configuraÃ§Ãµes simples e nÃ£o sensÃ­veis, definir a variÃ¡vel de ambiente diretamente no YAML Ã© o mais claro. Aqui, `Database:Provider=Sqlite` configura o app.
    2.  **Via `env_file`:** Para um conjunto de configuraÃ§Ãµes relacionadas (como nÃ­veis de log para diferentes partes do app), colocÃ¡-las em um arquivo `.env` separado no host (`logging.debug.env`) e referenciÃ¡-lo com `env_file:` mantÃ©m o YAML principal mais limpo. O Compose lÃª esse arquivo e injeta as variÃ¡veis no contÃªiner.
    3.  **Via `secrets` (definindo a fonte):** Completa a definiÃ§Ã£o do `secret` `todo-db-connection` referenciado no serviÃ§o. Como em dev estamos usando SQLite (configurado via `environment`), nÃ£o precisamos de uma string de conexÃ£o real, entÃ£o usamos um arquivo JSON vazio (`empty.json`) como fonte.
*   O exercÃ­cio instrui a rodar a aplicaÃ§Ã£o combinando o arquivo core (Listagem 10.3) e este override de dev (Listagem 10.4), usando `-f` e `-p`. A menÃ§Ã£o a um terceiro arquivo (`dev-windows.yml`) sugere que o path em `secrets: file:` pode precisar de ajuste para Windows, similar ao exercÃ­cio da p.138. O objetivo Ã© verificar se a configuraÃ§Ã£o (SQLite, logging Debug) foi aplicada corretamente olhando os logs.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar/Criar Arquivos:** No diretÃ³rio `ch10\exercises\todo-list-configured`, verifique/crie:
    *   `docker-compose.yml` (Listagem 10.3).
    *   `docker-compose-dev.yml` (Listagem 10.4).
    *   `docker-compose-dev-windows.yml` (Verifique seu conteÃºdo, provavelmente sÃ³ ajusta o path em `secrets: file:` para `./config/empty.json` ou `.\config\empty.json`).
    *   Pasta `config` contendo `logging.debug.env` e `empty.json`.
2.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    ```
3.  **Rodar Ambiente Dev:**
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-dev.yml -f ./docker-compose-dev-windows.yml -p todo-dev up -d
    ```
4.  **Testar e Ver Logs:**
    ```powershell
    Invoke-WebRequest http://localhost:8089/list
    docker logs --tail 10 todo-dev_todo-web_1 # Aumentei tail para 10
    ```
    Procure por logs detalhados (nÃ­vel Debug) e talvez alguma indicaÃ§Ã£o de SQLite nos logs.

---

ğŸ“„ PÃ¡gina 198

ğŸ” Resumo dos principais pontos:

*   Figura 10.9: Mostra saÃ­da do exercÃ­cio anterior, com logs detalhados (SQL do SQLite) confirmando config Debug.
*   Deployment Dev usou variÃ¡veis de ambiente e secrets (carregados de arquivos).
*   **Abordagem Alternativa:** Usar variÃ¡veis de ambiente *do host* para fornecer valores *dentro* do Compose file.
    *   Sintaxe: `${VARIAVEL_HOST}` ou `${VARIAVEL_HOST:-valor_padrao}`.
    *   Compose substitui o token pelo valor da variÃ¡vel de ambiente da mÃ¡quina onde `docker-compose` estÃ¡ rodando.
    *   Se variÃ¡vel nÃ£o existir, usa `valor_padrao` (se fornecido).
    *   Torna deploy mais portÃ¡vel (pode mudar config alterando variÃ¡veis do host, sem mudar arquivos Compose). Ãštil para mÃºltiplos servers de teste com configs diferentes.
*   **Listagem 10.5:** Trecho de override para Teste (`docker-compose-test.yml`), usando substituiÃ§Ã£o de variÃ¡vel para a porta.
    ```yaml
    todo-web:
      ports:
        - "${TODO_WEB_PORT}:80" # Porta do host vem da variÃ¡vel de ambiente
      environment:
        - Database:Provider=Postgres # Configura para Postgres
      env_file:
        - ./config/logging.information.env # Logging menos verboso
      networks:
        - app-net
    ```
*   Arquivo `test.yml` tambÃ©m inclui serviÃ§o `todo-db` e `secret` para conexÃ£o (nÃ£o mostrados na listagem).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz outra tÃ©cnica de configuraÃ§Ã£o flexÃ­vel no Docker Compose: **substituiÃ§Ã£o de variÃ¡veis de ambiente**.
*   Em vez de fixar um valor diretamente no arquivo `.yml` (como `ports: - "8089:80"`), vocÃª pode usar a sintaxe `${NOME_VARIAVEL_HOST}`. Quando `docker-compose` processar o arquivo, ele procurarÃ¡ por uma variÃ¡vel de ambiente chamada `NOME_VARIAVEL_HOST` na mÃ¡quina onde o comando `docker-compose` estÃ¡ sendo executado. Se encontrar, ele substituirÃ¡ `${NOME_VARIAVEL_HOST}` pelo valor dessa variÃ¡vel.
*   A sintaxe `${NOME_VARIAVEL_HOST:-valor_padrao}` adiciona um valor padrÃ£o a ser usado caso a variÃ¡vel de ambiente nÃ£o esteja definida no host.
*   **Vantagem:** Permite controlar partes da configuraÃ§Ã£o do Compose (como portas publicadas, tags de imagem, talvez atÃ© nomes de arquivos de config) atravÃ©s de variÃ¡veis de ambiente externas, sem precisar modificar os arquivos `.yml`. Isso Ã© Ãºtil para pipelines de CI/CD ou para configurar o mesmo conjunto de arquivos `.yml` de forma ligeiramente diferente em mÃ¡quinas diferentes.
*   A Listagem 10.5 mostra isso sendo usado para a porta do serviÃ§o `todo-web` no ambiente de teste. A porta externa serÃ¡ definida pelo valor da variÃ¡vel `TODO_WEB_PORT` no host. Este arquivo tambÃ©m configura o app para usar Postgres e um nÃ­vel de log diferente (Information).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Definir VariÃ¡vel no Host (PowerShell - temporÃ¡rio para sessÃ£o atual):**
    ```powershell
    $env:TODO_WEB_PORT = 8877
    echo $env:TODO_WEB_PORT # Verificar
    ```
*   **Verificar SubstituiÃ§Ã£o com `docker-compose config`:** (Assumindo que `docker-compose-test.yml` existe em `todo-list-configured`)
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-test.yml -f ./docker-compose-test-windows.yml config
    ```
    Procure pela seÃ§Ã£o `ports:` do `todo-web`. Deve mostrar `- "8877:80"`, confirmando que a variÃ¡vel do host foi usada. Se vocÃª *nÃ£o* definir `TODO_WEB_PORT` antes, e o YAML usar `${TODO_WEB_PORT}`, o Compose darÃ¡ um aviso e usarÃ¡ uma string vazia (provavelmente causando erro ou porta aleatÃ³ria). Se usar `${TODO_WEB_PORT:-8080}`, usaria 8080 como padrÃ£o.

---

ğŸ“„ PÃ¡gina 199

ğŸ” Resumo dos principais pontos:

*   **Arquivo `.env`:** Feature do Compose para facilitar gerenciamento de variÃ¡veis de ambiente.
    *   Se Compose encontra arquivo chamado `.env` no diretÃ³rio atual, ele automaticamente carrega as definiÃ§Ãµes `CHAVE=VALOR` desse arquivo como variÃ¡veis de ambiente *antes* de processar o comando (`up`, `config`, etc.).
    *   VariÃ¡veis do `.env` podem ser usadas para substituiÃ§Ã£o (`${VAR}`) dentro dos arquivos `.yml`.
    *   VariÃ¡veis do `.env` tambÃ©m podem configurar o *prÃ³prio* `docker-compose` (ex: quais arquivos `-f` usar por padrÃ£o, qual `-p` usar por padrÃ£o).
*   **ExercÃ­cio Try-it-now:** Rodar ambiente Teste *sem* especificar `-f` ou `-p`, usando `.env`.
    *   Navegar para `ch10/exercises/todo-list-configured` (ou `...-windows`).
    *   Rodar: `docker-compose up -d`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz o arquivo `.env`, uma convenÃ§Ã£o usada pelo `docker-compose` para simplificar a configuraÃ§Ã£o e execuÃ§Ã£o.
*   **Como funciona:**
    1.  Quando vocÃª executa `docker-compose` (qualquer comando como `up`, `down`, `config`) em um diretÃ³rio.
    2.  Ele automaticamente procura por um arquivo chamado `.env` nesse diretÃ³rio.
    3.  Se encontrado, ele lÃª as linhas no formato `CHAVE=VALOR` e as carrega como variÃ¡veis de ambiente para o processo do `docker-compose`.
*   **Usos do `.env`:**
    1.  **SubstituiÃ§Ã£o em `.yml`:** As variÃ¡veis carregadas do `.env` podem ser usadas com a sintaxe `${VAR}` dentro dos seus arquivos `docker-compose.yml` (como visto com `TODO_WEB_PORT` na pÃ¡gina anterior).
    2.  **ConfiguraÃ§Ã£o do Compose:** VariÃ¡veis especiais no `.env` podem controlar o comportamento padrÃ£o do `docker-compose`:
        *   `COMPOSE_PROJECT_NAME`: Define o nome do projeto padrÃ£o (equivalente a usar `-p`).
        *   `COMPOSE_FILE`: Define uma lista de arquivos Compose a serem usados por padrÃ£o (equivalente a usar mÃºltiplos `-f`). O separador da lista Ã© definido por `COMPOSE_PATH_SEPARATOR` (geralmente `:` no Linux/Mac, `;` no Windows).
*   O exercÃ­cio instrui a ir para a pasta `todo-list-configured` (que contÃ©m um `.env` prÃ©-configurado para o ambiente de Teste) e simplesmente rodar `docker-compose up -d`. O `.env` fornecerÃ¡ o nome do projeto e a lista de arquivos a serem usados, alÃ©m das variÃ¡veis para substituiÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    # Se vocÃª estava em ch10\exercises
    cd .\todo-list-configured\
    # Ou para a versÃ£o Windows se existir:
    # cd .\todo-list-configured-windows\
    ```
2.  **Examinar `.env`:** Abra o arquivo `.env` nesta pasta com VS Code. VocÃª verÃ¡ as variÃ¡veis sendo definidas (prÃ³xima pÃ¡gina mostra o conteÃºdo).
3.  **Rodar com `.env`:**
    ```powershell
    docker-compose up -d
    ```
    Observe a saÃ­da (prÃ³xima pÃ¡gina). O Compose deve indicar qual nome de projeto e quais arquivos estÃ¡ usando (baseado no `.env`) e iniciar os contÃªineres para o ambiente de Teste (incluindo `todo-db`).

---

ğŸ“„ PÃ¡gina 200

ğŸ” Resumo dos principais pontos:

*   Figura 10.10: Mostra a saÃ­da do `docker-compose up -d` usando `.env`. Indica criaÃ§Ã£o de contÃªineres web e db, e uso do projeto `todo_ch10`.
*   Explica que `.env` configurou o Compose para usar arquivos `test.yml` e nome do projeto `todo_ch10` por padrÃ£o.
*   **Listagem 10.6:** ConteÃºdo completo do arquivo `.env`.
    ```dotenv
    # container configuration - ports to publish:
    TODO_WEB_PORT=8877
    TODO_DB_PORT=5432

    # compose configuration - files and project name:
    COMPOSE_PATH_SEPARATOR=; # Separador para Windows
    COMPOSE_FILE=docker-compose.yml;docker-compose-test.yml # Arquivos a usar
    COMPOSE_PROJECT_NAME=todo_ch10 # Nome do projeto
    ```
*   Explica as variÃ¡veis no `.env`:
    *   `TODO_WEB_PORT`, `TODO_DB_PORT`: SerÃ£o usadas para substituiÃ§Ã£o (`${...}`) nos arquivos `.yml`.
    *   `COMPOSE_PATH_SEPARATOR`: Define o caractere (`;` para Windows) usado para separar mÃºltiplos arquivos em `COMPOSE_FILE`.
    *   `COMPOSE_FILE`: Lista os arquivos Compose a serem lidos por padrÃ£o.
    *   `COMPOSE_PROJECT_NAME`: Define o nome do projeto padrÃ£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a Figura 10.10, confirmando que o `docker-compose up -d` (sem flags extras) usou as configuraÃ§Ãµes do arquivo `.env` para iniciar o ambiente de Teste com o nome de projeto `todo_ch10`.
*   A Listagem 10.6 revela o conteÃºdo do arquivo `.env` que tornou isso possÃ­vel:
    *   Define valores para `TODO_WEB_PORT` e `TODO_DB_PORT`, que serÃ£o substituÃ­dos nos `.yml` onde `${TODO_WEB_PORT}` ou `${TODO_DB_PORT}` forem usados.
    *   Define `COMPOSE_PATH_SEPARATOR` como `;` (correto para Windows; seria `:` para Linux/Mac).
    *   Define `COMPOSE_FILE` para usar `docker-compose.yml` e `docker-compose-test.yml` por padrÃ£o.
    *   Define `COMPOSE_PROJECT_NAME` como `todo_ch10` por padrÃ£o.
*   O arquivo `.env` Ã© uma maneira conveniente de definir um "ambiente padrÃ£o" para um diretÃ³rio, simplificando os comandos `docker-compose` para o caso de uso mais comum naquele diretÃ³rio.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar ServiÃ§os:** (ApÃ³s `up` da pÃ¡gina anterior)
    ```powershell
    docker-compose ps
    # Deve mostrar 'todo_ch10_todo-db_1' e 'todo_ch10_todo-web_1' rodando.
    # Note o prefixo 'todo_ch10_' vindo do COMPOSE_PROJECT_NAME.
    ```
2.  **Verificar Porta:**
    ```powershell
    docker-compose port todo-web 80
    # Deve retornar a porta definida em TODO_WEB_PORT no .env (ex: 8877).
    ```
3.  **Experimentar (Opcional):**
    *   Edite o arquivo `.env`. Mude `COMPOSE_FILE` para usar `docker-compose-dev.yml` em vez de `test.yml`. Mude `COMPOSE_PROJECT_NAME` para `todo-dev-default`.
    *   Rode `docker-compose down`.
    *   Rode `docker-compose up -d`.
    *   Verifique com `docker-compose ps`. Agora deve iniciar o ambiente Dev com o novo nome de projeto.

---

ğŸ“„ PÃ¡gina 201

ğŸ” Resumo dos principais pontos:

*   Arquivo `.env` captura settings padrÃ£o, pode ser modificado para mudar o padrÃ£o (ex: de Teste para Dev).
*   LimitacÃ£o: Compose sÃ³ procura por arquivo chamado `.env`. NÃ£o pode especificar nome diferente, dificultando *troca fÃ¡cil* entre defaults de ambientes diferentes via mÃºltiplos arquivos `.env`.
*   **SumÃ¡rio das OpÃ§Ãµes de ConfiguraÃ§Ã£o no Compose:**
    1.  **`environment:` (direto no YAML):** Mais simples, claro para settings nÃ£o-sensÃ­veis.
    2.  **`secrets:` (carregado de arquivo):** Mais flexÃ­vel (fonte pode ser arquivo local no Compose, ou secret real em Swarm/K8s), ideal para dados sensÃ­veis.
    3.  **`env_file:`:** Ãštil para agrupar settings compartilhadas entre serviÃ§os. LÃª arquivo local, injeta como variÃ¡veis de ambiente no container.
    4.  **`.env` + SubstituiÃ§Ã£o (`${VAR}`):** Define defaults/configura Compose. Bom para definir ambiente padrÃ£o ou permitir overrides externos.
*   **10.4 Reducing duplication with extension fields** (Reduzindo duplicaÃ§Ã£o com campos de extensÃ£o)
    *   Mesmo com overrides, pode haver duplicaÃ§Ã£o *dentro* de um mesmo arquivo Compose, se mÃºltiplos serviÃ§os compartilham configuraÃ§Ãµes idÃªnticas (ex: mesma config de logging, mesmas labels padrÃ£o).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina resume as quatro principais maneiras de injetar configuraÃ§Ã£o usando Docker Compose, destacando os prÃ³s e contras de cada uma:
    1.  `environment:` direto no YAML: Simples e legÃ­vel para poucos valores nÃ£o-sensÃ­veis.
    2.  `secrets:`: PadrÃ£o recomendado para dados sensÃ­veis, portÃ¡vel entre Compose e orquestradores.
    3.  `env_file:`: Bom para organizar conjuntos de variÃ¡veis, mas valores ainda sÃ£o injetados como variÃ¡veis de ambiente (menos seguro que `secrets` para dados sensÃ­veis).
    4.  SubstituiÃ§Ã£o `${VAR}` (com ou sem `.env`): Permite parametrizaÃ§Ã£o externa, mas pode tornar o YAML menos auto-contido.
*   A seÃ§Ã£o 10.4 introduz uma tÃ©cnica para lidar com outro tipo de duplicaÃ§Ã£o: quando vocÃª tem **blocos de configuraÃ§Ã£o idÃªnticos** que precisam ser repetidos para vÃ¡rios serviÃ§os *dentro do mesmo arquivo* `docker-compose.yml` (ou dentro de um conjunto de arquivos que estÃ£o sendo mesclados).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revise as Listagens 10.3, 10.4, 10.5 e 10.6 para ver exemplos de `environment`, `secrets`, `env_file` e substituiÃ§Ã£o `${VAR}`.
*   O prÃ³ximo passo mostrarÃ¡ como usar "extension fields" para evitar repetir, por exemplo, a mesma configuraÃ§Ã£o de `logging:` ou `labels:` para mÃºltiplos serviÃ§os.

---

ğŸ“„ PÃ¡gina 202

ğŸ” Resumo dos principais pontos:

*   **Extension Fields (Campos de ExtensÃ£o YAML):**
    *   Forma de definir blocos YAML reutilizÃ¡veis.
    *   Definidos fora das seÃ§Ãµes top-level (`services`, `networks`), nomeados com `&` (Ã¢ncora YAML).
    *   Prefixo `x-` Ã© convenÃ§Ã£o (nÃ£o obrigatÃ³rio).
    *   Reutilizados dentro do arquivo usando `<<: *` (merge YAML).
*   **Listagem 10.7:** Exemplo de definiÃ§Ã£o de extension fields (em `image-gallery/docker-compose-prod.yml`).
    ```yaml
    # Define bloco 'logging' com config de logging
    x-logging: &logging # Ã‚ncora '&logging'
      logging:
        driver: "fluentd" # Exemplo
        options:
          max-size: "10m"
    # Define bloco 'labels' com label padrÃ£o
    x-labels: &labels # Ã‚ncora '&labels'
      app-name: image-gallery
    ```
*   DiferenÃ§a nas definiÃ§Ãµes: `x-logging` inclui a chave `logging:`, `x-labels` nÃ£o inclui a chave `labels:`. Isso afeta como sÃ£o usados.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina introduz os **Campos de ExtensÃ£o (Extension Fields)**, que sÃ£o uma funcionalidade do prÃ³prio **YAML** (nÃ£o especÃ­fica do Docker Compose, mas suportada por ele) para definir blocos de cÃ³digo reutilizÃ¡veis. Isso ajuda a seguir o princÃ­pio DRY (Don't Repeat Yourself).
*   **DefiniÃ§Ã£o:**
    *   VocÃª cria um bloco YAML em um nÃ­vel superior (fora de `services`, `networks`, etc.).
    *   Por convenÃ§Ã£o, o nome da chave comeÃ§a com `x-` (ex: `x-logging`, `x-labels`).
    *   VocÃª associa uma **Ã¢ncora** YAML a esse bloco usando `&<nome_ancora>` (ex: `&logging`, `&labels`).
*   **Listagem 10.7** mostra dois exemplos:
    *   `x-logging`: Define um bloco completo de configuraÃ§Ã£o de `logging:`, incluindo `driver:` e `options:`. A Ã¢ncora Ã© `&logging`.
    *   `x-labels`: Define *apenas* um par chave-valor (`app-name: image-gallery`) que se destina a ser *parte* de uma seÃ§Ã£o `labels:` maior. A Ã¢ncora Ã© `&labels`.
*   A diferenÃ§a na estrutura (incluir ou nÃ£o a chave principal como `logging:` ou `labels:` na definiÃ§Ã£o da extensÃ£o) impacta como vocÃª a utiliza com a sintaxe de merge (prÃ³xima pÃ¡gina).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Abra o arquivo `ch10\exercises\image-gallery\docker-compose-prod.yml` no VS Code. Localize essas seÃ§Ãµes `x-logging:` e `x-labels:` no inÃ­cio do arquivo (ou em algum nÃ­vel superior).
*   Note a estrutura de cada uma.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Extension Fields + Ã‚ncoras YAML: Ã‰ como criar um "carimbo" (`&logging`) com um conjunto de configuraÃ§Ãµes padrÃ£o.
*   Uso com Merge (prÃ³xima pÃ¡gina): Ã‰ como pegar o carimbo (`*logging`) e aplicÃ¡-lo (`<<:`) em vÃ¡rios lugares do seu documento (definiÃ§Ãµes de serviÃ§o).

---

ğŸ“„ PÃ¡gina 203

ğŸ” Resumo dos principais pontos:

*   **Uso de Extension Fields:**
    *   Usa sintaxe de merge YAML `<<: *<nome_ancora>`.
*   **Listagem 10.8:** Exemplo de uso das extensÃµes dentro de um serviÃ§o (`iotd`).
    ```yaml
    services:
      iotd:
        image: diamol/ch18-image-of-the-day # Imagem
        ports: # Propriedade especÃ­fica do serviÃ§o
          - 8080:80
        <<: *logging # MERGE 1: Insere todo o bloco 'logging:' definido em &logging
        labels: # Propriedade especÃ­fica do serviÃ§o
          <<: *labels # MERGE 2: Insere o conteÃºdo de &labels DENTRO da seÃ§Ã£o 'labels:'
          public: api # Outra label especÃ­fica
    ```
*   **ExplicaÃ§Ã£o dos Merges:**
    *   `<<: *logging`: Como `&logging` jÃ¡ continha a chave `logging:`, ele insere toda essa seÃ§Ã£o diretamente sob `iotd:`.
    *   `<<: *labels`: Como `&labels` continha apenas `app-name: image-gallery`, ele insere esse par chave-valor *dentro* da seÃ§Ã£o `labels:` jÃ¡ existente do serviÃ§o, junto com a label `public: api`.
*   Resultado: Compose processa o arquivo, substituindo as referÃªncias `<<: *...` pelo conteÃºdo das Ã¢ncoras correspondentes.
*   **ExercÃ­cio Try-it-now:** Usar `docker-compose config` para ver o resultado da mesclagem com extensÃµes.
    *   Navegar para `../image-gallery` (relativo a `numbers`).
    *   Comando: `docker-compose -f ./docker-compose.yml -f ./docker-compose-prod.yml config`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina mostra como **usar** os campos de extensÃ£o definidos anteriormente. A sintaxe chave Ã© `<<: *<nome_ancora>`. O `<<:` Ã© o operador de merge do YAML, e `*<nome_ancora>` Ã© uma referÃªncia Ã  Ã¢ncora definida com `&<nome_ancora>`.
*   A Listagem 10.8 ilustra os dois casos:
    *   **Merge de Bloco Inteiro:** `<<: *logging` insere todo o bloco `logging:` que foi definido na Ã¢ncora `&logging`. Isso funciona porque a definiÃ§Ã£o da extensÃ£o (`x-logging`) jÃ¡ incluÃ­a a chave principal (`logging:`). Ã‰ Ãºtil para aplicar um conjunto completo de configuraÃ§Ãµes (como logging, healthcheck, deploy) a vÃ¡rios serviÃ§os.
    *   **Merge Dentro de um Bloco:** `<<: *labels` estÃ¡ *dentro* da seÃ§Ã£o `labels:` do serviÃ§o. Isso insere apenas o *conteÃºdo* da Ã¢ncora `&labels` (o par `app-name: image-gallery`) naquela posiÃ§Ã£o, permitindo combinar labels reutilizÃ¡veis com labels especÃ­ficas do serviÃ§o (`public: api`). Isso funciona porque a definiÃ§Ã£o da extensÃ£o (`x-labels`) *nÃ£o* incluÃ­a a chave principal (`labels:`).
*   O comando `docker-compose config` Ã© novamente usado para visualizar o resultado final *apÃ³s* o Compose realizar essas substituiÃ§Ãµes/merges.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    # Se estava em ch10\exercises\numbers
    cd ..\image-gallery
    # Se estava em ch10\exercises
    cd .\image-gallery
    # Caminho completo: C:\DockerLabs\diamol\ch10\exercises\image-gallery
    ```
2.  **Verificar ConfiguraÃ§Ã£o Mesclada:**
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-prod.yml config
    ```
3.  **Analisar SaÃ­da:** Encontre a definiÃ§Ã£o do serviÃ§o `iotd` na saÃ­da. Confirme que ela agora contÃ©m a seÃ§Ã£o `logging:` completa (vinda de `*logging`) e que a seÃ§Ã£o `labels:` contÃ©m tanto `app-name: image-gallery` (vinda de `*labels`) quanto `public: api`. Verifique se outros serviÃ§os (como `accesslog`, `image-gallery`) tambÃ©m receberam as configuraÃ§Ãµes de logging/labels via merge.

---

ğŸ“„ PÃ¡gina 204

ğŸ” Resumo dos principais pontos:

*   Figura 10.11: Mostra trechos da saÃ­da do `config`, confirmando que os campos de extensÃ£o foram mesclados corretamente nos serviÃ§os.
*   Extension fields sÃ£o Ãºteis para garantir boas prÃ¡ticas (ex: settings de log padrÃ£o, labels padrÃ£o) consistentemente entre serviÃ§os.
*   NÃ£o sÃ£o usados em todos os apps, mas bom ter na "caixa de ferramentas" para evitar copiar/colar YAML.
*   **LimitaÃ§Ã£o Importante:** Extension fields (Ã¢ncoras/referÃªncias YAML) **nÃ£o funcionam entre arquivos separados**.
    *   NÃ£o se pode definir `&ancora` em `docker-compose.yml` e usar `*ancora` em `override.yml`.
    *   RestriÃ§Ã£o do YAML, nÃ£o do Compose.
*   **10.5 Understanding the configuration workflow with Docker** (Entendendo o fluxo de trabalho de configuraÃ§Ã£o com Docker)
    *   Valor de ter toda a config de deploy em artefatos versionados (Git).
    *   Permite deploy de qualquer versÃ£o, devs podem replicar stack prod localmente.
    *   VariaÃ§Ãµes entre ambientes sÃ£o necessÃ¡rias; Compose permite capturÃ¡-las mantendo artefatos em source control.
    *   RecapitulaÃ§Ã£o das Ã¡reas de variaÃ§Ã£o gerenciadas com Compose:
        1.  **ComposiÃ§Ã£o da AplicaÃ§Ã£o:** Nem todo ambiente roda tudo (ex: monitoramento sÃ³ em prod?). Overrides adicionam/removem serviÃ§os.
        2.  **ConfiguraÃ§Ã£o do ContÃªiner:** Propriedades que mudam (portas, paths de volume). Overrides + redes isoladas permitem rodar mÃºltiplos ambientes em um host.
        3.  **ConfiguraÃ§Ã£o da AplicaÃ§Ã£o:** Comportamento interno (logging, features, conexÃµes BD). Overrides + env_files + secrets + substituiÃ§Ã£o ${VAR}.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que a mesclagem com extension fields funcionou como esperado (Figura 10.11). Reitera seu valor para padronizaÃ§Ã£o e evitar repetiÃ§Ã£o.
*   Aponta uma **limitaÃ§Ã£o crucial**: Ã¢ncoras e referÃªncias YAML (`&` e `*`) sÃ³ funcionam *dentro do mesmo arquivo YAML* ou dentro do conjunto de arquivos que estÃ¡ sendo processado *simultaneamente* pelo parser YAML. VocÃª nÃ£o pode definir uma Ã¢ncora em `docker-compose.yml` e referenciÃ¡-la em `docker-compose.prod.yml` quando eles sÃ£o mesclados pelo Compose usando `-f` (pois o Compose processa e mescla as *estruturas de dados* resultantes, nÃ£o o texto YAML bruto com as Ã¢ncoras).
*   A seÃ§Ã£o 10.5 faz um **resumo geral** do capÃ­tulo, conectando as diferentes tÃ©cnicas de configuraÃ§Ã£o do Compose (overrides, `-p`, redes, `environment`, `secrets`, `env_file`, substituiÃ§Ã£o `${VAR}`, extension fields) ao objetivo maior de gerenciar mÃºltiplos ambientes de forma consistente e sustentÃ¡vel.
*   O fluxo ideal Ã© ter a definiÃ§Ã£o da aplicaÃ§Ã£o versionada no Git, permitindo que qualquer versÃ£o seja implantada e que os desenvolvedores possam facilmente rodar uma configuraÃ§Ã£o similar Ã  de produÃ§Ã£o localmente. O Compose, com suas funcionalidades de override e parametrizaÃ§Ã£o, Ã© a ferramenta que permite que a *mesma imagem Docker* seja usada em todos esses cenÃ¡rios, apenas com configuraÃ§Ãµes diferentes aplicadas em tempo de execuÃ§Ã£o. As trÃªs Ã¡reas principais onde a configuraÃ§Ã£o varia (composiÃ§Ã£o, configuraÃ§Ã£o do contÃªiner, configuraÃ§Ã£o da aplicaÃ§Ã£o) sÃ£o bem cobertas pelas features do Compose.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. Ã‰ uma seÃ§Ã£o de recapitulaÃ§Ã£o e consolidaÃ§Ã£o dos conceitos e tÃ©cnicas vistas no CapÃ­tulo 10.

---

ğŸ“„ PÃ¡gina 205

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 10.12): Ilustra app To-Do rodando em config Dev (SQLite) e Test (Postgres) no mesmo host, usando redes isoladas e portas diferentes.
*   **Takeaway Importante:** Fluxo de configuraÃ§Ã£o usa a *mesma imagem Docker* em todos os ambientes.
    *   Pipeline CI/CD produz imagem testada (release candidate).
    *   Deploy em smoke-test usa imagem + config smoke.
    *   Se passar, deploy em UAT usa *mesma imagem* + config UAT.
    *   Se passar, deploy em Prod (Swarm/K8s) usa *mesma imagem* + config Prod (via manifestos do orquestrador).
    *   Garante que o software testado Ã© o mesmo que vai para produÃ§Ã£o.
*   **10.6 Lab** (LaboratÃ³rio)

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 10.12 visualiza o resultado prÃ¡tico das tÃ©cnicas do capÃ­tulo: rodar instÃ¢ncias funcionalmente diferentes (Dev com SQLite, Teste com Postgres) da *mesma aplicaÃ§Ã£o* (baseada na mesma imagem `diamol/ch06-todo-list`) lado a lado na mesma mÃ¡quina, graÃ§as ao isolamento de rede e configuraÃ§Ã£o via Compose.
*   O ponto mais crucial do capÃ­tulo Ã© reforÃ§ado: o objetivo final desse gerenciamento de configuraÃ§Ã£o Ã© permitir que a **mesma imagem Docker, construÃ­da e testada uma Ãºnica vez, seja promovida atravÃ©s de todos os ambientes atÃ© a produÃ§Ã£o**. Apenas a *configuraÃ§Ã£o externa* (fornecida via Docker Compose para dev/teste, ou via manifestos de Swarm/Kubernetes para produÃ§Ã£o) muda em cada estÃ¡gio. Isso elimina o "drift" e aumenta a confianÃ§a nas implantaÃ§Ãµes.
*   Prepara para o Lab do CapÃ­tulo 10.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. Consolida o fluxo de trabalho ideal habilitado pelo Docker e Compose.

---

ğŸ“„ PÃ¡gina 206

ğŸ” Resumo dos principais pontos:

*   **ContinuaÃ§Ã£o do Lab 10:**
    *   Objetivo: Construir definiÃ§Ãµes de ambiente (Dev e Test) para o app To-Do, garantindo que ambos possam rodar no mesmo host.
*   **Requisitos Ambiente Dev:**
    1.  Deve ser o **padrÃ£o** (rodar com `docker-compose up`).
    2.  Usar banco de dados de arquivo local (SQLite).
    3.  Publicar na porta **8089**.
    4.  Usar imagem `v2` do app To-Do (`diamol/ch06-todo-list:v2`).
*   **Requisitos Ambiente Test:**
    1.  Deve ser rodado com arquivos `-f` especÃ­ficos e nome de projeto `-p`.
    2.  Usar contÃªiner de banco de dados (Postgres) separado.
    3.  Usar um **volume** (Docker Volume) para persistir dados do Postgres.
    4.  Publicar na porta **8080**.
    5.  Usar imagem `latest` do app To-Do (`diamol/ch06-todo-list` ou `:latest`).
*   **Dicas:**
    *   Similar aos exercÃ­cios `todo-list-configured`.
    *   DiferenÃ§a principal: uso de Volume Docker para Postgres no Teste.
    *   ContÃªiner Postgres usa variÃ¡vel de ambiente `PGDATA` para definir onde salvar dados; usar isso com definiÃ§Ã£o de volume no Compose.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina detalha o **Lab do CapÃ­tulo 10**. A tarefa Ã© criar um conjunto de arquivos Docker Compose para gerenciar dois ambientes (Dev e Test) para a aplicaÃ§Ã£o To-Do.
*   **Ambiente Dev (PadrÃ£o):** Deve ser configurado para ser o padrÃ£o ao rodar `docker-compose up` no diretÃ³rio do lab. Isso provavelmente envolverÃ¡ o uso de um arquivo `.env` (como nas p.199-200) para especificar os arquivos Compose (`core.yml` + `dev.yml`) e o nome do projeto Dev por padrÃ£o. As configuraÃ§Ãµes especÃ­ficas sÃ£o: usar SQLite (via `environment`), porta 8089, e a imagem `:v2`.
*   **Ambiente Test (ExplÃ­cito):** Deve ser iniciado usando flags `-f` e `-p` explÃ­citas. ConfiguraÃ§Ãµes: usar Postgres em outro contÃªiner, usar um Volume Docker nomeado para os dados do Postgres (montado no path definido por `PGDATA` dentro do contÃªiner Postgres), porta 8080, e a imagem `:latest` do To-Do.
*   A dica sobre `PGDATA` Ã© chave para o ambiente de Teste. VocÃª precisarÃ¡ definir um serviÃ§o `postgres` (baseado em `diamol/postgres` ou `postgres:alpine`), definir um Volume Docker nomeado na seÃ§Ã£o `volumes:` do Compose, e montar esse volume no caminho que o Postgres usa para dados (que pode ser influenciado pela variÃ¡vel `PGDATA`, ou usar o padrÃ£o `/var/lib/postgresql/data`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 10:**
    1.  Crie uma pasta para o lab (ex: `ch10\lab`).
    2.  **Arquivo Core (`docker-compose.yml`):** Defina o serviÃ§o `todo-web` bÃ¡sico (sÃ³ `image`, talvez `networks`). Defina o serviÃ§o `todo-db` bÃ¡sico (sÃ³ `image`, `networks`). Defina a seÃ§Ã£o `networks`. Defina a seÃ§Ã£o `volumes` (vazia por enquanto). Defina a seÃ§Ã£o `secrets` (referenciando `todo-db-connection`).
    3.  **Arquivo Override Dev (`docker-compose.dev.yml`):**
        *   No `todo-web`: adicione `ports: ["8089:80"]`, `image: diamol/ch06-todo-list:v2`, `environment: [Database:Provider=SQLite]`.
        *   Adicione `secrets: todo-db-connection: file: ./config/empty.json`.
        *   Adicione `networks: app-net: name: todo-dev-net`.
    4.  **Arquivo Override Test (`docker-compose.test.yml`):**
        *   No `todo-web`: adicione `ports: ["8080:80"]`, `image: diamol/ch06-todo-list:latest`, `environment: [Database:Provider=Postgres]`, `depends_on: [todo-db]`.
        *   No `todo-db`: adicione `restart: unless-stopped`, `volumes: [todo-db-data:/var/lib/postgresql/data]`. (Verifique o path correto do PGDATA se necessÃ¡rio). Adicione `environment` se precisar definir user/pass/db (pode nÃ£o ser necessÃ¡rio com a imagem `diamol/postgres`).
        *   Adicione `volumes: todo-db-data: {}` (para criar volume nomeado gerenciado pelo Compose).
        *   Adicione `secrets: todo-db-connection: file: ./config/secrets.json` (crie este arquivo com a string de conexÃ£o correta, usando `Host=todo-db`).
        *   Adicione `networks: app-net: name: todo-test-net`.
    5.  **Arquivo `.env`:**
        ```dotenv
        COMPOSE_PATH_SEPARATOR=;
        COMPOSE_FILE=docker-compose.yml;docker-compose.dev.yml
        COMPOSE_PROJECT_NAME=todo-dev
        ```
        *(Ajuste `COMPOSE_FILE` se usar arquivos -windows.yml)*.
    6.  **Arquivos de Config:** Crie a pasta `config` com `empty.json` e `secrets.json`.
    7.  **Testar Dev:** `cd ch10\lab`, `docker-compose up -d`. Acesse `:8089`.
    8.  **Testar Test:** `docker-compose down`, depois `docker-compose -f ...core.yml -f ...test.yml [-f ...test-windows.yml] -p todo-test up -d`. Acesse `:8080`. Verifique persistÃªncia com `down`/`up`.

---

ğŸ“„ PÃ¡gina 207

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 10 no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Fornece o link para a soluÃ§Ã£o completa do Lab 10 no repositÃ³rio do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 10, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch10/lab/README.md

---

ğŸ“„ PÃ¡gina 208

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, separando o Lab 10 do inÃ­cio do CapÃ­tulo 11.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 209

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 11: Building and testing applications with Docker and Docker Compose** (Construindo e testando aplicaÃ§Ãµes com Docker e Docker Compose)
*   **AutomaÃ§Ã£o** Ã© central no Docker (Dockerfiles, Compose files, CLIs).
*   CLIs se encaixam bem em processos automatizados (jobs agendados, gatilhos de cÃ³digo).
*   CompatÃ­vel com qualquer servidor de automaÃ§Ã£o que rode scripts.
*   Foco Cap. 11: **IntegraÃ§Ã£o ContÃ­nua (CI)** com Docker.
    *   **CI:** Processo automatizado (build + testes) que roda regularmente.
    *   Resultado saudÃ¡vel = cÃ³digo mais recente estÃ¡ bom, empacotado, pronto para deploy (release candidate).
    *   CI tradicional era complexo (gerenciar servidor de build, ferramentas).
    *   Docker simplifica cada parte do processo CI.
*   **11.1 How the CI process works with Docker** (Como o processo de CI funciona com Docker)
    *   CI Ã© pipeline: CÃ³digo Fonte -> Passos (Build, Teste) -> Artefato testado/implantÃ¡vel.
    *   Desafio CI tradicional: Pipelines Ãºnicos por projeto/tecnologia, servidor de build complexo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 11, que conecta o Docker ao mundo da **automaÃ§Ã£o**, especificamente Ã  **IntegraÃ§Ã£o ContÃ­nua (CI)**.
*   A ideia da CI Ã© automatizar o processo de pegar o cÃ³digo mais recente do controle de versÃ£o, compilÃ¡-lo, executar testes automatizados e, se tudo passar, gerar um pacote pronto para ser implantado (um "release candidate"). Isso Ã© feito frequentemente (a cada commit, ou diariamente) para detectar problemas o mais cedo possÃ­vel.
*   O capÃ­tulo argumenta que o Docker **simplifica enormemente** a CI. Servidores de CI tradicionais eram difÃ­ceis de manter, precisando ter todas as ferramentas (SDKs, runtimes, etc.) para todos os projetos da empresa instaladas e atualizadas. Com o Docker, o prÃ³prio processo de build e teste pode rodar *dentro* de contÃªineres, eliminando a necessidade de instalar essas ferramentas no servidor de CI.
*   A seÃ§Ã£o 11.1 comeÃ§a descrevendo o fluxo bÃ¡sico de um pipeline de CI e os desafios da abordagem tradicional.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker build` (geralmente via `docker-compose build`) para a etapa de build no pipeline.
*   VocÃª usarÃ¡ `docker run` ou `docker-compose up` para executar testes (unitÃ¡rios dentro do build, ou de integraÃ§Ã£o/end-to-end com a aplicaÃ§Ã£o rodando).
*   O artefato final do pipeline serÃ¡ uma ou mais **imagens Docker** testadas, prontas para serem enviadas a um registro.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Pipeline de CI: Uma linha de montagem automatizada na fÃ¡brica de software. Entra matÃ©ria-prima (cÃ³digo), passa por etapas de montagem (build) e controle de qualidade (testes), e sai um produto acabado e inspecionado (imagem Docker).
*   CI com Docker vs. Tradicional: Ã‰ a diferenÃ§a entre ter uma linha de montagem que usa ferramentas manuais especÃ­ficas para cada produto (tradicional, difÃ­cil de manter) e uma linha que usa robÃ´s flexÃ­veis que carregam suas prÃ³prias ferramentas de uma biblioteca central (Docker, fÃ¡cil de manter e adaptar).

---

ğŸ“„ PÃ¡gina 210

ğŸ” Resumo dos principais pontos:

*   Docker traz **consistÃªncia** ao CI: mesmos passos, mesmo tipo de artefato (imagem Docker) para todos os projetos.
*   Diagrama (Figura 11.1): Passos tÃ­picos de pipeline CI com Docker.
    *   Trigger (Code Change / Schedule) -> Get Code -> Build Image (inclui Unit Tests) -> Push Image -> Run Integration Tests (com Compose) -> Run End-to-End Tests (com Compose) -> (ImplÃ­cito: Imagem testada pronta).
*   **Como Docker Ã© usado:**
    *   Build/CompilaÃ§Ã£o em contÃªineres (sem SDKs no server CI).
    *   Testes unitÃ¡rios rodam *dentro* do `docker build` (se falham, build falha).
    *   Testes mais complexos (integraÃ§Ã£o, E2E): usar `docker-compose` para subir aplicaÃ§Ã£o + contÃªiner de teste separado.
*   **Infraestrutura CI NecessÃ¡ria:**
    *   Controle de VersÃ£o centralizado (ex: GitHub, GitLab, Gogs).
    *   Registro Docker (ex: Docker Hub, registro privado, registry container).
    *   Servidor de AutomaÃ§Ã£o (ex: Jenkins, GitLab CI, GitHub Actions, Azure DevOps).
*   OpÃ§Ãµes: ServiÃ§os gerenciados (misturar ou all-in-one) OU rodar sua prÃ³pria infra CI em contÃªineres Docker.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina detalha como um pipeline de CI funciona especificamente com Docker, destacando a consistÃªncia que ele traz. A Figura 11.1 mostra um fluxo comum:
    1.  **Gatilho:** Uma mudanÃ§a no cÃ³digo ou um agendamento inicia o pipeline.
    2.  **Obter CÃ³digo:** O cÃ³digo fonte Ã© baixado do sistema de controle de versÃ£o.
    3.  **Construir Imagem:** `docker build` (provavelmente multi-estÃ¡gio) Ã© executado. Testes unitÃ¡rios rÃ¡pidos podem (e devem) ser executados *dentro* deste estÃ¡gio de build. Se eles falharem, a imagem nÃ£o Ã© construÃ­da e o pipeline para.
    4.  **Enviar Imagem:** Se o build for bem-sucedido, a imagem resultante Ã© enviada (`push`) para um Registro Docker (pode ser um registro intermediÃ¡rio de CI).
    5.  **Testes de IntegraÃ§Ã£o/E2E:** A aplicaÃ§Ã£o (possivelmente multi-contÃªiner) Ã© iniciada usando `docker-compose up` a partir das imagens recÃ©m-construÃ­das. Um outro contÃªiner, contendo os scripts de teste, tambÃ©m Ã© iniciado e executa testes contra a aplicaÃ§Ã£o rodando.
    6.  **Resultado:** Se todos os testes passarem, a imagem (ou conjunto de imagens) Ã© considerada um "release candidate" vÃ¡lido.
*   A pÃ¡gina tambÃ©m lista os componentes de infraestrutura necessÃ¡rios para suportar esse pipeline (Git, Registro, Servidor de CI) e menciona que vocÃª pode usar serviÃ§os gerenciados na nuvem ou hospedar tudo vocÃª mesmo, inclusive rodando a prÃ³pria infraestrutura de CI em contÃªineres Docker.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª verÃ¡ como configurar um `Dockerfile` para incluir testes unitÃ¡rios.
*   VocÃª usarÃ¡ `docker-compose` para definir nÃ£o apenas a aplicaÃ§Ã£o, mas tambÃ©m um serviÃ§o de "teste" que executa scripts contra os outros serviÃ§os.
*   O prÃ³ximo exercÃ­cio mostrarÃ¡ como rodar a infraestrutura de CI (Gogs, Registry, Jenkins) localmente usando Docker Compose.

---

ğŸ“„ PÃ¡gina 211

ğŸ” Resumo dos principais pontos:

*   **11.2 Spinning up build infrastructure with Docker** (Criando infraestrutura de build com Docker)
*   Rodar infra prÃ³pria pode ser Ãºtil (soberania de dados, velocidade local, backup de serviÃ§os).
*   Componentes (Git server, Docker Registry, CI server) podem rodar em contÃªineres com software open source.
    *   **Gogs:** Servidor Git leve.
    *   **Docker Registry (Open Source):** Registro bÃ¡sico.
    *   **Jenkins:** Servidor de automaÃ§Ã£o popular.
*   Arquivo Docker Compose (`ch11/exercises/infrastructure`) define essa infraestrutura.
*   Setup difere ligeiramente entre Linux/Windows (provavelmente devido a montagem de volumes/pipes).
*   NecessÃ¡rio alias `registry.local` no arquivo `hosts` (se nÃ£o feito no Cap. 5).
*   **ExercÃ­cio Try-it-now:** Iniciar a infraestrutura de build com Compose.
    *   Navegar para `ch11/exercises/infrastructure`.
    *   Comando Linux: `docker-compose -f ...yml -f ...linux.yml up -d`.
    *   Comando Windows: `docker-compose -f ...yml -f ...windows.yml up -d`.
    *   Adicionar `registry.local` ao `hosts` (se necessÃ¡rio).
    *   Verificar contÃªineres: `docker container ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o mostra como Ã© viÃ¡vel (e Ã s vezes desejÃ¡vel) rodar sua prÃ³pria infraestrutura de CI/CD localmente, usando contÃªineres Docker para hospedar as ferramentas necessÃ¡rias. As ferramentas open source escolhidas sÃ£o:
    *   **Gogs:** Uma alternativa leve e auto-hospedada ao GitHub/GitLab para gerenciamento de cÃ³digo fonte Git.
    *   **Docker Registry:** O mesmo contÃªiner de registro bÃ¡sico visto no CapÃ­tulo 5, para armazenar as imagens construÃ­das.
    *   **Jenkins:** Um servidor de automaÃ§Ã£o extremamente popular e flexÃ­vel para executar os pipelines de CI/CD.
*   Um arquivo `docker-compose.yml` (com overrides especÃ­ficos para Linux/Windows) Ã© fornecido para iniciar esses trÃªs serviÃ§os como contÃªineres interconectados.
*   A necessidade de ter o alias `registry.local` no arquivo `hosts` Ã© relembrada, pois Jenkins e Gogs podem precisar se referir ao registro por esse nome.
*   O exercÃ­cio instrui a iniciar essa pilha de infraestrutura usando `docker-compose up -d` com os arquivos apropriados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch11\exercises\infrastructure
    ```
2.  **Verificar/Adicionar `registry.local` ao Hosts:** (Se nÃ£o fez na p.93)
    *   Abra PowerShell como Admin.
    *   `Add-Content -Value "127.0.0.1 registry.local" -Path C:\Windows\System32\drivers\etc\hosts`
3.  **Iniciar Infraestrutura:**
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-windows.yml up -d
    ```
4.  **Verificar ContÃªineres:**
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-windows.yml ps
    # Ou docker ps
    # VocÃª deve ver containers para gogs, registry, jenkins rodando.
    ```
    Anote as portas publicadas (Gogs: 3000, Jenkins: 8080, Registry: 5000).

---

ğŸ“„ PÃ¡gina 212

ğŸ” Resumo dos principais pontos:

*   Figura 11.2: Mostra a saÃ­da do `up` da infraestrutura e `docker ps` listando os 3 contÃªineres (gogs, jenkins, registry).
*   NÃ­veis diferentes de automaÃ§Ã£o no setup das ferramentas:
    *   **Registry:** Funciona sem setup extra.
    *   **Jenkins:** Usa plugins e pode ser configurado via scripts na inicializaÃ§Ã£o do contÃªiner (imagem `diamol/jenkins` jÃ¡ faz isso).
    *   **Gogs:** Requer configuraÃ§Ã£o manual inicial via web UI.
*   **ConfiguraÃ§Ã£o Manual do Gogs:**
    *   **ExercÃ­cio Try-it-now:** Acessar Gogs (`http://localhost:3000`).
        *   Tela de instalaÃ§Ã£o inicial (Figura 11.3). Valores prÃ©-configurados devem estar OK.
        *   Clicar "Install Gogs".

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que os trÃªs contÃªineres da infraestrutura de CI/CD (Gogs, Jenkins, Registry) estÃ£o rodando.
*   Explica que as imagens Docker usadas para essas ferramentas tÃªm nÃ­veis diferentes de automaÃ§Ã£o de configuraÃ§Ã£o inicial:
    *   O Registry Ã© simples e nÃ£o precisa de configuraÃ§Ã£o.
    *   O Jenkins usado aqui (`diamol/jenkins`) Ã© mais sofisticado e inclui scripts que rodam na primeira inicializaÃ§Ã£o para instalar plugins, criar usuÃ¡rios e configurar jobs automaticamente.
    *   O Gogs (`diamol/gogs` provavelmente) requer uma configuraÃ§Ã£o manual inicial atravÃ©s de sua interface web na primeira vez que Ã© executado.
*   O exercÃ­cio guia atravÃ©s dessa configuraÃ§Ã£o inicial do Gogs: acessar a UI web na porta 3000, aceitar as configuraÃ§Ãµes padrÃ£o na tela de instalaÃ§Ã£o e clicar em "Install".

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Acessar Gogs:** Abra o navegador em `http://localhost:3000`.
2.  **Instalar Gogs:** Na pÃ¡gina de instalaÃ§Ã£o que aparece (similar Ã  Figura 11.3):
    *   Verifique se os caminhos e configuraÃ§Ãµes parecem corretos (geralmente os padrÃµes funcionam se o Compose foi bem feito).
    *   Role para baixo e clique no botÃ£o "Install Gogs".
    *   Aguarde a instalaÃ§Ã£o (deve ser rÃ¡pido). VocÃª serÃ¡ redirecionado para a pÃ¡gina de login/registro.

---

ğŸ“„ PÃ¡gina 213

ğŸ” Resumo dos principais pontos:

*   Figura 11.3: Mostra a tela de instalaÃ§Ã£o do Gogs.
*   ApÃ³s instalaÃ§Ã£o, Gogs redireciona para login. Sem conta default.
*   **ExercÃ­cio Try-it-now:** Registrar usuÃ¡rio no Gogs.
    *   Clicar "Register".
    *   Criar usuÃ¡rio com **username `diamol`** (importante, pois Jenkins espera esse nome).
    *   Email/senha podem ser quaisquer. (Figura 11.4).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s a instalaÃ§Ã£o rÃ¡pida do Gogs, a interface web mostra a pÃ¡gina de login. Como nenhum usuÃ¡rio foi criado ainda, Ã© preciso registrar um novo usuÃ¡rio.
*   O exercÃ­cio instrui a criar um usuÃ¡rio com o **nome de usuÃ¡rio `diamol`**. Isso Ã© crucial porque o job prÃ©-configurado no Jenkins que rodaremos mais tarde estÃ¡ configurado para interagir com o Gogs usando este nome de usuÃ¡rio especÃ­fico. A senha e o email podem ser quaisquer que vocÃª escolha e lembre.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Registrar no Gogs:** Na pÃ¡gina de login/registro do Gogs (`http://localhost:3000/user/sign_up` ou similar):
    *   Username: `diamol`
    *   Email: (qualquer um, ex: `diamol@example.com`)
    *   Password: (crie e lembre-se de uma senha)
    *   Confirm Password: (repita a senha)
    *   Clique "Create New Account".

---

ğŸ“„ PÃ¡gina 214

ğŸ” Resumo dos principais pontos:

*   Figura 11.4: Mostra a tela de registro do Gogs.
*   ApÃ³s registro, fazer login com `diamol` / sua senha.
*   **ExercÃ­cio Try-it-now:** Criar um repositÃ³rio Git no Gogs.
    *   Acessar `http://localhost:3000/repo/create`.
    *   Nome do RepositÃ³rio: `diamol`.
    *   Deixar outros detalhes vazios. (Figura 11.5).
*   FrustraÃ§Ã£o com setup manual em Docker; ideal Ã© automaÃ§Ã£o total, mas nem sempre possÃ­vel. Autor poderia ter prÃ©-configurado imagem Gogs, mas quis mostrar a realidade.
*   **Jenkins:** ExperiÃªncia melhor, pois imagem `diamol/jenkins` usa scripts para automaÃ§Ã£o.
    *   Instala plugins, cria usuÃ¡rios, cria pipeline job automaticamente.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s criar o usuÃ¡rio `diamol`, vocÃª faz login no Gogs. O prÃ³ximo passo manual Ã© criar um repositÃ³rio Git vazio onde o cÃ³digo fonte do livro serÃ¡ posteriormente enviado (`push`). O exercÃ­cio instrui a criar um repositÃ³rio chamado `diamol`.
*   O autor faz uma pausa para comentar sobre a inconveniÃªncia dessa configuraÃ§Ã£o manual do Gogs, contrastando-a com a experiÃªncia mais automatizada que se espera do Jenkins (que serÃ¡ visto a seguir). Isso serve para ilustrar que nem todas as aplicaÃ§Ãµes empacotadas em Docker sÃ£o igualmente fÃ¡ceis de automatizar na inicializaÃ§Ã£o.
*   Antecipa que a imagem Jenkins usada (`diamol/jenkins`) jÃ¡ vem com um job de pipeline prÃ©-configurado, graÃ§as a scripts de inicializaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Login no Gogs:** FaÃ§a login com `diamol` e a senha que vocÃª criou.
2.  **Criar RepositÃ³rio:**
    *   Clique no '+' no canto superior direito e selecione "New Repository", OU acesse `http://localhost:3000/repo/create`.
    *   Repository Name: `diamol`
    *   Description: (opcional)
    *   Visibility: Deixe "Private" ou mude para "Public".
    *   Initialize Repository: Deixe desmarcado (criaremos um repo vazio).
    *   Clique "Create Repository". VocÃª serÃ¡ levado Ã  pÃ¡gina do repositÃ³rio vazio.

---

ğŸ“„ PÃ¡gina 215

ğŸ” Resumo dos principais pontos:

*   Figura 11.5: Mostra a tela de criaÃ§Ã£o de repositÃ³rio no Gogs.
*   **ExercÃ­cio Try-it-now:** Acessar Jenkins (`http://localhost:8080`).
    *   UI do Jenkins (Figura 11.6) mostra job `diamol` prÃ©-configurado, com status falho.
    *   Fazer login: clicar "Log In", usar user `diamol`, pass `diamol`.
*   Job Jenkins falhou porque foi configurado para buscar cÃ³digo do Gogs, que estava vazio.
*   RepositÃ³rio do livro jÃ¡ Ã© Git; adicionar Gogs local como `remote` e fazer `push`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com o repositÃ³rio `diamol` criado no Gogs, agora podemos verificar o Jenkins.
*   O exercÃ­cio instrui a acessar a interface web do Jenkins na porta 8080. Como esperado (devido aos scripts de automaÃ§Ã£o na imagem `diamol/jenkins`), jÃ¡ existe um job chamado `diamol`. Ele estÃ¡ com status de falha porque, na primeira execuÃ§Ã£o (automÃ¡tica ou agendada), ele tentou buscar o cÃ³digo do repositÃ³rio `diamol/diamol` no Gogs, mas o repositÃ³rio estava vazio ou nÃ£o existia completamente configurado naquele momento.
*   Ã‰ preciso fazer login no Jenkins usando as credenciais padrÃ£o `diamol`/`diamol` (tambÃ©m prÃ©-configuradas pelos scripts).
*   O prÃ³ximo passo serÃ¡ popular o repositÃ³rio Gogs com o cÃ³digo fonte do livro, que jÃ¡ estÃ¡ no seu disco local como um repositÃ³rio Git (clonado do GitHub no Cap. 1). Faremos isso adicionando o Gogs local como um novo "destino remoto" (remote) ao repositÃ³rio Git local e enviando (`push`) o cÃ³digo para ele.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Acessar Jenkins:** Abra o navegador em `http://localhost:8080`.
2.  **Login Jenkins:** Clique em "Log in" (canto superior direito). Use `diamol` / `diamol`.
3.  **Observar Job:** VocÃª verÃ¡ o job `diamol` na lista, provavelmente com um Ã­cone vermelho ou cinza indicando falha ou nÃ£o execuÃ§Ã£o recente.

---

ğŸ“„ PÃ¡gina 216

ğŸ” Resumo dos principais pontos:

*   Figura 11.6: Mostra a UI do Jenkins com o job `diamol` falho.
*   **ExercÃ­cio Try-it-now:** Adicionar Gogs como remote e enviar cÃ³digo.
    *   Comandos (no terminal, na pasta raiz do cÃ³digo clonado, ex: `C:\DockerLabs\diamol`):
        *   `git remote add local http://localhost:3000/diamol/diamol.git` (Adiciona remote chamado 'local')
        *   `git push local` (Envia branch atual - `master` ou `main` - para o remote 'local')
    *   Gogs pedirÃ¡ login (usar `diamol` / senha do Gogs).
*   Agora cÃ³digo fonte estÃ¡ no Gogs local.
*   Job Jenkins verifica mudanÃ§as a cada minuto; se houver, dispara pipeline.
*   Primeira execuÃ§Ã£o falhou, Jenkins pode ter pausado agendamento. Precisa rodar manualmente agora.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina executa o passo crucial de enviar o cÃ³digo fonte do livro (que vocÃª tem localmente) para o servidor Gogs (rodando no contÃªiner).
*   Os comandos Git sÃ£o executados a partir do diretÃ³rio raiz onde vocÃª clonou o repositÃ³rio `diamol` do GitHub.
    *   `git remote add local <url>`: Adiciona um "apelido" (`local`) para um novo servidor Git remoto. A URL Ã© o endereÃ§o HTTP do repositÃ³rio `diamol` que vocÃª criou no Gogs.
    *   `git push local`: Envia o branch atual do seu repositÃ³rio local para o servidor remoto associado ao apelido `local`. O Gogs exigirÃ¡ autenticaÃ§Ã£o (usuÃ¡rio `diamol` e a senha que vocÃª definiu para o Gogs).
*   ApÃ³s o `push`, o cÃ³digo estarÃ¡ no Gogs. O Jenkins estÃ¡ configurado para verificar esse repositÃ³rio Gogs periodicamente. No entanto, como o primeiro build falhou, pode ser necessÃ¡rio iniciar o job manualmente para "reativar" o processo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar para Raiz do CÃ³digo:** Abra PowerShell/terminal e vÃ¡ para a pasta onde clonou o repositÃ³rio.
    ```powershell
    cd C:\DockerLabs\diamol
    ```
2.  **Adicionar Remote Gogs:**
    ```powershell
    git remote add local http://localhost:3000/diamol/diamol.git
    # Verificar se foi adicionado (opcional): git remote -v
    ```
3.  **Enviar CÃ³digo para Gogs:**
    ```powershell
    git push local
    ```
    Quando pedir Username e Password, use `diamol` e a senha que vocÃª criou para o *Gogs*. A saÃ­da deve indicar que os objetos foram enviados e o branch foi configurado no remoto `local`.
4.  **(Opcional) Verificar no Gogs:** Atualize a pÃ¡gina do repositÃ³rio `diamol` no Gogs (`http://localhost:3000/diamol/diamol`). VocÃª deve ver os arquivos e pastas do livro agora.

---

ğŸ“„ PÃ¡gina 217

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Iniciar build manualmente no Jenkins.
    *   Acessar pÃ¡gina do job: `http://localhost:8080/job/diamol`.
    *   Tela da Figura 11.7.
    *   Clicar "Build Now" no menu esquerdo (requer estar logado como `diamol`).
*   Aguardar build completar (1 min aprox.). PÃ¡gina atualiza.
*   Figura 11.8: Mostra job `diamol` com build #2 bem-sucedido.
*   **Como funciona? Docker-in-Docker "Trick":**
    *   Pipeline Jenkins roda *dentro* do contÃªiner Jenkins.
    *   Esse contÃªiner pode executar comandos `docker` e `docker-compose`.
    *   Comandos sÃ£o enviados para a API do Docker Engine *do host* (sua mÃ¡quina Windows).
    *   Isso Ã© habilitado montando o canal de comunicaÃ§Ã£o da API do host para dentro do contÃªiner Jenkins.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com o cÃ³digo no Gogs, o exercÃ­cio instrui a iniciar manualmente o build no Jenkins. Acessando a pÃ¡gina do job `diamol` e clicando em "Build Now", o Jenkins buscarÃ¡ o cÃ³digo mais recente do Gogs e executarÃ¡ os passos definidos no pipeline (que estÃ¡ em um arquivo `Jenkinsfile` dentro do cÃ³digo fonte).
*   ApÃ³s a conclusÃ£o bem-sucedida (indicada por um status verde ou azul na Figura 11.8), a pÃ¡gina explica *como* o Jenkins, rodando ele mesmo em um contÃªiner, consegue executar comandos `docker` que afetam o host.
*   A tÃ©cnica envolve montar o "canal de comunicaÃ§Ã£o" que a CLI `docker` local usa para falar com o Docker Engine do host (um *socket* no Linux, um *named pipe* no Windows) como um volume dentro do contÃªiner Jenkins. Quando o Jenkins (ou um script executado por ele) roda o comando `docker` *dentro* do contÃªiner, esse comando se conecta ao socket/pipe montado, que na verdade o direciona para a API do Docker Engine *do host*. Isso permite que o contÃªiner Jenkins controle o Docker na mÃ¡quina onde ele mesmo estÃ¡ rodando.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Iniciar Build:** No Jenkins (`http://localhost:8080/job/diamol`), clique em "Build Now".
2.  **Observar:** Veja o "Build History" no canto inferior esquerdo. Um novo build (provavelmente #2) aparecerÃ¡ e comeÃ§arÃ¡ a piscar. Clique nele para ver o progresso. Ele passarÃ¡ por estÃ¡gios (Checkout, Verify, Build, Test, Push). Aguarde atÃ© que fique verde (Success).
3.  **Entender a Montagem:** Relembre o `docker-compose-windows.yml` da infraestrutura (p. 211). Ele provavelmente tinha uma seÃ§Ã£o `volumes:` para o serviÃ§o `jenkins` que montava `\\.\pipe\docker_engine` (o named pipe do Docker no Windows).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Docker-in-Docker "Trick": Ã‰ como dar ao seu contÃªiner Jenkins um "telefone direto" (o socket/pipe montado) para a "central de controle" (API do Docker Engine do host), permitindo que ele dÃª ordens como se estivesse do lado de fora.

---

ğŸ“„ PÃ¡gina 218

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 11.9): Ilustra como o contÃªiner Jenkins se conecta Ã  API Docker do host via socket (Linux) ou named pipe (Windows) montado como volume.
*   CLI Docker conecta Ã  API local via socket (Linux) ou named pipe (Windows).
*   Canal pode ser usado como bind mount para contÃªineres.
*   Permite cenÃ¡rios onde app em contÃªiner consulta ou controla outros contÃªineres no mesmo host.
*   **PreocupaÃ§Ã£o de SeguranÃ§a:** ContÃªiner ganha acesso *total* ao Docker do host. Usar apenas com imagens confiÃ¡veis (`diamol` sÃ£o confiÃ¡veis).
*   **Listagem 11.1:** Trechos dos Compose files (`docker-compose.yml`, `...linux.yml`, `...windows.yml`) mostrando a montagem do volume para o serviÃ§o `jenkins`.
    *   Linux: `source: /var/run/docker.sock`, `target: /var/run/docker.sock`.
    *   Windows: `type: npipe`, `source: \\.\pipe\docker_engine`, `target: \\.\pipe\docker_engine`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 11.9 visualiza a tÃ©cnica "Docker-out-of-Docker" (o contÃªiner Jenkins controla o Docker do host, nÃ£o rodando um Docker aninhado): o socket (Linux) ou named pipe (Windows) do host Ã© mapeado para o mesmo caminho dentro do contÃªiner Jenkins.
*   A Listagem 11.1 mostra as definiÃ§Ãµes exatas nos arquivos `docker-compose.yml` (com overrides) que habilitam essa conexÃ£o:
    *   No Linux, o socket Unix padrÃ£o `/var/run/docker.sock` do host Ã© montado no mesmo caminho dentro do contÃªiner.
    *   No Windows, o named pipe `\\.\pipe\docker_engine` do host Ã© montado (usando `type: npipe`) no mesmo caminho dentro do contÃªiner.
*   Quando a CLI `docker` dentro do contÃªiner Jenkins tenta se conectar ao Docker Engine (usando seu endereÃ§o padrÃ£o), ela encontra esse socket/pipe montado e acaba falando com o Engine do host.
*   **ImplicaÃ§Ã£o de SeguranÃ§a:** Isso Ã© muito poderoso, mas tambÃ©m perigoso. O contÃªiner Jenkins (e qualquer processo rodando dentro dele) agora tem o mesmo nÃ­vel de controle sobre o Docker que vocÃª tem no host. Ele pode iniciar, parar, remover qualquer contÃªiner, volume, rede, etc. Por isso, sÃ³ se deve usar essa tÃ©cnica com imagens de contÃªiner (como a `diamol/jenkins` aqui) em que se confia completamente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª jÃ¡ rodou o `docker-compose... up -d` que usou essa configuraÃ§Ã£o na Listagem 11.1 (parte do `docker-compose-windows.yml`). O build bem-sucedido no Jenkins confirma que a conexÃ£o com o Docker Engine do host estÃ¡ funcionando.

---

ğŸ“„ PÃ¡gina 219

ğŸ” Resumo dos principais pontos:

*   Infraestrutura CI pronta: Jenkins conecta ao Engine Docker host, e tambÃ©m conecta ao Gogs e Registry (via DNS Docker, pois estÃ£o na mesma rede Compose).
*   Pipeline CI roda comando Ãºnico para build; complexidade estÃ¡ nos Dockerfiles e Compose files.
*   **11.3 Capturing build settings with Docker Compose** (Capturando configuraÃ§Ãµes de build com Docker Compose)
    *   Job Jenkins construiu nova versÃ£o do app Random Number (Cap. 8).
    *   App usa overrides Compose para capturar settings de build.
*   **Listagem 11.2:** Arquivo Compose core (`docker-compose.yml` em `ch11/exercises`) para Random Number.
    *   Usa **substituiÃ§Ã£o de variÃ¡veis de ambiente** no nome da imagem.
    ```yaml
    services:
      numbers-api:
        image: ${REGISTRY:-docker.io}/diamol/ch11-numbers-api:v3-build-${BUILD_NUMBER:-local}
        networks:
          - app-net
      numbers-web:
        image: ${REGISTRY:-docker.io}/diamol/ch11-numbers-web:v3-build-${BUILD_NUMBER:-local}
        # ... resto ...
    ```
    *   Sintaxe `${VAR:-default}`: Usa valor da variÃ¡vel `VAR` do host/ambiente. Se `VAR` nÃ£o existir ou vazia, usa `default`.
    *   `REGISTRY`: Permite direcionar push/pull para registro local (`registry.local:5000`) ou Docker Hub (`docker.io`).
    *   `BUILD_NUMBER`: Permite incluir nÃºmero do build CI na tag da imagem. PadrÃ£o `local` para builds de dev.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que a infraestrutura de CI estÃ¡ funcional e que o pipeline do Jenkins executarÃ¡ comandos Docker/Compose. A complexidade real do build estÃ¡ encapsulada nos Dockerfiles (multi-estÃ¡gio) e nos arquivos Compose que definem como construir.
*   A seÃ§Ã£o 11.3 foca em como o **Docker Compose** Ã© usado *dentro* do pipeline de CI para gerenciar o processo de **build** das imagens da aplicaÃ§Ã£o (neste caso, o app Random Number).
*   A Listagem 11.2 mostra um arquivo `docker-compose.yml` inteligente que usa **substituiÃ§Ã£o de variÃ¡veis de ambiente** (lembra do Cap. 10?) diretamente nos nomes das imagens.
    *   `image: ${REGISTRY:-docker.io}/diamol/ch11-numbers-api:v3-build-${BUILD_NUMBER:-local}`
    *   Isso torna o mesmo arquivo Compose utilizÃ¡vel tanto localmente quanto no CI:
        *   **Localmente (Dev):** As variÃ¡veis `REGISTRY` e `BUILD_NUMBER` provavelmente nÃ£o estarÃ£o definidas. O Compose usarÃ¡ os defaults, resultando em nomes como `docker.io/diamol/ch11-numbers-api:v3-build-local` (que Ã© o mesmo que `diamol/ch11-numbers-api:v3-build-local` pois `docker.io` Ã© o padrÃ£o).
        *   **No CI (Jenkins):** O pipeline definirÃ¡ as variÃ¡veis de ambiente antes de chamar `docker-compose`. Por exemplo, `REGISTRY=registry.local:5000` e `BUILD_NUMBER=2`. O Compose entÃ£o usarÃ¡ esses valores, resultando em nomes como `registry.local:5000/diamol/ch11-numbers-api:v3-build-2`.
*   Essa tÃ©cnica permite versionar e direcionar imagens de forma flexÃ­vel usando o mesmo arquivo Compose base.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Verificar SubstituiÃ§Ã£o Local:**
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch11\exercises
    docker-compose -f docker-compose.yml config | Select-String "image:"
    # Deve mostrar as imagens com '-local' no final da tag.
    ```
*   **Simular CI:**
    ```powershell
    $env:REGISTRY="registry.local:5000"
    $env:BUILD_NUMBER="99"
    docker-compose -f docker-compose.yml config | Select-String "image:"
    # AGORA deve mostrar 'registry.local:5000' e '-99' nos nomes das imagens.
    Remove-Variable REGISTRY # Limpar variÃ¡veis
    Remove-Variable BUILD_NUMBER
    ```

---

ğŸ“„ PÃ¡gina 220

ğŸ” Resumo dos principais pontos:

*   VariÃ¡veis de ambiente com defaults (`${VAR:-default}`) sÃ£o padrÃ£o Ãºtil para suportar CI e build local com mesmos artefatos.
*   Comando `docker-compose build`: ConstrÃ³i imagens para serviÃ§os que tÃªm seÃ§Ã£o `build:` definida.
    *   Geralmente em arquivo override (`docker-compose-build.yml`).
*   Ã‰ boa prÃ¡tica construir com Compose (mesmo para 1 imagem) para usar nome/tag definido no Compose.
*   **Labels de Imagem:** Metadados chave/valor adicionados Ã  imagem no build (`LABEL` no Dockerfile).
    *   Ficam embutidos na imagem (viajam com push/pull).
    *   Ãšteis para **rastreabilidade/auditoria** no CI/CD: incluir info do build (nÃºmero, job, commit Git) como labels.
*   **Listagem 11.3:** Trecho de Dockerfile (`numbers-api/Dockerfile.v4`) usando `ARG` e `LABEL`.
    ```dockerfile
    # ...
    FROM diamol/dotnet-aspnet
    ARG BUILD_NUMBER=0 # Define argumento de build com valor default
    ARG BUILD_TAG=local # Define outro argumento de build
    LABEL version="3.0" # Label fixa
    LABEL build_number=${BUILD_NUMBER} # Label usa valor do ARG
    LABEL build_tag=${BUILD_TAG} # Label usa valor do ARG
    ENTRYPOINT ["dotnet", "Numbers.Api.dll"]
    # ...
    ```
*   InstruÃ§Ã£o `ARG`: Similar a `ENV`, mas sÃ³ existe durante o **build**. NÃ£o fica disponÃ­vel no contÃªiner em runtime. Usada para passar parÃ¢metros para o build.
*   InstruÃ§Ã£o `LABEL`: Adiciona metadados Ã  imagem. Pode usar variÃ¡veis `ARG` para valores dinÃ¢micos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz o comando `docker-compose build`. Assim como `docker-compose up` inicia serviÃ§os, `docker-compose build` constrÃ³i as imagens para os serviÃ§os que tÃªm uma seÃ§Ã£o `build:` definida em seus arquivos Compose (geralmente em um arquivo override especÃ­fico para build).
*   Enfatiza a importÃ¢ncia de adicionar **Labels** Ã s imagens durante o build. Labels sÃ£o metadados (como `autor="Alexandre"`, `git-commit="abc1234"`) que ficam permanentemente associados Ã  imagem. No contexto de CI/CD, Ã© crucial usar labels para registrar informaÃ§Ãµes sobre *como* e *quando* a imagem foi construÃ­da (ex: qual job, qual nÃºmero de build, qual commit do cÃ³digo fonte). Isso cria uma trilha de auditoria essencial.
*   A Listagem 11.3 mostra como definir labels no Dockerfile usando a instruÃ§Ã£o `LABEL`.
*   Introduz a instruÃ§Ã£o `ARG`. Ela Ã© usada para definir variÃ¡veis que podem ser passadas para o processo de build (via `docker build --build-arg NOME=valor` ou via seÃ§Ã£o `args:` no Compose). A diferenÃ§a principal para `ENV` Ã© que `ARG` sÃ³ existe *durante o build* e nÃ£o persiste no ambiente do contÃªiner final. Ã‰ ideal para passar informaÃ§Ãµes como nÃºmeros de build ou credenciais temporÃ¡rias necessÃ¡rias apenas para construir a imagem.
*   No exemplo, `ARG BUILD_NUMBER=0` define um argumento `BUILD_NUMBER` com valor padrÃ£o `0`. A instruÃ§Ã£o `LABEL build_number=${BUILD_NUMBER}` usa o valor desse argumento (que pode ser sobrescrito no build) para definir a label.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Analisar Dockerfile:** Abra `ch11\exercises\numbers\numbers-api\Dockerfile.v4` no VS Code. Veja as instruÃ§Ãµes `ARG` e `LABEL`.
*   **Build com `docker-compose build`:** (PrÃ³ximo exercÃ­cio).
*   **Build com `docker build` e `--build-arg`:**
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch11\exercises\numbers
    # Build passando argumentos para sobrescrever defaults do ARG
    docker image build -f numbers-api/Dockerfile.v4 `
      --build-arg BUILD_NUMBER=77 `
      --build-arg BUILD_TAG="manual-test" `
      -t numbers-api-manual .

    # Inspecionar labels da imagem criada
    docker image inspect -f '{{.Config.Labels}}' numbers-api-manual
    # Deve mostrar build_number:77 e build_tag:manual-test
    ```

---

ğŸ“„ PÃ¡gina 221

ğŸ” Resumo dos principais pontos:

*   Como settings do ambiente CI chegam aos `ARG`s do Dockerfile via `docker-compose build`: atravÃ©s da seÃ§Ã£o `build: args:` no arquivo Compose override.
*   **Listagem 11.4:** ConteÃºdo do `docker-compose-build.yml`.
    ```yaml
    # Define extensÃ£o 'args' reutilizÃ¡vel
    x-args: &args
      args: # SeÃ§Ã£o 'args' dentro do build:
        BUILD_NUMBER: ${BUILD_NUMBER:-0} # Usa variÃ¡vel de ambiente do host/CI
        BUILD_TAG: ${BUILD_TAG:-local} # Usa variÃ¡vel de ambiente do host/CI

    services:
      numbers-api:
        build:
          context: numbers # DiretÃ³rio relativo ao Compose file com cÃ³digo/Dockerfile
          dockerfile: numbers-api/Dockerfile.v4 # Path relativo ao context
          <<: *args # Mescla a extensÃ£o 'args' aqui
      numbers-web:
        build:
          context: numbers
          dockerfile: numbers-web/Dockerfile.v4
          <<: *args # Mescla a extensÃ£o 'args' aqui
    ```
*   **ExplicaÃ§Ã£o da SeÃ§Ã£o `build:`:**
    *   `context:`: Path para o diretÃ³rio de build (contÃ©m Dockerfile e arquivos a serem copiados).
    *   `dockerfile:`: Path para o Dockerfile (relativo ao `context`).
    *   `args:`: Define os argumentos (`ARG`) a serem passados para o `docker build`. Usa substituiÃ§Ã£o `${VAR}` para pegar valores do ambiente do host (onde `docker-compose build` roda).
*   Uso de extension field (`x-args: &args`) e merge (`<<: *args`) para evitar repetir a seÃ§Ã£o `args:` para ambos os serviÃ§os.
*   Valores default (`:-0`, `:-local`) garantem que build funcione mesmo fora do CI (quando variÃ¡veis nÃ£o estÃ£o setadas).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina conecta tudo: como as variÃ¡veis de ambiente definidas no ambiente de CI (Jenkins) sÃ£o passadas para os argumentos `ARG` dentro do Dockerfile quando se usa `docker-compose build`.
*   A chave estÃ¡ na seÃ§Ã£o `build: args:` dentro da definiÃ§Ã£o do serviÃ§o no arquivo `docker-compose-build.yml` (Listagem 11.4).
*   **Fluxo:**
    1.  O pipeline de CI (Jenkins) define variÃ¡veis de ambiente (ex: `BUILD_NUMBER=2`, `BUILD_TAG="jenkins-job-2"`).
    2.  O pipeline executa `docker-compose -f ... -f docker-compose-build.yml build`.
    3.  `docker-compose` lÃª o `docker-compose-build.yml`.
    4.  Na seÃ§Ã£o `build: args:`, ele encontra `BUILD_NUMBER: ${BUILD_NUMBER:-0}`.
    5.  Ele substitui `${BUILD_NUMBER}` pelo valor da variÃ¡vel de ambiente do CI (`2`). O valor resultante `BUILD_NUMBER: 2` Ã© passado como argumento para o `docker build`.
    6.  Dentro do `Dockerfile`, a instruÃ§Ã£o `ARG BUILD_NUMBER=0` recebe o valor `2` passado pelo build.
    7.  A instruÃ§Ã£o `LABEL build_number=${BUILD_NUMBER}` usa esse valor `2`.
*   O uso de campos de extensÃ£o (`x-args`) Ã© apenas uma tÃ©cnica YAML para nÃ£o repetir a seÃ§Ã£o `args:` para os serviÃ§os `numbers-api` e `numbers-web`, jÃ¡ que ambos usam os mesmos argumentos.
*   Os valores default (`:-0`, `:-local`) na seÃ§Ã£o `args:` do Compose garantem que o `docker-compose build` funcione mesmo se as variÃ¡veis de ambiente nÃ£o estiverem definidas (ex: rodando localmente), passando os defaults para o `docker build`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Verificar Build com Compose:** (Estando em `ch11/exercises`)
    ```powershell
    # Limpar variÃ¡veis de ambiente (simular build local)
    Remove-Variable BUILD_NUMBER -ErrorAction SilentlyContinue
    Remove-Variable BUILD_TAG -ErrorAction SilentlyContinue

    # Construir usando Compose (deve usar defaults '0' e 'local')
    docker-compose -f docker-compose.yml -f docker-compose-build.yml build

    # Verificar labels (deve mostrar 0, local)
    docker image inspect -f '{{.Config.Labels}}' diamol/ch11-numbers-api:v3-build-local

    # Simular ambiente CI e construir novamente
    $env:BUILD_NUMBER="5"
    $env:BUILD_TAG="ci-run-5"
    docker-compose -f docker-compose.yml -f docker-compose-build.yml build

    # Verificar labels (deve mostrar 5, ci-run-5)
    docker image inspect -f '{{.Config.Labels}}' diamol/ch11-numbers-api:v3-build-5
    ```

---

ğŸ“„ PÃ¡gina 222

ğŸ” Resumo dos principais pontos:

*   Defaults em mÃºltiplos lugares (Compose `args:`, Dockerfile `ARG`) garantem que build funcione em diferentes cenÃ¡rios (CI, local com Compose, local sÃ³ com `docker build`).
*   **ExercÃ­cio Try-it-now:** Construir imagem API usando `docker image build` diretamente (sem Compose), passando um `--build-arg`.
    *   Navegar para `ch11/exercises/numbers`.
    *   Comando: `docker image build -f numbers-api/Dockerfile.v4 --build-arg BUILD_TAG=ch11 -t numbers-api .`
    *   Verificar labels: `docker image inspect -f '{{.Config.Labels}}' numbers-api`.
*   Resultado esperado (Figura 11.11): Label `build_tag` usa valor passado (`ch11`), label `build_number` usa default `0` do `ARG` no Dockerfile (pois nÃ£o foi passado no comando).
*   ConclusÃ£o sobre `ARG`/`LABEL`: Importante para rastreabilidade (imagem -> build CI -> commit cÃ³digo).
*   **11.4 Writing CI jobs with no dependencies except Docker** (Escrevendo jobs CI sem dependÃªncias exceto Docker)
    *   Reafirma benefÃ­cio: Builds rodam em containers (via multi-stage), server CI sÃ³ precisa de Docker (+Compose). NÃ£o precisa de .NET SDK, Java SDK, etc. instalados no host CI.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina demonstra que, graÃ§as aos valores padrÃ£o definidos na instruÃ§Ã£o `ARG` dentro do Dockerfile, Ã© possÃ­vel construir a imagem usando o comando `docker image build` diretamente, mesmo sem usar o Docker Compose.
*   O exercÃ­cio mostra como passar um valor para um argumento (`--build-arg BUILD_TAG=ch11`) e omitir outro (`BUILD_NUMBER`). O resultado (Figura 11.11) confirma que o valor passado Ã© usado, e o omitido usa o default definido no `ARG` do Dockerfile. Isso garante flexibilidade mÃ¡xima no processo de build.
*   A seÃ§Ã£o 11.4 retorna ao ponto central do capÃ­tulo: a simplicidade dos **jobs de CI** quando se usa Docker. Como toda a complexidade de compilaÃ§Ã£o e empacotamento estÃ¡ encapsulada nos Dockerfiles (geralmente multi-estÃ¡gio), o servidor de CI (Jenkins, GitLab CI, GitHub Actions, etc.) nÃ£o precisa ter nenhuma ferramenta de desenvolvimento especÃ­fica instalada. Sua Ãºnica dependÃªncia real Ã© ter o Docker (e talvez o Docker Compose) disponÃ­vel para executar os comandos `docker build` ou `docker-compose build`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o principal (Build sem Compose):**
    ```powershell
    # Navegar para
    cd C:\DockerLabs\diamol\ch11\exercises\numbers

    # Construir com docker build, passando um arg
    docker image build -f numbers-api/Dockerfile.v4 --build-arg BUILD_TAG=ch11 -t numbers-api-direct .

    # Verificar labels
    docker image inspect -f '{{.Config.Labels}}' numbers-api-direct
    # Confirme build_tag=ch11 e build_number=0
    ```
*   O servidor Jenkins que vocÃª rodou anteriormente sÃ³ tem Docker e Java (para rodar o prÃ³prio Jenkins) instalados; ele consegue construir o app .NET Core porque o build .NET ocorre *dentro* de um contÃªiner iniciado pelo `docker build`, usando a imagem `golden/dotnetcore-sdk`.

---

ğŸ“„ PÃ¡gina 223

ğŸ” Resumo dos principais pontos:

*   BenefÃ­cio de CI sÃ³ com Docker: Server CI simples, scripts de build simples, fÃ¡cil migrar entre serviÃ§os CI (Docker Hub, GitHub Actions, Azure DevOps).
*   Exemplo Jenkins: Job configurado via arquivo `Jenkinsfile` no source control.
*   **Listagem 11.5:** Trecho do `Jenkinsfile` e do script `01-build.bat` chamado por ele.
    *   **Jenkinsfile (EstÃ¡gio 'Build'):**
        *   `dir('ch11/exercises')`: Muda diretÃ³rio para onde estÃ£o os Compose files.
        *   `sh 'chmod +x ./ci/01-build.bat'`: Torna script executÃ¡vel (Linux).
        *   `sh './ci/01-build.bat'`: Executa o script de build.
    *   **Script `01-build.bat`:**
        *   ContÃ©m apenas o comando `docker-compose` para construir as imagens, usando os arquivos `-f` core e de build, e a flag `--pull`.
        ```bash
        docker-compose \
        -f docker-compose.yml \
        -f docker-compose-build.yml \
        build --pull
        ```
*   Flag `--pull`: Faz `docker-compose build` (e `docker build`) tentar baixar versÃµes mais recentes das imagens base (`FROM ...`) antes de construir. Boa prÃ¡tica para CI.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como o job Jenkins (`diamol`) que rodou anteriormente Ã© configurado. Ele usa um arquivo chamado `Jenkinsfile` (uma convenÃ§Ã£o do Jenkins para definir pipelines como cÃ³digo, armazenado junto com o cÃ³digo fonte).
*   A Listagem 11.5 mostra um estÃ¡gio ('Build') desse `Jenkinsfile`. A lÃ³gica importante Ã© que ele simplesmente **executa um script externo** (`./ci/01-build.bat`).
*   O conteÃºdo do script `01-build.bat` Ã© revelado: Ã© apenas o comando `docker-compose build` que jÃ¡ usamos, combinando os arquivos `core` e `build` e adicionando a flag `--pull`.
*   **AbstraÃ§Ã£o:** A complexidade do build estÃ¡ no `docker-compose build` (que por sua vez usa os Dockerfiles). O pipeline Jenkins em si Ã© muito simples, apenas orquestrando a execuÃ§Ã£o de scripts. Isso torna o pipeline fÃ¡cil de entender e portÃ¡vel para outras ferramentas de CI (que tambÃ©m podem executar scripts).
*   A flag `--pull` Ã© recomendada para builds CI para garantir que vocÃª esteja sempre usando as Ãºltimas atualizaÃ§Ãµes de seguranÃ§a e patches das suas imagens base, detectando problemas de compatibilidade o mais cedo possÃ­vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Explorar Arquivos:**
    *   Abra `ch11\exercises\Jenkinsfile` (ou `Jenkinsfile.windows`) no VS Code. Veja a estrutura do pipeline com mÃºltiplos `stage`s.
    *   Abra `ch11\exercises\ci\01-build.bat`. Veja o comando `docker-compose build`.
    *   Abra outros scripts em `ci\` (00-verify, 02-test, 03-push) para ver os comandos Docker/Compose que eles executam para os outros estÃ¡gios do pipeline.
*   O build que vocÃª rodou manualmente com `docker-compose ... build` na p.221 executa essencialmente o mesmo que o estÃ¡gio 'Build' do Jenkins.

---

ğŸ“„ PÃ¡gina 224

ğŸ” Resumo dos principais pontos:

*   Script `01-build.bat` roda `docker-compose build`. SaÃ­da Ã© capturada nos logs do Jenkins.
*   **ExercÃ­cio Try-it-now:** Ver logs do build no Jenkins UI.
    *   Acessar job `diamol` (`http://localhost:8080/job/diamol`).
    *   Clicar no build #2 (o bem-sucedido).
    *   Clicar em "Logs" (ou "Console Output").
    *   Expandir passos; verÃ¡ saÃ­da padrÃ£o do `docker build`. (Figura 11.12).
*   Pipeline segue padrÃ£o: chama script que roda comando Docker/Compose. PortÃ¡vel entre ferramentas CI.
*   **Outros EstÃ¡gios do Pipeline Jenkins:**
    *   **Verify (`00-verify.bat`):** Imprime versÃµes `docker` e `docker-compose` (verifica dependÃªncias, registra versÃµes usadas).
    *   **Build (`01-build.bat`):** Roda `docker-compose build --pull`. VariÃ¡vel `REGISTRY` definida no Jenkinsfile para tag local (`registry.local:5000`).
    *   **Test (`02-test.bat`):** Roda `docker-compose up -d`, `docker ps`, `docker-compose down`. Teste simples (app sobe/desce sem erro). Real seria rodar testes E2E em outro container.
    *   **Push (`03-push.bat`):** Roda `docker-compose push`. Envia imagens (com tag do registro local) para `registry.local:5000`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra como visualizar os logs detalhados de um build especÃ­fico no Jenkins, confirmando que a saÃ­da Ã© a mesma que vocÃª veria ao rodar `docker-compose build` localmente.
*   Detalha os outros estÃ¡gios do pipeline `diamol` definido no `Jenkinsfile`, explicando o propÃ³sito de cada script correspondente na pasta `ci/`:
    *   `00-verify`: Um passo inicial para garantir que `docker` e `docker-compose` estÃ£o disponÃ­veis e registrar suas versÃµes.
    *   `01-build`: ConstrÃ³i as imagens (como jÃ¡ visto).
    *   `02-test`: Executa um teste de "fumaÃ§a" muito bÃ¡sico: consegue iniciar a aplicaÃ§Ã£o completa com `docker-compose up` e depois derrubÃ¡-la com `down` sem erros? Um teste real aqui seria mais complexo, envolvendo a execuÃ§Ã£o de testes de integraÃ§Ã£o ou end-to-end contra a aplicaÃ§Ã£o rodando.
    *   `03-push`: Se os estÃ¡gios anteriores (Build, Test) passaram, envia as imagens construÃ­das (que foram tagueadas para `registry.local:5000` devido Ã  variÃ¡vel `REGISTRY` no Jenkinsfile) para o registro Docker local (rodando no contÃªiner `registry`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Ver Logs Jenkins:**
    *   VÃ¡ para `http://localhost:8080/job/diamol/`.
    *   Clique no nÃºmero do Ãºltimo build bem-sucedido (ex: #2) na seÃ§Ã£o "Build History".
    *   No menu esquerdo da pÃ¡gina do build, clique em "Console Output".
    *   Examine os logs, procurando pela saÃ­da dos comandos `docker-compose build`.
2.  **Verificar Imagens no Registro Local:** (PrÃ³ximo exercÃ­cio farÃ¡ isso via API, mas vocÃª pode listar localmente tambÃ©m)
    ```powershell
    # Imagens construÃ­das pelo Jenkins (build #2) devem ter a tag '-build-2'
    docker image ls registry.local:5000/diamol/ch11-numbers-api*
    docker image ls registry.local:5000/diamol/ch11-numbers-web*
    ```

---

ğŸ“„ PÃ¡gina 225

ğŸ” Resumo dos principais pontos:

*   Figura 11.12: Mostra logs do estÃ¡gio Build no Jenkins UI.
*   EstÃ¡gios CI sÃ£o sequenciais; falha em um para o job.
*   ImplicaÃ§Ã£o: Registro sÃ³ contÃ©m imagens que passaram no build e teste (release candidates).
*   **ExercÃ­cio Try-it-now:** Verificar imagens no registro local via API REST.
    *   Build #1 falhou (sem cÃ³digo), build #2 sucesso. Esperado: sÃ³ tags `-build-2` no registro.
    *   Comandos `curl` para API do registro local (`http://registry.local:5000/v2/...`):
        *   `/_catalog` (listar repositÃ³rios).
        *   `/<repo>/tags/list` (listar tags de um repositÃ³rio especÃ­fico).
*   Figura 11.13: Mostra saÃ­da do `curl` confirmando apenas tags `-build-2` presentes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a que, como os estÃ¡gios do pipeline sÃ£o sequenciais e uma falha interrompe o processo, o estÃ¡gio `Push` sÃ³ Ã© alcanÃ§ado se os estÃ¡gios `Build` e `Test` forem bem-sucedidos. Isso significa que qualquer imagem que chegue ao registro Docker pode ser considerada um "release candidate" que passou pelas verificaÃ§Ãµes automatizadas bÃ¡sicas.
*   O exercÃ­cio usa a API REST do registro Docker local (a mesma explorada no Lab do Cap. 5) para verificar programaticamente quais imagens e tags foram efetivamente enviadas pelo pipeline do Jenkins. SÃ£o usados dois endpoints:
    *   `/_catalog`: Lista todos os repositÃ³rios (nomes de imagem base) no registro.
    *   `/<namespace>/<nome_repo>/tags/list`: Lista todas as tags para um repositÃ³rio especÃ­fico.
*   A expectativa Ã© encontrar os repositÃ³rios `diamol/ch11-numbers-api` e `diamol/ch11-numbers-web`, e que cada um contenha apenas a tag `v3-build-2` (assumindo que o Ãºltimo build bem-sucedido foi o #2), confirmando que o build #1 falhou antes do push.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Consultar API do Registro (PowerShell):**
    ```powershell
    # Listar repositÃ³rios
    Invoke-WebRequest http://registry.local:5000/v2/_catalog | ConvertFrom-Json

    # Listar tags da API
    Invoke-WebRequest http://registry.local:5000/v2/diamol/ch11-numbers-api/tags/list | ConvertFrom-Json

    # Listar tags da Web
    Invoke-WebRequest http://registry.local:5000/v2/diamol/ch11-numbers-web/tags/list | ConvertFrom-Json
    ```
    Confirme que a saÃ­da corresponde Ã  expectativa (apenas tags `-build-2`).

---

ğŸ“„ PÃ¡gina 226

ğŸ” Resumo dos principais pontos:

*   Figura 11.13: Mostra saÃ­da da consulta Ã  API do registro.
*   Pipeline CI apresentado Ã© simples, mas mostra estÃ¡gios chave e boas prÃ¡ticas.
*   Chave: Docker faz trabalho pesado, pipeline sÃ³ orquestra scripts. PortÃ¡vel entre ferramentas CI.
*   **11.5 Understanding containers in the CI process** (Entendendo contÃªineres no processo CI)
    *   Compilar/rodar apps em containers Ã© sÃ³ o comeÃ§o.
    *   ConsistÃªncia Docker permite adicionar mais features ao pipeline.
*   Diagrama (Figura 11.14): Pipeline CI mais extenso ("production-grade").
    *   Adiciona estÃ¡gios: **Scan Image** (varredura de seguranÃ§a) e **Sign Image** (assinatura digital).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a descriÃ§Ã£o do pipeline CI bÃ¡sico, enfatizando sua simplicidade e portabilidade devido Ã  abstraÃ§Ã£o fornecida pelo Docker.
*   A seÃ§Ã£o 11.5 introduz a ideia de **pipelines de CI mais avanÃ§ados**, que aproveitam ainda mais o ecossistema Docker para adicionar etapas de seguranÃ§a e garantia de qualidade.
*   A Figura 11.14 ilustra um pipeline que inclui:
    *   **Scan Image:** ApÃ³s construir a imagem (e antes de enviÃ¡-la), uma ferramenta de varredura de seguranÃ§a (como Anchore - visto no Cap. 17, ou outras como Trivy, Snyk, Clair) Ã© usada para analisar a imagem em busca de vulnerabilidades conhecidas (CVEs) em seus componentes (SO base, bibliotecas, dependÃªncias). Se vulnerabilidades crÃ­ticas forem encontradas, o pipeline pode ser configurado para falhar.
    *   **Sign Image:** Se a imagem passar na varredura e nos testes, ela pode ser digitalmente assinada. A assinatura atesta que a imagem foi produzida por um pipeline confiÃ¡vel e nÃ£o foi adulterada. Ambientes de produÃ§Ã£o podem ser configurados para *somente* executar imagens que possuam uma assinatura vÃ¡lida de uma fonte confiÃ¡vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Embora o pipeline Jenkins atual nÃ£o inclua Scan/Sign, vocÃª pode explorar ferramentas como:
    *   **Trivy:** Ferramenta de scan open source fÃ¡cil de usar. Pode rodar como contÃªiner: `docker run aquasec/trivy image <nome_imagem>:<tag>` para escanear uma imagem localmente.
    *   **Docker Content Trust (Notary):** Mecanismo do Docker para assinatura e verificaÃ§Ã£o de imagens. Requer configuraÃ§Ã£o mais complexa.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Scan Image: Controle de qualidade que verifica se hÃ¡ peÃ§as defeituosas ou perigosas no produto antes de embalar.
*   Sign Image: Colocar um selo de autenticidade/lacre na embalagem do produto, garantindo que ele veio da fÃ¡brica certa e nÃ£o foi violado.

---

ğŸ“„ PÃ¡gina 227

ğŸ” Resumo dos principais pontos:

*   Esse fluxo avanÃ§ado Ã© chamado de **Secure Software Supply Chain** (Cadeia de Suprimentos de Software Segura).
*   Importante para garantir confianÃ§a no software implantado.
*   Ferramentas de scan podem falhar build se problemas forem encontrados.
*   Ambiente de produÃ§Ã£o pode ser configurado para sÃ³ rodar imagens assinadas.
*   Garante que containers em prod vieram do pipeline confiÃ¡vel, foram testados e escaneados.
*   Checks (scan, sign) operam em containers/imagens -> aplicam-se consistentemente a todas as tecnologias. Builds/Dockerfiles podem variar, mas pipeline CI Ã© o mesmo.
*   **11.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Construir seu prÃ³prio pipeline CI (simples).
    *   Base: App To-Do (Cap. 6), cÃ³digo em `ch11/lab`.
    *   Infra: Jenkinsfile, scripts CI, core Compose file jÃ¡ existem.
*   **Sua Tarefa:**
    1.  Escrever arquivo override `docker-compose-build.yml` com settings de build (context, dockerfile, args).
    2.  Criar novo job Jenkins para rodar pipeline do lab (`Jenkinsfile` em `ch11/lab`).
    3.  Fazer push das mudanÃ§as (novo `.yml`, talvez `Jenkinsfile` ajustado) para o repo `diamol` no Gogs local.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre seguranÃ§a no pipeline, chamando essa abordagem de "Secure Software Supply Chain". O objetivo Ã© ter alta confianÃ§a de que as imagens Docker implantadas sÃ£o seguras e autÃªnticas. A consistÃªncia do Docker permite aplicar essas verificaÃ§Ãµes de seguranÃ§a (scan, sign) da mesma forma para qualquer aplicaÃ§Ã£o, independentemente da tecnologia interna.
*   O **Lab do CapÃ­tulo 11** pede para vocÃª configurar um pipeline CI bÃ¡sico para a aplicaÃ§Ã£o To-Do, similar ao que foi feito para o app Random Number.
*   A maior parte jÃ¡ estÃ¡ pronta (Jenkinsfile, scripts). Suas tarefas sÃ£o:
    1.  **Criar o `docker-compose-build.yml`:** Baseado na Listagem 11.4 (p. 221), vocÃª precisa definir a seÃ§Ã£o `build:` para o serviÃ§o `todo-web` (a imagem do To-Do). Especifique o `context` (provavelmente `.`, se o build for rodado da pasta `ch11/lab/todo-list`), o `dockerfile` (o Dockerfile do To-Do), e a seÃ§Ã£o `args:` para passar `BUILD_NUMBER` e `BUILD_TAG` (usando substituiÃ§Ã£o `${...:-...}` como no exemplo).
    2.  **Criar Job Jenkins:** No Jenkins UI (`http://localhost:8080`), criar um novo item (job), provavelmente do tipo "Pipeline". VocÃª pode usar a opÃ§Ã£o "Copy from" para copiar a configuraÃ§Ã£o do job `diamol` existente. A principal mudanÃ§a serÃ¡ apontar para o `Jenkinsfile` correto (o que estÃ¡ na pasta `ch11/lab`, nÃ£o o de `ch11/exercises`).
    3.  **Enviar CÃ³digo para Gogs:** Adicionar o novo `docker-compose-build.yml` ao Git, fazer commit, e fazer `push` para o `remote local` (Gogs). Isso (apÃ³s o prÃ³ximo poll do Jenkins) deve disparar o novo job.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 11:**
    1.  **Criar `docker-compose-build.yml`:** Na pasta `ch11\lab`, crie o arquivo. Adapte a Listagem 11.4. O serviÃ§o serÃ¡ `todo-web`, o `context` provavelmente serÃ¡ `./todo-list`, e o `dockerfile` serÃ¡ algo como `todo-list/Dockerfile` (verifique os nomes reais). Mantenha a extensÃ£o `x-args` e o merge `<<: *args`.
    2.  **Criar Job Jenkins:**
        *   Em `http://localhost:8080`, clique "New Item".
        *   Nome: `todo-pipeline` (ou similar).
        *   Selecione "Pipeline".
        *   Pode haver opÃ§Ã£o "Copy from": digite `diamol` e OK.
        *   Na configuraÃ§Ã£o do job, vÃ¡ para a seÃ§Ã£o "Pipeline". Mude o "Script Path" de `ch11/exercises/Jenkinsfile` para `ch11/lab/Jenkinsfile` (ou `Jenkinsfile.windows`). Salve.
    3.  **Commit & Push:**
        ```powershell
        # Estando na raiz C:\DockerLabs\diamol
        git add .\ch11\lab\docker-compose-build.yml # Adiciona novo arquivo
        git commit -m "Add build config for todo-list lab"
        git push local # Push para Gogs
        ```
    4.  **Verificar Jenkins:** ApÃ³s o push, vÃ¡ para o Jenkins e veja se o job `todo-pipeline` inicia automaticamente (ou inicie manualmente com "Build Now"). Monitore a execuÃ§Ã£o.

---

ğŸ“„ PÃ¡gina 228

ğŸ” Resumo dos principais pontos:

*   Dica para Lab 11: Pode haver falhas iniciais; verificar logs e ajustar. NinguÃ©m acerta job Jenkins de primeira.
*   Link para soluÃ§Ã£o no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Oferece uma dica realista e encorajadora para o Lab 11: Ã© normal que pipelines de CI nÃ£o funcionem perfeitamente na primeira tentativa. A depuraÃ§Ã£o envolve olhar os logs do Jenkins para entender onde falhou e ajustar a configuraÃ§Ã£o (no `Jenkinsfile`, nos scripts `ci/`, ou nos arquivos Compose) atÃ© funcionar.
*   Fornece o link para a soluÃ§Ã£o de referÃªncia.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se o job `todo-pipeline` falhar no Jenkins, clique no build falho, vÃ¡ para "Console Output" e analise os logs de erro para descobrir o que deu errado (ex: caminho de arquivo incorreto, comando falhou, permissÃµes, etc.).

---

ğŸ“„ PÃ¡gina 229

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, separando o Lab 11 do inÃ­cio da Parte 3.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 230

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 231

ğŸ” Resumo dos principais pontos:

*   **InÃ­cio da Parte 3: Running at scale with a container orchestrator** (Rodando em escala com um orquestrador de contÃªineres)
*   **OrquestraÃ§Ã£o:** Gerenciar aplicaÃ§Ãµes conteinerizadas em **mÃºltiplos servidores** (um cluster).
*   Usa as mesmas imagens Docker e formato de arquivo similar (Docker Compose).
*   DiferenÃ§a: VocÃª *declara* o estado desejado ao cluster, e o *orquestrador* gerencia os contÃªineres para vocÃª (criaÃ§Ã£o, distribuiÃ§Ã£o, monitoramento, substituiÃ§Ã£o).
*   Foco da Parte 3: **Docker Swarm** (orquestrador simples e poderoso, embutido no Docker).
*   TÃ³picos: Deploy de stacks (apps) no Swarm, upgrades/rollbacks, conexÃ£o do pipeline CI/CD para deploy contÃ­nuo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina introduz a Parte 3, que eleva o nÃ­vel do gerenciamento de contÃªineres do single-host (com Docker e Compose) para mÃºltiplos hosts (clusters) usando **orquestradores**.
*   A ideia chave da orquestraÃ§Ã£o Ã© a **abstraÃ§Ã£o e automaÃ§Ã£o em escala**. Em vez de vocÃª decidir manualmente em qual servidor rodar qual contÃªiner, vocÃª entrega a definiÃ§Ã£o da sua aplicaÃ§Ã£o (geralmente em um formato YAML, como o Docker Compose estendido para Swarm, ou os manifestos do Kubernetes) para o orquestrador. O orquestrador, entÃ£o, automaticamente:
    *   Distribui os contÃªineres pelos servidores disponÃ­veis no cluster (agendamento/scheduling).
    *   Monitora a saÃºde dos contÃªineres (usando health checks).
    *   Substitui contÃªineres que falham.
    *   Gerencia redes para comunicaÃ§Ã£o entre contÃªineres no cluster.
    *   Facilita atualizaÃ§Ãµes e rollbacks.
*   Esta parte do livro focarÃ¡ no **Docker Swarm**, que Ã© o orquestrador nativo incluÃ­do com o Docker Engine, conhecido por sua simplicidade em comparaÃ§Ã£o com o Kubernetes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ comandos `docker swarm ...` para inicializar e gerenciar um cluster Swarm (pode ser simulado no seu Docker Desktop ou usando Play With Docker).
*   VocÃª usarÃ¡ `docker stack deploy` para implantar aplicaÃ§Ãµes definidas em arquivos Compose no cluster Swarm.
*   VocÃª usarÃ¡ `docker service ...` para gerenciar os serviÃ§os rodando no Swarm.

---

ğŸ“„ PÃ¡gina 232

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, separando a introduÃ§Ã£o da Parte 3 do inÃ­cio do CapÃ­tulo 12.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 233

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 12: Understanding orchestration: Docker Swarm and Kubernetes** (Entendendo orquestraÃ§Ã£o: Docker Swarm e Kubernetes)
*   RevisÃ£o: Meio do livro, confortÃ¡vel com Docker/Compose. PrÃ³ximo passo: produÃ§Ã£o (mÃºltiplas mÃ¡quinas).
*   OrquestraÃ§Ã£o: Camada de gerenciamento que coordena mÃ¡quinas e containers.
*   Principais orquestradores: **Docker Swarm** e **Kubernetes (K8s)**.
*   CapÃ­tulo focarÃ¡ em **Docker Swarm** (embutido, poderoso).
*   Kubernetes: Mais complexo, livro "Learn K8s in a Month of Lunches" recomendado.
*   ComeÃ§ar com Swarm Ã© bom mesmo se objetivo for K8s (curva aprendizado mais suave).
*   **12.1 What is a container orchestrator?** (O que Ã© um orquestrador de contÃªineres?)
    *   Compose Ã© bom para single machine, mas nÃ£o para produÃ§Ã£o (sem alta disponibilidade - HA).
    *   Orquestrador: Agrupa muitas mÃ¡quinas em **cluster**. Gerencia containers, distribui trabalho, balanceia carga, substitui falhas.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 12, que introduz formalmente a **orquestraÃ§Ã£o de contÃªineres**.
*   ContrapÃµe o Docker Compose (Ã³timo para desenvolvimento e ambientes simples em um Ãºnico host) com a necessidade de **alta disponibilidade (HA)** e **escalabilidade** em produÃ§Ã£o, que exigem mÃºltiplos servidores gerenciados por um orquestrador.
*   Apresenta os dois principais orquestradores atuais: **Docker Swarm** e **Kubernetes**. O capÃ­tulo focarÃ¡ no Swarm por ser mais simples e integrado ao Docker, mas reconhece a popularidade e complexidade do Kubernetes, sugerindo um aprendizado sequencial (Swarm primeiro pode facilitar K8s depois).
*   A seÃ§Ã£o 12.1 define o que um orquestrador faz: transforma um conjunto de servidores individuais em um **pool unificado de recursos (cluster)** e automatiza o ciclo de vida dos contÃªineres sobre esse pool (implantaÃ§Ã£o, escalonamento, balanceamento de carga, recuperaÃ§Ã£o de falhas).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O Docker Desktop tem um modo Swarm embutido que pode ser ativado para simular um cluster de um nÃ³ Ãºnico, permitindo rodar os comandos `docker swarm`, `docker stack`, `docker service` localmente.
*   Alternativamente, pode-se usar playgrounds online como Play With Docker (pwd.docker.com) para criar clusters Swarm multi-nÃ³ temporÃ¡rios gratuitamente.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Orquestrador: Pense em um maestro regendo uma grande orquestra (o cluster de servidores). O maestro (orquestrador) tem a partitura (definiÃ§Ã£o da aplicaÃ§Ã£o), decide quais mÃºsicos (servidores) tocam quais partes (contÃªineres), garante que todos estejam sincronizados, e substitui um mÃºsico que desmaiar (falha de nÃ³/contÃªiner) por outro reserva, sem parar a mÃºsica (aplicaÃ§Ã£o).

---

ğŸ“„ PÃ¡gina 234

ğŸ” Resumo dos principais pontos:

*   CriaÃ§Ã£o do cluster: Instalar Docker em cada mÃ¡quina, uni-las com Swarm/K8s.
*   Gerenciamento remoto via CLI ou UI.
*   Diagrama (Figura 12.1): Mostra mÃºltiplos servidores Docker sendo gerenciados como um Ãºnico cluster pelo Orquestrador, que roda os contÃªineres.
*   **Capacidades extras do Orquestrador:**
    *   Banco de dados distribuÃ­do no cluster (armazena definiÃ§Ãµes de apps, estado).
    *   Agendador (Scheduler): Decide onde rodar contÃªineres (baseado em recursos, constraints).
    *   Sistema de Heartbeat: Monitora saÃºde dos nÃ³s do cluster.
*   **Fluxo de Deploy no Orquestrador:**
    1.  Envia YAML para cluster.
    2.  Cluster armazena definiÃ§Ã£o.
    3.  Scheduler aloca contÃªineres em nÃ³s com capacidade.
    4.  Cluster monitora: se nÃ³/contÃªiner falha, substitui em outro nÃ³.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como um cluster Ã© formado (instalando Docker e usando comandos do orquestrador para unir os nÃ³s) e como ele Ã© gerenciado (remotamente). A Figura 12.1 ilustra que, para o usuÃ¡rio/administrador, o cluster aparece como uma entidade Ãºnica, escondendo a complexidade dos servidores individuais.
*   Detalha os componentes internos chave de um orquestrador que permitem a automaÃ§Ã£o:
    *   **Banco de Dados DistribuÃ­do:** Armazena de forma confiÃ¡vel (replicada) o "estado desejado" das aplicaÃ§Ãµes e o estado atual do cluster.
    *   **Agendador (Scheduler):** O "cÃ©rebro" que decide em qual nÃ³ (servidor) um novo contÃªiner deve ser executado, considerando fatores como recursos disponÃ­veis, afinidade/anti-afinidade, etc.
    *   **Monitoramento (Heartbeat):** Mecanismo pelo qual os nÃ³s se comunicam e o gerenciamento central sabe quais nÃ³s estÃ£o ativos e saudÃ¡veis.
*   O fluxo bÃ¡sico de implantaÃ§Ã£o Ã© descrito: vocÃª envia a definiÃ§Ã£o da sua aplicaÃ§Ã£o (YAML) para o cluster, ele a armazena e o agendador cuida de criar os contÃªineres nos nÃ³s apropriados. O sistema entÃ£o trabalha continuamente para manter a aplicaÃ§Ã£o rodando conforme definido, substituindo automaticamente contÃªineres ou nÃ³s que falharem.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker stack deploy -c meu-app.yml my-app-stack` para enviar a definiÃ§Ã£o ao cluster Swarm.
*   VocÃª usarÃ¡ `docker service ls`, `docker service ps <service>` para ver onde o Swarm decidiu rodar seus contÃªineres (em qual nÃ³).

---

ğŸ“„ PÃ¡gina 235

ğŸ” Resumo dos principais pontos:

*   Orquestrador abstrai detalhes de servidores/redes/storage. Trabalha-se com cluster como unidade Ãºnica via API/CLI.
*   **Features extras do Orquestrador:** Networking, ConfiguraÃ§Ã£o, Storage (alÃ©m do gerenciamento de containers).
*   Diagrama (Figura 12.2): Mostra cluster com:
    *   **Ingress Networking:** TrÃ¡fego externo chega a qualquer nÃ³, cluster roteia para container correto (mesmo em outro nÃ³).
    *   **Overlay Networking:** Rede virtual que conecta containers atravÃ©s de mÃºltiplos nÃ³s.
    *   **Config Objects / Secrets:** Armazenamento seguro de configuraÃ§Ã£o no cluster, disponibilizado para containers.
    *   **Shared Storage / Volumes:** Volumes que podem ser acessados por containers em diferentes nÃ³s (requer driver de volume especÃ­fico).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a a abstraÃ§Ã£o fornecida pelo orquestrador: vocÃª define o que quer rodar, e ele cuida de onde e como, escondendo os detalhes dos servidores individuais.
*   A Figura 12.2 introduz funcionalidades adicionais importantes que os orquestradores oferecem para suportar aplicaÃ§Ãµes distribuÃ­das em cluster:
    *   **Rede (Networking):**
        *   **Ingress:** Como o trÃ¡fego *externo* (de usuÃ¡rios, outras redes) entra no cluster e chega ao serviÃ§o correto, nÃ£o importa em qual nÃ³ o usuÃ¡rio se conectou ou em qual nÃ³ o contÃªiner estÃ¡ rodando. O cluster faz o roteamento interno.
        *   **Overlay:** Como contÃªineres de serviÃ§os *diferentes* (rodando em nÃ³s diferentes) se comunicam entre si de forma transparente, como se estivessem na mesma rede local.
    *   **ConfiguraÃ§Ã£o (Config Objects / Secrets):** Mecanismos para armazenar dados de configuraÃ§Ã£o (normais ou sensÃ­veis/secretos) centralmente no cluster e montÃ¡-los de forma segura dentro dos contÃªineres que precisam deles.
    *   **Armazenamento (Shared Storage / Volumes):** SoluÃ§Ãµes (geralmente via plugins/drivers) para que volumes possam ser acessados por contÃªineres independentemente do nÃ³ em que estÃ£o rodando, essencial para aplicaÃ§Ãµes stateful escaladas ou com alta disponibilidade.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ redes `overlay` (jÃ¡ viu `docker network create --driver overlay` no Cap. 12).
*   VocÃª usarÃ¡ a rede `ingress` implicitamente ao publicar portas de serviÃ§os no Swarm (`ports:` na definiÃ§Ã£o do serviÃ§o no Compose).
*   VocÃª usarÃ¡ `docker config create` e `docker secret create` e as seÃ§Ãµes `configs:` e `secrets:` no Compose para gerenciamento de configuraÃ§Ã£o no Swarm (Cap. 13).
*   Volumes em Swarm serÃ£o discutidos no Cap. 13.

---

ğŸ“„ PÃ¡gina 236

ğŸ” Resumo dos principais pontos:

*   **12.2 Setting up a Docker Swarm cluster** (Configurando um cluster Docker Swarm)
*   Deploy de orquestrador com Swarm Ã© super fÃ¡cil (features embutidas no Docker Engine).
*   **InicializaÃ§Ã£o:** Comando `docker swarm init` ativa modo Swarm.
    *   **ExercÃ­cio Try-it-now:** Rodar `docker swarm init`.
    *   Pode dar erro se mÃ¡quina tiver mÃºltiplos IPs; Docker pede para especificar qual usar (`--advertise-addr <ip>`).
*   Figura 12.3: Mostra saÃ­da do `swarm init`.
    *   Informa que Swarm foi inicializado.
    *   MÃ¡quina atual tornou-se **Manager**.
    *   Mostra comando (`docker swarm join --token <TOKEN> <IP>:<PORT>`) para outras mÃ¡quinas se juntarem como **Workers**.
*   **Roles no Cluster:**
    *   **Managers:** Gerenciam o cluster (BD, API, scheduling, monitoramento).
    *   **Workers:** Apenas rodam contÃªineres agendados pelos managers (e reportam status). Managers tambÃ©m podem rodar contÃªineres.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o inicia a configuraÃ§Ã£o prÃ¡tica de um cluster **Docker Swarm**. A grande vantagem Ã© que as funcionalidades do Swarm jÃ¡ vÃªm incluÃ­das no Docker Engine; vocÃª sÃ³ precisa "ativÃ¡-las".
*   O comando `docker swarm init` transforma um Docker Engine standalone em um **nÃ³ Manager** de um novo cluster Swarm. Este primeiro nÃ³ se torna o lÃ­der inicial do cluster.
*   O comando Ã© muito simples. Se a mÃ¡quina tiver apenas uma interface de rede principal, ele geralmente funciona sem argumentos. Se tiver mÃºltiplas, vocÃª precisa especificar qual endereÃ§o IP os outros nÃ³s devem usar para se conectar a este manager usando a flag `--advertise-addr`.
*   A saÃ­da do `swarm init` Ã© importante: ela confirma a criaÃ§Ã£o do Swarm e fornece o **comando exato**, incluindo um **token de seguranÃ§a secreto**, que vocÃª deve executar em *outras* mÃ¡quinas Docker para que elas se juntem a este Swarm como nÃ³s **Workers**.
*   A pÃ¡gina introduz os dois papÃ©is principais dos nÃ³s em um cluster Swarm:
    *   **Managers:** SÃ£o o cÃ©rebro do Swarm. Eles mantÃªm o estado do cluster, recebem comandos, decidem onde rodar os contÃªineres e monitoram tudo. Para alta disponibilidade, vocÃª deve ter mÃºltiplos managers (3 ou 5 Ã© o recomendado).
    *   **Workers:** SÃ£o os mÃºsculos do Swarm. Sua funÃ§Ã£o principal Ã© executar os contÃªineres conforme instruÃ­do pelos managers.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Ativar Modo Swarm no Docker Desktop (Simula Cluster de 1 NÃ³):**
    ```powershell
    docker swarm init
    ```
    *   VocÃª verÃ¡ uma saÃ­da similar Ã  Figura 12.3. Seu Docker Desktop agora estÃ¡ rodando como um Manager de um Swarm de um nÃ³ sÃ³. VocÃª pode ignorar o comando `docker swarm join` por enquanto, pois nÃ£o temos outros nÃ³s para adicionar neste setup simples.
*   **Verificar Status:**
    ```powershell
    docker info # Procure pela seÃ§Ã£o 'Swarm: active'
    docker node ls # Deve listar um nÃ³, com 'Leader' como Manager Status
    ```

---

ğŸ“„ PÃ¡gina 237

ğŸ” Resumo dos principais pontos:

*   `swarm init` sÃ³ precisa ser feito uma vez (no primeiro manager).
*   NÃ³s se juntam usando `docker swarm join` com token do manager. Token funciona como senha.
*   Pode-se obter tokens para workers ou novos managers a partir de um manager existente.
*   **ExercÃ­cio Try-it-now:** Obter tokens e listar nÃ³s.
    *   Comandos (executar em um manager):
        *   `docker swarm join-token worker` (mostra comando/token para adicionar worker).
        *   `docker swarm join-token manager` (mostra comando/token para adicionar *outro* manager).
        *   `docker node ls` (lista todos os nÃ³s no Swarm).
*   Figura 12.4: Mostra saÃ­da dos comandos, incluindo os tokens (omitidos por seguranÃ§a no livro) e a lista com um Ãºnico nÃ³ (o manager atual).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como adicionar mais nÃ³s a um Swarm existente. O processo requer:
    1.  Acesso a um nÃ³ que jÃ¡ Ã© Manager no Swarm.
    2.  Obter o **token de junÃ§Ã£o** apropriado nesse manager. Existem tokens separados para adicionar novos Workers e para adicionar novos Managers. Esses tokens sÃ£o segredos que autorizam o novo nÃ³ a se juntar ao cluster.
    3.  Executar o comando `docker swarm join --token <TOKEN> <IP_DO_MANAGER>:<PORTA>` na mÃ¡quina que vocÃª quer adicionar ao Swarm.
*   O exercÃ­cio demonstra como obter esses comandos/tokens usando `docker swarm join-token worker` e `docker swarm join-token manager` a partir do nÃ³ manager que vocÃª inicializou. TambÃ©m mostra novamente `docker node ls` para visualizar os membros atuais do cluster.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Executar no seu Docker Desktop (que Ã© o manager):**
    ```powershell
    # Obter comando/token para adicionar workers (apenas para ver)
    docker swarm join-token worker

    # Obter comando/token para adicionar managers (apenas para ver)
    docker swarm join-token manager

    # Listar nÃ³s (deve mostrar apenas seu nÃ³ local como Leader)
    docker node ls
    ```
*   **Importante:** Mantenha os tokens secretos! Se alguÃ©m tiver o token e acesso Ã  rede do seu manager, pode adicionar nÃ³s nÃ£o autorizados ao seu cluster. Os tokens podem ser rotacionados se comprometidos (`docker swarm join-token --rotate ...`).

---

ğŸ“„ PÃ¡gina 238

ğŸ” Resumo dos principais pontos:

*   Swarm de nÃ³ Ãºnico funciona igual a multi-nÃ³, mas sem HA ou escalabilidade real.
*   Diagrama (Figura 12.5): Compara Swarm de 1 nÃ³ (dev/test) vs. Swarm multi-nÃ³ (produÃ§Ã£o com managers/workers).
*   Vantagem Swarm vs. K8s: Simplicidade de setup/gerenciamento do cluster.
    *   Swarm: `docker swarm init` + `docker swarm join` em cada nÃ³.
    *   Mesmo processo simples para produÃ§Ã£o e teste.
*   Com Swarm de 1 nÃ³ pronto, pode-se explorar como apps rodam sob orquestraÃ§Ã£o.
*   **12.3 Running applications as Docker Swarm services** (Rodando aplicaÃ§Ãµes como serviÃ§os Docker Swarm)
    *   No Swarm, nÃ£o se roda "containers", mas sim **deploy de "services"**.
    *   Swarm roda contÃªineres *para* o serviÃ§o.
    *   **ServiÃ§o:** AbstraÃ§Ã£o sobre contÃªineres; pode ter mÃºltiplas **rÃ©plicas** (contÃªineres idÃªnticos).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina destaca que um Swarm de um nÃ³ Ãºnico (como o que vocÃª tem rodando no Docker Desktop agora) Ã© funcionalmente idÃªntico a um cluster maior para fins de aprendizado dos comandos e conceitos de serviÃ§o, embora obviamente nÃ£o forneÃ§a os benefÃ­cios de alta disponibilidade (HA) ou capacidade de escalar para alÃ©m dos recursos da sua mÃ¡quina. A Figura 12.5 ilustra a diferenÃ§a de arquitetura entre teste (1 nÃ³) e produÃ§Ã£o (mÃºltiplos nÃ³s com redundÃ¢ncia de managers).
*   Reafirma a simplicidade da configuraÃ§Ã£o do cluster Swarm como uma vantagem sobre o Kubernetes.
*   A seÃ§Ã£o 12.3 introduz o conceito central de como as aplicaÃ§Ãµes sÃ£o executadas no Swarm: atravÃ©s de **ServiÃ§os (Services)**. Em vez de usar `docker run` para criar um contÃªiner individual, vocÃª usa `docker service create` para definir um serviÃ§o. O Swarm entÃ£o se encarrega de criar e gerenciar o nÃºmero desejado de contÃªineres (chamados **rÃ©plicas**) para executar aquele serviÃ§o. Um serviÃ§o Ã© a unidade de aplicaÃ§Ã£o gerenciada pelo orquestrador.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Seu Docker Desktop em modo Swarm estÃ¡ pronto.
*   O prÃ³ximo comando serÃ¡ `docker service create`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   ServiÃ§o vs. ContÃªiner: Pense no serviÃ§o como a "descriÃ§Ã£o do trabalho" (ex: "precisamos de 3 caixas no supermercado"). Os contÃªineres/rÃ©plicas sÃ£o os "funcionÃ¡rios" individuais que efetivamente realizam aquele trabalho. O Swarm (gerente) lÃª a descriÃ§Ã£o do trabalho e contrata/demite/substitui os funcionÃ¡rios conforme necessÃ¡rio para cumprir a meta (ex: manter 3 caixas abertos).

---

ğŸ“„ PÃ¡gina 239

ğŸ” Resumo dos principais pontos:

*   DefiniÃ§Ã£o de serviÃ§o inclui info similar a `docker run` (imagem, env vars, portas, nome).
*   Nome do serviÃ§o vira nome DNS na rede overlay do Swarm.
*   ServiÃ§o pode ter mÃºltiplas **rÃ©plicas**.
*   **ExercÃ­cio Try-it-now:** Criar um serviÃ§o simples.
    *   Comando: `docker service create --name timecheck --replicas 1 diamol/ch12-timecheck:1.0`.
    *   `--name timecheck`: Nome do serviÃ§o (e DNS).
    *   `--replicas 1`: NÃºmero desejado de contÃªineres/rÃ©plicas.
    *   `diamol/ch12-timecheck:1.0`: Imagem a ser usada.
    *   Listar serviÃ§os: `docker service ls`.
*   Figura 12.6: Mostra saÃ­da da criaÃ§Ã£o e listagem do serviÃ§o. `service ls` mostra nome, modo (replicated), rÃ©plicas (desejado/atual - 1/1), imagem, portas.
*   RÃ©plicas sÃ£o contÃªineres Docker normais.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz o comando `docker service create` para implantar uma aplicaÃ§Ã£o como um serviÃ§o no Swarm. As opÃ§Ãµes sÃ£o familiares, parecidas com `docker run`:
    *   `--name`: Define o nome do serviÃ§o. Este nome serÃ¡ usado para referenciar o serviÃ§o em outros comandos (`docker service ls`, `docker service rm`, etc.) e tambÃ©m funcionarÃ¡ como o nome DNS para outros serviÃ§os na mesma rede Swarm se comunicarem com ele.
    *   `--replicas`: Especifica quantos contÃªineres idÃªnticos o Swarm deve manter rodando para este serviÃ§o.
    *   `<imagem>:<tag>`: A imagem Docker a ser usada para criar as rÃ©plicas.
*   O exercÃ­cio cria um serviÃ§o chamado `timecheck` com 1 rÃ©plica, usando uma imagem simples que provavelmente imprime a hora periodicamente.
*   O comando `docker service ls` lista os serviÃ§os definidos no Swarm, mostrando seu estado (quantas rÃ©plicas estÃ£o rodando vs. quantas deveriam estar). A Figura 12.6 mostra que o serviÃ§o `timecheck` foi criado e tem 1/1 rÃ©plicas rodando.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar ServiÃ§o:**
    ```powershell
    docker service create --name timecheck --replicas 1 diamol/ch12-timecheck:1.0
    ```
    VocÃª verÃ¡ um ID longo sendo impresso, confirmando a criaÃ§Ã£o. O Docker baixarÃ¡ a imagem se necessÃ¡rio.
2.  **Listar ServiÃ§os:**
    ```powershell
    docker service ls
    ```
    VocÃª deve ver o serviÃ§o `timecheck` na lista com `REPLICAS 1/1`.

---

ğŸ“„ PÃ¡gina 240

ğŸ” Resumo dos principais pontos:

*   RÃ©plicas (containers) rodam em algum nÃ³; pode-se usar `docker container` nesse nÃ³, *mas* nÃ£o Ã© recomendado.
*   Swarm gerencia containers; se vocÃª interferir (ex: `docker rm`), Swarm corrige.
*   **ExercÃ­cio Try-it-now:** Demonstrar auto-recuperaÃ§Ã£o do Swarm.
    *   1. Listar rÃ©plicas/tarefas do serviÃ§o: `docker service ps timecheck`. (Mostra em qual nÃ³ estÃ¡ rodando, status).
    *   2. Listar contÃªineres no nÃ³: `docker container ls`. (Confirma que rÃ©plica Ã© um contÃªiner).
    *   3. Remover o contÃªiner da rÃ©plica: `docker container rm -f $( docker container ls --last 1 -q)`.
    *   4. Listar rÃ©plicas novamente: `docker service ps timecheck`.
*   Resultado esperado (Figura 12.7):
    *   `service ps` inicial mostra 1 rÃ©plica `Running`.
    *   ApÃ³s `rm`, o `service ps` mostra a rÃ©plica original como `Failed` e uma *nova* rÃ©plica `Running` (iniciada hÃ¡ poucos segundos). Swarm substituiu o contÃªiner removido.
*   **Gerenciamento no Swarm:** Usar comandos `docker service ...`, deixar Swarm gerenciar contÃªineres.
*   Comandos `docker service` operam no nÃ­vel do Swarm (agregam info de todos os nÃ³s/rÃ©plicas).
    *   Ex: `docker service logs` mostra logs de *todas* as rÃ©plicas.
    *   Ex: `docker service inspect` mostra definiÃ§Ã£o completa do serviÃ§o armazenada no cluster.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que, embora as rÃ©plicas de um serviÃ§o sejam contÃªineres Docker normais, vocÃª **nÃ£o deve** gerenciÃ¡-los diretamente usando comandos `docker container stop/rm`. O **Swarm** Ã© o responsÃ¡vel por manter o nÃºmero desejado de rÃ©plicas rodando.
*   O exercÃ­cio demonstra isso de forma prÃ¡tica:
    1.  `docker service ps <serviÃ§o>` (ps = process status, ou tasks) lista as tarefas (rÃ©plicas) do serviÃ§o, mostrando seu status e em qual nÃ³ estÃ£o rodando.
    2.  Identifica-se o contÃªiner correspondente Ã  rÃ©plica no nÃ³ local.
    3.  Remove-se *manualmente* esse contÃªiner.
    4.  Ao verificar novamente com `docker service ps`, observa-se que o Swarm detectou a ausÃªncia da rÃ©plica e **automaticamente iniciou uma nova** para restaurar o estado desejado (1 rÃ©plica). A rÃ©plica antiga Ã© marcada como `Failed`. Isso demonstra a capacidade de auto-recuperaÃ§Ã£o bÃ¡sica do Swarm.
*   A liÃ§Ã£o Ã©: interaja com suas aplicaÃ§Ãµes no Swarm usando os comandos `docker service ...` (e `docker stack ...`, como veremos), e nÃ£o os comandos `docker container ...` diretamente nas rÃ©plicas. Os comandos `service` sÃ£o projetados para operar no nÃ­vel do cluster, agregando informaÃ§Ãµes de todas as rÃ©plicas, onde quer que estejam rodando.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Listar Tarefas/RÃ©plicas:**
    ```powershell
    docker service ps timecheck
    # Anote o ID da tarefa e o NOME (ex: timecheck.1.xyz...)
    ```
2.  **Listar ContÃªineres Locais:**
    ```powershell
    docker container ls
    # Encontre o container cujo NOME corresponde ao da tarefa acima. Anote seu CONTAINER ID.
    ```
3.  **Remover ContÃªiner Manualmente:** (Use o CONTAINER ID)
    ```powershell
    docker container rm -f <CONTAINER_ID_da_replica>
    ```
4.  **Listar Tarefas Novamente:** Espere alguns segundos.
    ```powershell
    docker service ps timecheck
    ```
    VocÃª deverÃ¡ ver a tarefa antiga como `Failed` ou `Shutdown`, e uma nova tarefa (com nome diferente, ex: `timecheck.1.abc...`) no estado `Running`.

---

ğŸ“„ PÃ¡gina 241

ğŸ” Resumo dos principais pontos:

*   Figura 12.7: Mostra a saÃ­da do `service ps` antes e depois da remoÃ§Ã£o manual do contÃªiner, evidenciando a substituiÃ§Ã£o automÃ¡tica.
*   **ExercÃ­cio Try-it-now:** Usar comandos `docker service` para obter logs e inspecionar.
    *   Logs das Ãºltimas 10s: `docker service logs --since 10s timecheck`.
    *   Inspecionar serviÃ§o (mostrar sÃ³ imagem): `docker service inspect timecheck -f '{{.Spec.TaskTemplate.ContainerSpec.Image}}'`.
*   Figura 12.8: Mostra saÃ­da dos logs agregados e da imagem inspecionada.
*   DefiniÃ§Ã£o completa do serviÃ§o (`inspect` sem `-f`) Ã© armazenada no banco de dados do cluster (replicado entre managers).
*   **DiferenÃ§a Swarm vs. Compose:** Swarm armazena definiÃ§Ã£o no cluster; Compose depende do arquivo `.yml` local. Pode-se gerenciar serviÃ§os Swarm sem o YAML original.
*   **Atualizando ServiÃ§os:** Pode-se usar `docker service update` para mudar propriedades (ex: imagem). NÃ£o precisa repetir info inalterada.
*   Processo de deploy de atualizaÃ§Ãµes no cluster. Swarm faz **rolling update**: substitui rÃ©plicas gradualmente (remove antiga, inicia nova).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina demonstra o uso dos comandos `docker service logs` e `docker service inspect`.
    *   `service logs`: Agrega os logs de **todas as rÃ©plicas** de um serviÃ§o. Flags como `--since` ou `--tail` funcionam como no `container logs`.
    *   `service inspect`: Mostra a **definiÃ§Ã£o completa** do serviÃ§o como ela estÃ¡ armazenada no cluster Swarm. A flag `-f` (format) usa a sintaxe de templates Go para extrair apenas partes especÃ­ficas da saÃ­da JSON.
*   Um ponto crucial de distinÃ§Ã£o entre Swarm e Compose Ã© destacado: o Swarm **armazena a definiÃ§Ã£o do serviÃ§o** em seu banco de dados distribuÃ­do interno. Isso significa que, uma vez que um serviÃ§o Ã© criado, vocÃª pode gerenciÃ¡-lo (inspecionar, atualizar, remover) de qualquer nÃ³ manager, mesmo que vocÃª nÃ£o tenha mais o arquivo YAML original que o definiu. O Compose, sendo client-side, sempre precisa do arquivo YAML.
*   Introduz o comando `docker service update` como a forma de aplicar mudanÃ§as a um serviÃ§o existente (ex: usar uma nova versÃ£o da imagem). O Swarm gerencia essa atualizaÃ§Ã£o atravÃ©s de um **rolling update**: ele substitui as rÃ©plicas uma por uma (ou em lotes, configurÃ¡vel) para tentar manter o serviÃ§o disponÃ­vel durante a atualizaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Ver Logs do ServiÃ§o:**
    ```powershell
    docker service logs --since 30s timecheck # Ver logs dos Ãºltimos 30s
    ```
    VocÃª verÃ¡ logs prefixados com o nome da rÃ©plica (ex: `timecheck.1.abc... | ...log message...`).
2.  **Inspecionar Imagem do ServiÃ§o:**
    ```powershell
    docker service inspect timecheck -f '{{.Spec.TaskTemplate.ContainerSpec.Image}}'
    # Deve mostrar 'diamol/ch12-timecheck:1.0@sha256:...'
    ```
3.  **Inspecionar Tudo (Opcional):**
    ```powershell
    docker service inspect timecheck
    # Veja a estrutura JSON completa da definiÃ§Ã£o do serviÃ§o no Swarm.
    ```

---

ğŸ“„ PÃ¡gina 242

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Atualizar o serviÃ§o `timecheck` para usar imagem `:2.0`.
    *   Comando: `docker service update --image diamol/ch12-timecheck:2.0 timecheck`.
    *   Listar rÃ©plicas: `docker service ps timecheck`.
    *   Verificar logs: `docker service logs --since 20s timecheck`.
*   SaÃ­da do `service ps` mostra duas instÃ¢ncias: a antiga (`:1.0`) e a nova (`:2.0`) que a substituiu.
*   Logs do serviÃ§o incluem ID da rÃ©plica, permitindo ver logs da versÃ£o antiga e da nova.
*   Figura 12.9: Mostra a saÃ­da do `update`, `ps` (com rÃ©plica antiga `Shutdown` e nova `Running`), e `logs` mostrando logs de ambas as versÃµes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra o processo de atualizaÃ§Ã£o de um serviÃ§o no Swarm usando `docker service update`. A flag `--image` especifica a nova imagem a ser usada (`diamol/ch12-timecheck:2.0`).
*   Ao executar o update, o Swarm inicia o rolling update:
    1.  Ele pode parar a rÃ©plica antiga (rodando `:1.0`).
    2.  Ele inicia uma nova rÃ©plica usando a imagem `:2.0`.
    3.  Ele monitora a nova rÃ©plica (verificando se ela inicia e, se houver health check, se fica saudÃ¡vel).
    4.  Se tudo OK, a atualizaÃ§Ã£o Ã© considerada completa para essa rÃ©plica. (Como sÃ³ temos 1 rÃ©plica, o processo termina aqui).
*   O comando `docker service ps timecheck` mostra o histÃ³rico das rÃ©plicas. VocÃª verÃ¡ a rÃ©plica original com status `Shutdown` (ou similar) e a nova rÃ©plica com status `Running`.
*   O comando `docker service logs` agora mostrarÃ¡ logs de ambas as rÃ©plicas (a antiga antes de parar, e a nova depois de iniciar), prefixados com seus respectivos IDs, permitindo ver a transiÃ§Ã£o. A imagem `:2.0` foi feita para logar a versÃ£o, confirmando a atualizaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Atualizar ServiÃ§o:**
    ```powershell
    docker service update --image diamol/ch12-timecheck:2.0 timecheck
    ```
    Observe a saÃ­da (ex: `timecheck updated`).
2.  **Listar Tarefas/RÃ©plicas:** Espere alguns segundos.
    ```powershell
    docker service ps timecheck
    ```
    VocÃª verÃ¡ a tarefa antiga (`.1`) como `Shutdown` e uma nova (`.2`, ou talvez `.1` reutilizado dependendo do Swarm) como `Running`, usando a imagem `:2.0`.
3.  **Verificar Logs:**
    ```powershell
    docker service logs --since 30s timecheck
    ```
    Procure por logs da rÃ©plica antiga (`:1.0`) e da nova rÃ©plica (que deve logar `version: 2.0`).

---

ğŸ“„ PÃ¡gina 243

ğŸ” Resumo dos principais pontos:

*   Figura 12.9: Mostra logs confirmando update para v2.0.
*   Orquestradores usam **staged rollouts** (atualizaÃ§Ãµes em etapas) para manter app online.
*   Swarm substitui rÃ©plicas uma por uma (padrÃ£o) ou em lotes. Se mÃºltiplas rÃ©plicas, sempre hÃ¡ algumas rodando.
*   Comportamento do rolling update Ã© configurÃ¡vel (serÃ¡ visto depois).
    *   Ex: 10 rÃ©plicas, atualizar 2 por vez, checar saÃºde antes de prosseguir.
*   Diagrama (Figura 12.10): Ilustra rolling update em progresso. Algumas rÃ©plicas rodam versÃ£o antiga, outras rodam a nova.
*   Durante rollout, ambas as versÃµes estÃ£o ativas; gerenciar experiÃªncia do usuÃ¡rio Ã© responsabilidade do app/dev.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica o conceito de **Staged Rollout** (lanÃ§amento em etapas) ou **Rolling Update**, que Ã© o mecanismo padrÃ£o usado por orquestradores (Swarm, K8s) para atualizar aplicaÃ§Ãµes sem causar downtime (ou minimizando-o).
*   A ideia Ã© substituir as rÃ©plicas que rodam a versÃ£o antiga pelas que rodam a versÃ£o nova **gradualmente**, em vez de parar todas as antigas e depois iniciar todas as novas.
*   O comportamento padrÃ£o do Swarm Ã© substituir uma rÃ©plica por vez, mas isso Ã© configurÃ¡vel (ex: atualizar X rÃ©plicas em paralelo, esperar Y segundos entre lotes, verificar health check por Z segundos).
*   A Figura 12.10 visualiza um momento *durante* o rollout, onde o cluster tem rÃ©plicas de ambas as versÃµes (antiga e nova) rodando simultaneamente. O trÃ¡fego de entrada pode ser direcionado para qualquer uma delas.
*   **ImplicaÃ§Ã£o:** A aplicaÃ§Ã£o precisa ser projetada para lidar com essa coexistÃªncia temporÃ¡ria de versÃµes diferentes, especialmente se houver mudanÃ§as no esquema do banco de dados ou formato da API entre as versÃµes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O update que vocÃª fez na pÃ¡gina anterior foi um rolling update muito simples (1 rÃ©plica -> 1 rÃ©plica). Se vocÃª tivesse escalado para `--replicas 3` antes do update, veria o Swarm substituindo as rÃ©plicas uma apÃ³s a outra (no comportamento padrÃ£o).
*   O CapÃ­tulo 14 detalharÃ¡ como configurar o processo de `update_config` no Compose file para controlar o paralelismo, monitoramento e ordem do rollout.

---

ğŸ“„ PÃ¡gina 244

ğŸ” Resumo dos principais pontos:

*   Rolling updates automatizados sÃ£o grande melhoria sobre releases manuais.
*   Suportam auto-recuperaÃ§Ã£o: update checa saÃºde de novas rÃ©plicas; se falharem, update pode pausar automaticamente.
*   Swarm armazena especificaÃ§Ã£o *anterior* do serviÃ§o.
*   **Rollback Manual:** Se update causar problema nÃ£o detectado pelo health check, pode-se reverter para versÃ£o anterior com um comando.
*   **ExercÃ­cio Try-it-now:** Reverter (rollback) o update do `timecheck` para a versÃ£o anterior (1.0).
    *   Comando: `docker service update --rollback timecheck`.
    *   Listar rÃ©plicas: `docker service ps timecheck`.
    *   Verificar logs: `docker service logs --since 25s timecheck`.
*   Rollback funciona como update (staged rollout), mas usa a spec *anterior* armazenada no Swarm (nÃ£o precisa especificar imagem `:1.0`).
*   Ãštil se update quebrar app de forma sutil (nÃ£o pega no health check).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina destaca mais benefÃ­cios dos rolling updates gerenciados pelo orquestrador:
    *   **SeguranÃ§a:** O processo pode incorporar checagens de saÃºde. Se as novas rÃ©plicas falharem nos health checks durante o rollout, o processo pode ser pausado automaticamente, prevenindo uma falha total da aplicaÃ§Ã£o.
    *   **Reversibilidade (Rollback):** O Swarm (e K8s) mantÃ©m um histÃ³rico das configuraÃ§Ãµes do serviÃ§o. Se vocÃª implantar uma atualizaÃ§Ã£o (ex: v2.0) e depois perceber que ela introduziu um bug nÃ£o detectado pelos testes automÃ¡ticos ou health checks, vocÃª pode facilmente acionar um **rollback** para a configuraÃ§Ã£o *anterior* (ex: v1.0) com um Ãºnico comando (`docker service update --rollback <serviÃ§o>`).
*   O exercÃ­cio demonstra esse rollback manual. O comando `--rollback` instrui o Swarm a aplicar a especificaÃ§Ã£o que o serviÃ§o tinha *antes* do Ãºltimo `update`. O Swarm entÃ£o executa outro rolling update, substituindo as rÃ©plicas v2.0 por novas rÃ©plicas v1.0. Os comandos `ps` e `logs` confirmarÃ£o a reversÃ£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Executar Rollback:**
    ```powershell
    docker service update --rollback timecheck
    ```
2.  **Listar Tarefas/RÃ©plicas:** Espere alguns segundos.
    ```powershell
    docker service ps timecheck
    ```
    VocÃª verÃ¡ a tarefa v2.0 (`.2` ou `.1`) como `Shutdown` e uma nova tarefa (`.3` ou `.2`) como `Running`, usando a imagem `:1.0` novamente.
3.  **Verificar Logs:**
    ```powershell
    docker service logs --since 30s timecheck
    ```
    VocÃª verÃ¡ logs da rÃ©plica v2.0 sendo parada e da nova rÃ©plica v1.0 iniciando e logando `version: 1.0`.

---

ğŸ“„ PÃ¡gina 245

ğŸ” Resumo dos principais pontos:

*   Figura 12.11: Mostra a saÃ­da do rollback, com rÃ©plica v2 Shutdown e nova rÃ©plica v1 Running, e logs confirmando a reversÃ£o.
*   Gerenciamento no Swarm Ã© via `services`. Outros recursos (networks, ports) funcionam de forma similar.
*   **12.4 Managing network traffic in the cluster** (Gerenciando trÃ¡fego de rede no cluster)
    *   ComunicaÃ§Ã£o app-a-app no Swarm Ã© TCP/IP padrÃ£o (para os apps).
    *   Apps usam DNS (nome do serviÃ§o) -> Docker DNS retorna IP -> App conecta ao IP.
    *   Containers podem estar em nÃ³s diferentes; comunicaÃ§Ã£o transparente.
    *   Swarm faz "mÃ¡gica" de rede por baixo dos panos.
    *   **Rede Overlay (Overlay Network):** Tipo de rede Docker para Swarm.
        *   Rede virtual que se estende por *todos* os nÃ³s do cluster.
        *   ServiÃ§os anexados a mesma overlay podem se comunicar usando nome do serviÃ§o como DNS.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a parte de updates/rollbacks e inicia a discussÃ£o sobre **redes no Docker Swarm** (SeÃ§Ã£o 12.4).
*   Para as aplicaÃ§Ãµes rodando nos contÃªineres, a comunicaÃ§Ã£o entre serviÃ§os parece simples: um serviÃ§o `web` apenas tenta se conectar a `http://api/...`. Ele usa o nome do serviÃ§o (`api`) como um nome DNS normal.
*   O Swarm fornece a infraestrutura de rede para que isso funcione, mesmo que os contÃªineres `web` e `api` estejam rodando em servidores fÃ­sicos diferentes dentro do cluster. A principal ferramenta para isso Ã© a **Rede Overlay**.
*   Uma rede overlay Ã© um tipo especial de rede Docker (`--driver overlay`) que cria uma rede virtual privada que "cobre" (overlays) todos os nÃ³s participantes do Swarm. Quando vocÃª conecta serviÃ§os a essa rede, o Swarm garante que:
    *   Cada rÃ©plica receba um IP nessa rede virtual.
    *   O DNS interno do Swarm resolva os nomes dos serviÃ§os para os IPs apropriados dentro dessa rede.
    *   O trÃ¡fego possa fluir entre contÃªineres em nÃ³s diferentes de forma transparente atravÃ©s dessa rede virtual.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O comando para criar uma rede overlay Ã© `docker network create --driver overlay <nome_rede>`. (VocÃª usarÃ¡ isso no prÃ³ximo exercÃ­cio).
*   Ao definir serviÃ§os (com `docker service create` ou em um Compose file para `docker stack deploy`), vocÃª os conecta Ã  rede overlay usando a flag `--network <nome_rede>`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Rede Overlay: Ã‰ como criar uma rede Wi-Fi privada (`<nome_rede>`) que magicamente cobre todos os prÃ©dios do seu campus (cluster Swarm). Qualquer dispositivo (contÃªiner) conectado a essa Wi-Fi pode falar com outros dispositivos na mesma Wi-Fi usando seus nomes, nÃ£o importa em qual prÃ©dio eles estejam fisicamente.

---

ğŸ“„ PÃ¡gina 246

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 12.12): Mostra duas redes overlay separadas (`App A`, `App B`) em um cluster multi-nÃ³. Cada rede conecta os serviÃ§os de sua aplicaÃ§Ã£o, mas isola `App A` de `App B`.
*   Rede overlay permite comunicaÃ§Ã£o intra-app e isolamento inter-apps.
*   **DiferenÃ§a DNS: Compose vs. Swarm Overlay:**
    *   Compose (rede bridge padrÃ£o): DNS retorna *todos* IPs das rÃ©plicas escaladas. Cliente escolhe/balanceia. NÃ£o escala bem para centenas de rÃ©plicas.
    *   Swarm (rede overlay): DNS retorna *um Ãºnico IP Virtual (VIP)* para o serviÃ§o, compartilhado por todas as rÃ©plicas.
*   **ExercÃ­cio Try-it-now:** Criar rede overlay e serviÃ§os API do app Image Gallery.
    *   1. Remover serviÃ§o `timecheck`: `docker service rm timecheck`.
    *   2. Criar rede overlay: `docker network create --driver overlay iotd-net`.
    *   3. Criar serviÃ§o API `iotd` (3 rÃ©plicas) na rede: `docker service create --detach --replicas 3 --network iotd-net --name iotd diamol/ch09-image-of-the-day`.
    *   4. Criar serviÃ§o API `accesslog` (2 rÃ©plicas) na rede: `docker service create --detach --replicas 2 --network iotd-net --name accesslog diamol/ch09-access-log`.
    *   5. Verificar serviÃ§os: `docker service ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 12.12 ilustra que vocÃª pode ter mÃºltiplas redes overlay em um mesmo cluster, e elas funcionam como redes privadas separadas, garantindo isolamento entre diferentes aplicaÃ§Ãµes.
*   A pÃ¡gina aponta uma diferenÃ§a importante no funcionamento do DNS entre as redes padrÃ£o do Compose (tipo `bridge`) e as redes `overlay` do Swarm, especialmente para serviÃ§os escalados:
    *   **Compose/Bridge:** Retorna mÃºltiplos IPs.
    *   **Swarm/Overlay:** Retorna um Ãºnico **IP Virtual (VIP)**. Este VIP atua como um endereÃ§o de balanceador de carga interno para o serviÃ§o. O Swarm automaticamente distribui o trÃ¡fego enviado para o VIP entre as rÃ©plicas saudÃ¡veis do serviÃ§o. Isso Ã© mais eficiente e escalÃ¡vel do que o DNS round-robin do Compose.
*   O exercÃ­cio prepara o cenÃ¡rio para observar o VIP: remove o serviÃ§o antigo, cria uma rede overlay (`iotd-net`), e inicia os dois serviÃ§os de backend (`iotd` e `accesslog`) da aplicaÃ§Ã£o Image Gallery, conectando ambos Ã  rede `iotd-net` e escalando-os para mÃºltiplas rÃ©plicas.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Remover ServiÃ§o Antigo:**
    ```powershell
    docker service rm timecheck
    ```
2.  **Criar Rede Overlay:**
    ```powershell
    docker network create --driver overlay iotd-net
    ```
3.  **Criar ServiÃ§o `iotd`:**
    ```powershell
    docker service create --detach --replicas 3 --network iotd-net --name iotd diamol/ch09-image-of-the-day
    ```
4.  **Criar ServiÃ§o `accesslog`:**
    ```powershell
    docker service create --detach --replicas 2 --network iotd-net --name accesslog diamol/ch09-access-log
    ```
5.  **Verificar ServiÃ§os:**
    ```powershell
    docker service ls
    # VocÃª deve ver iotd (3/3) e accesslog (2/2)
    ```

---

ğŸ“„ PÃ¡gina 247

ğŸ” Resumo dos principais pontos:

*   Figura 12.13: Mostra saÃ­da do `service ls` com os novos serviÃ§os `iotd` e `accesslog` rodando com mÃºltiplas rÃ©plicas na rede overlay.
*   Comandos rodam igual em Swarm de 1 nÃ³ ou 500 nÃ³s (exceto que rÃ©plicas estariam distribuÃ­das).
*   **Verificando VIP:** Conectar a um terminal de uma rÃ©plica e usar `nslookup`.
*   **ExercÃ­cio Try-it-now:** Executar `nslookup` dentro de uma rÃ©plica.
    *   1. Executar shell interativo no Ãºltimo contÃªiner criado (serÃ¡ uma rÃ©plica `accesslog` ou `iotd`).
        *   Comando Linux: `docker container exec -it $(...) sh`.
        *   Comando Windows: `docker container exec -it $(...) cmd`.
    *   2. Dentro do contÃªiner, rodar `nslookup iotd` e `nslookup accesslog`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que os serviÃ§os foram criados com sucesso no Swarm (Figura 12.13).
*   O objetivo agora Ã© verificar experimentalmente o **IP Virtual (VIP)** retornado pelo DNS do Swarm para serviÃ§os em redes overlay.
*   A maneira de fazer isso Ã© entrar em um dos contÃªineres (rÃ©plicas) que estÃ£o conectados Ã  rede overlay (`iotd-net`) e usar uma ferramenta de consulta DNS como `nslookup` a partir de lÃ¡.
*   O exercÃ­cio instrui a usar `docker exec -it ...` para obter um shell dentro de uma das rÃ©plicas e, em seguida, executar `nslookup` para os nomes dos serviÃ§os (`iotd` e `accesslog`). A expectativa (baseada na explicaÃ§Ã£o anterior) Ã© que, mesmo com mÃºltiplas rÃ©plicas, o `nslookup` retorne apenas **um** endereÃ§o IP para cada serviÃ§o - o VIP.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Obter ID do Ãšltimo Container:**
    ```powershell
    $lastContainer = $(docker container ls --last 1 -q)
    echo $lastContainer
    ```
2.  **Executar Shell no Container:** (Assumindo imagem base Linux)
    ```powershell
    docker container exec -it $lastContainer sh
    ```
3.  **Dentro do Container (prompt / # ou /app #):**
    ```
    # Instalar dnsutils se nslookup nÃ£o estiver presente (comum em Alpine)
    # apk update && apk add bind-tools
    # Ou apt-get update && apt-get install dnsutils (se Debian/Ubuntu)

    # Executar nslookup
    nslookup iotd
    nslookup accesslog
    ```
4.  **Analisar SaÃ­da:** Para cada `nslookup`, procure a(s) linha(s) `Address:` na seÃ§Ã£o "Non-authoritative answer". VocÃª deve ver apenas **UM** endereÃ§o IP para `iotd` e **UM** endereÃ§o IP para `accesslog` (provavelmente diferentes entre si, mas apenas um por serviÃ§o). Anote esses IPs.
5.  **Sair:**
    ```
    exit
    ```

---

ğŸ“„ PÃ¡gina 248

ğŸ” Resumo dos principais pontos:

*   Figura 12.14: Mostra saÃ­da do `nslookup` dentro do contÃªiner, confirmando que apenas 1 IP (o VIP) Ã© retornado para cada serviÃ§o, mesmo com mÃºltiplas rÃ©plicas.
*   **VIP Networking:** Usado em overlay (Linux/Windows) para load balancing eficiente.
    *   DNS retorna 1 VIP constante (mesmo se escalar).
    *   Cliente envia trÃ¡fego para VIP.
    *   Camada de rede do SO (kernel) intercepta e distribui trÃ¡fego para uma das rÃ©plicas reais (ex: via IPVS no Linux).
*   Swarm usa VIP para comunicaÃ§Ã£o interna confiÃ¡vel e balanceada. Transparente para apps (usam DNS normal).
*   **TrÃ¡fego Externo (Ingress):** Swarm tambÃ©m simplifica como trÃ¡fego de fora entra no cluster. Problema complexo (qual nÃ³ recebe? onde estÃ£o as rÃ©plicas?).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma (Figura 12.14) que o `nslookup` para serviÃ§os em rede overlay retorna apenas um IP, o **VIP (Virtual IP)**.
*   Explica como o VIP funciona para balanceamento de carga interno:
    1.  O cliente DNS obtÃ©m apenas o VIP.
    2.  O cliente envia pacotes para o VIP.
    3.  O kernel do sistema operacional em *cada nÃ³* do Swarm Ã© configurado (pelo Swarm) para saber que aquele VIP corresponde a mÃºltiplas rÃ©plicas reais (possivelmente em outros nÃ³s).
    4.  O kernel intercepta o trÃ¡fego destinado ao VIP e o redireciona para uma das rÃ©plicas reais saudÃ¡veis, usando mecanismos eficientes de balanceamento de carga em nÃ­vel de rede (como IPVS no Linux).
*   Isso fornece um balanceamento de carga mais robusto e escalÃ¡vel do que o DNS round-robin do Compose. Para a aplicaÃ§Ã£o, tudo Ã© transparente.
*   Introduz o prÃ³ximo desafio de rede: como o trÃ¡fego vindo de **fora do cluster** (usuÃ¡rios na internet) chega ao serviÃ§o correto, considerando que o usuÃ¡rio pode se conectar a qualquer nÃ³ do cluster, e as rÃ©plicas do serviÃ§o podem estar em nÃ³s diferentes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. A compreensÃ£o do VIP Ã© conceitual.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   VIP (Balanceamento Interno): Ã‰ como ter um nÃºmero de telefone Ãºnico para o "Departamento de Vendas" (o serviÃ§o). Quando vocÃª liga para esse nÃºmero (envia trÃ¡fego para o VIP), a central telefÃ´nica do prÃ©dio (kernel+Swarm) automaticamente direciona sua chamada para um vendedor disponÃ­vel (rÃ©plica saudÃ¡vel), nÃ£o importa em qual sala (nÃ³) ele esteja.

---

ğŸ“„ PÃ¡gina 249

ğŸ” Resumo dos principais pontos:

*   **Rede Ingress (Ingress Network):** SoluÃ§Ã£o do Swarm para trÃ¡fego externo.
    *   Diagrama (Figura 12.15): Ilustra a rede Ingress.
    *   Todos os nÃ³s no Swarm escutam na porta publicada pelo serviÃ§o (ex: porta 80).
    *   Quando um nÃ³ recebe trÃ¡fego nessa porta, a Rede Ingress (que tambÃ©m Ã© uma overlay) roteia a requisiÃ§Ã£o para uma rÃ©plica saudÃ¡vel do serviÃ§o, *mesmo que a rÃ©plica esteja em outro nÃ³*.
*   Ingress = balanceador de carga distribuÃ­do embutido no Swarm.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica a **Rede Ingress** do Docker Swarm, que resolve o problema do acesso externo.
*   **Como funciona (Figura 12.15):**
    1.  VocÃª cria um serviÃ§o e publica uma porta (ex: `-p 80:8080`).
    2.  O Swarm automaticamente configura **todos os nÃ³s** do cluster para escutar naquela porta externa (porta 80 do host, neste exemplo).
    3.  Quando um usuÃ¡rio acessa `http://<IP_de_QUALQUER_NÃ“>:80`, aquele nÃ³ recebe a requisiÃ§Ã£o.
    4.  A rede Ingress (uma rede overlay especial gerenciada pelo Swarm) entra em aÃ§Ã£o. Ela sabe onde estÃ£o todas as rÃ©plicas saudÃ¡veis do serviÃ§o que publicou a porta 80.
    5.  A requisiÃ§Ã£o Ã© roteada (atravÃ©s da rede Ingress) para uma das rÃ©plicas saudÃ¡veis, nÃ£o importa em qual nÃ³ ela esteja rodando.
    6.  A rÃ©plica processa a requisiÃ§Ã£o e a resposta retorna pelo mesmo caminho.
*   **BenefÃ­cio:** VocÃª pode direcionar seus usuÃ¡rios (ou seu balanceador de carga externo) para o endereÃ§o IP de *qualquer* nÃ³ no seu Swarm, e o trÃ¡fego chegarÃ¡ ao serviÃ§o correto. A rede Ingress atua como um balanceador de carga distribuÃ­do na camada 4 (TCP/UDP) para portas publicadas.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª jÃ¡ usou a rede Ingress sem saber! Quando vocÃª usou `-p` (ou `ports:`) com `docker service create` ou `docker stack deploy` em modo Swarm, o Swarm configurou a rede Ingress para vocÃª.
*   O prÃ³ximo exercÃ­cio completarÃ¡ o deploy do Image Gallery adicionando o serviÃ§o web com uma porta publicada, que usarÃ¡ a rede Ingress.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Rede Ingress: Ã‰ como se todos os porteiros de todos os prÃ©dios do campus (todos os nÃ³s do Swarm) tivessem o mesmo nÃºmero de interfone pÃºblico (porta publicada, ex: 80). Quando um visitante liga para esse nÃºmero em qualquer portaria, o sistema central (Rede Ingress) sabe para qual apartamento especÃ­fico (rÃ©plica saudÃ¡vel) em qual prÃ©dio (nÃ³) encaminhar a chamada.

---

ğŸ“„ PÃ¡gina 250

ğŸ” Resumo dos principais pontos:

*   Rede Ingress Ã© padrÃ£o no Swarm ao publicar portas (`--publish` ou `ports:`). Complexo, mas fÃ¡cil de usar.
*   **ExercÃ­cio Try-it-now:** Implantar serviÃ§o web (`image-gallery`) do app, publicando porta via Ingress.
    *   Comando: `docker service create --detach --name image-gallery --network iotd-net --publish 8010:80 --replicas 2 diamol/ch09-image-gallery`.
    *   `--publish 8010:80`: Publica porta 80 interna na porta 8010 de *todos* os nÃ³s via Ingress.
    *   `--replicas 2`: Roda 2 instÃ¢ncias do web app.
    *   Listar serviÃ§os: `docker service ls`.
*   Resultado: ServiÃ§o rodando com mÃºltiplas rÃ©plicas, ouvindo em porta Ãºnica via Ingress. ImpossÃ­vel no Compose (conflito de porta), possÃ­vel no Swarm.
*   Figura 12.16: Mostra o `service ls` com o novo serviÃ§o `image-gallery` (2/2 rÃ©plicas) e a porta 8010 publicada.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que usar a rede Ingress Ã© automÃ¡tico no Swarm: basta publicar uma porta para o serviÃ§o.
*   O exercÃ­cio finaliza a implantaÃ§Ã£o da aplicaÃ§Ã£o Image Gallery no Swarm, criando o serviÃ§o frontend `image-gallery`.
    *   Ele Ã© conectado Ã  mesma rede overlay (`iotd-net`) que as APIs para comunicaÃ§Ã£o interna.
    *   Ele publica a porta 8010 (`--publish 8010:80`). Isso automaticamente configura a rede Ingress para que o acesso a `http://<qualquer_nÃ³>:8010` seja roteado para uma das rÃ©plicas do `image-gallery`.
    *   Ele Ã© iniciado com `--replicas 2`. O Swarm garantirÃ¡ que duas instÃ¢ncias desse contÃªiner estejam rodando, e a rede Ingress farÃ¡ o balanceamento de carga entre elas para requisiÃ§Ãµes vindas de fora.
*   Um ponto importante Ã© destacado: no Swarm, mÃºltiplas rÃ©plicas do *mesmo serviÃ§o* podem (e devem) ser acessadas atravÃ©s da *mesma porta publicada* via Ingress. Isso Ã© diferente do Docker Compose em um host Ãºnico, onde vocÃª nÃ£o pode ter mÃºltiplos contÃªineres tentando usar a mesma porta do host.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar ServiÃ§o Web:**
    ```powershell
    docker service create --detach --name image-gallery --network iotd-net --publish 8010:80 --replicas 2 diamol/ch09-image-gallery
    ```
2.  **Verificar ServiÃ§os:**
    ```powershell
    docker service ls
    ```
    VocÃª deve ver os 3 serviÃ§os (`accesslog`, `iotd`, `image-gallery`) rodando com o nÃºmero correto de rÃ©plicas e a porta `*:8010->80/tcp` listada para `image-gallery`.

---

ğŸ“„ PÃ¡gina 251

ğŸ” Resumo dos principais pontos:

*   Figura 12.16: SaÃ­da do `service ls`.
*   **Acesso ao App:**
    *   **Linux/Mac/DockerDesktop-Linux:** Acessar `http://localhost:8010`. Funciona.
    *   **Windows Containers (Nativo/Server/DockerDesktop-Windows):** **NÃƒO** funciona acessar via `localhost`. LimitaÃ§Ã£o de rede do Windows para Ingress Swarm. Precisa acessar pelo IP do nÃ³ a partir de *outra mÃ¡quina* na rede, ou usar workarounds.
*   Ã‰ uma das poucas grandes diferenÃ§as/limitaÃ§Ãµes de Windows Containers. Funciona em cenÃ¡rios remotos (teste/prod), mas nÃ£o localmente via `localhost`.
*   Autor muda para Linux containers para o resto do capÃ­tulo.
*   Figura 12.17: Mostra app Image Gallery acessado via navegador (presumivelmente de um host Linux/Mac). TrÃ¡fego roteado via Ingress para uma rÃ©plica web, que chama API via rede overlay.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com o serviÃ§o web rodando e a porta 8010 publicada via rede Ingress, a pÃ¡gina explica como acessÃ¡-lo. E aqui surge uma **incompatibilidade significativa** para usuÃ¡rios de **Windows Containers nativos**.
*   **Linux/Mac:** A rede Ingress no Linux/Mac permite que vocÃª acesse o serviÃ§o publicado usando `localhost:<porta_publicada>` na prÃ³pria mÃ¡quina onde o Swarm (mesmo de 1 nÃ³) estÃ¡ rodando.
*   **Windows Containers:** Devido a limitaÃ§Ãµes na implementaÃ§Ã£o da rede overlay/ingress no Windows, acessar `localhost:<porta_publicada>` **nÃ£o funciona** para serviÃ§os Swarm. O trÃ¡fego nÃ£o Ã© roteado corretamente pela rede Ingress nesse caso especÃ­fico. Para acessar um serviÃ§o rodando em um Swarm de Windows Containers, vocÃª precisa usar o endereÃ§o IP real do nÃ³ Swarm e acessÃ¡-lo de *outra mÃ¡quina* na mesma rede, ou usar configuraÃ§Ãµes de rede mais complexas (como modo de host direto, que tem outras implicaÃ§Ãµes).
*   O autor reconhece que isso Ã© uma limitaÃ§Ã£o frustrante do Windows e informa que usarÃ¡ Linux containers para os screenshots/exemplos seguintes onde o acesso via `localhost` Ã© necessÃ¡rio para a demonstraÃ§Ã£o.
*   A Figura 12.17 ilustra o fluxo completo do trÃ¡fego em um ambiente funcional (Linux): navegador -> nÃ³ Swarm:8010 -> Rede Ingress -> rÃ©plica `image-gallery` -> Rede Overlay -> rÃ©plica `iotd`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Se vocÃª estiver usando Docker Desktop no MODO LINUX:**
    *   Abra o navegador e acesse `http://localhost:8010`. Deve funcionar e mostrar a aplicaÃ§Ã£o Image Gallery.
*   **Se vocÃª estiver usando Docker Desktop no MODO WINDOWS (ou Windows Server):**
    *   Acessar `http://localhost:8010` **NÃƒO** funcionarÃ¡.
    *   Para testar, vocÃª precisaria:
        1.  Encontrar o endereÃ§o IP da sua mÃ¡quina Windows na sua rede local (ex: via `ipconfig`).
        2.  Acessar `http://<seu_ip_windows>:8010` de *outro dispositivo* (outro PC, celular) na mesma rede Wi-Fi/LAN. (Pode precisar ajustar firewall do Windows para permitir trÃ¡fego na porta 8010).
    *   *Devido a essa complexidade, pode ser mais fÃ¡cil seguir os prÃ³ximos exemplos que dependem de acesso `localhost` trocando o Docker Desktop para o modo Linux.*

---

ğŸ“„ PÃ¡gina 252

ğŸ” Resumo dos principais pontos:

*   Figura 12.17: Mostra o app rodando.
*   ReafirmaÃ§Ã£o: Tamanho do cluster (1 nÃ³ vs 50 nÃ³s) nÃ£o muda a forma de deploy/gerenciamento via Swarm. Comandos sÃ£o os mesmos.
*   **12.5 Understanding the choice between Docker Swarm and Kubernetes** (Entendendo a escolha entre Docker Swarm e Kubernetes)
    *   Swarm foi projetado para ser simples, estendendo conceitos do Compose.
    *   Outros orquestradores surgiram, mas hoje a escolha Ã© basicamente Swarm vs. Kubernetes (K8s).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a parte prÃ¡tica do Swarm, reforÃ§ando que os comandos e conceitos (`service create`, `service update`, `network create --driver overlay`, rede Ingress) funcionam da mesma forma independentemente do nÃºmero de nÃ³s no cluster.
*   A seÃ§Ã£o 12.5 inicia uma discussÃ£o comparativa importante: **Docker Swarm versus Kubernetes (K8s)**.
*   Contextualiza que o Swarm evoluiu naturalmente do Docker e do Compose, buscando simplicidade. O Kubernetes surgiu como um projeto separado (originado no Google), com um escopo mais amplo e complexo. Outros orquestradores existiram, mas Swarm e K8s se consolidaram como as principais opÃ§Ãµes. O objetivo desta seÃ§Ã£o Ã© ajudar o leitor a entender as diferenÃ§as e quando escolher um ou outro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. A discussÃ£o Ã© conceitual e estratÃ©gica.
*   O Docker Desktop permite habilitar um cluster Kubernetes local (single-node) tambÃ©m (em Settings > Kubernetes > Enable Kubernetes), caso vocÃª queira experimentar K8s futuramente.

---

ğŸ“„ PÃ¡gina 253

ğŸ” Resumo dos principais pontos:

*   **Kubernetes:**
    *   Mais popular, especialmente na nuvem (oferecido como serviÃ§o gerenciado por AWS, Azure, GCP).
    *   Nuvens cuidam do setup complexo do cluster K8s e gerenciamento dos nÃ³s (VMs).
    *   ExtensÃ­vel, integra bem com outros serviÃ§os da nuvem (load balancers, storage).
*   **Docker Swarm:**
    *   NÃ£o oferecido como serviÃ§o gerenciado nas nuvens principais.
    *   Para rodar Swarm na nuvem, precisa provisionar VMs e inicializar Swarm manualmente (mais trabalho).
*   Diagrama (Figura 12.18): Mostra a complexidade de recursos na nuvem (VMs, Load Balancer, Discos Gerenciados, Rede Virtual, etc.) que precisam ser gerenciados manualmente para rodar um Swarm em produÃ§Ã£o no Azure.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina compara Swarm e K8s sob a Ã³tica da **disponibilidade e facilidade de uso na nuvem pÃºblica**.
*   **Kubernetes (K8s)** tem a vantagem clara aqui: todos os grandes provedores (AWS com EKS, Azure com AKS, Google com GKE) oferecem **Kubernetes como ServiÃ§o Gerenciado**. Isso significa que vocÃª pode criar um cluster K8s complexo e pronto para produÃ§Ã£o com poucos cliques ou comandos, e o provedor de nuvem cuida da instalaÃ§Ã£o, gerenciamento e atualizaÃ§Ã£o

**You**

ğŸ§  Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), pÃ¡gina por pÃ¡gina, a apartir da pÃ¡gina 253.

Para cada pÃ¡gina, siga este modelo de resposta:

ğŸ“„ PÃ¡gina X

ğŸ” Resumo dos principais pontos:

\[bullet points com os tÃ³picos abordados na pÃ¡gina\]

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

\[explique os conceitos em portuguÃªs claro, com boa didÃ¡tica, como um professor\]

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

\[comandos ou trechos de cÃ³digo aplicÃ¡veis no Windows, usando Docker Desktop e Visual Studio Code\]

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

\[comparaÃ§Ãµes simples ou observaÃ§Ãµes para reforÃ§ar o entendimento\]

âœ… Importante: percorra automaticamente todas as pÃ¡ginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "prÃ³xima".

Quando terminar todas as pÃ¡ginas, diga:

"âœ… Fim da leitura. Todas as pÃ¡ginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

OK, Alexandre, retomando a anÃ¡lise a partir da pÃ¡gina 253.

---

ğŸ“„ PÃ¡gina 253

ğŸ” Resumo dos principais pontos:

*   **Kubernetes:**
    *   Mais popular, especialmente na nuvem (oferecido como serviÃ§o gerenciado por AWS, Azure, GCP).
    *   Nuvens cuidam do setup complexo do cluster K8s e gerenciamento dos nÃ³s (VMs).
    *   ExtensÃ­vel, integra bem com outros serviÃ§os da nuvem (load balancers, storage).
*   **Docker Swarm:**
    *   NÃ£o oferecido como serviÃ§o gerenciado nas nuvens principais.
    *   Para rodar Swarm na nuvem, precisa provisionar VMs e inicializar Swarm manualmente (mais trabalho).
*   Diagrama (Figura 12.18): Mostra a complexidade de recursos na nuvem (VMs, Load Balancer, Discos Gerenciados, Rede Virtual, etc.) que precisam ser gerenciados manualmente para rodar um Swarm em produÃ§Ã£o no Azure.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina compara Swarm e K8s sob a Ã³tica da **disponibilidade e facilidade de uso na nuvem pÃºblica**.
*   **Kubernetes (K8s)** tem a vantagem clara aqui: todos os grandes provedores (AWS com EKS, Azure com AKS, Google com GKE) oferecem **Kubernetes como ServiÃ§o Gerenciado**. Isso significa que vocÃª pode criar um cluster K8s complexo e pronto para produÃ§Ã£o com poucos cliques ou comandos, e o provedor de nuvem cuida da instalaÃ§Ã£o, gerenciamento e atualizaÃ§Ã£o da infraestrutura do Kubernetes (os "nÃ³s mestres" ou "control plane") e dos nÃ³s trabalhadores (VMs). AlÃ©m disso, esses serviÃ§os K8s gerenciados integram-se facilmente com outros recursos da nuvem, como balanceadores de carga, armazenamento persistente, etc.
*   **Docker Swarm**, por outro lado, nÃ£o possui essa mesma oferta de serviÃ§o gerenciado nas principais nuvens. Se vocÃª quiser rodar Swarm na AWS, Azure ou GCP, vocÃª terÃ¡ que:
    1.  Criar as mÃ¡quinas virtuais (VMs) manualmente.
    2.  Instalar o Docker Engine em cada uma.
    3.  Executar `docker swarm init` e `docker swarm join` para formar o cluster.
    4.  Configurar manualmente a integraÃ§Ã£o com balanceadores de carga externos, armazenamento compartilhado, etc.
*   A Figura 12.18 ilustra essa complexidade adicional ao rodar Swarm na nuvem, mostrando todos os componentes de infraestrutura que vocÃª mesmo teria que configurar e gerenciar no Azure.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Experimentar K8s local:** O Docker Desktop (nas configuraÃ§Ãµes > Kubernetes) permite habilitar um cluster Kubernetes de nÃ³ Ãºnico localmente para aprendizado.
*   **Experimentar Swarm na Nuvem (Conceitual):** Envolveria usar o portal ou CLI do Azure/AWS/GCP para criar VMs Linux ou Windows, conectar-se a elas (via SSH ou RDP), instalar o Docker e rodar os comandos `swarm init/join`. Ferramentas como Terraform ou Pulumi podem automatizar isso.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   K8s Gerenciado na Nuvem: Comprar um carro pronto da concessionÃ¡ria, com garantia e manutenÃ§Ã£o incluÃ­das pelo fabricante.
*   Swarm na Nuvem (Manual): Comprar todas as peÃ§as separadamente e montar o carro vocÃª mesmo na sua garagem. Funciona, mas exige mais esforÃ§o e conhecimento de mecÃ¢nica.

---

ğŸ“„ PÃ¡gina 254

ğŸ” Resumo dos principais pontos:

*   Deploy de clusters Ã© menos frequente que deploy de apps.
*   **OperaÃ§Ãµes ContÃ­nuas: Swarm Ã© mais simples que K8s.**
    *   Swarm tem menos features, mas o suficiente para muitos, com fraÃ§Ã£o da complexidade do K8s.
    *   YAML do Swarm: ExtensÃ£o do Docker Compose (conciso, lÃ³gico).
    *   YAML do K8s: Muito mais complexo e verboso (5 a 10x mais YAML para mesma app).
*   **Conselho:** ComeÃ§ar com Swarm, migrar para K8s *se* precisar de feature que Swarm nÃ£o tem.
    *   Investimento em Dockerizar apps nÃ£o Ã© perdido (mesmas imagens).
*   **Fatores para Escolha (Swarm vs. K8s):**
    1.  **Infraestrutura:** Nuvem -> K8s gerenciado Ã© mais simples. Datacenter -> Swarm Ã© mais fÃ¡cil de operar. Time 100% Windows -> Swarm pode ser mais fÃ¡cil.
    2.  **Curva de Aprendizado:** Swarm Ã© extensÃ£o natural de Docker/Compose. K8s Ã© ecossistema totalmente novo.
    3.  **Conjunto de Features:** K8s Ã© mais rico/configurÃ¡vel (ex: blue/green deploy, autoscaling, RBAC). Swarm tem o essencial.
    4.  **Investimento Futuro:** K8s tem comunidade maior/mais ativa, novas features constantes. Swarm Ã© mais estÃ¡vel/maduro, menos features novas.
*   ProvÃ¡vel roadmap futuro leva a K8s, mas Swarm Ã© Ã³timo comeÃ§o. Exemplo: Visa usa Swarm em produÃ§Ã£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Embora configurar um cluster K8s gerenciado na nuvem seja mais fÃ¡cil, a pÃ¡gina argumenta que, no **dia a dia da operaÃ§Ã£o e implantaÃ§Ã£o de aplicaÃ§Ãµes**, o **Docker Swarm Ã© consideravelmente mais simples** que o Kubernetes.
*   **Complexidade:** O Swarm tem um conjunto menor de conceitos e funcionalidades, o que o torna mais fÃ¡cil de aprender e gerenciar. O Kubernetes tem muito mais opÃ§Ãµes, o que traz flexibilidade, mas tambÃ©m uma curva de aprendizado e complexidade operacional significativamente maiores.
*   **DefiniÃ§Ã£o da AplicaÃ§Ã£o (YAML):** O Swarm utiliza uma extensÃ£o do formato Docker Compose, que muitos jÃ¡ conhecem. O Kubernetes usa seus prÃ³prios arquivos de manifesto YAML, que sÃ£o notavelmente mais verbosos e complexos para descrever a mesma aplicaÃ§Ã£o.
*   **RecomendaÃ§Ã£o do Autor:** Para quem estÃ¡ comeÃ§ando com orquestraÃ§Ã£o, faz sentido iniciar com o Swarm. Se, e somente se, vocÃª encontrar uma necessidade que o Swarm nÃ£o atende, aÃ­ sim considere a migraÃ§Ã£o para o Kubernetes. A boa notÃ­cia Ã© que o trabalho de criar as imagens Docker Ã© reaproveitado em ambos.
*   A pÃ¡gina lista fatores importantes a considerar na decisÃ£o: o ambiente de infraestrutura (nuvem vs. datacenter, Windows vs. Linux), a capacidade e disposiÃ§Ã£o da equipe para aprender uma nova tecnologia complexa (K8s), as funcionalidades especÃ­ficas necessÃ¡rias, e a direÃ§Ã£o futura do desenvolvimento de cada plataforma.
*   Conclui que, embora K8s seja o destino provÃ¡vel para muitos a longo prazo, o Swarm Ã© uma plataforma de produÃ§Ã£o robusta e comprovada (citando o caso da Visa).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Os comandos que vocÃª aprendeu para `docker service` e `docker stack` (usando arquivos Compose) sÃ£o a base do gerenciamento no Swarm.
*   Para Kubernetes, vocÃª usaria uma ferramenta de linha de comando diferente (`kubectl`) e arquivos YAML com uma estrutura totalmente diferente para definir `Deployments`, `Services`, `Pods`, `Ingresses`, etc.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Swarm vs. K8s (Complexidade): Swarm Ã© como um smartphone com as funÃ§Ãµes essenciais bem feitas e fÃ¡ceis de usar. K8s Ã© como um smartphone topo de linha com centenas de apps e configuraÃ§Ãµes avanÃ§adas - poderoso, mas leva tempo para dominar tudo.
*   YAML Swarm vs. K8s: Descrever como fazer um sanduÃ­che. Swarm: "PÃ£o, queijo, presunto, alface. Montar." K8s: "Obtenha um objeto 'PÃ£o' tipo 'Integral', fatia 1. Obtenha objeto 'Queijo' tipo 'Prato', fatia 1. Aplique 'Queijo' sobre 'PÃ£o'. Obtenha 'Presunto'... etc." (Mais detalhado, mais controle, mas mais verboso).

---

ğŸ“„ PÃ¡gina 255

ğŸ” Resumo dos principais pontos:

*   **12.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Ganhar mais experiÃªncia com serviÃ§os Docker Swarm.
*   **Tarefa:** Rodar o app Random Number (Cap. 8) no cluster Swarm (pode ser o de 1 nÃ³ no Docker Desktop).
    *   Precisa de 2 serviÃ§os (`numbers-api`, `numbers-web`) e 1 rede overlay para conectÃ¡-los.
    *   Usar imagens prontas do Docker Hub:
        *   `diamol/ch08-numbers-api:v3`
        *   `diamol/ch08-numbers-web:v3`
*   SoluÃ§Ã£o no GitHub (poucos comandos).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O **Lab do CapÃ­tulo 12** pede para vocÃª aplicar os comandos `docker network create --driver overlay` e `docker service create` que aprendeu neste capÃ­tulo para implantar a aplicaÃ§Ã£o Random Number (API + Web) como serviÃ§os no seu ambiente Swarm.
*   Os passos sÃ£o:
    1.  Criar uma rede overlay (ex: `numbers-net`).
    2.  Criar o serviÃ§o `numbers-api` usando a imagem `:v3` fornecida, conectando-o Ã  rede `numbers-net`. VocÃª pode definir o nÃºmero de rÃ©plicas (ex: `--replicas 1` ou mais).
    3.  Criar o serviÃ§o `numbers-web` usando a imagem `:v3` fornecida, conectando-o Ã  rede `numbers-net` e **publicando uma porta** (ex: `--publish 8088:80`) para poder acessÃ¡-lo externamente via rede Ingress.
*   O objetivo Ã© praticar a criaÃ§Ã£o manual de serviÃ§os e redes no Swarm.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Modo Swarm:** Verifique com `docker info | Select-String "Swarm"`. Deve dizer `active`. Se nÃ£o, `docker swarm init`.
2.  **Limpar ServiÃ§os Anteriores (Opcional):**
    ```powershell
    docker service rm image-gallery iotd accesslog # Remove serviÃ§os do Image Gallery
    docker network rm iotd-net # Remove rede antiga
    ```
3.  **Criar Rede Overlay:**
    ```powershell
    docker network create --driver overlay numbers-net
    ```
4.  **Criar ServiÃ§o API:**
    ```powershell
    docker service create --detach --name numbers-api --network numbers-net --replicas 1 diamol/ch08-numbers-api:v3
    ```
5.  **Criar ServiÃ§o Web:**
    ```powershell
    docker service create --detach --name numbers-web --network numbers-net --publish 8088:80 --replicas 1 diamol/ch08-numbers-web:v3
    ```
6.  **Verificar:**
    ```powershell
    docker service ls
    # Deve mostrar ambos os serviÃ§os com 1/1 rÃ©plicas.
    ```
7.  **Testar (Se estiver em modo Linux no Docker Desktop):**
    *   Acesse `http://localhost:8088`. Clique no botÃ£o. Deve funcionar (pelas 3 primeiras vezes, lembre-se do healthcheck do v3 que deve reiniciar a API depois).

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch12/lab/README.md

---

ğŸ“„ PÃ¡gina 256

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 13: Deploying distributed applications as stacks in Docker Swarm** (Implantando aplicaÃ§Ãµes distribuÃ­das como stacks no Docker Swarm)
*   ConfissÃ£o: Criar serviÃ§os manualmente com `docker service create` (como no Cap. 12) nÃ£o Ã© feito em projetos reais.
*   Abordagem Real: Descrever a aplicaÃ§Ã£o em arquivo **YAML** (formato Docker Compose) e enviar ao manager Swarm.
*   Swarm lÃª o YAML e cria/atualiza os recursos (serviÃ§os, redes, etc.) para atingir o estado desejado. Mesma abordagem declarativa do Compose.
*   **Stack:** Recurso Swarm que agrupa os recursos de uma aplicaÃ§Ã£o definida em um arquivo Compose.
*   **Vantagem:** Usa o mesmo formato de arquivo (`docker-compose.yml`) familiar de dev/teste para deploys em produÃ§Ã£o no Swarm.
*   Mas hÃ¡ diferenÃ§as: algumas configs Compose sÃ³ fazem sentido em single-host (ex: `build`), outras sÃ³ em Swarm (`deploy`). EspecificaÃ§Ã£o Ã© flexÃ­vel.
*   Foco Cap. 13: Usar Compose para definir e implantar **stacks** no Swarm.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 13 corrigindo uma impressÃ£o do capÃ­tulo anterior: embora `docker service create` funcione, a maneira idiomÃ¡tica e prÃ¡tica de implantar aplicaÃ§Ãµes no Docker Swarm Ã© usando arquivos **Docker Compose** e o conceito de **Stack**.
*   **Stack:** Pense em uma "stack" (pilha) como a representaÃ§Ã£o, dentro do Swarm, de uma aplicaÃ§Ã£o completa definida em um arquivo Docker Compose. Quando vocÃª "implanta uma stack", vocÃª entrega seu arquivo `docker-compose.yml` ao Swarm manager.
*   **Processo:**
    1.  VocÃª escreve um `docker-compose.yml` descrevendo seus serviÃ§os, redes, volumes, configs, secrets.
    2.  VocÃª usa o comando `docker stack deploy -c <arquivo.yml> <nome_da_stack>`.
    3.  O Swarm manager lÃª o arquivo, compara com o estado atual dos recursos pertencentes Ã quela stack (se ela jÃ¡ existir) e cria, atualiza ou remove serviÃ§os, redes, etc., conforme necessÃ¡rio para corresponder Ã  definiÃ§Ã£o no arquivo.
*   **BenefÃ­cio:** Reutiliza o conhecimento e os arquivos do Docker Compose, tornando a transiÃ§Ã£o de desenvolvimento (Compose) para produÃ§Ã£o (Swarm stack) mais suave.
*   **AdaptaÃ§Ã£o:** O formato do arquivo Compose usado para stacks Swarm Ã© o mesmo do Compose standalone, mas o Swarm *ignora* certas seÃ§Ãµes que nÃ£o fazem sentido em um cluster (como `build`) e *interpreta* seÃ§Ãµes especÃ­ficas do Swarm (principalmente a seÃ§Ã£o `deploy` dentro de um serviÃ§o).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ arquivos `docker-compose.yml` (ou apenas `.yml`) como entrada.
*   O comando principal serÃ¡ `docker stack deploy`.
*   Outros comandos relacionados: `docker stack ls`, `docker stack ps`, `docker stack services`, `docker stack rm`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `docker service create` vs. `docker stack deploy`:
    *   `service create`: Como dar instruÃ§Ãµes verbais separadas ao gerente da loja para contratar um caixa, depois um empacotador, depois um estoquista.
    *   `stack deploy`: Como entregar ao gerente um documento completo (o YAML) descrevendo *toda* a equipe necessÃ¡ria (caixas, empacotadores, estoquistas, suas funÃ§Ãµes, quantos de cada) e deixar que ele cuide de contratar/organizar todos de uma vez.

---

ğŸ“„ PÃ¡gina 257

ğŸ” Resumo dos principais pontos:

*   **13.1 Using Docker Compose for production deployments** (Usando Docker Compose para implantaÃ§Ãµes de produÃ§Ã£o)
*   Poder real do Swarm vem do Compose: mesmo formato de dev/teste a prod.
*   Exemplo Simples (`v1.yml`): Deploy bÃ¡sico do To-Do app.
*   **Listagem 13.1:** Arquivo Compose (`v1.yml`) para deploy Swarm.
    ```yaml
    version: "3.7"
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
    ```
*   Este arquivo pode ser usado com `docker-compose up` (cria 1 container) OU com `docker stack deploy` (cria 1 serviÃ§o com 1 rÃ©plica).
*   **Stack:** Agrupa recursos (serviÃ§os, redes, etc.) de um deploy Swarm.
*   **ExercÃ­cio Try-it-now:** Implantar o Compose simples como stack.
    *   Garantir modo Swarm ativo. Navegar para `ch13/exercises`.
    *   Comando deploy: `docker stack deploy -c ./todo-list/v1.yml todo`. (`todo` Ã© o nome da stack).
    *   Verificar stacks: `docker stack ls`.
    *   Verificar serviÃ§os criados: `docker service ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A seÃ§Ã£o 13.1 demonstra como atÃ© mesmo um arquivo `docker-compose.yml` muito simples, sem nenhuma configuraÃ§Ã£o especÃ­fica do Swarm, pode ser implantado como uma **stack** no Docker Swarm.
*   A Listagem 13.1 mostra um arquivo mÃ­nimo para o To-Do app, apenas especificando a imagem e a porta.
*   O comando chave Ã© `docker stack deploy`. Ele recebe:
    *   `-c <arquivo.yml>`: O caminho para o arquivo Compose que define a aplicaÃ§Ã£o.
    *   `<nome_da_stack>`: Um nome que vocÃª escolhe para agrupar todos os recursos criados a partir deste arquivo (serviÃ§os, redes, etc.).
*   O exercÃ­cio instrui a implantar a Listagem 13.1 como uma stack chamada `todo`. O Swarm lerÃ¡ o arquivo e criarÃ¡ um serviÃ§o chamado `todo_todo-web` (prefixado pelo nome da stack) com 1 rÃ©plica, conectado a uma rede padrÃ£o criada para a stack (ou uma rede definida no YAML), e publicarÃ¡ a porta 8080 via rede Ingress.
*   Os comandos `docker stack ls` e `docker service ls` sÃ£o usados para verificar que a stack e o serviÃ§o foram criados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Modo Swarm:** `docker info | Select-String "Swarm"` (deve ser `active`).
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch13\exercises
    ```
3.  **Implantar Stack:**
    ```powershell
    docker stack deploy -c ./todo-list/v1.yml todo
    ```
    Observe a saÃ­da (criando rede, criando serviÃ§o).
4.  **Verificar Stacks:**
    ```powershell
    docker stack ls
    # Deve listar a stack 'todo'
    ```
5.  **Verificar ServiÃ§os:**
    ```powershell
    docker service ls
    # Deve listar o serviÃ§o 'todo_todo-web' com 1/1 rÃ©plicas.
    ```

---

ğŸ“„ PÃ¡gina 258

ğŸ” Resumo dos principais pontos:

*   Figura 13.1: Mostra saÃ­da do `stack deploy`, `stack ls`, `service ls`. Confirma criaÃ§Ã£o da rede padrÃ£o e do serviÃ§o `todo_todo-web`.
*   Acesso ao app (Linux `localhost:8080`, Windows requer IP externo) funciona como antes.
*   BenefÃ­cio: Mesmo arquivo Compose simples jÃ¡ dÃ¡ alta disponibilidade (HA) no Swarm (se multi-nÃ³, Swarm substitui rÃ©plica se nÃ³ falhar).
*   **SeÃ§Ã£o `deploy:` no Compose:** Adiciona configuraÃ§Ãµes especÃ­ficas do Swarm.
    *   Ignorada por `docker-compose up`, usada por `docker stack deploy`.
*   **Listagem 13.2:** Trecho de `v2.yml` com seÃ§Ã£o `deploy:`.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
        deploy: # Configs especÃ­ficas do Swarm
          replicas: 2 # Rodar 2 instÃ¢ncias
          resources: # Limites de recursos
            limits:
              cpus: "0.50" # Max 50% de 1 core CPU por rÃ©plica
              memory: 100M # Max 100 MB de RAM por rÃ©plica
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que o deploy da stack funcionou e que o acesso ao app Ã© o mesmo de antes (com a ressalva do `localhost` no Windows). Mesmo com esse deploy simples, se o Swarm tivesse mÃºltiplos nÃ³s, a aplicaÃ§Ã£o jÃ¡ teria um nÃ­vel bÃ¡sico de alta disponibilidade.
*   Introduz a seÃ§Ã£o `deploy:` dentro da definiÃ§Ã£o de um serviÃ§o no arquivo Compose. Esta seÃ§Ã£o Ã© a maneira padrÃ£o de especificar configuraÃ§Ãµes que sÃ£o relevantes *apenas* para o modo Swarm (e Kubernetes, com algumas adaptaÃ§Ãµes de sintaxe em outra ferramenta chamada Kompose).
*   **Importante:** O `docker-compose` (standalone) **ignora** completamente a seÃ§Ã£o `deploy:`. O `docker stack deploy` (Swarm) **lÃª e aplica** as configuraÃ§Ãµes dentro de `deploy:`.
*   A Listagem 13.2 mostra exemplos de configuraÃ§Ãµes comuns em `deploy:`:
    *   `replicas: 2`: Instrui o Swarm a manter 2 rÃ©plicas (contÃªineres) rodando para este serviÃ§o, em vez do padrÃ£o 1.
    *   `resources: limits:`: Define limites mÃ¡ximos de consumo de CPU e memÃ³ria para cada rÃ©plica. Isso Ã© crucial em produÃ§Ã£o para evitar que um serviÃ§o "fuja" e consuma todos os recursos do nÃ³, afetando outros serviÃ§os. `cpus: "0.50"` limita a meio core de CPU. `memory: 100M` limita a 100 Megabytes de RAM.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar `v2.yml`:** Na pasta `ch13\exercises\todo-list`, crie um arquivo `v2.yml` com o conteÃºdo completo (incluindo `version:` e a seÃ§Ã£o `services:` da Listagem 13.2).
2.  **(Opcional) Testar com Compose:** Rode `docker-compose -f v2.yml config`. Note que a seÃ§Ã£o `deploy:` *nÃ£o aparece* na saÃ­da final, pois o Compose a ignora.
3.  O prÃ³ximo exercÃ­cio usarÃ¡ `docker stack deploy` com este arquivo `v2.yml`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   SeÃ§Ã£o `deploy:`: InstruÃ§Ãµes extras na planta da casa que sÃ³ o sÃ­ndico do condomÃ­nio (Swarm) entende, nÃ£o o construtor de uma Ãºnica casa (Compose). Ex: "Construa duas casas deste modelo" (`replicas: 2`), "Cada casa nÃ£o pode usar mais que X de eletricidade" (`resources: limits:`).

---

ğŸ“„ PÃ¡gina 259

ğŸ” Resumo dos principais pontos:

*   Config `deploy:` sÃ£o bÃ¡sicas para produÃ§Ã£o (mÃºltiplas rÃ©plicas para HA/carga, limites para proteger cluster).
*   Calcular limites exige conhecer pico de uso do app (mÃ©tricas do Cap. 9 ajudam).
*   `limits:` de CPU/memÃ³ria sÃ£o importantes.
*   **Atualizando Stack:** Mesmo comando `docker stack deploy -c <novo_arquivo.yml> <nome_stack_existente>`.
    *   Swarm compara novo YAML com estado atual e aplica diferenÃ§as.
*   **ExercÃ­cio Try-it-now:** Atualizar stack `todo` usando `v2.yml`. Verificar tarefas.
    *   Comando deploy: `docker stack deploy -c ./todo-list/v2.yml todo`.
    *   Verificar tarefas/rÃ©plicas do serviÃ§o: `docker service ps todo_todo-web`.
*   Figura 13.2: SaÃ­da da atualizaÃ§Ã£o e `service ps`.
*   ObservaÃ§Ã£o: `service ps` mostra rÃ©plica original (`.1`) sendo substituÃ­da (`Shutdown`) por duas novas (`.1` e `.2` - *Nota: Swarm reusa o Ã­ndice 1*).
*   **Por que a rÃ©plica original foi substituÃ­da, nÃ£o apenas adicionada uma nova?** Porque adicionar `resources: limits:` muda a definiÃ§Ã£o do contÃªiner, exigindo recriaÃ§Ã£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a a importÃ¢ncia de `replicas` e `resources: limits:` para produÃ§Ã£o. Determinar os limites corretos Ã© um processo que envolve observar o consumo real da aplicaÃ§Ã£o sob carga (usando ferramentas como Prometheus/Grafana).
*   Explica que para **atualizar** uma stack existente, usa-se o **mesmo comando `docker stack deploy`**, apenas fornecendo o novo arquivo YAML (ou o mesmo arquivo modificado) e o mesmo nome da stack. O Swarm Ã© inteligente o suficiente para calcular as diferenÃ§as ("diff") entre a nova definiÃ§Ã£o e a que estÃ¡ rodando e aplicar apenas as mudanÃ§as necessÃ¡rias (ex: escalar rÃ©plicas, atualizar imagem, mudar limites).
*   O exercÃ­cio aplica o arquivo `v2.yml` (que define `replicas: 2` e `limits`) Ã  stack `todo` existente.
*   A anÃ¡lise da saÃ­da do `docker service ps todo_todo-web` (Figura 13.2) revela um detalhe importante: nÃ£o apenas uma nova rÃ©plica foi adicionada para chegar a 2, mas a rÃ©plica original tambÃ©m foi substituÃ­da. A razÃ£o Ã© que a adiÃ§Ã£o da seÃ§Ã£o `resources: limits:` na definiÃ§Ã£o do serviÃ§o alterou a especificaÃ§Ã£o fundamental de como as rÃ©plicas devem ser criadas, forÃ§ando o Swarm a recriar todas elas com os novos limites aplicados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Atualizar Stack:** (Estando em `ch13/exercises`)
    ```powershell
    docker stack deploy -c ./todo-list/v2.yml todo
    ```
    Observe a saÃ­da (ex: `Updating service todo_todo-web`).
2.  **Verificar Tarefas:** Espere alguns segundos para o update completar.
    ```powershell
    docker service ps todo_todo-web
    ```
    VocÃª deve ver a tarefa original (`.1`) com status `Shutdown` e duas novas tarefas (provavelmente `.1` e `.2` nos nomes) com status `Running`, ambas usando a mesma imagem mas agora com limites aplicados.
3.  **Verificar ServiÃ§o:**
    ```powershell
    docker service ls
    # Deve mostrar todo_todo-web com REPLICAS 2/2
    ```

---

ğŸ“„ PÃ¡gina 260

ğŸ” Resumo dos principais pontos:

*   Figura 13.2: Confirma substituiÃ§Ã£o de rÃ©plicas na atualizaÃ§Ã£o com limites.
*   Containers rodam sem limites por padrÃ£o; OK para dev/teste, mas ruim para prod (risco de consumo total de recursos).
*   `limits` sÃ£o aplicados na criaÃ§Ã£o do container; mudÃ¡-los causa recriaÃ§Ã£o/update.
*   **Gerenciamento de Stacks:**
    *   Agrupam apps; cluster pode ter muitos apps/stacks.
    *   Gerenciar app como um todo via `docker stack ...` (ls, services, ps, rm).
*   **ExercÃ­cio Try-it-now:** Explorar comandos `stack`.
    *   Listar serviÃ§os da stack `todo`: `docker stack services todo`.
    *   Listar rÃ©plicas/tarefas de todos os serviÃ§os da stack: `docker stack ps todo`.
    *   Remover a stack inteira: `docker stack rm todo`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica por que os limites de recursos causam a recriaÃ§Ã£o das rÃ©plicas: os limites sÃ£o parte fundamental da definiÃ§Ã£o do contÃªiner e sÃ³ podem ser aplicados quando ele Ã© criado.
*   Introduz os comandos `docker stack ...` como a forma principal de gerenciar uma aplicaÃ§Ã£o implantada como um todo no Swarm. Eles oferecem uma visÃ£o agregada dos recursos pertencentes a uma stack especÃ­fica:
    *   `docker stack ls`: Lista todas as stacks rodando no Swarm.
    *   `docker stack services <stack_name>`: Lista apenas os serviÃ§os que pertencem a essa stack.
    *   `docker stack ps <stack_name>`: Lista todas as tarefas/rÃ©plicas de todos os serviÃ§os pertencentes a essa stack.
    *   `docker stack rm <stack_name>`: Remove a stack e **todos os recursos associados a ela** que foram criados pelo deploy (serviÃ§os, e redes/configs/secrets *nÃ£o* marcados como `external`). **Use com cuidado!**
*   O exercÃ­cio demonstra esses comandos para visualizar e depois remover completamente a stack `todo`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Explorar Stack `todo`:**
    ```powershell
    docker stack services todo
    # Deve listar apenas 'todo_todo-web'

    docker stack ps todo
    # Deve listar as 2 rÃ©plicas 'Running' e a 1 'Shutdown' de 'todo_todo-web'
    ```
2.  **Remover Stack:**
    ```powershell
    docker stack rm todo
    ```
    Observe a saÃ­da (removendo serviÃ§o, removendo rede).
3.  **Verificar RemoÃ§Ã£o:**
    ```powershell
    docker stack ls # NÃ£o deve listar mais 'todo'
    docker service ls # NÃ£o deve listar mais 'todo_todo-web'
    docker network ls # Rede padrÃ£o da stack 'todo' (ex: 'todo_default') deve ter sido removida
    ```

---

ğŸ“„ PÃ¡gina 261

ğŸ” Resumo dos principais pontos:

*   Figura 13.3: Mostra saÃ­da dos comandos `stack services`, `stack ps`, `stack rm`.
*   Gerenciamento via `stack` funciona mesmo para apps grandes (dezenas de serviÃ§os, centenas de rÃ©plicas).
*   DefiniÃ§Ãµes ficam no cluster DB; nÃ£o precisa do YAML para `stack rm` ou outros comandos de gerenciamento. DB Ã© replicado entre managers (seguro).
*   PrÃ³ximo: ConfiguraÃ§Ã£o de app no Swarm usando arquivos armazenados no cluster.
*   **13.2 Managing app configuration with config objects** (Gerenciando configuraÃ§Ã£o de app com objetos de configuraÃ§Ã£o)
*   Apps precisam carregar config do ambiente (env vars, arquivos).
*   Docker provÃª ambiente (seta env vars, constrÃ³i filesystem).
*   ProduÃ§Ã£o (Swarm): Usa **Docker Config Objects** armazenados no cluster.
*   Diagrama (Figura 13.4): Mostra fluxo de config em diferentes ambientes.
    *   Dev (Compose): Env Vars / Bind Mounts.
    *   Prod (Swarm): Config Objects / Secrets.
    *   Mesma imagem Docker em todos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a introduÃ§Ã£o Ã s stacks, reforÃ§ando que elas simplificam o gerenciamento de aplicaÃ§Ãµes complexas no Swarm e que as definiÃ§Ãµes persistem no cluster, diferentemente do Compose.
*   A seÃ§Ã£o 13.2 introduz a maneira especÃ­fica do Swarm (e K8s tem algo similar) para lidar com **arquivos de configuraÃ§Ã£o**: os **Objetos de ConfiguraÃ§Ã£o (Config Objects)** ou simplesmente **Configs**.
*   A ideia Ã© que, em vez de montar arquivos de configuraÃ§Ã£o do host via bind mount (o que exigiria que o arquivo existisse em todos os nÃ³s do cluster onde uma rÃ©plica pudesse rodar) ou embuti-los na imagem (o que Ã© ruim para flexibilidade e seguranÃ§a), vocÃª **carrega o conteÃºdo do arquivo de configuraÃ§Ã£o para dentro do cluster Swarm como um objeto gerenciado**.
*   Esse "Config Object" Ã© armazenado de forma segura no banco de dados distribuÃ­do do Swarm. Ao definir um serviÃ§o, vocÃª pode especificar que ele precisa de um determinado Config Object. O Swarm entÃ£o garante que o conteÃºdo desse objeto seja disponibilizado como um arquivo *dentro* do filesystem de cada rÃ©plica daquele serviÃ§o, onde quer que ela rode no cluster.
*   A Figura 13.4 ilustra o conceito geral: a *mesma imagem Docker* Ã© usada em Dev e Prod, mas a *fonte* da configuraÃ§Ã£o muda. Em Dev (Compose), podem ser variÃ¡veis de ambiente ou bind mounts locais. Em Prod (Swarm), sÃ£o Config Objects e Secrets gerenciados pelo cluster.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ o comando `docker config create` para criar um Config Object no Swarm a partir de um arquivo local.
*   VocÃª usarÃ¡ a seÃ§Ã£o `configs:` no arquivo Compose (similar a `secrets:`) para declarar que um serviÃ§o precisa usar um Config Object e onde montÃ¡-lo como arquivo dentro do contÃªiner.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Config Object: Ã‰ como fazer upload de um documento importante (arquivo de configuraÃ§Ã£o) para um cofre central seguro no condomÃ­nio (cluster Swarm). Quando um morador autorizado (rÃ©plica de serviÃ§o) precisa do documento, o sÃ­ndico (Swarm) entrega uma cÃ³pia diretamente dentro do apartamento dele (monta como arquivo no contÃªiner).

---

ğŸ“„ PÃ¡gina 262

ğŸ” Resumo dos principais pontos:

*   Configs sÃ£o recurso de primeira classe em orquestradores.
*   Desacopla deploy da app do gerenciamento de config.
    *   Equipe de config pode gerenciar configs em sistema central e carregÃ¡-los no cluster Swarm, separadamente do deploy da app.
*   **Configs:** Criados a partir de arquivo existente.
*   **ExercÃ­cio Try-it-now:** Criar um Config Object a partir de arquivo JSON local.
    *   App To-Do usa JSON para config. Config padrÃ£o (na imagem) usa SQLite. Nova config usarÃ¡ Postgres.
    *   Comando: `docker config create todo-list-config ./todo-list/configs/config.json`.
    *   Verificar configs: `docker config ls`.
*   Figura 13.5: Mostra saÃ­da da criaÃ§Ã£o e listagem do config. Objeto criado com nome e ID longo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a que os **Configs** (e Secrets) sÃ£o entidades gerenciadas pelo prÃ³prio orquestrador, o que permite separar as responsabilidades: a equipe de desenvolvimento entrega a imagem Docker, e a equipe de operaÃ§Ãµes (ou automaÃ§Ã£o) carrega a configuraÃ§Ã£o apropriada para o ambiente (Teste, UAT, ProduÃ§Ã£o) no cluster como Config Objects. A definiÃ§Ã£o do serviÃ§o no deploy apenas *referencia* o Config Object pelo nome.
*   O comando `docker config create <nome_do_config> <caminho_arquivo_local>` Ã© usado para carregar o conteÃºdo de um arquivo do seu sistema local para dentro do Swarm como um Config Object com o nome especificado.
*   O exercÃ­cio cria um config chamado `todo-list-config` a partir de um arquivo `config.json` local. Este arquivo contÃ©m as configuraÃ§Ãµes para o app To-Do usar o banco de dados PostgreSQL.
*   O comando `docker config ls` lista os Config Objects que existem no cluster Swarm.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:** (Estando em `ch13/exercises`)
    ```powershell
    # O arquivo deve estar em ./todo-list/configs/config.json
    # Verifique o conteÃºdo (opcional):
    # Get-Content ./todo-list/configs/config.json | ConvertFrom-Json
    ```
2.  **Criar Config Object:**
    ```powershell
    docker config create todo-list-config ./todo-list/configs/config.json
    ```
3.  **Listar Configs:**
    ```powershell
    docker config ls
    # Deve listar 'todo-list-config'
    ```

---

ğŸ“„ PÃ¡gina 263

ğŸ” Resumo dos principais pontos:

*   Gerenciamento de Configs: `docker config rm`, `docker config inspect`.
*   `inspect` mostra conteÃºdo do config.
*   **Importante:** Configs **NÃƒO sÃ£o para dados sensÃ­veis**.
    *   ConteÃºdo *nÃ£o* Ã© criptografado no Swarm DB nem em trÃ¢nsito para os nÃ³s.
*   **ExercÃ­cio Try-it-now:** Inspecionar o config object criado.
    *   Comando: `docker config inspect --pretty todo-list-config`. (`--pretty` formata a saÃ­da).
*   Figura 13.6: Mostra saÃ­da do `inspect`, incluindo metadados e o conteÃºdo JSON original do arquivo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz os comandos para gerenciar Config Objects existentes: `docker config rm` (remover) e `docker config inspect` (ver detalhes).
*   Uma advertÃªncia crucial Ã© dada: **Config Objects nÃ£o sÃ£o seguros para dados confidenciais!** O conteÃºdo deles Ã© armazenado em texto plano (ou no formato original) no banco de dados do Swarm e transmitido sem criptografia adicional para os nÃ³s que precisam dele. Qualquer pessoa com acesso ao Swarm manager (ou potencialmente Ã  rede do cluster) pode conseguir ler o conteÃºdo de um Config Object.
*   O exercÃ­cio usa `docker config inspect --pretty` para visualizar o conteÃºdo do `todo-list-config` que foi criado. A flag `--pretty` apenas formata a saÃ­da JSON para ser mais legÃ­vel. O resultado (Figura 13.6) mostra claramente o conteÃºdo do arquivo `config.json` original, confirmando a falta de criptografia.
*   Para dados sensÃ­veis (senhas, chaves API, certificados), deve-se usar o recurso irmÃ£o: **Secrets** (prÃ³xima seÃ§Ã£o).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Inspecionar Config:**
    ```powershell
    docker config inspect --pretty todo-list-config
    ```
    Examine a saÃ­da. Na chave `Spec.Data`, vocÃª verÃ¡ o conteÃºdo do `config.json` (provavelmente codificado em Base64, mas `inspect --pretty` pode decodificÃ¡-lo para exibiÃ§Ã£o).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Config Object vs. Secret:
    *   Config Object: Um memorando importante deixado no quadro de avisos do condomÃ­nio. Todos os moradores autorizados (rÃ©plicas) podem ler.
    *   Secret: Uma carta registrada entregue em um envelope lacrado e seguro apenas ao morador especÃ­fico que precisa dela. SÃ³ ele consegue ler o conteÃºdo.

---

ğŸ“„ PÃ¡gina 264

ğŸ” Resumo dos principais pontos:

*   Figura 13.6: Confirma que conteÃºdo do config Ã© visÃ­vel.
*   Fluxo de trabalho pode ser separado (ops cria configs, dev/pipeline deploya app que usa configs).
*   **Consumindo Configs em ServiÃ§os:** Especificados no Compose file.
*   **Listagem 13.3:** Trecho de `v3.yml` mostrando uso do config no serviÃ§o `todo-web`.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
        configs: # SeÃ§Ã£o para usar configs
          - source: todo-list-config # Nome do Config Object no Swarm
            target: /app/config/config.json # Caminho onde arquivo serÃ¡ montado no container
        #... (outras configs do serviÃ§o)
    #... (outras seÃ§Ãµes do YAML)
    configs: # SeÃ§Ã£o para declarar configs usados na stack
      todo-list-config:
        external: true # Indica que 'todo-list-config' jÃ¡ existe no Swarm
    ```
*   **ExplicaÃ§Ã£o:**
    *   SeÃ§Ã£o `configs:` dentro do serviÃ§o `todo-web`: Mapeia o `source` (nome do Config Object no Swarm) para o `target` (caminho do arquivo dentro do contÃªiner). App lerÃ¡ `/app/config/config.json`.
    *   Sintaxe curta (omitir `target`) existe, mas nÃ£o recomendada (path default varia com OS). Melhor ser explÃ­cito.
    *   SeÃ§Ã£o `configs:` no nÃ­vel raiz do YAML: Declara os Config Objects que a stack usarÃ¡. `external: true` significa que o `docker stack deploy` *nÃ£o* tentarÃ¡ criar o `todo-list-config`; ele espera que jÃ¡ exista no Swarm (criado via `docker config create`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra como um serviÃ§o (definido em um arquivo Compose para `docker stack deploy`) consome um Config Object que jÃ¡ existe no Swarm.
*   **Listagem 13.3** ilustra as duas partes necessÃ¡rias:
    1.  **Dentro da definiÃ§Ã£o do serviÃ§o (`services: todo-web: configs:`):**
        *   Especifica-se qual Config Object usar (`source: todo-list-config`).
        *   Especifica-se onde o conteÃºdo desse objeto deve aparecer como um arquivo dentro do contÃªiner (`target: /app/config/config.json`). A aplicaÃ§Ã£o `todo-web` estÃ¡ programada para procurar um arquivo de configuraÃ§Ã£o nesse local.
    2.  **No nÃ­vel raiz do arquivo Compose (`configs:`):**
        *   Declara-se o nome do Config Object (`todo-list-config:`).
        *   Usa-se `external: true` para dizer ao Swarm: "Este config object jÃ¡ foi criado externamente (via `docker config create`), nÃ£o tente criÃ¡-lo como parte desta stack". Se `external: true` fosse omitido, o Swarm esperaria encontrar a definiÃ§Ã£o do *conteÃºdo* do config dentro do prÃ³prio arquivo Compose (usando `file:` ou `content:`), o que Ã© menos comum para configs gerenciados separadamente.
*   Essa separaÃ§Ã£o (criar config via `docker config create`, referenciar como `external: true` no deploy da stack) reforÃ§a o desacoplamento entre gerenciamento de configuraÃ§Ã£o e implantaÃ§Ã£o da aplicaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar `v3.yml`:** Na pasta `ch13/exercises/todo-list`, crie `v3.yml` contendo:
    *   `version: '3.7'`
    *   A seÃ§Ã£o `services:` com `todo-web` (Listagem 13.3) e tambÃ©m um serviÃ§o `todo-db` (baseado em `diamol/postgres`, conectado Ã  `app-net`, sem `ports` publicados, talvez com `restart: unless-stopped`).
    *   A seÃ§Ã£o `networks:` definindo `app-net` (pode ser criada pela stack ou `external: true` se vocÃª criar antes).
    *   A seÃ§Ã£o `configs:` no nÃ­vel raiz (Listagem 13.3).
2.  O prÃ³ximo exercÃ­cio usarÃ¡ este arquivo para atualizar a stack.

---

ğŸ“„ PÃ¡gina 265

ğŸ” Resumo dos principais pontos:

*   Fluxo de deploy: criar configs primeiro, depois deployar stack que os usa.
*   Arquivo `v3.yml` tambÃ©m inclui serviÃ§o para BD SQL (Postgres) para compartilhar entre rÃ©plicas web.
*   **ExercÃ­cio Try-it-now:** Atualizar stack `todo` usando `v3.yml`.
    *   Comando: `docker stack deploy -c ./todo-list/v3.yml todo`.
    *   Verificar serviÃ§os: `docker stack services todo`.
*   Resultado: Swarm cria serviÃ§o `todo-db` e atualiza `todo_todo-web` para usar o config object. RÃ©plica `todo_todo-web` Ã© substituÃ­da.
*   Figura 13.7: Mostra saÃ­da do deploy `v3`, com criaÃ§Ã£o/atualizaÃ§Ã£o de serviÃ§os.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina descreve o fluxo recomendado: primeiro garantir que os Config Objects necessÃ¡rios existam no cluster (via `docker config create`) e depois implantar a stack (`docker stack deploy`) que os referencia como `external: true`.
*   Menciona que o arquivo completo `v3.yml` tambÃ©m define o serviÃ§o do banco de dados PostgreSQL (`todo-db`), necessÃ¡rio agora que a configuraÃ§Ã£o do `todo-web` foi alterada (pelo `todo-list-config`) para usar Postgres em vez de SQLite. Ambos os serviÃ§os (`todo-web` e `todo-db`) serÃ£o conectados Ã  mesma rede overlay para poderem se comunicar.
*   O exercÃ­cio instrui a aplicar esta nova configuraÃ§Ã£o Ã  stack `todo` existente (que estava vazia apÃ³s o `rm` da p.260). O Swarm criarÃ¡ a rede (se nÃ£o externa), o serviÃ§o `todo-db`, e o serviÃ§o `todo-web` (que agora usa o `todo-list-config`).
*   A Figura 13.7 mostra a saÃ­da esperada, indicando que os recursos estÃ£o sendo criados/atualizados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Config Existe:** (JÃ¡ criado na p.262)
    ```powershell
    docker config ls # Deve listar 'todo-list-config'
    ```
2.  **Verificar/Criar `v3.yml`:** Certifique-se que `ch13\exercises\todo-list\v3.yml` existe e contÃ©m as definiÃ§Ãµes completas para `version`, `services` (`todo-web` e `todo-db`), `networks` (definindo `app-net`), e `configs` (declarando `todo-list-config` como externo).
3.  **Implantar/Atualizar Stack:**
    ```powershell
    docker stack deploy -c ./todo-list/v3.yml todo
    ```
4.  **Verificar ServiÃ§os:**
    ```powershell
    docker stack services todo
    # Deve listar 'todo_todo-db' (1/1) e 'todo_todo-web' (1/1, ou o nÂº de rÃ©plicas definido em v3.yml)
    ```

---

ğŸ“„ PÃ¡gina 266

ğŸ” Resumo dos principais pontos:

*   App agora configurado para usar Postgres (via config object).
*   **Problema:** Acessar app (`http://localhost:8080`) nÃ£o funciona. Logs do `todo-web` mostram erros de conexÃ£o com BD.
*   **Causa:** Config object (`todo-list-config`) definiu *que* usar Postgres, mas *nÃ£o forneceu* os detalhes de conexÃ£o (string de conexÃ£o). App falha ao tentar conectar.
*   **Dados SensÃ­veis:** NÃ£o devem ir em Config Objects (nÃ£o criptografados).
    *   Ex: Strings de conexÃ£o (com user/pass), URLs de produÃ§Ã£o, chaves API.
    *   Usar defesa em profundidade; mesmo se acesso ao cluster for difÃ­cil, nÃ£o expor segredos em configs.
*   SoluÃ§Ã£o Swarm: **Secrets** para dados confidenciais.
*   **13.3 Managing confidential settings with secrets** (Gerenciando configuraÃ§Ãµes confidenciais com secrets)
    *   Secrets: Recurso Swarm, funciona quase igual a Configs.
    *   Criados de arquivo local -> armazenados no cluster DB.
    *   Referenciados no serviÃ§o -> montados como arquivo no container.
    *   **DiferenÃ§a Chave:** ConteÃºdo sÃ³ legÃ­vel em texto plano *dentro* do contÃªiner.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina revela que, apesar do deploy bem-sucedido da stack `v3`, a aplicaÃ§Ã£o ainda nÃ£o funciona. A investigaÃ§Ã£o dos logs do `todo-web` mostraria que ele estÃ¡ tentando se conectar ao Postgres (conforme instruÃ­do pelo `todo-list-config`), mas estÃ¡ falhando porque nÃ£o sabe *como* se conectar (falta a string de conexÃ£o).
*   Isso leva Ã  introduÃ§Ã£o dos **Secrets** do Docker Swarm. Eles sÃ£o projetados especificamente para armazenar e distribuir informaÃ§Ãµes **confidenciais** de forma segura, resolvendo a limitaÃ§Ã£o dos Config Objects (que sÃ£o inseguros para dados sensÃ­veis).
*   O mecanismo dos Secrets Ã© muito similar ao dos Configs:
    1.  VocÃª cria um Secret no Swarm a partir de um arquivo local (`docker secret create`).
    2.  O Swarm armazena o conteÃºdo do secret de forma segura.
    3.  VocÃª referencia o Secret na definiÃ§Ã£o do serviÃ§o no seu arquivo Compose (`services: ... secrets:`).
    4.  VocÃª declara o Secret como `external: true` na seÃ§Ã£o raiz `secrets:` do Compose.
    5.  O Swarm monta o conteÃºdo do Secret como um arquivo dentro das rÃ©plicas do serviÃ§o.
*   A **grande diferenÃ§a** Ã© a seguranÃ§a: Secrets sÃ£o criptografados em repouso (no DB do Swarm) e em trÃ¢nsito (entre managers e workers). O conteÃºdo sÃ³ Ã© descriptografado e disponibilizado em texto plano dentro do ambiente isolado do contÃªiner que o solicitou (geralmente em um filesystem temporÃ¡rio em memÃ³ria, `tmpfs`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker secret create` (similar a `docker config create`).
*   VocÃª usarÃ¡ a seÃ§Ã£o `secrets:` no Compose (exatamente como usou para configs, Listagens 10.3 e 10.4).
*   O prÃ³ximo exercÃ­cio criarÃ¡ o secret com a string de conexÃ£o do Postgres.

---

ğŸ“„ PÃ¡gina 267

ğŸ” Resumo dos principais pontos:

*   Secrets sÃ£o criptografados em repouso (DB Swarm) e em trÃ¢nsito (manager -> worker).
*   Descriptografados apenas dentro do container (ex: em `tmpfs`).
*   Usar secret para string de conexÃ£o do To-Do app.
*   **ExercÃ­cio Try-it-now:** Criar e inspecionar um secret.
    *   Comando criar: `docker secret create todo-list-secret ./todo-list/secrets/secrets.json`.
    *   Comando inspecionar: `docker secret inspect --pretty todo-list-secret`.
*   Figura 13.8: SaÃ­da do `secret inspect`. Mostra apenas metadados (ID, nome, timestamps), **NÃƒO** o conteÃºdo do secret. Confirma seguranÃ§a.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina detalha as garantias de seguranÃ§a dos **Secrets**: criptografia ponta a ponta desde o armazenamento no cluster atÃ© a entrega no nÃ³ worker, com descriptografia ocorrendo apenas dentro do ambiente de execuÃ§Ã£o do contÃªiner (geralmente montado em um local seguro como `/run/secrets/<secret_name>` por padrÃ£o, ou no `target` especificado).
*   O exercÃ­cio demonstra a criaÃ§Ã£o de um secret (`docker secret create`) e a tentativa de inspecionÃ¡-lo (`docker secret inspect`).
*   O resultado crucial (Figura 13.8) Ã© que `docker secret inspect` **NÃƒO revela o conteÃºdo** do secret, ao contrÃ¡rio do `docker config inspect`. Isso comprova que, uma vez criado no Swarm, o conteÃºdo do secret nÃ£o pode ser facilmente lido por administradores ou usuÃ¡rios com acesso ao cluster, aumentando a seguranÃ§a.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Arquivo Local:** Certifique-se que o arquivo `ch13\exercises\todo-list\secrets\secrets.json` existe e contÃ©m a string de conexÃ£o JSON correta (como na p.137).
2.  **Criar Secret:**
    ```powershell
    docker secret create todo-list-secret ./todo-list/secrets/secrets.json
    ```
3.  **Listar Secrets:**
    ```powershell
    docker secret ls
    # Deve listar 'todo-list-secret'
    ```
4.  **Inspecionar Secret:**
    ```powershell
    docker secret inspect --pretty todo-list-secret
    ```
    Observe a saÃ­da. VocÃª verÃ¡ informaÃ§Ãµes como ID, Version, CreatedAt, UpdatedAt, Spec (com Name e Labels), mas **NÃƒO** verÃ¡ a string de conexÃ£o JSON em lugar nenhum.

---

ğŸ“„ PÃ¡gina 268

ğŸ” Resumo dos principais pontos:

*   Secret armazenado; agora usar no deploy da app (`v4.yml`).
*   Sintaxe Compose para `secrets` Ã© muito similar a `configs`.
*   **Listagem 13.4:** Trechos de `v4.yml` adicionando uso do secret.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
        configs: # Ainda usa o config object para settings nÃ£o-sensÃ­veis
          - source: todo-list-config
            target: /app/config/config.json
        secrets: # Adiciona o uso do secret
          - source: todo-list-secret # Nome do Secret no Swarm
            target: /app/config/secrets.json # Onde montar como arquivo
        #...
    secrets: # Declara o secret como externo
      todo-list-secret:
        external: true
    ```
*   ConteÃºdo do secret (JSON da string de conexÃ£o) serÃ¡ carregado em `/app/config/secrets.json`, que o app tambÃ©m lÃª como fonte de config.
*   App agora terÃ¡ config completa (tipo de BD do config object, conexÃ£o do secret).
*   **ExercÃ­cio Try-it-now:** Implantar `v4.yml` para atualizar a stack `todo`.
    *   Comando: `docker stack deploy -c ./todo-list/v4.yml todo`.
    *   Verificar tarefas: `docker stack ps todo`.
*   Deploy atualiza sÃ³ serviÃ§o `todo-web` (adicionou `secrets:`), mas CLI pode listar ambos como "updating".
*   Figura 13.9: SaÃ­da do deploy `v4`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Agora que o secret `todo-list-secret` (contendo a string de conexÃ£o) existe no Swarm, podemos atualizar a definiÃ§Ã£o da aplicaÃ§Ã£o para usÃ¡-lo.
*   A Listagem 13.4 mostra as modificaÃ§Ãµes no arquivo Compose (`v4.yml`):
    *   Dentro do serviÃ§o `todo-web`, uma seÃ§Ã£o `secrets:` Ã© adicionada (similar Ã  seÃ§Ã£o `configs:` existente) para mapear o `source: todo-list-secret` para o `target: /app/config/secrets.json`. A aplicaÃ§Ã£o To-Do estÃ¡ configurada para ler ambos os arquivos (`config.json` e `secrets.json`) e mesclar as configuraÃ§Ãµes.
    *   Na seÃ§Ã£o raiz `secrets:`, o `todo-list-secret` Ã© declarado como `external: true`, indicando que ele jÃ¡ existe no Swarm.
*   Com essa atualizaÃ§Ã£o, a rÃ©plica do `todo-web` terÃ¡:
    *   O arquivo `config.json` (do Config Object) dizendo para usar `Database:Provider=Postgres`.
    *   O arquivo `secrets.json` (do Secret) fornecendo a string de conexÃ£o para o `Host=todo-db`.
*   O exercÃ­cio instrui a aplicar este `v4.yml` Ã  stack `todo`. Isso causarÃ¡ um rolling update apenas no serviÃ§o `todo-web` (pois sÃ³ ele mudou), substituindo a rÃ©plica existente por uma nova que terÃ¡ o secret montado. A saÃ­da do `docker stack deploy` pode ser um pouco enganosa e listar todos os serviÃ§os como "updating", mesmo que apenas alguns sejam realmente alterados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar/Criar `v4.yml`:** Certifique-se que `ch13\exercises\todo-list\v4.yml` existe e contÃ©m as seÃ§Ãµes `services: todo-web: secrets:` e a seÃ§Ã£o raiz `secrets:` conforme a Listagem 13.4 (alÃ©m do resto da definiÃ§Ã£o da stack v3).
2.  **Implantar AtualizaÃ§Ã£o v4:**
    ```powershell
    docker stack deploy -c ./todo-list/v4.yml todo
    ```
    Observe a saÃ­da.
3.  **Verificar Tarefas:** Espere alguns segundos.
    ```powershell
    docker stack ps todo
    ```
    VocÃª deve ver que a rÃ©plica de `todo-web` foi substituÃ­da (a antiga estarÃ¡ `Shutdown`, a nova `Running`), enquanto a rÃ©plica de `todo-db` permaneceu a mesma.

---

ğŸ“„ PÃ¡gina 269

ğŸ” Resumo dos principais pontos:

*   Figura 13.9: SaÃ­da do deploy v4.
*   App agora funciona corretamente (`http://localhost:8080` ou IP externo no Windows).
*   Figura 13.10: Diagrama da arquitetura final rodando no Swarm.
    *   TrÃ¡fego externo -> NÃ³ Swarm -> Rede Ingress -> RÃ©plica `todo-web`.
    *   RÃ©plica `todo-web` -> Rede Overlay -> RÃ©plica `todo-db`.
    *   `todo-web` lÃª config do Config Object e do Secret montados pelo Swarm.
*   Arquitetura funciona igual em Swarms de qualquer tamanho (cluster abstrai hardware).
*   **Importante sobre Updates de Config/Secret:**
    *   Configs e Secrets no Swarm sÃ£o **imutÃ¡veis** (nÃ£o podem ser atualizados no local).
    *   Para mudar config:
        1.  Criar *novo* config/secret com conteÃºdo atualizado e nome *diferente*.
        2.  Atualizar arquivo Compose para referenciar o *novo* nome.
        3.  Fazer `docker stack deploy` com Compose atualizado.
    *   Isso *sempre* causa update do serviÃ§o (recriaÃ§Ã£o das rÃ©plicas).
*   ComparaÃ§Ã£o com K8s: K8s *permite* atualizar ConfigMaps/Secrets existentes, mas apps podem nÃ£o pegar a mudanÃ§a sem reinÃ­cio de qualquer forma. Swarm Ã© consistente (mudanÃ§a = update do serviÃ§o).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que, com o secret fornecendo a string de conexÃ£o, a aplicaÃ§Ã£o To-Do agora funciona corretamente, conectando-se ao banco de dados Postgres.
*   A Figura 13.10 resume a arquitetura completa em execuÃ§Ã£o no Swarm, destacando o fluxo de rede (Ingress para acesso externo, Overlay para comunicaÃ§Ã£o interna) e o fornecimento de configuraÃ§Ã£o via Configs e Secrets gerenciados pelo cluster.
*   Um ponto tÃ©cnico **muito importante** sobre Configs e Secrets no Docker Swarm Ã© que eles sÃ£o **imutÃ¡veis**. Uma vez que vocÃª cria um `docker config create meu-cfg ...` ou `docker secret create minha-senha ...`, o conteÃºdo associado a esses nomes (`meu-cfg`, `minha-senha`) **nÃ£o pode ser alterado**.
*   Se vocÃª precisar atualizar uma configuraÃ§Ã£o ou um segredo, o processo Ã©:
    1.  Crie um **novo** objeto com um nome **diferente** (ex: `meu-cfg-v2`, `minha-senha-nova`).
    2.  Modifique seu arquivo `docker-compose.yml` para que o serviÃ§o passe a referenciar o **novo** nome na seÃ§Ã£o `configs:` ou `secrets:`.
    3.  Execute `docker stack deploy` com o arquivo `.yml` modificado.
*   Isso garante que as atualizaÃ§Ãµes de configuraÃ§Ã£o sejam explÃ­citas e sempre resultem em um rolling update do serviÃ§o, garantindo que as novas rÃ©plicas recebam a configuraÃ§Ã£o atualizada. A pÃ¡gina contrasta isso com o Kubernetes, que permite alterar objetos existentes, mas o que pode levar a inconsistÃªncias se as aplicaÃ§Ãµes nÃ£o recarregarem a configuraÃ§Ã£o dinamicamente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Simular Update de Config:**
    1.  Crie um novo arquivo de config local, ex: `config_v2.json`, com um valor diferente.
    2.  Crie um novo config object no Swarm: `docker config create todo-list-config-v2 ./config_v2.json`.
    3.  Edite `v4.yml` (ou crie `v4.1.yml`) e mude a referÃªncia no serviÃ§o `todo-web` e na seÃ§Ã£o raiz `configs:` de `todo-list-config` para `todo-list-config-v2`.
    4.  Execute `docker stack deploy -c v4.1.yml todo`. O serviÃ§o `todo-web` serÃ¡ atualizado para usar a nova configuraÃ§Ã£o.

---

ğŸ“„ PÃ¡gina 270

ğŸ” Resumo dos principais pontos:

*   Atualizar serviÃ§os Ã© normal e esperado (features, seguranÃ§a OS/deps). MÃ­nimo: mensalmente.
*   PrÃ³ximo: AplicaÃ§Ãµes stateful no Swarm. Containers sÃ£o substituÃ­dos, precisa de Volumes. Volumes funcionam diferente no Swarm.
*   **13.4 Storing data with volumes in the Swarm** (Armazenando dados com volumes no Swarm)
    *   RevisÃ£o Cap. 6: Volumes tÃªm ciclo de vida separado, dados persistem. App atualizado anexa mesmo volume.
    *   Conceito igual em orquestradores (Compose usa `volumes:` no serviÃ§o).
    *   **DiferenÃ§a Swarm:** Onde os dados do volume sÃ£o *fisicamente* armazenados?
        *   PadrÃ£o Swarm: **Volume Local** no nÃ³ onde a rÃ©plica estÃ¡ rodando.
*   **Problema Volume Local:** Se rÃ©plica Ã© substituÃ­da e agendada em *outro* nÃ³, ela *nÃ£o* terÃ¡ acesso aos dados do volume local do nÃ³ original.
*   **SoluÃ§Ã£o 1 (Simples, Limitada): Pinning de ServiÃ§o (Constraints).**
    *   ForÃ§ar rÃ©plicas de um serviÃ§o a rodarem *sempre* em um nÃ³ (ou conjunto de nÃ³s) especÃ­fico que tem os dados.
    *   Funciona para: dados precisam sobreviver a updates, mas app nÃ£o precisa de >1 rÃ©plica e pode tolerar indisponibilidade se *aquele* nÃ³ especÃ­fico falhar.
    *   ImplementaÃ§Ã£o: Adicionar **label** ao nÃ³, usar **constraint** de posicionamento (`deploy: placement: constraints:`) no serviÃ§o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina faz a transiÃ§Ã£o para o Ãºltimo tÃ³pico de recursos do Swarm no capÃ­tulo: **Volumes** para aplicaÃ§Ãµes stateful, como o banco de dados `todo-db`.
*   O conceito de usar volumes para persistÃªncia Ã© o mesmo do Cap. 6. A complicaÃ§Ã£o no Swarm (e K8s) surge porque as rÃ©plicas podem rodar em **nÃ³s diferentes** do cluster.
*   Por padrÃ£o, quando vocÃª define um volume nomeado em uma stack Swarm (como faremos), o Swarm cria um **Volume Local** no disco do nÃ³ onde a rÃ©plica Ã© agendada.
*   **Problema:** Se a rÃ©plica `todo-db.1` roda no `node-A` e escreve dados no volume local `db-data@node-A`, e depois essa rÃ©plica falha e o Swarm a substitui por `todo-db.2` no `node-B`, a nova rÃ©plica `todo-db.2` **nÃ£o terÃ¡ acesso** aos dados que estavam em `db-data@node-A`. Os dados foram perdidos para a aplicaÃ§Ã£o (embora ainda existam fisicamente no disco do `node-A`).
*   A primeira soluÃ§Ã£o apresentada para lidar com isso (para casos de uso especÃ­ficos) Ã© usar **Constraints de Posicionamento (Placement Constraints)**. VocÃª pode "etiquetar" (`label`) um nÃ³ especÃ­fico (ex: o que tem um disco rÃ¡pido ou grande) e depois dizer ao Swarm, na definiÃ§Ã£o do serviÃ§o `todo-db`, que ele *sÃ³ pode* rodar rÃ©plicas nos nÃ³s que possuem aquela etiqueta. Isso garante que a rÃ©plica (e suas substitutas) sempre rodem no mesmo nÃ³, podendo assim reutilizar o volume local daquele nÃ³. A desvantagem Ã© que isso cria um ponto Ãºnico de falha: se aquele nÃ³ especÃ­fico cair, o serviÃ§o ficarÃ¡ indisponÃ­vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker node update --label-add <chave>=<valor> <nome_no>` para adicionar uma label a um nÃ³.
*   VocÃª usarÃ¡ a seÃ§Ã£o `deploy: placement: constraints:` no arquivo Compose para especificar as labels que o nÃ³ deve ter para rodar o serviÃ§o.
*   O prÃ³ximo exercÃ­cio demonstrarÃ¡ isso (embora em um Swarm de 1 nÃ³, o efeito do constraint seja trivial).

---

ğŸ“„ PÃ¡gina 271

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Adicionar label `storage=raid` ao nÃ³ Swarm.
    *   Comando: `docker node update --label-add storage=raid $(docker node ls -q)`. (Aplica ao nÃ³ Ãºnico no setup atual).
*   SaÃ­da Ã© sÃ³ ID do nÃ³.
*   **Listagem 13.5:** Trecho de `v5.yml` com `constraints` e `volumes` para `todo-db`.
    ```yaml
    services:
      todo-db:
        image: diamol/postgres:11.5
        volumes: # Monta volume nomeado no path de dados do Postgres
          - todo-db-data:/var/lib/postgresql/data
        deploy:
          placement:
            constraints: # SÃ³ roda em nÃ³s com esta label
              - node.labels.storage == raid
        #...
    volumes: # Declara o volume nomeado a ser criado/usado pela stack
      todo-db-data: {} # {} indica usar driver padrÃ£o (local)
    ```
*   **ExplicaÃ§Ã£o:**
    *   `deploy: placement: constraints:` forÃ§a rÃ©plica a rodar em nÃ³ com `storage=raid`.
    *   `volumes:` (no serviÃ§o) monta o volume `todo-db-data` no diretÃ³rio de dados do Postgres.
    *   `volumes:` (raiz) declara `todo-db-data`. `{}` vazio usa o driver de volume padrÃ£o do Swarm (geralmente `local`, significando volume no disco do nÃ³ onde a rÃ©plica roda).
*   **ExercÃ­cio Try-it-now:** Verificar volumes antes e depois de deployar `v5.yml`.
    *   `docker volume ls -q` (antes).
    *   `docker stack deploy -c ./todo-list/v5.yml todo` (Linux) ou `v5-windows.yml` (Windows).
    *   `docker volume ls -q` (depois).
*   Figura 13.11: SaÃ­da mostrando muitos volumes (incluindo anÃ´nimos de execuÃ§Ãµes anteriores) e o novo volume nomeado `todo_todo-db-data` (prefixado pela stack).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O primeiro exercÃ­cio adiciona uma label `storage=raid` ao Ãºnico nÃ³ do Swarm local. Em um cluster real, vocÃª adicionaria essa label apenas aos nÃ³s que tivessem o armazenamento desejado.
*   A Listagem 13.5 mostra como usar essa label na definiÃ§Ã£o do serviÃ§o `todo-db` dentro do arquivo Compose (`v5.yml`):
    *   A seÃ§Ã£o `deploy: placement: constraints:` contÃ©m uma lista de condiÃ§Ãµes. `node.labels.storage == raid` instrui o Swarm a agendar rÃ©plicas *apenas* em nÃ³s que possuam a label `storage` com o valor `raid`.
    *   A seÃ§Ã£o `volumes:` dentro do serviÃ§o monta um volume chamado `todo-db-data` no diretÃ³rio `/var/lib/postgresql/data` (local padrÃ£o de dados do Postgres).
    *   A seÃ§Ã£o `volumes:` no nÃ­vel raiz do YAML declara o volume `todo-db-data`. DeixÃ¡-lo vazio (`{}`) significa que o Swarm deve usar o driver de volume padrÃ£o (que Ã© `local` na maioria das instalaÃ§Ãµes), criando o volume no disco local do nÃ³ selecionado pela constraint.
*   O segundo exercÃ­cio implanta essa stack `v5`. Antes e depois, `docker volume ls` Ã© usado para observar os volumes. O importante Ã© notar que, apÃ³s o deploy, um novo volume nomeado `todo_todo-db-data` aparecerÃ¡ (o Swarm prefixa volumes definidos na stack com o nome da stack). Volumes anÃ´nimos criados por execuÃ§Ãµes anteriores (sem `-v` ou volume nomeado) tambÃ©m podem estar presentes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Adicionar Label ao NÃ³:**
    ```powershell
    docker node update --label-add storage=raid $(docker node ls -q)
    # Verificar (opcional): docker node inspect $(docker node ls -q) -f '{{.Spec.Labels}}'
    ```
2.  **Verificar Volumes Atuais:**
    ```powershell
    docker volume ls -q
    ```
3.  **Verificar/Criar `v5.yml` ou `v5-windows.yml`:** Certifique-se que o arquivo existe em `ch13/exercises/todo-list` e contÃ©m a Listagem 13.5 (mais o serviÃ§o `todo-web` de antes, `version`, `networks`, `configs`, `secrets`). O `v5-windows.yml` provavelmente ajusta o path no `volumes:` do serviÃ§o `todo-db` para `C:\...` se necessÃ¡rio para a imagem Postgres Windows.
4.  **Implantar Stack v5:** (Use o arquivo apropriado)
    ```powershell
    docker stack deploy -c ./todo-list/v5.yml todo
    # Ou: docker stack deploy -c ./todo-list/v5-windows.yml todo
    ```
5.  **Verificar Volumes Novamente:**
    ```powershell
    docker volume ls
    # Procure pelo volume chamado 'todo_todo-db-data'.
    ```

---

ğŸ“„ PÃ¡gina 272

ğŸ” Resumo dos principais pontos:

*   Figura 13.11: Confirma criaÃ§Ã£o do volume nomeado `todo_todo-db-data`.
*   DiferenÃ§a: Volumes anÃ´nimos (`VOLUME` no Dockerfile ou `-v` sem nome) sÃ£o removidos com a stack (`stack rm`). Volumes nomeados no Compose (`volumes:` raiz) *nÃ£o* sÃ£o removidos com a stack, garantindo persistÃªncia.
*   Deploy com constraint garante:
    *   RÃ©plica do BD roda no nÃ³ com label `storage=raid`.
    *   NÃ³ cria volume local `todo_todo-db-data`.
    *   Se rÃ©plica falhar/for atualizada, substituta roda no *mesmo* nÃ³ (devido Ã  constraint) e re-anexa o *mesmo* volume local.
*   Dados persistem entre containers, desde que o nÃ³ esteja disponÃ­vel.
*   **ExercÃ­cio Try-it-now:** Atualizar versÃ£o do Postgres para testar persistÃªncia.
    *   Arquivo `v6.yml` (ou `v6-windows.yml`) Ã© idÃªntico ao v5, exceto por usar imagem Postgres mais nova.
    *   1. Deploy v6: `docker stack deploy -c ./todo-list/v6.yml todo` (ou `v6-windows.yml`).
    *   2. Verificar tarefas: `docker stack ps todo`.
    *   3. Verificar volumes: `docker volume ls -q`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina esclarece uma diferenÃ§a importante no ciclo de vida: volumes **nomeados** definidos na seÃ§Ã£o `volumes:` raiz do Compose para uma stack **nÃ£o sÃ£o removidos** quando a stack Ã© removida (`docker stack rm`). Isso Ã© diferente de volumes *anÃ´nimos* ou volumes criados implicitamente pela stack (sem definiÃ§Ã£o na raiz), que *sÃ£o* removidos com a stack. Usar volumes nomeados declarados na raiz Ã©, portanto, essencial para dados que precisam sobreviver alÃ©m da prÃ³pria stack.
*   Reafirma que a combinaÃ§Ã£o da **constraint de posicionamento** (forÃ§ando o serviÃ§o a rodar em um nÃ³ especÃ­fico) e o **volume nomeado local** nesse nÃ³ garante que os dados do banco de dados persistam atravÃ©s de reinÃ­cios ou atualizaÃ§Ãµes da rÃ©plica do banco de dados, *contanto que aquele nÃ³ especÃ­fico esteja funcional*.
*   O exercÃ­cio demonstra isso na prÃ¡tica, simulando uma atualizaÃ§Ã£o do servidor PostgreSQL. Um novo arquivo Compose (`v6.yml`) Ã© idÃªntico ao `v5.yml`, exceto pela tag da imagem do Postgres. Ao aplicar `v6.yml` com `docker stack deploy`:
    *   O Swarm detecta a mudanÃ§a na imagem do serviÃ§o `todo-db`.
    *   Ele executa um rolling update para `todo-db`.
    *   A nova rÃ©plica Ã© agendada no *mesmo* nÃ³ (devido Ã  constraint).
    *   A nova rÃ©plica anexa o *mesmo* volume nomeado `todo_todo-db-data` que a rÃ©plica antiga usava.
    *   Nenhum volume novo Ã© criado, e os dados sÃ£o preservados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **(Opcional) Adicionar Dados:** Acesse `http://localhost:8080` (ou IP externo) e adicione/verifique itens na lista To-Do (para ter dados a preservar).
2.  **Verificar/Criar `v6.yml`:** Certifique-se que `v6.yml` (ou `v6-windows.yml`) existe e apenas atualiza a tag da imagem `diamol/postgres`.
3.  **Implantar AtualizaÃ§Ã£o v6:**
    ```powershell
    docker stack deploy -c ./todo-list/v6.yml todo
    # Ou: docker stack deploy -c ./todo-list/v6-windows.yml todo
    ```
4.  **Verificar Tarefas:** Espere update completar.
    ```powershell
    docker stack ps todo
    # Veja rÃ©plica antiga de todo-db como Shutdown, nova como Running.
    ```
5.  **Verificar Volumes:**
    ```powershell
    docker volume ls -q
    # O nÃºmero de volumes nÃ£o deve ter aumentado. O volume 'todo_todo-db-data' ainda existe.
    ```
6.  **Testar PersistÃªncia:** Acesse `http://localhost:8080` novamente. Os itens que vocÃª tinha devem continuar lÃ¡, mesmo apÃ³s a atualizaÃ§Ã£o do BD.

---

ğŸ“„ PÃ¡gina 273

ğŸ” Resumo dos principais pontos:

*   Figura 13.12: Mostra saÃ­da do `stack ps` e `volume ls` apÃ³s update v6, confirmando substituiÃ§Ã£o da rÃ©plica e reuso do volume.
*   Exemplo simples; storage fica mais complexo com requisitos diferentes.
*   **Problema do Volume Local (Revisitado):** Dados locais nÃ£o sÃ£o replicados. Se nÃ³ com constraint falhar, app stateful fica indisponÃ­vel.
*   SoluÃ§Ã£o para HA real de dados: **Storage DistribuÃ­do**.
    *   Docker tem sistema de plugins para **drivers de volume**.
    *   Permite Swarm usar storage externo (cloud - EBS, Azure Disk; datacenter - SAN, Ceph, etc.) como backend para volumes.
    *   ConfiguraÃ§Ã£o depende da infra; mas consumo no Compose Ã© similar (especifica driver e opÃ§Ãµes no `volumes:` raiz).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que a atualizaÃ§Ã£o do Postgres funcionou e os dados persistiram graÃ§as ao volume nomeado e Ã  constraint.
*   No entanto, volta a apontar a **limitaÃ§Ã£o fundamental** da abordagem com volume local + constraint: ela nÃ£o fornece verdadeira alta disponibilidade (HA) para os dados. Se o Ãºnico nÃ³ permitido pela constraint falhar, o serviÃ§o stateful (o banco de dados) nÃ£o poderÃ¡ ser iniciado em outro lugar, e a aplicaÃ§Ã£o ficarÃ¡ degradada ou indisponÃ­vel.
*   A soluÃ§Ã£o para HA de dados em um cluster Ã© usar **armazenamento distribuÃ­do** ou **armazenamento compartilhado** que seja acessÃ­vel por *todos* (ou mÃºltiplos) nÃ³s do cluster. O Docker Swarm suporta isso atravÃ©s de **drivers de volume de terceiros**.
*   **Drivers de Volume:** SÃ£o plugins que ensinam o Docker a interagir com sistemas de armazenamento externos. Existem drivers para:
    *   Storage em bloco da nuvem (AWS EBS, Azure Disk, Google Persistent Disk): O driver pode montar/desmontar o disco no nÃ³ correto.
    *   Sistemas de arquivos de rede (NFS, CIFS/SMB).
    *   Sistemas de arquivos distribuÃ­dos (GlusterFS, Ceph).
    *   SoluÃ§Ãµes de Storage Definido por Software (SDS) especÃ­ficas para contÃªineres.
*   Ao usar um driver de volume externo, vocÃª o especifica na definiÃ§Ã£o do volume no arquivo Compose (`volumes: meu-volume-distribuido: driver: meu-driver-externo`). O Swarm entÃ£o usa esse driver para provisionar e anexar o volume ao nÃ³ apropriado, e o volume pode potencialmente ser re-anexado a um nÃ³ diferente se a rÃ©plica se mover, mantendo o acesso aos dados. A configuraÃ§Ã£o inicial do driver no cluster depende do provedor do driver e da infraestrutura.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Configurar drivers de volume externos estÃ¡ fora do escopo do Docker Desktop bÃ¡sico. Em produÃ§Ã£o, vocÃª seguiria a documentaÃ§Ã£o do driver escolhido (ex: driver CSI para Kubernetes/Swarm com Azure Disk, ou driver NFS).
*   A definiÃ§Ã£o no Compose seria algo como:
    ```yaml
    services:
      todo-db:
        # ...
        volumes:
          - todo-db-shared:/var/lib/postgresql/data
    volumes:
      todo-db-shared:
        driver: azure_file # Exemplo hipotÃ©tico para Azure Files
        driver_opts: # OpÃ§Ãµes especÃ­ficas do driver
          share_name: meu-share-postgres
          storage_account_name: minha-conta
    ```

---

ğŸ“„ PÃ¡gina 274

ğŸ” Resumo dos principais pontos:

*   **13.5 Understanding how the cluster manages stacks** (Entendendo como o cluster gerencia stacks)
*   Stack = grupo lÃ³gico de recursos gerenciados pelo cluster.
*   Recursos (volumes, secrets, configs, networks, services) tÃªm comportamentos diferentes no gerenciamento pelo Swarm.
*   Diagrama (Figura 13.13): Resume gerenciamento de cada tipo de recurso pela stack.
*   **Resumo do Gerenciamento por Tipo:**
    1.  **Volumes:**
        *   AnÃ´nimos/ImplÃ­citos: Criados/Removidos com a stack.
        *   Nomeados (declarados na raiz `volumes:`): Criados pela stack (se nÃ£o `external`), **NÃƒO** removidos com `stack rm`. Persistem alÃ©m da stack.
    2.  **Secrets & Configs:**
        *   Criados externamente (`docker secret/config create`).
        *   Declarados como `external: true` na stack.
        *   NÃƒO sÃ£o criados/removidos pela stack. Ciclo de vida independente. ImutÃ¡veis.
    3.  **Networks:**
        *   Podem ser gerenciadas externamente (criadas antes, `external: true`) OU pela stack (criadas/removidas com `stack deploy/rm`).
        *   Stack sempre garante que serviÃ§os estejam em uma rede (cria `_default` se nenhuma for especificada).
    4.  **Services:**
        *   Criados/Atualizados/Removidos por `stack deploy/rm`.
        *   Swarm monitora continuamente, mantÃ©m rÃ©plicas desejadas, substitui falhas.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o final resume como o Docker Swarm gerencia os diferentes tipos de recursos (volumes, secrets, configs, redes, serviÃ§os) que compÃµem uma **stack**. O comportamento nÃ£o Ã© uniforme para todos os tipos.
*   A Figura 13.13 e a lista subsequente esclarecem o ciclo de vida de cada um em relaÃ§Ã£o Ã  stack:
    *   **Volumes:** O ponto chave Ã© a diferenÃ§a entre volumes anÃ´nimos/implÃ­citos (que morrem com a stack) e volumes **nomeados e declarados na seÃ§Ã£o `volumes:` raiz** (que **sobrevivem** Ã  remoÃ§Ã£o da stack). Para persistÃªncia real, use volumes nomeados declarados.
    *   **Secrets e Configs:** SÃ£o tipicamente gerenciados **fora** do ciclo de vida da stack. VocÃª os cria antes (`docker secret/config create`), e a stack apenas os referencia como `external: true`. Eles nÃ£o sÃ£o afetados por `stack rm`. Lembre-se que sÃ£o imutÃ¡veis.
    *   **Networks:** Podem ser gerenciadas pela stack (criadas no `deploy`, removidas no `rm`) ou externamente (criadas antes, `external: true`). Uma rede padrÃ£o Ã© criada se nenhuma for especificada.
    *   **Services:** SÃ£o o principal recurso gerenciado pela stack. `stack deploy` os cria ou atualiza, `stack rm` os remove. O Swarm ativamente gerencia suas rÃ©plicas enquanto a stack existe.
*   Compreender essas diferenÃ§as Ã© importante para evitar surpresas (como perder dados de um volume anÃ´nimo ao remover uma stack de teste).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revise os arquivos Compose (`.yml`) usados no capÃ­tulo. Identifique:
    *   Volumes nomeados declarados na raiz (ex: `todo-db-data:` na p.271).
    *   Secrets/Configs declarados na raiz com `external: true` (ex: `todo-list-config: external: true` na p.264).
    *   Redes declaradas na raiz (podem ter `external: true` ou nÃ£o).
*   Experimente remover uma stack (`docker stack rm todo`) e depois verificar quais recursos ainda existem (`docker volume ls`, `docker secret ls`, `docker config ls`, `docker network ls`). Os volumes nomeados e os secrets/configs externos devem permanecer.

---

ğŸ“„ PÃ¡gina 275

ğŸ” Resumo dos principais pontos:

*   Stack agrupa componentes, mas **NÃƒO define dependÃªncias** entre serviÃ§os.
*   `stack deploy` inicia rÃ©plicas o mais rÃ¡pido possÃ­vel, sem ordem garantida entre serviÃ§os.
*   NÃ£o se pode forÃ§ar Swarm a iniciar ServiÃ§o A completamente antes do ServiÃ§o B.
*   Precisa assumir ordem aleatÃ³ria; usar **health checks** e **dependency checks** nas imagens.
*   Checks permitem "fail fast", cluster repara -> app auto-reparÃ¡vel (self-healing).
*   **13.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Ganhar experiÃªncia com deploy de stack Swarm e arquivos Compose.
*   **Tarefa:** Escrever arquivo Compose (`.yml`) Ãºnico para deploy **produÃ§Ã£o** do app Image Gallery (Cap. 9).
    *   **Requisitos:**
        1.  ServiÃ§o `access-log` (imagem `diamol/ch09-access-log`), 3 rÃ©plicas, interno (sem porta publicada).
        2.  ServiÃ§o `iotd` (imagem `diamol/ch09-image-of-the-day`), 5 rÃ©plicas, porta 8088 publicada.
        3.  ServiÃ§o `image-gallery` (imagem `diamol/ch09-image-gallery`), 2 rÃ©plicas, porta 80 publicada.
        4.  Todos os componentes com limites (`deploy: resources: limits:`) sensatos de CPU/memÃ³ria. (Pode exigir tentativa e erro para achar valores).
        5.  App deve funcionar apÃ³s `docker stack deploy`.
    *   Sem volumes/configs/secrets complexos neste app.
*   SoluÃ§Ã£o no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Um ponto crucial sobre `docker stack deploy` (e deploy em orquestradores em geral) Ã© reiterado: ele **nÃ£o garante a ordem de inicializaÃ§Ã£o** entre serviÃ§os diferentes. O orquestrador tenta iniciar tudo o mais rÃ¡pido possÃ­vel. Portanto, vocÃª **nÃ£o pode** confiar que o serviÃ§o `todo-db` estarÃ¡ totalmente pronto quando o `todo-web` iniciar.
*   Ã‰ por isso que as tÃ©cnicas do CapÃ­tulo 8 (dependency checks na inicializaÃ§Ã£o, health checks para monitoramento contÃ­nuo) sÃ£o **essenciais** em ambientes orquestrados. Eles permitem que os contÃªineres lidem com a inicializaÃ§Ã£o fora de ordem e sinalizem seu estado real para o orquestrador, habilitando a auto-recuperaÃ§Ã£o.
*   O **Lab do CapÃ­tulo 13** Ã© um exercÃ­cio prÃ¡tico para consolidar o uso de `docker stack deploy` com um arquivo Compose que define mÃºltiplos serviÃ§os, rÃ©plicas e limites de recursos.
*   A tarefa Ã© criar um Ãºnico arquivo `.yml` para a aplicaÃ§Ã£o Image Gallery, especificando:
    *   Os trÃªs serviÃ§os (`access-log`, `iotd`, `image-gallery`) com suas respectivas imagens do Cap. 9.
    *   O nÃºmero de `replicas:` desejado para cada um dentro da seÃ§Ã£o `deploy:`.
    *   As `ports:` publicadas para `iotd` (8088) e `image-gallery` (80). `access-log` nÃ£o tem portas publicadas.
    *   A seÃ§Ã£o `deploy: resources: limits:` para *cada* serviÃ§o, com valores que vocÃª estime serem razoÃ¡veis (ex: `cpus: "0.25"`, `memory: 128M`). Encontrar valores ideais requereria monitoramento real (Cap. 9).
    *   Garantir que todos os serviÃ§os estejam conectados a uma rede overlay (definida na seÃ§Ã£o `networks:` raiz, ou deixar o Swarm criar uma `_default`).
*   O objetivo final Ã© ter um arquivo `.yml` que, ao ser implantado com `docker stack deploy -c <seu_arquivo.yml> image-gallery-stack`, resulte na aplicaÃ§Ã£o funcionando corretamente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 13:**
    1.  Crie um arquivo `image-gallery-stack.yml` (ou similar) na pasta `ch13\lab`.
    2.  Adicione `version: '3.7'`.
    3.  Adicione seÃ§Ã£o `networks:` definindo uma rede overlay, ex: `app-net: driver: overlay`.
    4.  Adicione seÃ§Ã£o `services:`.
    5.  Para cada serviÃ§o (`access-log`, `iotd`, `image-gallery`):
        *   Defina `image:` com a imagem correta (`diamol/ch09-...`).
        *   Adicione `networks: [app-net]`.
        *   Adicione `deploy:`:
            *   Dentro de `deploy:`, adicione `replicas:` com o nÃºmero pedido (3, 5, 2).
            *   Dentro de `deploy:`, adicione `resources: limits:` com `cpus:` e `memory:` (ex: "0.1", "64M").
        *   Para `iotd` e `image-gallery`, adicione a seÃ§Ã£o `ports:` com os mapeamentos pedidos (`["8088:80"]`, `["80:80"]`).
    6.  **Implantar:** `docker stack deploy -c image-gallery-stack.yml ig-stack`.
    7.  **Verificar:** `docker stack services ig-stack`, `docker stack ps ig-stack`.
    8.  **Testar:** Acesse `http://localhost` (porta 80). Deve mostrar a aplicaÃ§Ã£o. (Lembre-se da limitaÃ§Ã£o do Windows para `localhost` se estiver nesse modo).

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch13/lab/README.md

---

ğŸ“„ PÃ¡gina 276

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 14: Automating releases with upgrades and rollbacks** (Automatizando lanÃ§amentos com atualizaÃ§Ãµes e reversÃµes)
*   AtualizaÃ§Ãµes de contÃªineres devem ter **zero downtime**, gerenciadas pelo orquestrador.
*   PrÃ©-requisitos: capacidade extra no cluster, health checks nas imagens.
*   Processo jÃ¡ visto com `docker stack deploy` no Cap. 13.
*   Foco Cap. 14: ConfiguraÃ§Ã£o detalhada do processo de update/rollback no Swarm.
*   ImportÃ¢ncia: Entender e configurar rollouts evita problemas em produÃ§Ã£o. Foco em Swarm, mas conceitos similares em K8s.
*   Objetivo: Deployments frequentes e confiÃ¡veis (funcionam ou revertem automaticamente).
*   **14.1 The application upgrade process with Docker** (O processo de atualizaÃ§Ã£o de aplicaÃ§Ã£o com Docker)
    *   Imagens Docker parecem simples, mas hÃ¡ mÃºltiplas "cadÃªncias" de atualizaÃ§Ã£o a considerar.
    *   Diagrama (Figura 14.1): Mostra dependÃªncias de uma imagem .NET Core/Linux.
        *   App -> Deps Nuget -> Runtime .NET -> SDK .NET -> Imagem Base OS -> Kernel OS.
    *   NecessÃ¡rio atualizar imagem nÃ£o sÃ³ por mudanÃ§as no app, mas tambÃ©m por updates em qualquer camada abaixo (seguranÃ§a OS, patches runtime, etc.).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 14, focado em aprofundar o processo de **atualizaÃ§Ã£o (upgrade)** e **reversÃ£o (rollback)** de aplicaÃ§Ãµes rodando como stacks/serviÃ§os no Docker Swarm.
*   O objetivo ideal Ã© ter atualizaÃ§Ãµes com **zero downtime**, onde o orquestrador substitui as rÃ©plicas antigas pelas novas de forma gradual e segura, usando os health checks para validar o sucesso.
*   Embora `docker stack deploy` jÃ¡ faÃ§a isso com configuraÃ§Ãµes padrÃ£o, este capÃ­tulo mostrarÃ¡ como **customizar** o processo de rollout (quantas rÃ©plicas atualizar por vez, quanto tempo esperar, o que fazer em caso de falha, etc.) para tornÃ¡-lo mais rÃ¡pido e/ou mais seguro, dependendo das necessidades da aplicaÃ§Ã£o. Entender essas configuraÃ§Ãµes Ã© crucial para evitar problemas em produÃ§Ã£o.
*   A seÃ§Ã£o 14.1 comeÃ§a contextualizando *por que* as atualizaÃ§Ãµes sÃ£o frequentes e necessÃ¡rias. NÃ£o Ã© apenas quando o cÃ³digo da sua aplicaÃ§Ã£o muda. Uma imagem Docker Ã© construÃ­da sobre vÃ¡rias camadas de dependÃªncias (Figura 14.1): o cÃ³digo da aplicaÃ§Ã£o depende de bibliotecas, que rodam sobre um runtime (ex: .NET, Java), que por sua vez roda sobre uma imagem base de sistema operacional. Qualquer uma dessas camadas pode receber atualizaÃ§Ãµes (especialmente de seguranÃ§a). Portanto, Ã© uma boa prÃ¡tica reconstruir e reimplantar suas imagens regularmente (ex: mensalmente, ou atÃ© diariamente via CI) para incorporar essas atualizaÃ§Ãµes das camadas base, mesmo que seu prÃ³prio cÃ³digo nÃ£o tenha mudado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ a seÃ§Ã£o `deploy: update_config:` e `deploy: rollback_config:` dentro das definiÃ§Ãµes de serviÃ§o no seu arquivo Compose para controlar o comportamento dos updates e rollbacks no Swarm.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   CadÃªncias de Update: Pense no seu celular. VocÃª atualiza seus apps (sua aplicaÃ§Ã£o). Mas, de vez em quando, o sistema operacional (Android/iOS - imagem base OS/runtime) tambÃ©m precisa ser atualizado. Ignorar atualizaÃ§Ãµes do SO pode deixar seu celular vulnerÃ¡vel. O mesmo vale para imagens Docker.

---

ğŸ“„ PÃ¡gina 277

ğŸ” Resumo dos principais pontos:

*   Ideal: Pipeline CI/CD rodando diariamente para pegar updates das bases.
*   Releases frequentes e automatizados aumentam confianÃ§a e agilidade (vs. releases manuais raras e arriscadas).
*   ConfianÃ§a vem de releases bem-sucedidos, que dependem de **health checks** (sem eles, nÃ£o hÃ¡ auto-recuperaÃ§Ã£o ou updates/rollbacks seguros).
*   Exemplo: App Random Number (Cap. 8) com overrides Compose (Cap. 10).
    *   `docker-compose.yml` (core), `prod.yml` (override prod), `vX.yml` (overrides de versÃ£o).
*   **Problema:** `docker stack deploy` *nÃ£o* suporta mÃºltiplos arquivos `-f` como `docker-compose`.
*   **SoluÃ§Ã£o:** Usar `docker-compose config` para *primeiro* mesclar os arquivos em um Ãºnico YAML final, e *depois* usar `docker stack deploy -c` com esse arquivo mesclado.
*   **ExercÃ­cio Try-it-now:** Deployar versÃ£o inicial (build 1) do Random Number.
    *   Garantir modo Swarm ativo.
    *   1. Mesclar arquivos core + prod: `docker-compose -f ./numbers/core.yml -f ./numbers/prod.yml config > stack.yml`.
    *   2. Implantar stack com arquivo mesclado: `docker stack deploy -c stack.yml numbers`.
    *   3. Verificar serviÃ§os: `docker stack services numbers`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina argumenta a favor de **lanÃ§amentos frequentes e automatizados**, habilitados por um pipeline CI/CD robusto e pela confianÃ§a gerada por health checks que permitem rollouts/rollbacks seguros. O objetivo Ã© tornar o release um evento "chato" e rotineiro, nÃ£o um trauma.
*   Introduz o cenÃ¡rio prÃ¡tico do capÃ­tulo: usar a aplicaÃ§Ã£o Random Number e gerenciÃ¡-la com uma estrutura de arquivos Compose com overrides (um core, um para produÃ§Ã£o, e outros para versÃµes especÃ­ficas).
*   Aponta uma **limitaÃ§Ã£o importante do `docker stack deploy`**: ele **nÃ£o** aceita mÃºltiplos arquivos `-f` para mesclagem como o `docker-compose` faz. VocÃª sÃ³ pode fornecer *um* arquivo `-c`.
*   A soluÃ§Ã£o/workaround Ã© usar o `docker-compose config` (que *aceita* mÃºltiplos `-f`) para prÃ©-processar e mesclar todos os arquivos necessÃ¡rios em um Ãºnico arquivo YAML final (redirecionando a saÃ­da para `stack.yml`, por exemplo), e entÃ£o usar `docker stack deploy -c stack.yml ...`.
*   O exercÃ­cio demonstra esse fluxo de duas etapas: primeiro mesclar o `core.yml` e `prod.yml` em `stack.yml`, e depois implantar `stack.yml` no Swarm.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Swarm:** `docker info | Select-String "Swarm"` (active).
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch14\exercises
    ```
3.  **Mesclar Arquivos:** (Verifique se os arquivos se chamam `docker-compose.yml` e `prod.yml` na pasta `numbers`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml config > stack.yml
    # Verifique o conteÃºdo de stack.yml (opcional): Get-Content stack.yml
    ```
4.  **Implantar Stack:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
5.  **Verificar ServiÃ§os:**
    ```powershell
    docker stack services numbers
    # Deve mostrar numbers_numbers-api e numbers_numbers-web
    ```

---

ğŸ“„ PÃ¡gina 278

ğŸ” Resumo dos principais pontos:

*   Figura 14.2: Mostra saÃ­da da mesclagem e deploy. Stack `numbers` criada com serviÃ§os `api` (replicated) e `web` (global).
*   **Modo Global vs. Replicated:**
    *   `prod.yml` configura `numbers-web` como `deploy: mode: global`.
    *   ServiÃ§o Global: Roda **uma** rÃ©plica em **cada** nÃ³ do Swarm. Ãštil para agentes de monitoramento, ou frontends leves onde 1 por nÃ³ Ã© suficiente.
    *   ServiÃ§o Replicated (padrÃ£o): Roda nÃºmero `replicas:` desejado, distribuÃ­do entre nÃ³s disponÃ­veis.
*   `prod.yml` tambÃ©m configura `numbers-web` com `ports: mode: host`.
*   **Modo Host vs. Ingress:**
    *   `mode: host` (em `ports`): bypassa rede Ingress. Container publica porta *diretamente* no IP do nÃ³ onde estÃ¡ rodando.
    *   SÃ³ pode ter *um* container por nÃ³ usando a mesma porta host nesse modo. Funciona bem com `mode: global` (garante 1 rÃ©plica por nÃ³).
    *   Pode ter melhor performance (sem hop extra do Ingress), mas menos flexibilidade (precisa saber IP do nÃ³ especÃ­fico, ou ter LB externo apontando para todos os nÃ³s).
*   Deploy atual usa imagens v1 (sem health checks). App API tem bug (falha apÃ³s 3 chamadas).
*   Pode-se acessar `http://localhost`. MÃºltiplas chamadas usam LB do Swarm entre as 6 rÃ©plicas da API. Eventualmente, todas quebrarÃ£o e app para de funcionar (sem auto-reparo).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa o resultado do deploy inicial, notando que o serviÃ§o `numbers-web` foi implantado de forma diferente dos exemplos anteriores, usando `mode: global` e `ports: mode: host`.
*   **Modo de ServiÃ§o (`deploy: mode:`):**
    *   `replicated` (PadrÃ£o): VocÃª especifica quantas rÃ©plicas quer (`replicas: N`), e o Swarm as distribui entre os nÃ³s disponÃ­veis. Ã‰ o modo mais comum.
    *   `global`: O Swarm automaticamente roda exatamente uma rÃ©plica em *cada* nÃ³ ativo do cluster. Se um nÃ³ Ã© adicionado, ele ganha uma rÃ©plica. Se um nÃ³ sai, sua rÃ©plica Ã© removida. Ãštil para serviÃ§os que precisam estar presentes em todos os nÃ³s (ex: agentes de log ou monitoramento).
*   **Modo de PublicaÃ§Ã£o de Porta (`ports: mode:`):**
    *   `ingress` (PadrÃ£o): Usa a rede Ingress do Swarm. Todos os nÃ³s escutam na porta publicada, e o Swarm roteia para as rÃ©plicas. Permite mÃºltiplas rÃ©plicas ouvindo na mesma porta externa.
    *   `host`: O contÃªiner se liga diretamente Ã  porta no IP do nÃ³ onde ele estÃ¡ rodando. A rede Ingress *nÃ£o* Ã© usada. SÃ³ pode haver um contÃªiner por nÃ³ usando essa porta especÃ­fica. Frequentemente usado em conjunto com `deploy: mode: global` para garantir que nÃ£o haja conflitos de porta. Pode oferecer performance ligeiramente melhor em alguns casos, mas perde o balanceamento de carga e a abstraÃ§Ã£o do Ingress (vocÃª precisa saber o IP do nÃ³ para acessÃ¡-lo, ou usar um balanceador externo).
*   O deploy atual (v1 sem health checks) Ã© frÃ¡gil. Embora o Swarm balanceie a carga inicial entre as 6 rÃ©plicas da API, o bug eventualmente travarÃ¡ todas elas, e como nÃ£o hÃ¡ health check, o Swarm nÃ£o farÃ¡ nada para corrigi-las.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Detalhes do ServiÃ§o Web:**
    ```powershell
    docker service inspect --pretty numbers_numbers-web
    # Procure por "Mode: global" e "PublishMode: host"
    ```
2.  **Testar App (Modo Linux):**
    *   Acesse `http://localhost` (porta 80, modo host).
    *   Clique "Get Random Number" mÃºltiplas vezes (mais de 18 vezes para garantir quebrar todas as 6 rÃ©plicas da API).
    *   Eventualmente, o botÃ£o deve parar de retornar nÃºmeros e mostrar erro.
    *   Verifique `docker service ps numbers_numbers-api`. Todas as 6 rÃ©plicas devem estar `Running`, mesmo que a aplicaÃ§Ã£o interna tenha falhado.

---

ğŸ“„ PÃ¡gina 279

ğŸ” Resumo dos principais pontos:

*   Figura 14.3: Mostra a saÃ­da do `stack ps` apÃ³s o deploy inicial v1 (API x6, Web x1 - global).
*   Falta de health checks impede auto-reparo e updates seguros.
*   **Deploy v2:** Usa imagens v2 (com health checks).
    *   `v2.yml`: Override que muda `image:` para tag `:v2`.
    *   `prod-healthcheck.yml`: Override que adiciona seÃ§Ã£o `healthcheck:` com parÃ¢metros (`interval`, `timeout`, etc.) aos serviÃ§os.
*   Agora, com health checks, Swarm pode detectar API quebrada e substituÃ­-la.
*   **ExercÃ­cio Try-it-now:** Mesclar arquivos para v2+healthcheck e atualizar stack.
    *   1. Mesclar: `docker-compose -f ...core -f ...prod -f ...prod-healthcheck -f ...v2 config > stack.yml`. Adiciona `--log-level ERROR` para suprimir warnings de Compose sobre campos nÃ£o usados (como `depends_on`).
    *   2. Atualizar: `docker stack deploy -c stack.yml numbers`.
    *   3. Verificar tarefas: `docker stack ps numbers`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina enfatiza que o deploy `v1` Ã© inseguro devido Ã  falta de health checks. A soluÃ§Ã£o Ã© implantar a `v2` das imagens, que os possuem.
*   O processo de atualizaÃ§Ã£o para `v2` envolve mesclar *mais* arquivos override usando `docker-compose config`:
    *   `docker-compose.yml`: Base.
    *   `prod.yml`: Configs de produÃ§Ã£o (rÃ©plicas API, modo global/host web).
    *   `prod-healthcheck.yml`: Adiciona/configura os parÃ¢metros de `healthcheck` para os serviÃ§os (intervalo, retries, etc.).
    *   `v2.yml`: Muda a `image:` para usar a tag `:v2`.
*   A flag `--log-level ERROR` Ã© adicionada ao `docker-compose config` para evitar mensagens de aviso sobre chaves YAML (como `depends_on`) que podem existir nos arquivos mas sÃ£o ignoradas pelo `stack deploy`.
*   O `stack.yml` resultante Ã© entÃ£o usado para atualizar a stack `numbers`. A expectativa Ã© que o Swarm execute um rolling update para ambos os serviÃ§os, substituindo as rÃ©plicas `v1` por `v2`.
*   Com a `v2` rodando (e os health checks definidos em `prod-healthcheck.yml` ativos), o Swarm agora *poderÃ¡* detectar quando as rÃ©plicas da API falharem e as substituirÃ¡ automaticamente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Mesclar Arquivos para v2:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/v2.yml --log-level ERROR config > stack.yml
    ```
2.  **Atualizar Stack para v2:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
3.  **Verificar Tarefas:** Espere o update completar.
    ```powershell
    docker stack ps numbers
    ```
    VocÃª verÃ¡ as rÃ©plicas v1 como `Shutdown` e as novas rÃ©plicas v2 como `Running`.

---

ğŸ“„ PÃ¡gina 280

ğŸ” Resumo dos principais pontos:

*   Update v2 substitui rÃ©plicas v1.
*   AnÃ¡lise do Rollout PadrÃ£o Swarm:
    *   PadrÃ£o: Para rÃ©plicas (`replicated`), para antigas (`stop`) *antes* de iniciar novas (`start`).
    *   Justificativa `stop-first`: Garante que nÂº total de rÃ©plicas nunca exceda o desejado. NecessÃ¡rio para `mode: host` (evita conflito de porta).
    *   Desvantagem `stop-first`: Reduz capacidade temporariamente durante update.
*   Figura 14.4: Mostra saÃ­da do `stack ps` durante o update v2. Destaca que rÃ©plicas v1 sÃ£o paradas (`Shutdown`) antes das v2 iniciarem (`Starting`/`Running`).
*   **ConfiguraÃ§Ã£o PadrÃ£o de Update Swarm:**
    *   Cautelosa: 1 rÃ©plica por vez (`parallelism: 1`).
    *   Verifica se container inicia corretamente.
    *   *NÃ£o* espera health check por padrÃ£o (`monitor: 0s`).
    *   Se falha (container nÃ£o inicia), pausa rollout (`failure_action: pause`).
*   Problemas com padrÃ£o: `stop-first` arriscado (e se nova nÃ£o funcionar?). Pausar em falha pode deixar sistema inconsistente.
*   Melhor: Configurar rollout com opÃ§Ãµes mais sensatas.
*   **14.2 Configuring production rollouts with Compose** (Configurando rollouts de produÃ§Ã£o com Compose)
    *   App v2 agora Ã© auto-reparÃ¡vel (health checks -> Swarm substitui API falha).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa o comportamento do rolling update que acabou de ocorrer para a v2. Destaca a estratÃ©gia padrÃ£o do Swarm: **`stop-first`**. Para cada rÃ©plica a ser atualizada, o Swarm primeiro para a rÃ©plica antiga e depois inicia a nova. Embora isso garanta que o nÃºmero de rÃ©plicas nunca exceda o configurado e seja necessÃ¡rio para o modo `host`, reduz a capacidade da aplicaÃ§Ã£o durante a atualizaÃ§Ã£o.
*   AlÃ©m disso, a configuraÃ§Ã£o padrÃ£o completa do update Ã© descrita como "cautelosa", mas talvez nÃ£o ideal:
    *   `parallelism: 1`: Atualiza apenas uma rÃ©plica de cada vez. Lento para serviÃ§os com muitas rÃ©plicas.
    *   `monitor: 0s`: ApÃ³s iniciar a nova rÃ©plica, o Swarm *nÃ£o* espera para ver se ela fica saudÃ¡vel (via health check) antes de prosseguir para a prÃ³xima. Ele sÃ³ verifica se o contÃªiner iniciou.
    *   `failure_action: pause`: Se uma nova rÃ©plica falhar ao iniciar, o Swarm simplesmente *para* o processo de atualizaÃ§Ã£o, deixando o serviÃ§o em um estado misto (algumas rÃ©plicas novas, outras antigas).
*   A pÃ¡gina critica essas padrÃµes, especialmente `stop-first` e `pause`, sugerindo que hÃ¡ configuraÃ§Ãµes melhores para a maioria dos cenÃ¡rios.
*   A seÃ§Ã£o 14.2 introduz como customizar essas configuraÃ§Ãµes usando a seÃ§Ã£o `deploy: update_config:` no arquivo Compose.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Testar Auto-Reparo (v2):**
    *   Com a stack `v2` rodando, acesse `http://localhost` e cause o bug na API (muitos cliques).
    *   Aguarde (depende do `interval` e `retries` do healthcheck definido em `prod-healthcheck.yml`).
    *   Use `docker service ps numbers_numbers-api`. VocÃª deverÃ¡ ver rÃ©plicas antigas como `Failed` ou `Shutdown` e novas rÃ©plicas `Running` que o Swarm iniciou automaticamente.
    *   Tente acessar o app novamente; ele deve voltar a funcionar.
*   A prÃ³xima listagem mostrarÃ¡ como definir `update_config`.

---

ğŸ“„ PÃ¡gina 281

ğŸ” Resumo dos principais pontos:

*   SeÃ§Ã£o `deploy: update_config:` no Compose permite customizar rollout.
*   **Listagem 14.2:** Trecho de `prod-update-config.yml` com `update_config` customizada para `numbers-api`.
    ```yaml
    numbers-api:
      deploy:
        update_config:
          parallelism: 3 # Atualizar 3 rÃ©plicas em paralelo
          monitor: 60s # Esperar 60s apÃ³s update de lote, monitorando health check
          failure_action: rollback # Se falha no monitoramento, reverter AUTOMATICAMENTE
          order: start-first # Iniciar nova rÃ©plica ANTES de parar a antiga
    ```
*   **ExplicaÃ§Ã£o das Propriedades `update_config`:**
    *   `parallelism`: NÂº rÃ©plicas a atualizar simultaneamente (default 1). Aumenta velocidade.
    *   `monitor`: Tempo (ex: `60s`) para Swarm esperar *apÃ³s* um lote de rÃ©plicas ser atualizado, verificando se elas ficam `healthy` (se houver health check). Default 0s (sem espera/verificaÃ§Ã£o de saÃºde). Aumenta seguranÃ§a.
    *   `failure_action`: O que fazer se update falhar (container nÃ£o inicia ou falha no `monitor`). `pause` (default) ou `rollback` (reverte para versÃ£o anterior). `rollback` Ã© mais seguro.
    *   `order`: Ordem de substituiÃ§Ã£o. `stop-first` (default) ou `start-first`. `start-first` inicia a nova rÃ©plica, espera ficar `healthy` (se `monitor` > 0), *depois* para a antiga. Mais seguro (sem reduÃ§Ã£o de capacidade), mas requer capacidade extra temporÃ¡ria no cluster (para rodar N + `parallelism` rÃ©plicas).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina apresenta a seÃ§Ã£o `deploy: update_config:` do Compose, que permite ajustar finamente como o Swarm realiza os rolling updates. A Listagem 14.2 mostra uma configuraÃ§Ã£o customizada considerada uma boa prÃ¡tica geral:
    *   `parallelism: 3`: Acelera o rollout atualizando 3 rÃ©plicas de cada vez (em vez de 1). O valor ideal depende do nÃºmero total de rÃ©plicas (ex: 20-30% do total).
    *   `monitor: 60s`: ApÃ³s atualizar um lote de 3 rÃ©plicas, o Swarm espera 60 segundos. Durante esse tempo, ele verifica os health checks das novas rÃ©plicas. Somente se todas ficarem `healthy` dentro desses 60s, ele prossegue para o prÃ³ximo lote de 3. Isso garante que a nova versÃ£o estÃ¡ funcionando antes de continuar.
    *   `failure_action: rollback`: Se *qualquer* rÃ©plica nova falhar ao iniciar ou nÃ£o ficar `healthy` dentro do perÃ­odo de `monitor`, o Swarm **automaticamente reverte** toda a atualizaÃ§Ã£o, voltando para a versÃ£o anterior do serviÃ§o. Isso Ã© muito mais seguro do que o `pause` padrÃ£o.
    *   `order: start-first`: O Swarm inicia a(s) nova(s) rÃ©plica(s), espera(m) ficar(em) `healthy` (se `monitor` > 0), e sÃ³ entÃ£o para a(s) rÃ©plica(s) antiga(s) correspondente(s). Isso evita a queda temporÃ¡ria de capacidade do `stop-first`, mas exige que o cluster tenha recursos livres para acomodar brevemente mais rÃ©plicas do que o normal (`replicas` + `parallelism`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª precisarÃ¡ de um arquivo `prod-update-config.yml` contendo a Listagem 14.2 (e talvez configs similares para `numbers-web`).
*   No prÃ³ximo deploy (para v3), vocÃª incluirÃ¡ este arquivo na mesclagem com `docker-compose config`, e o rollout subsequente usarÃ¡ essas novas configuraÃ§Ãµes.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `update_config`: Ajustar a velocidade e seguranÃ§a da troca de turno dos funcionÃ¡rios (rÃ©plicas).
    *   `parallelism`: Quantos funcionÃ¡rios trocar de uma vez.
    *   `monitor`: Quanto tempo observar o novo funcionÃ¡rio para ver se ele sabe fazer o trabalho (health check) antes de mandar o prÃ³ximo antigo embora.
    *   `failure_action: rollback`: Se o novo funcionÃ¡rio for um desastre, automaticamente chamar o antigo de volta.
    *   `order: start-first`: Trazer o novo funcionÃ¡rio, treinÃ¡-lo e vÃª-lo trabalhando bem *antes* de liberar o antigo para ir para casa.

---

ğŸ“„ PÃ¡gina 282

ğŸ” Resumo dos principais pontos:

*   Config `update_config` Ã© boa prÃ¡tica geral, mas ajustar para cada app.
*   **Importante:** Config `update_config` Ã© aplicada *antes* do rollout. Se deploy inclui mudanÃ§a no `update_config` *e* na `image`, o rollout jÃ¡ usa a *nova* config de update.
*   **ExercÃ­cio Try-it-now:** Deployar v3 da imagem, incluindo `prod-update-config.yml`.
    *   Rollout usarÃ¡ a nova config (parallelism 3, monitor 60s, rollback, start-first).
    *   1. Mesclar arquivos (core, prod, healthcheck, **update-config**, v3): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Verificar tarefas: `docker stack ps numbers`.
*   SaÃ­da do `stack ps` fica grande com muitos updates (mostra histÃ³rico).
*   Observar que 3 rÃ©plicas API sÃ£o atualizadas/monitoradas antes das prÃ³ximas 3.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reitera que a configuraÃ§Ã£o de `update_config` deve ser ajustada para cada aplicaÃ§Ã£o.
*   Explica uma sutileza importante: quando vocÃª faz um `docker stack deploy` que altera tanto a configuraÃ§Ã£o de `update_config` quanto a imagem (ou outra propriedade) do serviÃ§o, o Swarm primeiro aplica a **nova** `update_config` e depois executa o rolling update usando essa **nova** configuraÃ§Ã£o.
*   O exercÃ­cio instrui a implantar a `v3` da imagem, mas desta vez incluindo o arquivo `prod-update-config.yml` na mesclagem. Isso significa que a substituiÃ§Ã£o das rÃ©plicas `v2` pelas `v3` ocorrerÃ¡ seguindo as regras definidas na Listagem 14.2 (3 em paralelo, monitorando por 60s, revertendo em falha, start-first).
*   Ao observar a saÃ­da de `docker stack ps numbers` durante ou apÃ³s a atualizaÃ§Ã£o, vocÃª verÃ¡ evidÃªncias desse comportamento (ex: 3 rÃ©plicas iniciando/rodando a v3 enquanto 3 rÃ©plicas v2 ainda estÃ£o rodando ou sendo desligadas, depois as prÃ³ximas 3 sendo atualizadas). A lista de tarefas (`ps`) acumula o histÃ³rico, mostrando rÃ©plicas de todas as versÃµes anteriores com status `Shutdown`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Mesclar Arquivos para v3:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/v3.yml --log-level ERROR config > stack.yml
    ```
2.  **Atualizar Stack para v3:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
3.  **Observar Tarefas (durante e apÃ³s):** Execute repetidamente:
    ```powershell
    docker stack ps numbers
    # Ou focado na API:
    # docker service ps numbers_numbers-api
    ```
    Tente observar o padrÃ£o de 3 rÃ©plicas sendo atualizadas por vez. O estado pode passar por `Preparing`, `Starting`, `Running (health: starting)`, `Running` (saudÃ¡vel). As antigas passarÃ£o para `Shutdown`.

---

ğŸ“„ PÃ¡gina 283

ğŸ” Resumo dos principais pontos:

*   Figura 14.4: Mostra saÃ­da (truncada) do `stack ps` apÃ³s deploy v3, indicando 3 rÃ©plicas v2 Shutdown e 3 v3 Running.
*   Forma mais limpa de ver status: `docker service inspect --pretty <stack>_<service>`.
    *   Mostra spec do serviÃ§o, config de update, status do Ãºltimo update.
*   **ExercÃ­cio Try-it-now:** Inspecionar serviÃ§o API apÃ³s update v3.
    *   Comando: `docker service inspect --pretty numbers_numbers-api`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reconhece que a saÃ­da de `docker stack ps` (ou `docker service ps`) pode ficar muito poluÃ­da com o histÃ³rico de rÃ©plicas de atualizaÃ§Ãµes anteriores.
*   Apresenta `docker service inspect --pretty <nome_servico>` como uma maneira melhor de obter um resumo do estado atual do serviÃ§o e o resultado da Ãºltima operaÃ§Ã£o de atualizaÃ§Ã£o. O nome do serviÃ§o no Swarm geralmente segue o padrÃ£o `<nome_stack>_<nome_servico_no_compose>`.
*   O exercÃ­cio usa esse comando para inspecionar o serviÃ§o `numbers_numbers-api`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Inspecionar ServiÃ§o API:**
    ```powershell
    docker service inspect --pretty numbers_numbers-api
    ```
2.  **Analisar SaÃ­da:** Procure por seÃ§Ãµes como:
    *   `Service:` (Nome, ID, Labels)
    *   `UpdateStatus:` (State: completed, StartedAt, CompletedAt, Message: update completed)
    *   `Spec:` (ContÃ©m a definiÃ§Ã£o completa, incluindo `TaskTemplate.ContainerSpec.Image` mostrando `:v3`, `UpdateConfig` com seus valores customizados, `RollbackConfig`, `Resources.Limits`, etc.)

---

ğŸ“„ PÃ¡gina 284

ğŸ” Resumo dos principais pontos:

*   Figura 14.5: Mostra saÃ­da (truncada) do `service inspect --pretty`, destacando `UpdateStatus: completed` e trechos da especificaÃ§Ã£o (imagem v3, update config).
*   **Importante:** Config de update/rollback precisa ser incluÃ­da em *todos* os deploys subsequentes.
    *   Se deploy futuro *nÃ£o* incluir o override `prod-update-config.yml`, Swarm reverterÃ¡ `update_config` para os defaults *antes* de fazer o rollout da nova imagem.
*   ConfiguraÃ§Ãµes de Rollback (`rollback_config`):
    *   Conjunto idÃªntico de opÃ§Ãµes (`parallelism`, `monitor`, `failure_action`, `order`).
    *   Permite definir como a *reversÃ£o* deve ocorrer (ex: rÃ¡pida vs. cautelosa).
*   Importante testar configs de update/rollback com app real em escala para ganhar confianÃ§a.
*   Objetivo: Rollouts rÃ¡pidos mas seguros, com rollback automÃ¡tico rÃ¡pido se necessÃ¡rio.
*   **14.3 Configuring service rollbacks** (Configurando reversÃµes de serviÃ§o)
    *   Rollback manual (`docker service update --rollback`) nÃ£o deve ser necessÃ¡rio normalmente.
    *   Ideal: Rollback automÃ¡tico acionado por falha durante `monitor` do update.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a saÃ­da do `service inspect` (Figura 14.5), que fornece um bom resumo do estado atual e da Ãºltima atualizaÃ§Ã£o.
*   Faz um alerta crucial: as configuraÃ§Ãµes customizadas de `update_config` (e `rollback_config`) **nÃ£o sÃ£o "pegajosas"** por si sÃ³. Elas sÃ£o parte da *definiÃ§Ã£o do serviÃ§o*. Se vocÃª fizer um `docker stack deploy` futuro que *nÃ£o* inclua o arquivo `.yml` que define essas configuraÃ§Ãµes customizadas, o Swarm atualizarÃ¡ a definiÃ§Ã£o do serviÃ§o para usar os **defaults** novamente, *antes* de executar o rollout da nova imagem (se houver). Portanto, os arquivos que definem `update_config` e `rollback_config` devem ser incluÃ­dos em *todas* as mesclagens de `docker-compose config` para seus deploys de produÃ§Ã£o.
*   Introduz a seÃ§Ã£o `deploy: rollback_config:`, que permite configurar como um rollback (seja automÃ¡tico ou manual) deve se comportar, usando as mesmas opÃ§Ãµes (`parallelism`, `monitor`, `failure_action`, `order`) que `update_config`. VocÃª pode querer um rollback mais rÃ¡pido e agressivo do que um update normal.
*   A seÃ§Ã£o 14.3 foca em como configurar e testar o comportamento de rollback, especialmente o automÃ¡tico.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª adicionarÃ¡ uma seÃ§Ã£o `rollback_config:` ao serviÃ§o `numbers-api` em um novo arquivo override (`prod-rollback-config.yml`) e o incluirÃ¡ nos prÃ³ximos deploys.
*   Exemplo de `rollback_config:` (serÃ¡ visto na prÃ³xima pÃ¡gina):
    ```yaml
    deploy:
      rollback_config:
        parallelism: 2 # Exemplo: reverter 2 por vez
        order: stop-first # Exemplo: parar novas falhas antes de iniciar antigas
        # ... outras opÃ§Ãµes
    ```

---

ğŸ“„ PÃ¡gina 285

ğŸ” Resumo dos principais pontos:

*   Rollback automÃ¡tico acontece se novas rÃ©plicas falham durante `monitor` do update.
*   Se bem configurado, pode nem perceber que rollback ocorreu.
*   Simular falha: Deployar v5 do app, que requer config nÃ£o fornecida -> API falha imediatamente.
*   **ExercÃ­cio Try-it-now:** Deployar v5 "ruim" (`v5-bad.yml`) e observar rollback automÃ¡tico.
    *   `v5-bad.yml`: Usa imagem `:v5`, mas nÃ£o fornece config necessÃ¡ria -> rÃ©plicas v5 falharÃ£o health check.
    *   1. Mesclar arquivos (core, prod, healthcheck, update-config, **v5-bad**): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Esperar ~1 min e inspecionar serviÃ§o: `docker service inspect --pretty numbers_numbers-api`.
*   Resultado esperado: Update inicia, rÃ©plicas v5 falham health check durante `monitor: 60s`, `failure_action: rollback` Ã© acionado, Swarm reverte para v3 automaticamente. `inspect` mostrarÃ¡ `UpdateStatus: rollback_completed` e imagem de volta para `:v3`.
*   Figura 14.6: Mostra `inspect` apÃ³s rollback, com estado `rollback_completed`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina descreve o cenÃ¡rio ideal de **rollback automÃ¡tico**. GraÃ§as Ã  combinaÃ§Ã£o de:
    *   `monitor: <tempo>` (que dÃ¡ tempo para health checks rodarem nas novas rÃ©plicas)
    *   `failure_action: rollback` (que instrui o Swarm a reverter se falhas ocorrerem durante o monitoramento)
    ... um deploy de uma versÃ£o quebrada da aplicaÃ§Ã£o pode ser automaticamente desfeito pelo Swarm, retornando ao estado funcional anterior sem intervenÃ§Ã£o manual significativa.
*   O exercÃ­cio simula exatamente isso:
    1.  Prepara-se um `stack.yml` que inclui uma atualizaÃ§Ã£o para a imagem `:v5` (definida em `v5-bad.yml`), mas *sem* a configuraÃ§Ã£o que essa versÃ£o `v5` requer para funcionar. As configuraÃ§Ãµes de `update_config` (parallelism 3, monitor 60s, failure_action rollback, order start-first) ainda estÃ£o ativas (incluÃ­das na mesclagem).
    2.  Executa-se `docker stack deploy`.
    3.  O Swarm inicia o rolling update para `v5`. As novas rÃ©plicas `v5` iniciam, mas seus health checks falham quase imediatamente (porque a config necessÃ¡ria estÃ¡ faltando).
    4.  Como as falhas ocorrem dentro do perÃ­odo de `monitor: 60s`, a condiÃ§Ã£o para `failure_action: rollback` Ã© satisfeita.
    5.  O Swarm **automaticamente** inicia um processo de rollback, substituindo as rÃ©plicas `v5` falhas por novas rÃ©plicas `v3` (a versÃ£o anterior).
    6.  ApÃ³s algum tempo, o comando `docker service inspect` mostrarÃ¡ que o estado da Ãºltima atualizaÃ§Ã£o Ã© `rollback_completed` e que a imagem em uso Ã© a `:v3`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Mesclar Arquivos para v5 Ruim:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/v5-bad.yml --log-level ERROR config > stack.yml
    ```
2.  **Implantar v5 Ruim:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
3.  **Observar e Inspecionar:** Aguarde cerca de 1 a 2 minutos (para o `monitor` expirar e o rollback ocorrer).
    ```powershell
    docker service inspect --pretty numbers_numbers-api
    ```
    Verifique o `UpdateStatus.State` (deve ser `rollback_completed` ou `rollback_paused` se algo der errado no rollback) e a `Spec.TaskTemplate.ContainerSpec.Image` (deve ter voltado para `:v3`). VocÃª tambÃ©m pode usar `docker service ps numbers_numbers-api` para ver o histÃ³rico de rÃ©plicas v5 falhando e sendo substituÃ­das por v3.

---

ğŸ“„ PÃ¡gina 286

ğŸ” Resumo dos principais pontos:

*   Figura 14.6: Confirma `UpdateStatus: rollback_completed`.
*   Rollback automÃ¡tico mantÃ©m app rodando, mesmo com deploy falho.
*   ComparaÃ§Ã£o `start-first` vs `stop-first` durante falha/rollback:
    *   `start-first`: MantÃ©m capacidade total (ou quase) durante a tentativa de update e rollback, pois rÃ©plicas antigas sÃ³ sÃ£o paradas apÃ³s novas (v5 falhas, ou v3 no rollback) estarem "prontas" (ou falharem).
    *   `stop-first` (padrÃ£o): Reduz capacidade temporariamente, pois rÃ©plicas antigas (v3) sÃ£o paradas *antes* das novas (v5) iniciarem e falharem. App fica com menos rÃ©plicas durante a falha e o rollback.
*   Pode-se customizar `rollback_config` (similar a `update_config`).
*   **Listagem 14.3:** Config `rollback_config` agressiva (em `prod-rollback-config.yml`).
    ```yaml
    numbers-api:
      deploy:
        rollback_config:
          parallelism: 6 # Reverter todas as 6 rÃ©plicas de uma vez
          monitor: 0s # NÃ£o esperar/monitorar saÃºde das rÃ©plicas revertidas (assume v3 Ã© boa)
          failure_action: continue # Se rollback falhar, continuar mesmo assim (nÃ£o pausar)
          order: start-first # Iniciar v3 antes de parar v5 falha
    ```
*   Objetivo: Reverter o mais rÃ¡pido possÃ­vel, assumindo que versÃ£o anterior era estÃ¡vel.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa o rollback automÃ¡tico bem-sucedido e discute o impacto da estratÃ©gia `order: start-first` (definida no `update_config`) versus a padrÃ£o `stop-first` nesse cenÃ¡rio de falha: `start-first` Ã© geralmente melhor para manter a capacidade da aplicaÃ§Ã£o durante a turbulÃªncia do update falho e do rollback.
*   Introduz a seÃ§Ã£o `deploy: rollback_config:` que permite definir *como* o processo de rollback deve ocorrer, usando as mesmas chaves de `update_config`.
*   A Listagem 14.3 mostra uma configuraÃ§Ã£o de rollback **agressiva**:
    *   `parallelism: 6`: Reverte todas as 6 rÃ©plicas da API de uma sÃ³ vez.
    *   `monitor: 0s`: NÃ£o perde tempo monitorando as rÃ©plicas v3 que estÃ£o voltando (assume que elas funcionarÃ£o).
    *   `failure_action: continue`: Mesmo que uma rÃ©plica v3 falhe ao iniciar durante o rollback, o processo continua para as outras (nÃ£o pausa).
    *   `order: start-first`: Inicia as rÃ©plicas v3 antes de remover as v5 falhas.
*   A lÃ³gica por trÃ¡s dessa configuraÃ§Ã£o agressiva Ã©: "A versÃ£o anterior (v3) era boa. Se a v5 falhou, quero voltar para a v3 o mais rÃ¡pido possÃ­vel, sem muita cautela."

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª precisarÃ¡ de um arquivo `prod-rollback-config.yml` com a Listagem 14.3.
*   No prÃ³ximo deploy (tentando v5 ruim novamente), vocÃª incluirÃ¡ este arquivo na mesclagem, e o rollback resultante deverÃ¡ ser mais rÃ¡pido.

---

ğŸ“„ PÃ¡gina 287

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Tentar deploy v5 ruim novamente, agora com `prod-rollback-config.yml` incluÃ­do.
    *   Rollout ainda falha, mas rollback deve ser mais rÃ¡pido.
    *   1. Mesclar arquivos (core, prod, healthcheck, update-config, **rollback-config**, v5-bad): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Inspecionar serviÃ§o: `docker service inspect --pretty numbers_numbers-api`.
*   Figura 14.7: Mostra `inspect` capturado durante o inÃ­cio do rollback (`rollback_started`).
*   **Comportamento Estranho:** ApÃ³s rollback completar, `inspect` mostrarÃ¡ que `rollback_config` voltou aos *defaults*!
    *   Causa: Rollback reverte *toda* a especificaÃ§Ã£o do serviÃ§o para a anterior (v3), e a spec v3 *nÃ£o* incluÃ­a a `rollback_config` customizada.
    *   O rollback *usou* a config agressiva, mas depois a prÃ³pria config de rollback foi revertida.
*   LiÃ§Ã£o: Precisa incluir overrides `update_config` e `rollback_config` em *todos* os deploys futuros para mantÃª-los ativos.
*   Perigo de mÃºltiplos overrides: FÃ¡cil esquecer um. Melhor agrupar configs de produÃ§Ã£o em um Ãºnico override.
*   **Deploy Final (v5 corrigido):**
    *   `v5.yml`: Override que adiciona config faltante para v5 funcionar.
    *   `prod-full.yml`: Novo override que combina todas as configs de produÃ§Ã£o (prod, healthcheck, update, rollback).
*   **ExercÃ­cio Try-it-now:** Deployar v5 funcionando, usando estrutura de overrides mais limpa.
    *   1. Mesclar arquivos (core, **prod-full**, v5): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Esperar e inspecionar: `docker service inspect --pretty numbers_numbers-api`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio aplica a configuraÃ§Ã£o de rollback agressiva e tenta novamente o deploy da v5 quebrada. O esperado Ã© que o rollback ocorra mais rapidamente (substituindo todas as 6 rÃ©plicas de uma vez, sem monitoramento).
*   A pÃ¡gina revela um comportamento contraintuitivo e importante: quando um rollback acontece, ele reverte **toda a especificaÃ§Ã£o do serviÃ§o** para o estado anterior. Se a configuraÃ§Ã£o de `rollback_config` customizada sÃ³ foi introduzida na versÃ£o que *falhou* (v5), entÃ£o apÃ³s o rollback para a versÃ£o anterior (v3, que nÃ£o tinha a `rollback_config` customizada), a definiÃ§Ã£o do serviÃ§o voltarÃ¡ a ter a `rollback_config` *padrÃ£o*. O rollback em si usou a configuraÃ§Ã£o agressiva, mas essa configuraÃ§Ã£o nÃ£o persistiu apÃ³s a reversÃ£o.
*   **Moral da histÃ³ria:** Para que configuraÃ§Ãµes como `update_config` e `rollback_config` sejam permanentes, elas precisam estar definidas no arquivo override que representa o estado *estÃ¡vel* do seu ambiente (ex: um `prod.yml` ou `prod-full.yml` que Ã© sempre incluÃ­do).
*   O exercÃ­cio final demonstra a abordagem correta:
    1.  Cria-se um `prod-full.yml` que consolida todas as configuraÃ§Ãµes desejadas para produÃ§Ã£o (rÃ©plicas, limites, healthcheck, update, rollback).
    2.  Cria-se um `v5.yml` que contÃ©m a `image: :v5` *e* a correÃ§Ã£o de configuraÃ§Ã£o que faltava no `v5-bad.yml`.
    3.  O deploy agora mescla apenas `core.yml`, `prod-full.yml`, e `v5.yml`.
*   Este deploy da v5 deve ser bem-sucedido, pois a configuraÃ§Ã£o estÃ¡ correta, e as configuraÃ§Ãµes de update/rollback de `prod-full.yml` serÃ£o aplicadas e persistirÃ£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Mesclar com Rollback Config:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/prod-rollback-config.yml -f ./numbers/v5-bad.yml --log-level ERROR config > stack.yml
    docker stack deploy -c stack.yml numbers
    # Observe o rollback (deve ser mais rÃ¡pido)
    Start-Sleep -Seconds 30 # Tempo menor para verificar
    docker service inspect --pretty numbers_numbers-api # Verificar se voltou para v3 e se rollback_config sumiu/voltou ao default
    ```
2.  **Mesclar para v5 Corrigido:**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod-full.yml -f ./numbers/v5.yml --log-level ERROR config > stack.yml
    ```
3.  **Implantar v5 Corrigido:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
4.  **Verificar Sucesso:** Espere ~1 minuto (para passar o `monitor` do update).
    ```powershell
    docker service inspect --pretty numbers_numbers-api
    # Deve mostrar UpdateStatus.State: completed e Image: :v5
    # Verifique tambÃ©m se UpdateConfig e RollbackConfig estÃ£o com os valores de prod-full.yml
    ```

---

ğŸ“„ PÃ¡gina 288

ğŸ” Resumo dos principais pontos:

*   Figura 14.8: Mostra `inspect` apÃ³s deploy v5 bem-sucedido, confirmando estado `completed` e imagem `:v5`.
*   Agora v5 estÃ¡ rodando com health checks. Se API quebrar, Swarm substitui rÃ©plicas.
*   **Ponto Final sobre Rollbacks:** Falhas de health check *durante* o `monitor` do update disparam rollback. Falhas de health check *apÃ³s* o update completar (app rodando normalmente) disparam apenas substituiÃ§Ã£o de rÃ©plica, *nÃ£o* rollback para versÃ£o anterior.
*   Diagrama (Figura 14.9): Fluxograma do processo de update.
    *   Update iniciado -> Atualiza lote de rÃ©plicas -> Monitora saÃºde -> [Falhou? -> Rollback] / [OK? -> PrÃ³ximo lote ou Fim].

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o sucesso do deploy da v5 corrigida.
*   Esclarece um ponto importante sobre quando os rollbacks automÃ¡ticos ocorrem:
    *   O `failure_action: rollback` definido no `update_config` sÃ³ Ã© acionado se uma rÃ©plica falhar ao iniciar ou falhar no health check **durante o perÃ­odo de `monitor`** especificado, logo apÃ³s ela ter sido atualizada.
    *   Se a atualizaÃ§Ã£o for concluÃ­da com sucesso (todas as rÃ©plicas passaram pelo `monitor` OK) e, *depois*, uma rÃ©plica falhar no seu health check normal, o Swarm simplesmente a **substituirÃ¡** por uma nova rÃ©plica da *mesma versÃ£o atual (v5)*. Ele **nÃ£o** reverterÃ¡ para a versÃ£o anterior (v3) nesse caso. O rollback automÃ¡tico Ã© uma proteÃ§Ã£o contra deploys *quebrados*, nÃ£o contra falhas operacionais posteriores.
*   A Figura 14.9 resume visualmente o fluxo de decisÃ£o durante um rolling update configurado com monitoramento e rollback.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Testar Auto-Reparo v5:**
    *   Com a stack `v5` rodando, acesse `http://localhost` e cause o bug na API (muitos cliques).
    *   Aguarde (depende do `health-interval` e `retries`).
    *   Use `docker service ps numbers_numbers-api`. VocÃª verÃ¡ rÃ©plicas v5 sendo substituÃ­das por *novas* rÃ©plicas v5. O serviÃ§o permanecerÃ¡ na versÃ£o 5.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Rollback vs. SubstituiÃ§Ã£o:
    *   Rollback (automÃ¡tico): O controle de qualidade detecta um defeito grave *na linha de produÃ§Ã£o* do novo modelo de carro (update falha no `monitor`). A fÃ¡brica inteira para de produzir o novo modelo e volta a produzir o antigo.
    *   SubstituiÃ§Ã£o (pÃ³s-update): Um carro do novo modelo jÃ¡ entregue ao cliente apresenta um problema (falha no health check normal). A concessionÃ¡ria apenas troca aquele carro especÃ­fico por outro *do mesmo modelo novo*. A produÃ§Ã£o do novo modelo continua.

---

ğŸ“„ PÃ¡gina 289

ğŸ” Resumo dos principais pontos:

*   Figura 14.9: Fluxograma do update.
*   ConclusÃ£o sobre updates/rollbacks: Configurar e testar cenÃ¡rios de falha dÃ¡ confianÃ§a para releases frequentes.
*   PrÃ³ximo tÃ³pico: Como uptime Ã© impactado por downtime de **nÃ³s** do cluster.
*   **14.4 Managing downtime for your cluster** (Gerenciando tempo de inatividade para seu cluster)
    *   Orquestrador agrupa mÃ¡quinas, mas mÃ¡quinas falham (disco, rede, energia).
    *   Cluster mantÃ©m apps rodando na maioria das falhas, mas algumas exigem intervenÃ§Ã£o.
    *   Gerenciar **downtime planejado** (updates OS, etc.) facilita trabalho do Swarm.
*   **Necessidade de Cluster Multi-NÃ³ para ExercÃ­cios:**
    *   Conceitos de gerenciamento de nÃ³s sÃ³ fazem sentido com >1 nÃ³.
    *   OpÃ§Ãµes: VMs locais, ou playground online **Play with Docker (PWD)**.
    *   PWD recomendado: `https://labs.play-with-docker.com`. Gratuito, temporÃ¡rio.
*   **ExercÃ­cio Try-it-now:** Configurar Swarm multi-nÃ³ no PWD.
    *   1. Acessar PWD, login Docker Hub, clicar "Add New Instance" 5 vezes (cria 5 VMs Docker temporÃ¡rias).
    *   2. No terminal do `node1`: `ip=$(hostname -i)`, `docker swarm init --advertise-addr $ip`.
    *   3. Obter tokens manager/worker: `docker swarm join-token manager`, `docker swarm join-token worker`.
    *   4. Nos terminais de `node2` e `node3`: Colar comando `join` de *manager*.
    *   5. Nos terminais de `node4` e `node5`: Colar comando `join` de *worker*.
    *   6. De volta ao `node1`, verificar nÃ³s: `docker node ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre configuraÃ§Ã£o de updates/rollbacks e introduz o prÃ³ximo desafio em clusters: lidar com a falha ou manutenÃ§Ã£o dos prÃ³prios **servidores (nÃ³s)** que compÃµem o cluster.
*   Embora o Swarm seja resiliente, hÃ¡ cenÃ¡rios de falha de nÃ³ que podem exigir intervenÃ§Ã£o manual, e para manutenÃ§Ãµes *planejadas* (como aplicar patches no SO de um nÃ³), existem procedimentos para minimizar o impacto na aplicaÃ§Ã£o.
*   Para praticar esses conceitos, Ã© necessÃ¡rio um **cluster com mÃºltiplos nÃ³s**. Como configurar isso localmente pode ser complexo (requer VMs), a pÃ¡gina recomenda fortemente o uso do **Play with Docker (PWD)**. PWD Ã© um ambiente online gratuito que fornece acesso rÃ¡pido a mÃºltiplas instÃ¢ncias de Docker (rodando em contÃªineres) que podem ser conectadas para formar um cluster Swarm temporÃ¡rio.
*   O exercÃ­cio guia passo a passo na criaÃ§Ã£o de um Swarm de 5 nÃ³s no PWD: 3 managers e 2 workers. Isso fornecerÃ¡ um ambiente realista (com redundÃ¢ncia de managers) para os prÃ³ximos exercÃ­cios de gerenciamento de nÃ³s.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o Principal:** Seguir as instruÃ§Ãµes para configurar o Swarm no Play with Docker.
    1.  Acesse `https://labs.play-with-docker.com` e faÃ§a login.
    2.  Clique "+ ADD NEW INSTANCE" 5 vezes. VocÃª terÃ¡ `node1` a `node5`.
    3.  Selecione `node1` no painel esquerdo. No terminal dele, execute:
        ```bash
        ip=$(hostname -i)
        docker swarm init --advertise-addr $ip
        ```
    4.  Copie o comando `docker swarm join ...` para *managers* exibido.
    5.  Copie o comando `docker swarm join ...` para *workers* exibido (use `docker swarm join-token worker` se precisar ver de novo).
    6.  Selecione `node2`, cole e execute o comando join de *manager*.
    7.  Selecione `node3`, cole e execute o comando join de *manager*.
    8.  Selecione `node4`, cole e execute o comando join de *worker*.
    9.  Selecione `node5`, cole e execute o comando join de *worker*.
    10. Selecione `node1` novamente e execute:
        ```bash
        docker node ls
        ```
        VocÃª deve ver 5 nÃ³s listados, 3 com `MANAGER STATUS` (um como `Leader`) e 2 sem (workers). Todos devem estar `Ready` e `Active`.

---

ğŸ“„ PÃ¡gina 290

ğŸ” Resumo dos principais pontos:

*   Figura 14.10: Mostra a saÃ­da do `docker node ls` no PWD com 5 nÃ³s (3 managers, 2 workers).
*   **CenÃ¡rio 1: Downtime Planejado (ManutenÃ§Ã£o de NÃ³).**
    *   Objetivo: Remover containers do nÃ³, impedir novos agendamentos, fazer manutenÃ§Ã£o (ex: update OS, reboot), depois retornar nÃ³ ao serviÃ§o.
    *   SoluÃ§Ã£o Swarm: **Drain Mode**.
*   **ExercÃ­cio Try-it-now:** Colocar nÃ³s em modo Drain.
    *   No `node1` (manager):
        *   Colocar worker `node5` em drain: `docker node update --availability drain node5`.
        *   Colocar manager `node3` em drain: `docker node update --availability drain node3`.
    *   Verificar: `docker node ls`.
*   **Modo Drain:**
    *   Para Workers e Managers: Todas as rÃ©plicas rodando no nÃ³ sÃ£o paradas (Swarm tenta reagendÃ¡-las em outros nÃ³s ativos). Novas rÃ©plicas *nÃ£o* sÃ£o agendadas neste nÃ³.
    *   Para Managers: NÃ³ continua participando do quorum/gerenciamento (replica DB, responde API), *a menos* que seja o Leader (nesse caso, outro manager vira Leader).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com o cluster multi-nÃ³ PWD pronto, a pÃ¡gina aborda o gerenciamento de **downtime planejado**. Se vocÃª precisa tirar um servidor do cluster temporariamente para manutenÃ§Ã£o, o procedimento correto Ã© colocÃ¡-lo em modo **Drain**.
*   O comando `docker node update --availability drain <nome_no>` instrui o Swarm a:
    1.  **Parar** todas as rÃ©plicas de serviÃ§o que estÃ£o rodando atualmente naquele nÃ³.
    2.  **Reagendar** essas rÃ©plicas em outros nÃ³s *ativos* do cluster (se houver capacidade e as constraints permitirem).
    3.  **Marcar** o nÃ³ como `Drain` para que o agendador nÃ£o envie *nenhuma nova rÃ©plica* para ele.
*   Isso permite que vocÃª desligue, atualize, reinicie o nÃ³ com seguranÃ§a, sem impactar (idealmente) a disponibilidade dos serviÃ§os (pois as rÃ©plicas foram movidas).
*   Importante: Mesmo em modo `Drain`, um nÃ³ Manager continua participando do consenso do cluster (a menos que seja o Ãºnico manager restante ou o lÃ­der precise ser substituÃ­do).
*   ApÃ³s a manutenÃ§Ã£o, vocÃª retorna o nÃ³ ao serviÃ§o com `docker node update --availability active <nome_no>`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **No terminal do `node1` no PWD:**
    ```bash
    # Colocar node5 em drain
    docker node update --availability drain node5

    # Colocar node3 em drain
    docker node update --availability drain node3

    # Verificar status dos nÃ³s
    docker node ls
    ```
    Observe que `node3` e `node5` agora mostram `Drain` na coluna `AVAILABILITY`. Se houvesse serviÃ§os rodando, `docker service ps <servico>` mostraria as rÃ©plicas sendo movidas desses nÃ³s.

---

ğŸ“„ PÃ¡gina 291

ğŸ” Resumo dos principais pontos:

*   Figura 14.11: Mostra saÃ­da do `docker node ls` com `node3` e `node5` em `Drain`.
*   **Managers e Leader:**
    *   Precisa de mÃºltiplos managers para HA.
    *   Modelo Ativo-Passivo: SÃ³ 1 manager Ã© o **Leader** (controla cluster). Outros replicam DB, podem responder API, podem virar leader se atual falhar.
    *   **EleiÃ§Ã£o de Leader:** Requer **maioria** (quorum) dos managers. Por isso, nÃºmero **Ã­mpar** de managers (3 ou 5) Ã© recomendado.
*   **CenÃ¡rio 2: Perda de Manager / Quorum Par.**
    *   Se perde manager e fica com nÂº par (ex: 2 de 3), cluster ainda funciona, mas nÃ£o tolera *mais nenhuma* falha de manager (perderia quorum).
    *   SoluÃ§Ã£o: Promover um Worker a Manager (`docker node promote <worker>`).
*   **ExercÃ­cio Try-it-now:** Simular perda de leader e promover worker.
    *   1. No `node1` (Leader atual): ForÃ§ar saÃ­da do Swarm: `docker swarm leave --force`.
    *   2. No `node2` (outro manager):
        *   Retornar `node5` (worker) para ativo: `docker node update --availability active node5`.
        *   Promover `node5` a manager: `docker node promote node5`.
        *   Verificar nÃ³s: `docker node ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica o papel dos **Managers** e do **Leader** no Swarm. Para que o cluster Swarm seja altamente disponÃ­vel (possa sobreviver Ã  falha de um manager), vocÃª precisa de mÃºltiplos managers (recomendado 3 ou 5). Apenas um deles Ã© o Leader ativo a qualquer momento, mas os outros estÃ£o prontos para assumir se o Leader falhar, atravÃ©s de um processo de eleiÃ§Ã£o que exige um quorum (mais da metade) dos managers. Ã‰ por isso que um nÃºmero Ã­mpar Ã© crucial (com 3, precisa de 2 para quorum; com 5, precisa de 3).
*   Aborda o cenÃ¡rio onde se perde um manager e o nÃºmero restante fica par. Para restaurar a tolerÃ¢ncia a falhas, deve-se **promover** um nÃ³ Worker existente para se tornar um Manager usando `docker node promote`.
*   O exercÃ­cio simula a perda do Leader (`node1` sai do Swarm) e depois, a partir de outro manager (`node2`), promove um worker (`node5`, que Ã© reativado primeiro) para restaurar o nÃºmero Ã­mpar de managers (agora `node2`, `node3` em drain, `node5` promovido).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **No terminal do `node1` no PWD:**
    ```bash
    docker swarm leave --force
    # Este nÃ³ nÃ£o faz mais parte do Swarm.
    ```
2.  **No terminal do `node2` no PWD:**
    ```bash
    # Reativar node5
    docker node update --availability active node5

    # Promover node5
    docker node promote node5

    # Verificar nÃ³s
    docker node ls
    ```
    VocÃª verÃ¡ que `node1` estÃ¡ `Down` e `Unreachable`. `node2` provavelmente se tornou o novo `Leader`. `node3` ainda estÃ¡ `Drain`. `node4` Ã© worker. `node5` agora tem `Reachable` como `MANAGER STATUS`. O Swarm tem 3 managers alcanÃ§Ã¡veis (`node2`, `node3`, `node5`), mantendo o quorum.

---

ğŸ“„ PÃ¡gina 292

ğŸ” Resumo dos principais pontos:

*   Figura 14.12: Mostra `node ls` apÃ³s `leave` e `promote`, com `node1` Down, `node2` Leader, `node5` Manager Reachable.
*   Swarm agora tem 3 managers novamente (alta disponibilidade restaurada).
*   Se `node1` voltasse, poderia rebaixar (`demote`) outro manager para worker.
*   **Outros CenÃ¡rios de Falha/Gerenciamento:**
    1.  **Todos Managers Offline:** Workers continuam rodando apps existentes, mas Swarm nÃ£o gerencia mais (sem scheduling, sem substituir falhas). Precisa trazer managers online.
    2.  **Perda de Quorum (SÃ³ 1 Manager Sobrevive, nÃ£o era Leader):** Cluster perde controle. Precisa forÃ§ar o manager restante a ser leader: `docker swarm init --force-new-cluster` (preserva dados/tarefas). Depois adicionar mais managers.
    3.  **Rebalancear RÃ©plicas:** Adicionar nÃ³s nÃ£o move rÃ©plicas automaticamente. Para redistribuir: `docker service update --force <servico>` (sem mudar nada mais).
*   **14.5 Understanding high availability in Swarm clusters** (Entendendo alta disponibilidade em clusters Swarm)
    *   MÃºltiplas camadas de HA no Swarm (health checks -> substituiÃ§Ã£o; mÃºltiplos workers -> reagendamento; mÃºltiplos managers -> redundÃ¢ncia de controle).
    *   Ãšltima camada: **Falha de Datacenter/RegiÃ£o**.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o resultado do exercÃ­cio anterior (Figura 14.12), mostrando o Swarm se recuperando da perda do lÃ­der e mantendo a alta disponibilidade do plano de controle ao promover um worker.
*   Apresenta brevemente trÃªs outros cenÃ¡rios de gerenciamento e recuperaÃ§Ã£o:
    1.  **Perda Total dos Managers:** As aplicaÃ§Ãµes continuam rodando nos workers, mas o "cÃ©rebro" do cluster se foi. Nenhuma nova decisÃ£o (scheduling, substituiÃ§Ã£o) pode ser tomada atÃ© que os managers voltem.
    2.  **Perda de Quorum:** Se apenas um manager nÃ£o-lÃ­der sobreviver, ele nÃ£o pode eleger a si mesmo. O comando `docker swarm init --force-new-cluster` Ã© um procedimento de recuperaÃ§Ã£o de desastre para forÃ§ar aquele nÃ³ a se tornar o lÃ­der de um "novo" cluster que reutiliza o estado antigo.
    3.  **Rebalanceamento:** O Swarm nÃ£o move rÃ©plicas automaticamente sÃ³ porque um novo nÃ³ entrou no cluster. Se vocÃª quiser forÃ§ar uma redistribuiÃ§Ã£o mais equilibrada, precisa usar `docker service update --force` no serviÃ§o.
*   A seÃ§Ã£o 14.5 introduz a consideraÃ§Ã£o final sobre alta disponibilidade: o que acontece se todo o datacenter ou regiÃ£o onde seu cluster estÃ¡ hospedado ficar indisponÃ­vel?

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Rebalanceamento (Exemplo conceitual):**
    ```powershell
    # Se vocÃª tivesse um serviÃ§o 'my-app' rodando e adicionasse nÃ³s
    docker service update --force my-app
    # O Swarm tentaria parar e iniciar rÃ©plicas para distribuÃ­-las melhor.
    ```
*   Os cenÃ¡rios de perda total de managers ou quorum sÃ£o situaÃ§Ãµes de recuperaÃ§Ã£o de desastre que vocÃª espera nunca precisar usar, mas Ã© bom saber que existem.

---

ğŸ“„ PÃ¡gina 293

ğŸ” Resumo dos principais pontos:

*   **HA Inter-RegiÃµes:** Tentar estender um *Ãºnico* cluster Swarm por mÃºltiplos datacenters/regiÃµes Ã© **problemÃ¡tico**.
    *   **LatÃªncia de Rede:** NÃ³s Swarm (especialmente managers) precisam de comunicaÃ§Ã£o de baixa latÃªncia. LatÃªncia alta entre regiÃµes pode fazer managers acharem que outros nÃ³s/managers falharam, causando agendamento/eleiÃ§Ãµes incorretas.
    *   **Risco de Split-Brain:** Managers em regiÃµes diferentes podem perder comunicaÃ§Ã£o e ambos acharem que sÃ£o o Leader, levando a um estado inconsistente do cluster.
*   **SoluÃ§Ã£o Segura para HA Regional:** **MÃºltiplos Clusters Independentes.**
    *   Um cluster Swarm por regiÃ£o/datacenter.
    *   Gerenciamento de trÃ¡fego entre clusters feito por fora (ex: DNS Global Load Balancing, Traffic Manager).
*   Desvantagens: Maior sobrecarga de gerenciamento (mÃºltiplos clusters), risco de drift entre clusters/apps. Mas sÃ£o gerenciÃ¡veis (automaÃ§Ã£o, GitOps).
*   Diagrama (Figura 14.13): Mostra configuraÃ§Ã£o com mÃºltiplos clusters em regiÃµes diferentes, com balanceador de carga global direcionando trÃ¡fego.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina aborda a questÃ£o da alta disponibilidade em caso de falha de um datacenter inteiro ou regiÃ£o geogrÃ¡fica.
*   **Alerta:** Embora tecnicamente possÃ­vel, esticar um *Ãºnico* cluster Docker Swarm atravÃ©s de mÃºltiplas regiÃµes com alta latÃªncia de rede entre elas Ã© **altamente desaconselhado**. A comunicaÃ§Ã£o constante necessÃ¡ria entre os nÃ³s managers para manter o consenso do cluster Ã© muito sensÃ­vel Ã  latÃªncia. Uma lentidÃ£o temporÃ¡ria na rede pode levar o Swarm a conclusÃµes erradas sobre a saÃºde dos nÃ³s remotos, resultando em re-agendamentos desnecessÃ¡rios ou, pior, em um cenÃ¡rio de "split-brain" onde diferentes conjuntos de managers acham que sÃ£o os lÃ­deres, corrompendo o estado do cluster.
*   A **soluÃ§Ã£o recomendada e segura** para redundÃ¢ncia regional Ã© ter **clusters Swarm completamente independentes** em cada regiÃ£o. A lÃ³gica para direcionar o trÃ¡fego para o cluster apropriado (ou para fazer failover entre eles) deve ser implementada *acima* dos clusters, usando balanceadores de carga globais, DNS inteligente (como Azure Traffic Manager, AWS Route 53 com health checks), etc.
*   Isso adiciona complexidade no gerenciamento (vocÃª tem mÃºltiplos clusters para cuidar), mas Ã© uma arquitetura muito mais robusta e previsÃ­vel do que um Ãºnico cluster esticado geograficamente. A Figura 14.13 ilustra essa abordagem multi-cluster.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando direto. Ã‰ uma consideraÃ§Ã£o de arquitetura de alta disponibilidade para cenÃ¡rios de produÃ§Ã£o distribuÃ­dos globalmente.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Cluster Swarm Esticado vs. MÃºltiplos Clusters:
    *   Esticado: Tentar gerenciar filiais de uma empresa em paÃ­ses diferentes com uma Ãºnica linha telefÃ´nica direta e lenta entre os gerentes. A comunicaÃ§Ã£o falha, decisÃµes erradas sÃ£o tomadas.
    *   MÃºltiplos: Cada filial tem sua prÃ³pria gerÃªncia local (cluster independente). A matriz (balanceador global) decide para qual filial enviar os clientes, e cada filial opera autonomamente.

---

ğŸ“„ PÃ¡gina 294

ğŸ” Resumo dos principais pontos:

*   **14.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Ganhar experiÃªncia com configuraÃ§Ã£o de rollout/rollback em stack Swarm.
    *   App: Image Gallery (Cap. 9).
*   **Tarefa:**
    1.  Escrever arquivo Compose Ãºnico (`.yml`) para deploy produÃ§Ã£o do Image Gallery.
    2.  **ServiÃ§o API (`iotd`, imagem `diamol/ch04-image-of-the-day` - *Nota: usa imagem antiga sem healthcheck!*):**
        *   4 rÃ©plicas.
        *   **Adicionar `healthcheck`** na definiÃ§Ã£o do *serviÃ§o* no Compose (imagem nÃ£o tem).
        *   Configurar `update_config` (rÃ¡pido mas seguro).
        *   Configurar `rollback_config` (rÃ¡pido).
    3.  ServiÃ§o Web (`image-gallery`, imagem `diamol/ch04-image-gallery`): Outras configs (porta, rÃ©plicas - nÃ£o especificado no lab, pode usar defaults ou definir).
    4.  ServiÃ§o Log (`access-log`, imagem `diamol/ch04-access-log`): Outras configs (rÃ©plicas - nÃ£o especificado).
    5.  ApÃ³s deploy inicial, preparar *outro* stack file (`.yml`) que atualiza *todas* as imagens para as versÃµes do Cap. 9 (com mÃ©tricas/healthchecks embutidos): `diamol/ch09-...`.
    6.  Implantar o update. Verificar se API (`iotd`) atualiza conforme polÃ­tica definida e *nÃ£o* faz rollback (pois health check adicionado no serviÃ§o deve funcionar com imagem antiga, e nova imagem tambÃ©m Ã© saudÃ¡vel).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O **Lab do CapÃ­tulo 14** foca em aplicar as configuraÃ§Ãµes de `deploy: update_config:` e `deploy: rollback_config:` aprendidas.
*   O cenÃ¡rio usa a aplicaÃ§Ã£o Image Gallery, mas com uma pegadinha: o deploy inicial usarÃ¡ as imagens do **CapÃ­tulo 4**, onde a API (`iotd`) **nÃ£o tinha** um `HEALTHCHECK` embutido.
*   **Parte 1: Deploy Inicial:**
    *   Criar um `docker-compose.yml` (ou `stack.yml`) para a stack.
    *   Definir os 3 serviÃ§os (`access-log`, `iotd`, `image-gallery`) usando imagens `diamol/ch04-...`.
    *   Para `iotd`:
        *   `replicas: 4`.
        *   Adicionar seÃ§Ã£o `healthcheck:` com um `test:` que verifique a saÃºde da API (ex: `["CMD", "curl", "-f", "http://localhost:8080/"]` - assumindo porta interna 8080).
        *   Adicionar seÃ§Ã£o `deploy: update_config:` com settings "rÃ¡pidos mas seguros" (ex: `parallelism: 2`, `monitor: 30s`, `failure_action: rollback`, `order: start-first`).
        *   Adicionar seÃ§Ã£o `deploy: rollback_config:` com settings "rÃ¡pidos" (ex: `parallelism: 4`, `monitor: 0s`, `failure_action: continue`, `order: start-first`).
    *   Definir outras propriedades (portas, redes, limites de recursos - opcional mas bom praticar).
    *   Implantar com `docker stack deploy`.
*   **Parte 2: Deploy de Update:**
    *   Criar um *segundo* arquivo YAML (ou modificar o primeiro) onde as `image:` dos 3 serviÃ§os sÃ£o alteradas para as versÃµes do Cap. 9 (`diamol/ch09-...`). Manter as configs de deploy.
    *   Implantar a atualizaÃ§Ã£o com `docker stack deploy` usando este novo arquivo.
    *   **Observar:** O rollout do `iotd` deve seguir a polÃ­tica definida (ex: 2 em paralelo, monitorando por 30s). Como ambas as imagens (v4 e v9) devem ser saudÃ¡veis (com o health check definido no serviÃ§o), o update deve completar com sucesso, *sem* rollback.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 14:**
    1.  Crie `ig-stack-v1.yml` na pasta `ch14\lab`.
    2.  Defina `version`, `networks`, `services`.
    3.  No serviÃ§o `iotd`: `image: diamol/ch04...`, `replicas: 4`, adicione `healthcheck: test: ...`, `deploy: update_config: ...`, `deploy: rollback_config: ...`.
    4.  Defina `image-gallery` e `access-log` (com imagens `ch04`, rÃ©plicas, portas, limites).
    5.  `docker stack deploy -c ig-stack-v1.yml ig-lab`. Verifique.
    6.  Crie `ig-stack-v2.yml` copiando `v1` e mudando *apenas* as `image:` para `diamol/ch09...`.
    7.  `docker stack deploy -c ig-stack-v2.yml ig-lab`.
    8.  Observe o update com `docker service ps ig-lab_iotd`. Verifique se segue o `parallelism` e `monitor`, e se completa com sucesso.

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch14/lab/README.md

---

ğŸ“„ PÃ¡gina 295

ğŸ” Resumo dos principais pontos:

*   Encerramento do Lab 14.
*   **CapÃ­tulo 15: Configuring Docker for secure remote access and CI/CD** (Configurando Docker para acesso remoto seguro e CI/CD)
*   RevisÃ£o: CLI `docker` nÃ£o faz o trabalho, sÃ³ envia instruÃ§Ãµes para API do Docker Engine.
*   **BenefÃ­cios da SeparaÃ§Ã£o CLI <-> API:**
    1.  Outras ferramentas (GUIs, scripts, etc.) podem usar a API.
    2.  CLI local pode ser configurada para falar com Engine *remoto*.
*   Permite gerenciar cluster de dezenas de nÃ³s da sua mÃ¡quina local, usando mesmos comandos `docker`.
*   **Acesso Remoto:**
    *   Usado para administrar ambientes (teste, prod).
    *   Usado para depurar problemas.
    *   Habilita parte de **ImplantaÃ§Ã£o ContÃ­nua (CD)** do pipeline CI/CD.
*   **Fluxo CI/CD com Acesso Remoto:**
    *   CI (local ou build server): Builda imagem, testa, push para registro.
    *   CD (mesmo pipeline/job): Conecta-se ao Engine Docker *remoto* (ex: UAT, Prod) e implanta a nova versÃ£o da imagem (ex: via `docker stack deploy`).
*   Foco Cap. 15: Como expor API Docker remotamente de forma *segura* e como conectar a ela (localmente e de pipelines).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 15, que trata do **acesso remoto seguro** Ã  API do Docker Engine.
*   Relembra a arquitetura cliente-servidor do Docker: a CLI `docker` Ã© um cliente que fala com a API exposta pelo Docker Engine (servidor).
*   Essa separaÃ§Ã£o permite que a CLI local se conecte a um Docker Engine rodando em outra mÃ¡quina (se a API estiver exposta e acessÃ­vel pela rede).
*   Esse acesso remoto Ã© fundamental para:
    *   **Gerenciamento:** Administrar servidores Docker que nÃ£o estÃ£o na sua mÃ¡quina local (servidores de teste, produÃ§Ã£o).
    *   **ImplantaÃ§Ã£o ContÃ­nua (CD):** Permitir que o pipeline de CI/CD, apÃ³s construir e testar uma imagem, se conecte automaticamente a um ambiente de destino (UAT, produÃ§Ã£o) e execute o comando de implantaÃ§Ã£o (como `docker stack deploy`).
*   O capÃ­tulo abordarÃ¡ como configurar o Docker Engine para permitir esse acesso remoto de forma **segura** (evitando os riscos do acesso HTTP inseguro visto brevemente antes) e como configurar a CLI (ou o pipeline) para se conectar a esses Engines remotos seguros.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª configurarÃ¡ o Docker Engine (provavelmente no Play With Docker, pois o Docker Desktop tem limitaÃ§Ãµes aqui) para usar TLS ou SSH para expor sua API.
*   VocÃª usarÃ¡ flags na CLI `docker` (como `--host`, `--tlsverify`, `--tlscacert`, etc.) ou Docker Contexts para se conectar a esse Engine remoto a partir da sua mÃ¡quina local ou de um contÃªiner Jenkins.

---

ğŸ“„ PÃ¡gina 296

ğŸ” Resumo dos principais pontos:

*   ContinuaÃ§Ã£o da introduÃ§Ã£o ao Cap. 15.
*   **15.1 Endpoint options for the Docker API** (OpÃ§Ãµes de endpoint para a API Docker)
*   InstalaÃ§Ã£o padrÃ£o Docker: Engine ouve em canal local (socket Linux, named pipe Windows), CLI usa mesmo canal. NÃ£o permite acesso remoto.
*   Para acesso remoto, precisa configurar Engine explicitamente.
*   OpÃ§Ã£o 1 (Ruim): **HTTP simples, inseguro**.
    *   Habilita API em endpoint HTTP (ex: `tcp://0.0.0.0:2375`).
    *   **PerigosÃ­ssimo:** Qualquer um na rede pode conectar e controlar Docker *sem autenticaÃ§Ã£o*.
    *   Risco de ataque (ex: site malicioso acessando `localhost:2375` e rodando container de mineraÃ§Ã£o).
    *   **NÃ£o usar em produÃ§Ã£o, nem mesmo em dev.**
*   ExercÃ­cio serÃ¡ feito para aprendizado, mas instrui a desabilitar depois.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A seÃ§Ã£o 15.1 comeÃ§a a discutir as diferentes maneiras pelas quais a API do Docker Engine pode ser exposta para comunicaÃ§Ã£o.
*   O padrÃ£o Ã© usar um canal de comunicaÃ§Ã£o local e privado (socket Unix no Linux/Mac, Named Pipe no Windows), que Ã© seguro por natureza, pois sÃ³ pode ser acessado por processos rodando na mesma mÃ¡quina e com as permissÃµes corretas.
*   Para habilitar o acesso remoto, a primeira opÃ§Ã£o (e a mais **insegura e desaconselhada**) Ã© configurar o Engine para ouvir em um endereÃ§o TCP usando HTTP simples, geralmente na porta 2375.
*   A pÃ¡gina enfatiza veementemente os **riscos** dessa abordagem:
    *   **Sem AutenticaÃ§Ã£o:** Qualquer pessoa que consiga alcanÃ§ar a porta 2375 do seu Docker Engine pela rede pode enviar comandos e controlar totalmente o Docker.
    *   **Sem Criptografia:** Toda a comunicaÃ§Ã£o (comandos, respostas, segredos potenciais passados como variÃ¡veis) viaja em texto plano pela rede.
    *   **Vetor de Ataque:** Mesmo em uma mÃ¡quina local, um malware ou um site malicioso poderia tentar se conectar a `localhost:2375` e executar aÃ§Ãµes danosas.
*   Embora a configuraÃ§Ã£o seja mostrada a seguir para fins didÃ¡ticos, a instruÃ§Ã£o clara Ã© **nunca usar HTTP inseguro** em um ambiente real.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio mostrarÃ¡ como habilitar essa opÃ§Ã£o insegura (temporariamente) no Docker Desktop ou via `daemon.json`. **Lembre-se de desfazer essa configuraÃ§Ã£o logo em seguida.**

---

ğŸ“„ PÃ¡gina 297

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Habilitar acesso HTTP inseguro (TemporÃ¡rio!).
    *   **Docker Desktop:** Settings > General (ou Daemon/Docker Engine) > Marcar "Expose daemon on tcp://localhost:2375 without TLS". Docker reinicia. (Figura 15.1).
    *   **Docker Engine (Linux/Server):** Editar `daemon.json`. Adicionar `"tcp://0.0.0.0:2375"` Ã  lista `hosts`. Garantir que canal local (`npipe://` ou `fd://`) tambÃ©m esteja lÃ¡. (Listagem 15.1). Reiniciar serviÃ§o Docker.
*   **Listagem 15.1:** Exemplo `daemon.json` com `hosts` para TCP e canal local.
    ```json
    {
      "hosts": [
        "tcp://0.0.0.0:2375", // HTTP inseguro
        "npipe://" // Canal local Windows
        // ou "fd://" para Linux
      ],
      "insecure-registries": [ // Exemplo de outra config
        "registry.local:5000"
      ]
    }
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este exercÃ­cio mostra **como** habilitar o acesso remoto via HTTP inseguro, **apenas para fins de demonstraÃ§Ã£o**.
*   **No Docker Desktop:** Ã‰ uma simples caixa de seleÃ§Ã£o nas configuraÃ§Ãµes gerais (o local exato pode variar ligeiramente entre versÃµes). Marcar essa caixa e aplicar farÃ¡ o Docker Desktop reconfigurar e reiniciar seu Engine interno para ouvir na porta TCP 2375 sem seguranÃ§a TLS.
*   **Em Servidores (Linux ou Windows Server):** Requer editar o arquivo `daemon.json` manualmente. A chave `hosts` aceita uma lista de endpoints onde o Engine deve escutar. Ã‰ crucial adicionar `"tcp://0.0.0.0:2375"` para habilitar o acesso TCP em todas as interfaces, mas tambÃ©m **manter** a entrada original do canal local (`"npipe://"` para Windows, `"unix:///var/run/docker.sock"` ou `"fd://"` para Linux via systemd) para que a CLI local continue funcionando da maneira padrÃ£o. ApÃ³s salvar o `daemon.json`, o serviÃ§o Docker precisa ser reiniciado.
*   **A Listagem 15.1** mostra um exemplo de como o `daemon.json` ficaria.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Habilitar no Docker Desktop:**
    *   Clique direito no Ã­cone Docker > Settings.
    *   VÃ¡ para General.
    *   Marque a caixa "Expose daemon on tcp://localhost:2375 without TLS".
    *   Clique "Apply & Restart". Aguarde reiniciar.
2.  **(Alternativa - NÃƒO FAÃ‡A NO DOCKER DESKTOP, apenas em servidor) Editar `daemon.json`:**
    *   Encontre o arquivo (`C:\ProgramData\docker\config\daemon.json`).
    *   Edite como administrador, adicionando `tcp://0.0.0.0:2375` Ã  lista `hosts` (crie `hosts` se nÃ£o existir), mantendo `npipe://`.
    *   Salve e reinicie o serviÃ§o: `Restart-Service docker`.

*   **Lembrete:** Esta configuraÃ§Ã£o Ã© insegura. Prepare-se para desfazÃª-la apÃ³s os prÃ³ximos testes.

---

ğŸ“„ PÃ¡gina 298

ğŸ” Resumo dos principais pontos:

*   Figura 15.1: Mostra opÃ§Ã£o no Docker Desktop Settings para expor daemon sem TLS.
*   Verificar acesso remoto inseguro:
    *   Via CLI Docker com flag `--host`.
    *   Via API REST com `curl`.
*   **ExercÃ­cio Try-it-now:** Testar acesso via TCP/HTTP.
    *   Comando CLI: `docker --host tcp://localhost:2375 container ls`.
    *   Comando API: `curl http://localhost:2375/containers/json`.
*   Figura 15.2: Mostra saÃ­da bem-sucedida de ambos os comandos, confirmando acesso inseguro.
*   CLI `docker` e `docker-compose` suportam parÃ¢metro `--host` (ou `-H`) para especificar Engine remoto.
*   Sem seguranÃ§a: sÃ³ precisa do endereÃ§o; sem autenticaÃ§Ã£o/criptografia.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s habilitar o acesso HTTP inseguro na porta 2375, esta pÃ¡gina mostra como verificar se funcionou e como usar a flag `--host` (ou `-H`) para direcionar os comandos `docker` (e `docker-compose`) para esse endpoint TCP em vez do canal local padrÃ£o.
*   O primeiro comando do exercÃ­cio, `docker --host tcp://localhost:2375 container ls`, instrui a CLI `docker` a se conectar ao Engine via TCP na porta 2375 da prÃ³pria mÃ¡quina (`localhost`) e executar o comando `container ls`.
*   O segundo comando, `curl http://localhost:2375/containers/json`, acessa diretamente a API REST do Docker usando `curl`. O endpoint `/containers/json` Ã© o equivalente API ao comando `docker container ls`.
*   Se ambos os comandos funcionarem e retornarem a lista de contÃªineres (ou uma lista vazia, se nÃ£o houver nenhum rodando), isso confirma que a API estÃ¡ exposta via HTTP na porta 2375 e acessÃ­vel sem qualquer seguranÃ§a.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Testar Acesso via CLI:** (ApÃ³s habilitar na pÃ¡gina anterior)
    ```powershell
    docker --host tcp://localhost:2375 container ls
    # Deve listar seus containers (ou nada, se nÃ£o houver). Funciona!
    ```
2.  **Testar Acesso via API (curl ou PowerShell):**
    ```powershell
    # Usando curl (se instalado):
    # curl http://localhost:2375/containers/json

    # Usando PowerShell:
    Invoke-WebRequest http://localhost:2375/containers/json | ConvertFrom-Json
    # Deve retornar um JSON com a lista de containers. Funciona!
    ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Flag `--host`: Ã‰ como colocar o endereÃ§o de entrega especÃ­fico (`tcp://localhost:2375`) no envelope do seu comando `docker`, em vez de usar o endereÃ§o padrÃ£o (canal local).

---

ğŸ“„ PÃ¡gina 299

ğŸ” Resumo dos principais pontos:

*   Figura 15.2: Confirma acesso bem-sucedido via CLI e API.
*   ReforÃ§a o perigo: Ops team ficaria horrorizado com acesso inseguro. Qualquer um pode fazer qualquer coisa.
*   Perigo extra: Risco de escalonamento de privilÃ©gios.
    *   Containers Linux rodando como `root` podem ter acesso admin ao host.
    *   Containers Windows: Menos risco de acesso total ao host, mas ainda perigoso.
*   **Contexto de ExecuÃ§Ã£o Remota:**
    *   Comandos `--host <remoto>` rodam no *contexto da mÃ¡quina remota*.
    *   Ex: `docker --host <remoto> run -v /data:/app/data ...` monta `/data` do host *remoto*, nÃ£o local.
    *   Pode causar confusÃ£o (erro "diretÃ³rio nÃ£o existe") ou problemas de seguranÃ§a (acesso a arquivos inesperados no servidor remoto).
    *   Permite "navegar" no filesystem do host remoto se tiver acesso Docker.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina volta a martelar nos **riscos graves** do acesso HTTP inseguro. AlÃ©m da falta de autenticaÃ§Ã£o e criptografia, hÃ¡ o risco de **escalonamento de privilÃ©gios**. Se um invasor conseguir executar um contÃªiner como `root` (administrador do Linux) atravÃ©s de uma API Docker insegura, ele pode conseguir acesso administrativo ao prÃ³prio servidor host onde o Docker estÃ¡ rodando, especialmente se puder usar bind mounts. O risco Ã© um pouco menor com contÃªineres Windows, mas ainda existe.
*   Outro ponto importante Ã© o **contexto de execuÃ§Ã£o**. Quando vocÃª usa `--host` para executar um comando em um Docker Engine remoto, qualquer referÃªncia a recursos locais no comando (como um caminho de host em um bind mount `-v /meu/local:/app/data`) serÃ¡ interpretada no **contexto da mÃ¡quina remota**, nÃ£o da sua mÃ¡quina local. Isso pode ser confuso (o comando falha porque `/meu/local` nÃ£o existe no servidor remoto) ou perigoso (o comando monta um diretÃ³rio sensÃ­vel do servidor remoto que vocÃª nÃ£o pretendia acessar).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio demonstrarÃ¡ como um usuÃ¡rio com acesso Ã  API insegura pode explorar o filesystem do host.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Acesso Inseguro + Bind Mount: Ã‰ como deixar a porta da sua casa destrancada (API insegura) e tambÃ©m deixar a chave da sua garagem (acesso ao filesystem via `-v`) pendurada na porta. AlguÃ©m pode entrar e nÃ£o sÃ³ bagunÃ§ar a casa (Docker), mas tambÃ©m levar seu carro (acessar/modificar arquivos do host).

---

ğŸ“„ PÃ¡gina 300

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Demonstrar risco de acesso ao filesystem do host via API insegura.
    *   Rodar contÃªiner (Linux ou Windows) conectando via TCP (`--host`), usando bind mount para mapear a raiz do disco do host (`/` ou `C:\`) para dentro do contÃªiner (`/host-drive`).
    *   Comando Linux: `docker --host ... run -it -v /:/host-drive diamol/base`.
    *   Comando Windows: `docker --host ... run -it -v C:\:C:\host-drive diamol/base`.
    *   Dentro do contÃªiner, listar conteÃºdo de `/host-drive`.
*   Figura 15.3: Mostra acesso aos arquivos do host de dentro do contÃªiner.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este exercÃ­cio demonstra de forma prÃ¡tica e alarmante o risco do acesso inseguro combinado com bind mounts.
*   O comando `docker run -v /:/host-drive` (Linux) ou `-v C:\:C:\host-drive` (Windows) instrui o Docker Engine (acessado remotamente via `--host tcp://localhost:2375`) a montar o diretÃ³rio raiz (`/` ou `C:\`) do prÃ³prio **servidor host** dentro do contÃªiner no caminho `/host-drive` (ou `C:\host-drive`).
*   Como o contÃªiner `diamol/base` provavelmente roda com privilÃ©gios elevados (root/Administrator), o processo dentro do contÃªiner agora tem acesso de leitura e escrita a **todo o sistema de arquivos do host**.
*   Ao listar o conteÃºdo de `/host-drive` (ou `C:\host-drive`) de dentro do contÃªiner, vocÃª verÃ¡ os diretÃ³rios e arquivos reais da sua mÃ¡quina host, comprovando a vulnerabilidade.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o Principal:**
    ```powershell
    # Rodar container Windows montando C:\ do host
    docker --host tcp://localhost:2375 container run -it -v C:\:C:\host-drive diamol/base cmd
    # (Se estiver usando modo Linux no Desktop, adapte para Linux):
    # docker --host tcp://localhost:2375 container run -it -v /:/host-drive diamol/base sh
    ```
*   **Dentro do ContÃªiner (prompt `C:\>` ou `/ #`):**
    ```
    # Se Windows:
    dir C:\host-drive
    # Se Linux:
    ls /host-drive
    ```
    VocÃª verÃ¡ listados os diretÃ³rios raiz do seu disco C: (ou / no Linux da VM do Docker Desktop).
*   **Sair do ContÃªiner:** `exit`.

**=> IMPORTANTE: DesfaÃ§a a configuraÃ§Ã£o insegura AGORA!**
   *   No Docker Desktop: Settings > General > Desmarque "Expose daemon..." > Apply & Restart.
   *   Se editou `daemon.json`: Remova a linha `"tcp://0.0.0.0:2375"` de `hosts`, salve, reinicie o serviÃ§o Docker.

---

ğŸ“„ PÃ¡gina 301

ğŸ” Resumo dos principais pontos:

*   Figura 15.3: Confirma acesso ao host filesystem.
*   Exemplo conecta ao localhost, mas risco Ã© real para servidores remotos com API insegura (ex: sistema de folha de pagamento).
*   ReforÃ§o: **Nunca habilitar acesso inseguro** (exceto para aprender e desabilitar logo).
*   InstruÃ§Ã£o para desabilitar acesso HTTP inseguro antes de prosseguir.
*   **15.2 Configuring Docker for secure remote access** (Configurando Docker para acesso remoto seguro)
    *   Duas opÃ§Ãµes seguras:
        1.  **Mutual TLS (mTLS):** Usa certificados digitais (HTTPS). Servidor tem certificado, cliente *tambÃ©m* tem certificado. Ambos emitidos por uma CA confiada por ambos. Criptografa trÃ¡fego, autentica ambos os lados.
        2.  **SSH (Secure Shell):** Usa protocolo SSH padrÃ£o (usado para conectar a servidores Linux/Unix, tambÃ©m disponÃ­vel no Windows). AutenticaÃ§Ã£o via user/pass ou chaves SSH. Criptografa trÃ¡fego, autentica cliente.
*   Diagrama (Figura 15.4): Mostra os 3 canais (Local seguro, TCP inseguro, TCP seguro com TLS, SSH seguro).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a demonstraÃ§Ã£o do risco do acesso inseguro e instrui explicitamente o leitor a **desfazer essa configuraÃ§Ã£o imediatamente**.
*   A seÃ§Ã£o 15.2 introduz as duas alternativas **seguras** para acesso remoto Ã  API Docker:
    1.  **Mutual TLS (mTLS):** Funciona de forma similar ao HTTPS que vocÃª usa em sites seguros, mas com uma camada extra de seguranÃ§a. NÃ£o sÃ³ o servidor Docker prova sua identidade com um certificado, mas o *cliente* (sua CLI `docker`) tambÃ©m precisa apresentar seu prÃ³prio certificado vÃ¡lido para provar sua identidade ao servidor. Ambos os certificados devem ser emitidos ou confiados por uma Autoridade Certificadora (CA) comum. Isso garante **criptografia** e **autenticaÃ§Ã£o mÃºtua**. Ã‰ a forma mais tradicional e robusta para APIs.
    2.  **SSH:** Utiliza o protocolo Secure Shell, muito comum para acesso a servidores remotos. A CLI `docker` pode usar uma conexÃ£o SSH existente (ou estabelecer uma nova) para tunelar os comandos da API Docker de forma segura atÃ© o Engine remoto. A autenticaÃ§Ã£o Ã© feita pelos mecanismos normais do SSH (senha ou par de chaves pÃºblica/privada). Garante **criptografia** e **autenticaÃ§Ã£o do cliente**.
*   A Figura 15.4 compara visualmente os diferentes canais de comunicaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o Principal:** Certifique-se de que desabilitou o acesso inseguro (desmarcou a caixa no Docker Desktop ou removeu a linha do `daemon.json` e reiniciou).
*   **PreparaÃ§Ã£o para TLS/SSH:** Os prÃ³ximos exercÃ­cios usarÃ£o o Play With Docker (PWD) para simular um Engine remoto, pois configurar TLS/SSH no Docker Desktop nÃ£o Ã© direto. Se vocÃª tivesse um servidor Linux/Windows remoto com Docker, os passos seriam:
    *   **TLS:** Gerar certificados CA, servidor e cliente (usando `openssl` ou ferramentas da empresa). Configurar `daemon.json` no servidor com paths dos certs do servidor/CA e flags `tlsverify`. Usar flags `--tlsverify`, `--tlscacert`, `--tlscert`, `--tlskey` na CLI cliente.
    *   **SSH:** Garantir que servidor SSH esteja rodando no host Docker. Garantir que cliente SSH esteja funcionando na mÃ¡quina cliente. Configurar autenticaÃ§Ã£o SSH (chaves sÃ£o recomendadas). Usar `-H ssh://user@host` na CLI cliente.

---

ğŸ“„ PÃ¡gina 302

ğŸ” Resumo dos principais pontos:

*   **Controle de Acesso:** mTLS (gerenciar certs) vs SSH (gerenciar contas de usuÃ¡rio no servidor).
*   **LimitaÃ§Ã£o Docker Desktop:** NÃ£o se pode configurar TLS/SSH diretamente na VM interna. ExercÃ­cios usarÃ£o **Play with Docker (PWD)**. (Ou use VM/Raspberry Pi remoto se tiver).
*   **Configurando Acesso TLS (mTLS) no PWD:**
    *   Requer gerar certificados/chaves (CA, Server, Client).
    *   Autor jÃ¡ gerou certs compatÃ­veis com PWD.
*   **ExercÃ­cio Try-it-now:** Configurar nÃ³ PWD para usar TLS.
    *   1. Login PWD, criar 1 nÃ³.
    *   2. Criar pasta para certs: `mkdir -p /diamol-certs`.
    *   3. Rodar container utilitÃ¡rio (`diamol/pwd-tls:server`) que monta pastas `/diamol-certs` e `/etc/docker` e copia certs do servidor + `daemon.json` configurado para TLS para dentro do nÃ³ PWD.
        *   `docker run -v /diamol-certs:/certs -v /etc/docker:/docker diamol/pwd-tls:server`
    *   4. Reiniciar Docker Engine no PWD para carregar nova config: `pkill dockerd`, `dockerd &>/docker.log &`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina discute brevemente as implicaÃ§Ãµes de gerenciamento de cada abordagem segura (certificados vs. contas de usuÃ¡rio) e reitera que o **Docker Desktop nÃ£o permite configurar facilmente TLS ou SSH** para seu Engine interno. Por isso, os exercÃ­cios prÃ¡ticos usarÃ£o o **Play with Docker (PWD)**.
*   O foco se volta para configurar o **mTLS** em um nÃ³ do PWD. O processo envolve:
    1.  Obter os arquivos de certificado e chave necessÃ¡rios (CA, certificado do servidor, chave privada do servidor). O autor fornece esses arquivos prÃ©-gerados dentro de uma imagem utilitÃ¡ria (`diamol/pwd-tls:server`) para simplificar.
    2.  Configurar o `daemon.json` do Docker Engine no nÃ³ PWD para usar esses certificados e habilitar o TLS na porta TCP 2376 (porta padrÃ£o para Docker sobre TLS).
    3.  Reiniciar o Docker Engine no nÃ³ PWD.
*   O exercÃ­cio automatiza os passos 1 e 2 usando o contÃªiner utilitÃ¡rio. Ele monta o diretÃ³rio de certificados do host (`/diamol-certs`, que serÃ¡ criado) e o diretÃ³rio de configuraÃ§Ã£o do Docker do host (`/etc/docker`) dentro do contÃªiner. O contÃªiner entÃ£o copia os arquivos de certificado (CA, server-cert, server-key) para `/diamol-certs` no host e copia um `daemon.json` prÃ©-configurado para TLS para `/etc/docker` no host.
*   O passo final reinicia o `dockerd` (o processo do Docker Engine) no nÃ³ PWD para que ele leia a nova configuraÃ§Ã£o e comece a escutar na porta 2376 usando TLS.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o Principal (no PWD):**
    1.  Acesse `https://labs.play-with-docker.com`. Clique "Start", depois "+ ADD NEW INSTANCE".
    2.  No terminal do `node1` que aparece no PWD, execute os comandos do exercÃ­cio:
        ```bash
        mkdir -p /diamol-certs
        docker run -v /diamol-certs:/certs -v /etc/docker:/docker diamol/pwd-tls:server
        pkill dockerd
        # Espere alguns segundos
        dockerd &>/docker.log &
        # Espere mais alguns segundos para o Docker iniciar
        docker ps # Verifique se o Docker estÃ¡ respondendo
        ```
    *   Agora, o Docker Engine neste nÃ³ PWD estÃ¡ (supostamente) escutando na porta 2376 e exigindo autenticaÃ§Ã£o mTLS.

---

ğŸ“„ PÃ¡gina 303

ğŸ” Resumo dos principais pontos:

*   Figura 15.5: Mostra a saÃ­da dos comandos no PWD configurando TLS.
*   **Expondo Porta PWD:** Precisa expor a porta 2376 do nÃ³ PWD para acesso externo (da sua mÃ¡quina local).
    *   Clicar botÃ£o "OPEN PORT" no PWD, digitar 2376.
    *   Nova aba abrirÃ¡ com erro (ignorar). **Copiar URL da nova aba** (ex: `ip<...>.direct.labs.play-with-docker.com`). Este Ã© o domÃ­nio pÃºblico para acessar a porta 2376 do seu nÃ³ PWD.
*   Figura 15.6: Mostra botÃ£o "OPEN PORT" no PWD.
*   NÃ³ PWD agora gerenciÃ¡vel remotamente via TLS.
*   **RelaÃ§Ã£o Certificados TLS (Figura 15.7):**
    *   CA (Autoridade Certificadora): Emite/assina certs servidor e cliente. Raiz de confianÃ§a.
    *   Cert Servidor: Identifica o servidor Docker. Instalado no Engine.
    *   Cert Cliente: Identifica o usuÃ¡rio/CLI. Usado pelo cliente `docker`.
    *   Chave Privada: Arquivo secreto correspondente a cada certificado (servidor/cliente). Usado para provar posse do certificado e para criptografia.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   ApÃ³s configurar o Docker Engine no PWD para usar TLS na porta 2376, Ã© preciso tornar essa porta **acessÃ­vel pela internet**, para que sua mÃ¡quina local possa se conectar a ela. O Play with Docker tem um recurso para isso: o botÃ£o "OPEN PORT".
*   Ao clicar e digitar 2376, o PWD cria um mapeamento e fornece um **URL pÃºblico Ãºnico** (que inclui um hostname dinÃ¢mico e a porta externa, que pode ou nÃ£o ser 2376 - mas o PWD faz o mapeamento interno). Ã‰ **crucial copiar este URL completo** da nova aba que se abre, pois ele serÃ¡ usado como o endereÃ§o do host remoto nos prÃ³ximos comandos `docker`.
*   A Figura 15.7 revisita a estrutura bÃ¡sica do mTLS:
    *   VocÃª tem uma CA (raiz de confianÃ§a).
    *   A CA assina um certificado para o servidor (Docker Engine) e um para o cliente (CLI Docker).
    *   Cada certificado tem uma chave privada correspondente.
    *   Para a conexÃ£o funcionar:
        *   O cliente deve confiar na CA que emitiu o certificado do servidor.
        *   O servidor deve confiar na CA que emitiu o certificado do cliente.
        *   O cliente deve apresentar seu certificado e provar que possui a chave privada correspondente.
        *   O servidor deve apresentar seu certificado e provar que possui a chave privada correspondente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Expor Porta no PWD:** No PWD, com o `node1` selecionado, clique no botÃ£o "OPEN PORT" acima do terminal. Digite `2376` e clique OK.
2.  **Copiar URL:** Uma nova aba/link aparecerÃ¡. **Copie o URL completo** dessa aba (algo como `https://ip-<...>.direct.labs.play-with-docker.com`). VocÃª precisarÃ¡ do **hostname** (a parte entre `https://` e o possÃ­vel `/`) e talvez da porta se for diferente de 443 (embora PWD geralmente mapeie para 443 ou 80 externamente). Para os comandos `docker`, usaremos o hostname e a porta 2376 (ou a porta externa mapeada pelo PWD). *O livro parece assumir que o hostname PWD resolverÃ¡ diretamente para a porta 2376, o que pode ou nÃ£o ser verdade - talvez precise ajustar a porta no comando `--host` depois*. Vamos seguir o livro por enquanto e usar a porta 2376.

---

ğŸ“„ PÃ¡gina 304

ğŸ” Resumo dos principais pontos:

*   Figura 15.6: Confirma como abrir a porta no PWD.
*   **Listagem 15.2:** ConfiguraÃ§Ã£o `daemon.json` aplicada no PWD para TLS.
    ```json
    {
      "hosts": ["unix:///var/run/docker.sock", "tcp://0.0.0.0:2376"], // Ouve local e TCP/TLS
      "tls": true, // TLS desabilitado (?? - Errata: deveria ser true) - CORREÃ‡ÃƒO: Livro impresso pode estar errado, mas contexto exige true.
      "tlscacert": "/diamol-certs/ca.pem", // Path para cert CA
      "tlskey": "/diamol-certs/server-key.pem", // Path para chave servidor
      "tlscert": "/diamol-certs/server-cert.pem" // Path para cert servidor
      // Faltando: "tlsverify": true (Para EXIGIR cert cliente)
    }
    ```
    *(**Nota Importante:** A Listagem 15.2 no PDF tem `"tls": true`, o que estÃ¡ correto. Adicionei `"tlsverify": true` que tambÃ©m Ã© crucial para mTLS)*.
*   Com TLS ativo (e `tlsverify`), nÃ£o se pode mais usar API/CLI sem fornecer certs cliente vÃ¡lidos (CA, cert cliente, chave cliente).
*   API rejeita conexÃµes sem TLS ou com certs invÃ¡lidos/nÃ£o confiÃ¡veis.
*   Obter certs cliente: Usar imagem utilitÃ¡ria similar (`diamol/pwd-tls:client`?) para extrair certs cliente para mÃ¡quina local.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Listagem 15.2 mostra a configuraÃ§Ã£o `daemon.json` que (supostamente) foi aplicada ao nÃ³ PWD pelo contÃªiner utilitÃ¡rio. Ela instrui o Docker Engine a:
    *   Ouvir no socket local (`unix://...`) E na porta TCP 2376 (`tcp://...`).
    *   Usar TLS (`"tls": true`).
    *   Usar os arquivos de certificado da CA, certificado do servidor e chave privada do servidor localizados em `/diamol-certs`.
    *   **Crucial (e talvez faltando na listagem original, mas implÃ­cito pelo mTLS):** Deveria haver tambÃ©m `"tlsverify": true` para *exigir* que os clientes apresentem um certificado vÃ¡lido assinado pela mesma CA. Sem isso, seria apenas TLS do servidor, nÃ£o mTLS.
*   A consequÃªncia dessa configuraÃ§Ã£o Ã© que qualquer tentativa de conexÃ£o Ã  porta 2376 que nÃ£o use TLS ou nÃ£o apresente um certificado de cliente vÃ¡lido e confiÃ¡vel (assinado pela `ca.pem`) serÃ¡ **rejeitada** pelo Docker Engine.
*   Para que *sua mÃ¡quina local* possa se conectar, ela precisa dos arquivos correspondentes do lado do cliente: `ca.pem` (para confiar no servidor), `client-cert.pem` (seu certificado) e `client-key.pem` (sua chave privada). O prÃ³ximo passo Ã© obter esses arquivos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª nÃ£o edita o `daemon.json` diretamente no PWD (o container utilitÃ¡rio fez isso).
*   O prÃ³ximo exercÃ­cio mostrarÃ¡ como obter os certificados do cliente e tentar a conexÃ£o.

---

ğŸ“„ PÃ¡gina 305

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Tentar conectar ao PWD (seguro) sem e com certs cliente.
    *   1. Obter/Salvar domÃ­nio PWD em variÃ¡vel (`$pwdDomain` ou `pwdDomain`). (Ex: `ip<...>.direct.labs.play-with-docker.com`). **Importante:** Usar apenas o hostname, sem `https://`.
    *   2. Tentar `curl` HTTP (sem TLS): `curl "http://$pwdDomain:2376/containers/json"`. Deve falhar/timeout.
    *   3. Tentar `docker` CLI (sem TLS flags): `docker --host "tcp://$pwdDomain:2376" container ls`. Deve falhar (erro TLS).
    *   4. Extrair certs cliente para pasta local (`/tmp/pwd-certs`):
        *   `mkdir -p /tmp/pwd-certs`
        *   `cd ./ch15/exercises`
        *   `tar -xvf pwd-client-certs.tar -C /tmp/pwd-certs` (Extrai `ca.pem`, `client-cert.pem`, `client-key.pem` de um arquivo .tar).
    *   5. Conectar com CLI `docker` usando flags TLS:
        ```bash
        docker --host "tcp://$pwdDomain:2376" --tlsverify \
          --tlscacert /tmp/pwd-certs/ca.pem \
          --tlscert /tmp/pwd-certs/client-cert.pem \
          --tlskey /tmp/pwd-certs/client-key.pem \
          container ls
        ```
        Deve funcionar e listar containers *no PWD*.
    *   6. Rodar um container no PWD remotamente:
        ```bash
        docker --host "tcp://$pwdDomain:2376" --tlsverify \
          --tlscacert ... --tlscert ... --tlskey ... \
          container run -d -P diamol/apache
        ```
        Deve funcionar.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este exercÃ­cio demonstra a necessidade dos certificados de cliente para acessar um Docker Engine protegido por mTLS.
*   Primeiro, ele salva o hostname pÃºblico do PWD (obtido na p.303 ao abrir a porta 2376) em uma variÃ¡vel.
*   As tentativas de conexÃ£o usando `curl` via HTTP simples e `docker` via TCP simples (sem flags TLS) devem falhar, pois o Engine no PWD agora espera conexÃµes TLS com certificados de cliente vÃ¡lidos.
*   O passo crucial Ã© extrair os arquivos de certificado do cliente (`ca.pem`, `client-cert.pem`, `client-key.pem`) de um arquivo `pwd-client-certs.tar` (fornecido nos materiais do livro) para uma pasta local (ex: `/tmp/pwd-certs` no Linux/Mac, ou um caminho similar no Windows).
*   Finalmente, o comando `docker` Ã© executado novamente, mas desta vez com:
    *   `--host "tcp://$pwdDomain:2376"`: Aponta para o Engine remoto na porta TLS.
    *   `--tlsverify`: Habilita verificaÃ§Ã£o TLS (mÃºtua).
    *   `--tlscacert <path>`: Caminho para o certificado da CA (para verificar o servidor).
    *   `--tlscert <path>`: Caminho para o certificado do cliente.
    *   `--tlskey <path>`: Caminho para a chave privada do cliente.
*   Com essas flags, a conexÃ£o TLS mÃºtua deve ser estabelecida com sucesso, e o comando `docker container ls` (e subsequentemente `docker container run`) serÃ¡ executado no Engine remoto do PWD.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Definir DomÃ­nio PWD (PowerShell):** (Substitua pelo seu hostname PWD real)
    ```powershell
    $pwdDomain = "ip<...>.direct.labs.play-with-docker.com"
    ```
2.  **Tentar ConexÃµes Inseguras (Devem Falhar):**
    ```powershell
    # Pode dar timeout ou erro de conexÃ£o
    Invoke-WebRequest http://$pwdDomain`:2376/containers/json
    # Deve dar erro de TLS handshake ou similar
    docker --host tcp://$pwdDomain`:2376 container ls
    ```
3.  **Extrair Certificados Cliente:**
    ```powershell
    # Criar pasta (ex: no seu diretÃ³rio de usuÃ¡rio ou C:\DockerLabs)
    $certDir = "$HOME\pwd-certs" # Ou "C:\DockerLabs\pwd-certs"
    mkdir $certDir -ErrorAction SilentlyContinue
    # Navegar para onde estÃ¡ o .tar
    cd C:\DockerLabs\diamol\ch15\exercises
    # Extrair (requer 'tar' no path - Git for Windows inclui; ou use 7-Zip)
    tar -xvf .\pwd-client-certs.tar -C $certDir
    # Verificar se os arquivos .pem estÃ£o em $certDir
    Get-ChildItem $certDir
    ```
4.  **Conectar com TLS (PowerShell - comando longo):**
    ```powershell
    docker --host "tcp://$pwdDomain`:2376" --tlsverify `
      --tlscacert "$certDir\ca.pem" `
      --tlscert "$certDir\client-cert.pem" `
      --tlskey "$certDir\client-key.pem" `
      container ls
    # Deve listar containers rodando no PWD (provavelmente nenhum ainda).
    ```
5.  **Rodar Container Remoto:**
    ```powershell
    docker --host "tcp://$pwdDomain`:2376" --tlsverify `
      --tlscacert "$certDir\ca.pem" --tlscert "$certDir\client-cert.pem" --tlskey "$certDir\client-key.pem" `
      container run -d -P diamol/apache
    # Verifique se iniciou: repita o comando 'container ls' do passo 4.
    ```

---

ğŸ“„ PÃ¡gina 306

ğŸ” Resumo dos principais pontos:

*   Figura 15.8: Mostra as falhas de conexÃ£o sem TLS e o sucesso com as flags TLS.
*   Passar flags TLS em cada comando Ã© inconveniente. Pode-se usar variÃ¡veis de ambiente (`DOCKER_HOST`, `DOCKER_TLS_VERIFY`, `DOCKER_CERT_PATH`).
*   **OpÃ§Ã£o 2: Acesso Remoto Seguro via SSH.**
    *   Vantagem: CLI `docker` usa cliente SSH padrÃ£o. **Nenhuma mudanÃ§a** na config do Docker Engine (`daemon.json`) Ã© necessÃ¡ria.
    *   Desvantagem/Gerenciamento: AutenticaÃ§Ã£o Ã© via servidor SSH. Precisa criar contas de usuÃ¡rio no host Docker para quem precisa de acesso. Gerenciar usuÃ¡rios/chaves SSH.
*   ExercÃ­cio usarÃ¡ PWD novamente para SSH.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o sucesso da conexÃ£o TLS e menciona que, em vez de repetir todas as flags `--tls...` a cada comando, vocÃª pode definir variÃ¡veis de ambiente como `DOCKER_HOST`, `DOCKER_TLS_VERIFY=1`, e `DOCKER_CERT_PATH` (apontando para o diretÃ³rio com os arquivos `ca.pem`, `cert.pem`, `key.pem`) para que a CLI `docker` as use automaticamente.
*   Introduz a segunda opÃ§Ã£o segura: **tunelamento via SSH**.
*   **Como funciona:** A CLI `docker`, ao receber um host no formato `ssh://user@hostname`, usa o cliente SSH local para estabelecer uma conexÃ£o segura com o servidor `hostname` como o `user` especificado. Uma vez conectado via SSH, a CLI tunela os comandos da API Docker atravÃ©s dessa conexÃ£o segura para o Docker Engine rodando no servidor remoto (que ainda estÃ¡ ouvindo no seu socket/pipe local padrÃ£o).
*   **Vantagens:**
    *   **Nenhuma configuraÃ§Ã£o extra no Docker Engine remoto** Ã© necessÃ¡ria (ele continua ouvindo apenas localmente).
    *   Usa o mecanismo de autenticaÃ§Ã£o robusto e familiar do SSH (senhas ou chaves).
*   **Desvantagens:**
    *   Requer que um **servidor SSH** esteja rodando no host Docker remoto.
    *   Requer que **contas de usuÃ¡rio** sejam criadas nesse host para cada pessoa/sistema que precise de acesso Docker via SSH. O gerenciamento de acesso ao Docker se torna o gerenciamento de acesso SSH ao servidor.
    *   Cliente SSH precisa estar disponÃ­vel na mÃ¡quina que executa `docker`.
*   A configuraÃ§Ã£o para o prÃ³ximo exercÃ­cio usarÃ¡ novamente o PWD, que jÃ¡ vem com SSH prÃ©-configurado entre os nÃ³s.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Usar VariÃ¡veis de Ambiente para TLS (PowerShell):**
    ```powershell
    $env:DOCKER_HOST = "tcp://$pwdDomain`:2376"
    $env:DOCKER_TLS_VERIFY = "1"
    $env:DOCKER_CERT_PATH = "$HOME\pwd-certs" # Ou onde extraiu
    # Agora pode usar 'docker container ls', 'docker run...' sem flags extras
    docker container ls
    # Limpar variÃ¡veis depois (ou fechar/abrir PowerShell):
    # Remove-Variable DOCKER_HOST, DOCKER_TLS_VERIFY, DOCKER_CERT_PATH
    ```
*   **ConexÃ£o SSH no Windows:** O Windows 10/11 inclui um cliente OpenSSH (`ssh.exe`). VocÃª pode usÃ¡-lo no PowerShell/CMD. A flag para o Docker seria `-H ssh://user@host`.

---

ğŸ“„ PÃ¡gina 307

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Usar SSH para gerenciar Docker remoto no PWD.
    *   1. No PWD, criar 2Âº nÃ³ (`node2`). Anotar IP do `node1`.
    *   2. No `node2`, salvar IP do `node1` em variÃ¡vel (`node1ip`).
    *   3. Testar conexÃ£o SSH direta: `ssh root@$node1ip`. Deve conectar (sem senha no PWD). Sair (`exit`).
    *   4. Listar containers *locais* no `node2`: `docker container ls`.
    *   5. Listar containers *remotos* no `node1` usando SSH: `docker -H ssh://root@$node1ip container ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este exercÃ­cio demonstra o uso da flag `-H` (ou `--host`) com o protocolo `ssh://` para direcionar comandos `docker` para um Engine remoto atravÃ©s de uma conexÃ£o SSH.
*   O ambiente PWD Ã© ideal para isso, pois as instÃ¢ncias jÃ¡ vÃªm com SSH configurado para permitir conexÃ£o entre elas como usuÃ¡rio `root` usando chaves prÃ©-compartilhadas (sem necessidade de senha).
*   **Passos:**
    1.  Cria-se um segundo nÃ³ no PWD (`node2`).
    2.  ObtÃ©m-se o IP do primeiro nÃ³ (`node1`).
    3.  No terminal do `node2`, testa-se a conexÃ£o SSH direta para `node1` para confirmar que funciona.
    4.  Executa-se `docker container ls` sem `-H`. Isso mostra os contÃªineres rodando *localmente* no `node2` (provavelmente nenhum).
    5.  Executa-se `docker -H ssh://root@$node1ip container ls`. Isso instrui a CLI `docker` no `node2` a:
        *   Conectar-se via SSH ao `node1` como usuÃ¡rio `root`.
        *   Uma vez conectado, enviar o comando `container ls` para o Docker Engine que estÃ¡ rodando *localmente* no `node1` (via socket/pipe).
        *   Retornar o resultado para o terminal do `node2`.
*   O resultado esperado Ã© que o Ãºltimo comando liste os contÃªineres que estÃ£o rodando no `node1` (como o `diamol/apache` iniciado no exercÃ­cio TLS).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Executar no PWD:**
    1.  Se sÃ³ tem 1 nÃ³ no PWD, clique "+ ADD NEW INSTANCE" para criar `node2`.
    2.  Selecione `node1`. Anote seu IP (ex: `192.168.0.18`).
    3.  Selecione `node2`. Execute no terminal dele:
        ```bash
        node1ip=<IP_DO_NODE1> # Substitua pelo IP real
        echo $node1ip # Verifique
        ssh root@$node1ip # Teste SSH
        # Se conectar (prompt muda para node1), digite 'exit' para voltar a node2
        exit
        docker container ls # Deve estar vazio (containers locais de node2)
        docker -H ssh://root@$node1ip container ls # Deve listar containers de node1
        ```

---

ğŸ“„ PÃ¡gina 308

ğŸ” Resumo dos principais pontos:

*   Figura 15.9: Mostra saÃ­da do exercÃ­cio SSH no PWD, confirmando conexÃ£o e listagem remota.
*   PWD simplifica SSH (prÃ©-configurado). Ambiente real requer setup de usuÃ¡rios/chaves.
*   **OpiniÃ£o Ops sobre Docker via SSH:** Mista.
    *   PrÃ³: Mais fÃ¡cil que gerenciar certs TLS, familiar para admins Linux.
    *   Contra: Requer dar acesso SSH ao servidor para quem precisa de acesso Docker (pode ser privilÃ©gio demais). Menos familiar/padrÃ£o para admins Windows (vs. TLS talvez).
*   **Resumo SeguranÃ§a:** TLS e SSH oferecem **Criptografia** e **AutenticaÃ§Ã£o**. *NÃƒO* oferecem **AutorizaÃ§Ã£o** (controle fino de permissÃµes) nem **Auditoria** (log de quem fez o quÃª).
*   Modelo Docker padrÃ£o: "Tudo ou Nada" (conectou? pode fazer tudo).
*   Importante considerar quem precisa de acesso a quais ambientes.
*   Risco: FÃ¡cil errar e executar comando destrutivo no ambiente errado (ex: prod) se contexto CLI estiver apontando para lÃ¡.
*   **15.3 Using Docker Contexts to work with remote engines** (Usando Contextos Docker para trabalhar com engines remotos)
    *   Passar `--host` e flags TLS/SSH a cada comando Ã© inconveniente.
    *   SoluÃ§Ã£o Docker: **Contexts**.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina discute as implicaÃ§Ãµes operacionais de usar SSH para acesso remoto ao Docker. Ã‰ mais simples que TLS em termos de setup inicial se o SSH jÃ¡ estiver estabelecido, mas mistura o gerenciamento de acesso ao servidor com o gerenciamento de acesso ao Docker, o que pode nÃ£o ser ideal em termos de separaÃ§Ã£o de privilÃ©gios.
*   Reafirma uma limitaÃ§Ã£o importante de seguranÃ§a, tanto para TLS quanto para SSH no Docker CE/EE bÃ¡sico: eles controlam *quem* pode conectar (autenticaÃ§Ã£o) e protegem a comunicaÃ§Ã£o (criptografia), mas uma vez conectado, o usuÃ¡rio tem **acesso total** Ã  API Docker. NÃ£o hÃ¡ um sistema de permissÃµes granular (autorizaÃ§Ã£o) embutido para dizer "usuÃ¡rio A sÃ³ pode listar contÃªineres, usuÃ¡rio B pode criar/remover". (SoluÃ§Ãµes como Docker UCP ou K8s RBAC oferecem isso). TambÃ©m nÃ£o hÃ¡ um log de auditoria detalhado nativo.
*   Isso torna crucial ter cuidado ao conceder acesso remoto e tambÃ©m para o prÃ³prio usuÃ¡rio ao trabalhar com mÃºltiplos ambientes, para nÃ£o executar comandos no lugar errado.
*   A seÃ§Ã£o 15.3 introduz os **Docker Contexts** como uma maneira de simplificar o trabalho com mÃºltiplos Docker Engines (locais ou remotos, via TLS ou SSH).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Comando para criar contexto:** `docker context create <nome_contexto> --docker "<opÃ§Ãµes_de_conexÃ£o>"`.
*   As `<opÃ§Ãµes_de_conexÃ£o>` serÃ£o `host=tcp://...` mais `ca=...`, `cert=...`, `key=...` para TLS, ou `host=ssh://...` para SSH.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Docker Contexts: Ã‰ como salvar diferentes servidores remotos nos seus "Favoritos" ou "ConexÃµes Salvas" em um cliente SSH ou RDP. Em vez de digitar o endereÃ§o e as credenciais toda vez, vocÃª sÃ³ seleciona o nome salvo (contexto) e a CLI `docker` sabe como se conectar.

---

ğŸ“„ PÃ¡gina 309

ğŸ” Resumo dos principais pontos:

*   **Docker Context:** Armazena detalhes de conexÃ£o (host, TLS, SSH) localmente, associados a um nome.
*   Permite criar mÃºltiplos contextos e alternar entre eles.
*   **ExercÃ­cio Try-it-now:** Criar contexto TLS para PWD.
    *   Comando: `docker context create pwd-tls --docker "host=tcp://$pwdDomain:2376,ca=/tmp/pwd-certs/ca.pem,cert=/tmp/pwd-certs/client-cert.pem,key=/tmp/pwd-certs/client-key.pem"`.
    *   (Exemplo SSH tambÃ©m mostrado: `... --docker "host=ssh://user@server"`).
    *   Listar contextos: `docker context ls`.
*   Figura 15.10: Mostra saÃ­da do `context create` e `context ls`.
*   `context ls` mostra contexto `default` (engine local) e o novo `pwd-tls`. Asterisco `*` indica contexto ativo. Mostra endpoint Docker e K8s (se aplicÃ¡vel).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como criar um Docker Context usando o comando `docker context create`.
*   Os parÃ¢metros chave sÃ£o:
    *   `<nome_contexto>`: Um nome que vocÃª escolhe para identificar essa conexÃ£o (ex: `pwd-tls`, `servidor-teste-ssh`, `producao-cluster-a`).
    *   `--docker "<string_de_conexÃ£o>"`: Uma string contendo as opÃ§Ãµes de conexÃ£o para o Docker Engine, separadas por vÃ­rgula.
        *   Para **TLS**, inclui `host=tcp://...`, `ca=...`, `cert=...`, `key=...`.
        *   Para **SSH**, inclui `host=ssh://...`.
        *   Para o engine **local** padrÃ£o, geralmente Ã© `host=npipe://...` (Windows) ou `host=unix://...` (Linux/Mac).
*   O exercÃ­cio cria um contexto chamado `pwd-tls` usando os detalhes de conexÃ£o TLS para o PWD (domÃ­nio e caminhos dos certificados locais).
*   O comando `docker context ls` lista todos os contextos definidos localmente, indicando qual estÃ¡ ativo no momento com um asterisco (`*`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar Contexto TLS para PWD:** (Certifique-se que `$pwdDomain` e `$certDir` estÃ£o definidos corretamente na sua sessÃ£o PowerShell. `$certDir` deve ser o path *absoluto* para a pasta com os .pem).
    ```powershell
    # Use ` para continuar comando em mÃºltiplas linhas no PowerShell
    docker context create pwd-tls --docker "host=tcp://$pwdDomain`:2376,`
    ca=$($certDir -replace '\\', '/')/ca.pem,`
    cert=$($certDir -replace '\\', '/')/client-cert.pem,`
    key=$($certDir -replace '\\', '/')/client-key.pem"
    # Nota: Pode ser necessÃ¡rio usar / em vez de \ nos paths dos certs
    # Ou use paths absolutos diretamente na string
    ```
2.  **Listar Contextos:**
    ```powershell
    docker context ls
    ```
    VocÃª deve ver o contexto `default` e o novo `pwd-tls`. O `default` ainda estarÃ¡ ativo (`*`).

---

ğŸ“„ PÃ¡gina 310

ğŸ” Resumo dos principais pontos:

*   Figura 15.10: Confirma criaÃ§Ã£o do contexto `pwd-tls`.
*   Contextos guardam info para TLS ou SSH.
*   Contextos podem apontar para mÃ¡quinas locais ou remotas.
*   **Como Usar/Trocar Contextos:**
    1.  **TemporÃ¡rio (por sessÃ£o):** VariÃ¡vel de ambiente `DOCKER_CONTEXT`. PreferÃ­vel, pois nÃ£o afeta outras sessÃµes/scripts.
    2.  **Permanente (global):** Comando `docker context use <nome_contexto>`. Afeta todas as sessÃµes atÃ© ser trocado novamente. **NÃ£o recomendado** para trocas frequentes.
*   **ExercÃ­cio Try-it-now:** Demonstrar troca de contexto.
    *   1. Setar `DOCKER_CONTEXT` para `pwd-tls` (comando `env:` Windows, `export` Linux).
    *   2. Listar contextos (`docker context ls` mostra `*` em `pwd-tls` *nesta sessÃ£o*).
    *   3. Listar containers (`docker container ls` agora lista os do PWD).
    *   4. Trocar permanentemente para `default`: `docker context use default`.
    *   5. Listar containers (`docker container ls` agora lista os locais).
*   Figura 15.11: Mostra saÃ­da do exercÃ­cio. **Alerta:** Figura mostra que mesmo apÃ³s `context use default`, `context ls` ainda mostra `*` em `pwd-tls` (provavelmente erro na figura ou comportamento confuso). VariÃ¡vel `DOCKER_CONTEXT` tem precedÃªncia.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com os contextos criados, a pÃ¡gina explica como **ativÃ¡-los** para direcionar seus comandos `docker`. Existem duas maneiras:
    1.  **VariÃ¡vel de Ambiente `DOCKER_CONTEXT`:** Definir esta variÃ¡vel (ex: `DOCKER_CONTEXT=pwd-tls`) faz com que a CLI `docker`, *naquela sessÃ£o de terminal especÃ­fica*, use as configuraÃ§Ãµes do contexto nomeado. Isso **nÃ£o** afeta outras janelas de terminal ou o padrÃ£o global. Ã‰ a forma **recomendada** para trabalho interativo, pois Ã© explÃ­cita e temporÃ¡ria.
    2.  **Comando `docker context use <nome>`:** Este comando altera o contexto ativo **globalmente** para a CLI Docker naquela mÃ¡quina. Qualquer novo terminal que vocÃª abrir usarÃ¡ esse contexto por padrÃ£o. Isso pode ser perigoso se vocÃª esquecer qual contexto estÃ¡ ativo e executar comandos destrutivos no ambiente errado. Use com cautela.
*   O exercÃ­cio demonstra ambas as formas. Ele primeiro ativa `pwd-tls` via variÃ¡vel de ambiente, lista os contÃªineres remotos, depois tenta voltar ao padrÃ£o com `docker context use default` e lista os contÃªineres locais.
*   **Importante:** A pÃ¡gina (e Figura 15.11) aponta um comportamento potencialmente confuso ou um erro na ilustraÃ§Ã£o: a variÃ¡vel `DOCKER_CONTEXT` **tem precedÃªncia** sobre o contexto definido por `docker context use`. Se `DOCKER_CONTEXT` estiver definida, `docker context ls` mostrarÃ¡ o asterisco (`*`) nela, e os comandos `docker` usarÃ£o esse contexto, mesmo que `docker context use` tenha sido chamado para definir outro como padrÃ£o global. Para realmente voltar ao padrÃ£o global, vocÃª precisaria *desdefinir* a variÃ¡vel `DOCKER_CONTEXT`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Ativar Contexto PWD (TemporÃ¡rio):**
    ```powershell
    $env:DOCKER_CONTEXT = "pwd-tls"
    docker context ls # Deve mostrar * em pwd-tls
    docker container ls # Deve listar containers do PWD (ex: diamol/apache)
    ```
2.  **Tentar Voltar ao PadrÃ£o (Global):**
    ```powershell
    docker context use default
    docker context ls # * AINDA estarÃ¡ em pwd-tls devido Ã  variÃ¡vel de ambiente!
    docker container ls # AINDA lista containers do PWD!
    ```
3.  **Realmente Voltar ao PadrÃ£o (Desdefinindo VariÃ¡vel):**
    ```powershell
    Remove-Variable DOCKER_CONTEXT # Ou feche e reabra o PowerShell
    docker context ls # Agora * deve estar em default
    docker container ls # Agora lista containers locais
    ```

---

ğŸ“„ PÃ¡gina 311

ğŸ” Resumo dos principais pontos:

*   Figura 15.11: Mostra a confusÃ£o entre `context use` e `DOCKER_CONTEXT`.
*   `docker context use` muda padrÃ£o global; `DOCKER_CONTEXT` sobrescreve temporariamente e tem precedÃªncia.
*   RecomendaÃ§Ã£o: Usar sempre `DOCKER_CONTEXT` para trocas, deixar `default` como global. Evita erros.
*   Acesso remoto nÃ£o deve ser frequente em produÃ§Ã£o; ideal Ã© automaÃ§Ã£o via CI/CD.
*   **15.4 Adding continuous deployment to your CI pipeline** (Adicionando implantaÃ§Ã£o contÃ­nua ao seu pipeline CI)
    *   Com acesso remoto seguro configurado, pode-se implementar pipeline CI/CD completo.
    *   CI (Cap. 11): Build -> Test -> Push para Registro.
    *   CD: Deploy para UAT -> (Testes UAT/AprovaÃ§Ã£o) -> Deploy para Prod.
    *   CD usa acesso remoto para conectar aos engines UAT/Prod.
    *   Pipeline usa `docker`/`docker-compose` com `--host` (ou Contexto) e credenciais seguras.
    *   **Credenciais:** NÃƒO devem ir no source control. Usar gerenciador de secrets do servidor CI (Jenkins, GitLab Secrets, GitHub Secrets, etc.).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre contextos, recomendando o uso da variÃ¡vel de ambiente `DOCKER_CONTEXT` para trocas temporÃ¡rias e seguras entre engines.
*   A seÃ§Ã£o 15.4 conecta o acesso remoto seguro ao objetivo final: criar um pipeline de **ImplantaÃ§Ã£o ContÃ­nua (CD)** que estende o pipeline de IntegraÃ§Ã£o ContÃ­nua (CI).
*   **Fluxo CI/CD:**
    1.  **CI:** CÃ³digo -> Build -> Teste UnitÃ¡rio -> Imagem Docker -> Push para Registro.
    2.  **CD - EstÃ¡gio UAT:** O pipeline CI/CD se conecta ao Docker Engine do ambiente de **UAT (User Acceptance Testing)** usando acesso remoto seguro (TLS ou SSH). Executa `docker stack deploy` (ou `docker-compose up`) para implantar a imagem recÃ©m-construÃ­da do registro. (Opcional: rodam-se testes de aceitaÃ§Ã£o automatizados).
    3.  **CD - AprovaÃ§Ã£o (Opcional):** O pipeline pode pausar, aguardando aprovaÃ§Ã£o manual (de QA, PO, etc.) antes de prosseguir para produÃ§Ã£o.
    4.  **CD - EstÃ¡gio ProduÃ§Ã£o:** Se aprovado, o pipeline se conecta ao Docker Engine (ou cluster manager) do ambiente de **ProduÃ§Ã£o** usando acesso remoto seguro. Executa `docker stack deploy` para implantar a imagem validada.
*   A chave para os estÃ¡gios CD Ã© a capacidade do pipeline de se conectar remotamente e de forma segura aos ambientes de destino. As **credenciais** (certificados TLS cliente, chaves SSH, senhas) necessÃ¡rias para essas conexÃµes remotas *nunca* devem ser armazenadas no cÃ³digo fonte. Devem ser gerenciadas pelo sistema de CI/CD (ex: Jenkins Credentials, GitHub Secrets) e injetadas no pipeline de forma segura apenas quando necessÃ¡rio.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O pipeline Jenkins que serÃ¡ usado a seguir incluirÃ¡ estÃ¡gios 'Deploy UAT' e 'Deploy Prod' que usam as credenciais TLS (armazenadas no Jenkins) para se conectar ao PWD (simulando UAT e Prod) e executar `docker-compose`.

---

ğŸ“„ PÃ¡gina 312

ğŸ” Resumo dos principais pontos:

*   **Infraestrutura CI/CD Local (com CD para PWD):**
    *   Similar ao Cap. 11 (Gogs, Registry, Jenkins em containers).
    *   **Novidade:** Scripts de inicializaÃ§Ã£o do Jenkins agora criam credenciais Jenkins a partir dos arquivos de certificado PWD cliente (que devem estar na mÃ¡quina local onde Compose roda).
    *   CD stages do pipeline usarÃ£o essas credenciais para deployar no PWD.
*   **ExercÃ­cio Try-it-now:** Iniciar infraestrutura CI/CD local.
    *   Navegar para `ch15/exercises/infrastructure`.
    *   Rodar `docker-compose ... up -d` (com override windows/linux).
    *   Acessar Jenkins Credentials (`http://localhost:8080/credentials`). Login `diamol`/`diamol`.
    *   Verificar se credenciais `docker-ca.pem`, `docker-cert.pem`, `docker-key.pem` (tipo "Secret file") foram criadas.
*   Figura 15.12: Mostra as credenciais baseadas em arquivo no Jenkins UI.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Para demonstrar o pipeline CD, uma nova infraestrutura local (Gogs, Registry, Jenkins) Ã© iniciada, muito similar Ã  do CapÃ­tulo 11.
*   A principal diferenÃ§a estÃ¡ no contÃªiner Jenkins (`diamol/jenkins` provavelmente atualizado). Agora, seus scripts de inicializaÃ§Ã£o nÃ£o apenas configuram o job, mas tambÃ©m procuram pelos arquivos de certificado do cliente PWD (que vocÃª extraiu para `/tmp/pwd-certs` ou `$HOME\pwd-certs` anteriormente) na mÃ¡quina host (onde o `docker-compose up` Ã© executado) e os carregam como **Credenciais** do tipo "Secret file" dentro do prÃ³prio Jenkins.
*   Isso permite que o pipeline Jenkins, ao executar os estÃ¡gios de CD, acesse esses certificados de forma segura (atravÃ©s do mecanismo `withCredentials` do Jenkins) para autenticar as conexÃµes TLS com o Docker Engine remoto no PWD.
*   O exercÃ­cio instrui a iniciar essa nova infraestrutura e verificar, na interface do Jenkins, se as credenciais dos certificados foram carregadas corretamente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Certificados Locais:** Verifique se os arquivos `ca.pem`, `client-cert.pem`, `client-key.pem` ainda existem na pasta que vocÃª usou no exercÃ­cio da p.305 (ex: `$HOME\pwd-certs`). O Jenkins Compose file provavelmente montarÃ¡ essa pasta.
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch15\exercises\infrastructure
    ```
3.  **Parar Infra Anterior (se rodando):**
    ```powershell
    # docker-compose -f ... down # Use os arquivos do cap 11 se necessÃ¡rio
    ```
4.  **Iniciar Nova Infra:**
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-windows.yml up -d
    ```
5.  **Verificar Credenciais Jenkins:**
    *   Aguarde Jenkins iniciar completamente (pode levar um minuto).
    *   Acesse `http://localhost:8080`.
    *   Login `diamol` / `diamol`.
    *   No menu esquerdo, vÃ¡ para "Manage Jenkins" -> "Manage Credentials".
    *   Em "Stores scoped to Jenkins", clique em "Jenkins".
    *   Em "Global credentials", vocÃª deve ver trÃªs entradas: `docker-ca.pem`, `docker-cert.pem`, `docker-key.pem`, tipo "Secret file". (Como na Figura 15.12).

---

ğŸ“„ PÃ¡gina 313

ğŸ” Resumo dos principais pontos:

*   Figura 15.12: Confirma credenciais no Jenkins.
*   Infra rodando, mas Gogs precisa de setup manual novamente (Install, Register user `diamol`, Create repo `diamol`). Ref Cap 11, Figuras 11.3-11.5.
*   Pipeline a ser rodado: Builda nova versÃ£o do app `timecheck` (Cap 12).
*   Precisa editar `Jenkinsfile` para adicionar **seu** domÃ­nio PWD.
*   Deploy CD usarÃ¡ PWD como ambiente UAT e Prod (simulaÃ§Ã£o).
*   **ExercÃ­cio Try-it-now:** Configurar Gogs, editar Jenkinsfile, push para Gogs, rodar build Jenkins.
    *   1. Setup Gogs (`http://localhost:3000`).
    *   2. Editar `Jenkinsfile` (ou `Jenkinsfile.windows`) em `ch15/exercises`.
        *   Achar seÃ§Ã£o `environment { ... }`.
        *   Substituir `pwd-domain` em `UAT_ENGINE` e `PROD_ENGINE` pelo seu domÃ­nio PWD real (copiado na p.303), **incluindo porta `:80`**. (PWD mapeia 80 externamente para 2376 interno).
    *   3. Commit e Push para Gogs local: `git remote add ch15 ...`, `git commit -a ...`, `git push ch15 ...`.
    *   4. Iniciar build no Jenkins (`http://localhost:8080/job/diamol/`, clicar "Build Now").

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com a infraestrutura rodando e as credenciais carregadas no Jenkins, os Ãºltimos passos de setup sÃ£o:
    1.  **Configurar o Gogs:** Repetir o processo manual do Cap. 11 (instalar, registrar usuÃ¡rio `diamol`, criar repo `diamol`).
    2.  **Configurar o Pipeline:** O pipeline Jenkins (definido no `Jenkinsfile`) precisa saber o endereÃ§o dos Docker Engines remotos para os estÃ¡gios de CD. Esses endereÃ§os sÃ£o definidos como variÃ¡veis de ambiente (`UAT_ENGINE`, `PROD_ENGINE`) dentro do prÃ³prio `Jenkinsfile`. VocÃª precisa **editar** o `Jenkinsfile` (na sua cÃ³pia local do cÃ³digo) e substituir o placeholder `pwd-domain` pelo **hostname pÃºblico real do seu PWD** (obtido na p.303), seguido por `:80` (pois o PWD expÃµe a porta 2376 mapeada para a porta 80 externamente).
    3.  **Enviar CÃ³digo Atualizado:** Fazer `commit` da mudanÃ§a no `Jenkinsfile` e `push` para o Gogs local.
    4.  **Iniciar Build:** Disparar o build manualmente no Jenkins.
*   O pipeline entÃ£o executarÃ¡: CI (Build, Test, Push to local Registry) -> CD (Deploy to UAT via TLS para PWD) -> Pausa para AprovaÃ§Ã£o -> CD (Deploy to Prod via TLS para PWD).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Setup Gogs:** Repita os passos das p.212-214 (Install, Register `diamol`, Create repo `diamol`).
2.  **Editar Jenkinsfile:**
    *   Abra `C:\DockerLabs\diamol\ch15\exercises\Jenkinsfile.windows` (ou `Jenkinsfile` se usar Linux no PWD) no VS Code.
    *   Localize a seÃ§Ã£o `environment { ... }`.
    *   Substitua `<your-pwd-domain-from-the-address-bar>` pelo seu hostname PWD real (sem `https://`). Mantenha `:80` no final. Exemplo:
        ```groovy
        UAT_ENGINE = "ip172-18-0-59-bngh3ebjagq000ddjbv0.direct.labs.play-with-docker.com:80"
        PROD_ENGINE = "ip172-18-0-59-bngh3ebjagq000ddjbv0.direct.labs.play-with-docker.com:80"
        ```
    *   Salve o arquivo.
3.  **Commit & Push:**
    ```powershell
    # Estando na raiz C:\DockerLabs\diamol
    git remote remove ch15 # Remover se existir de antes
    git remote add ch15 http://localhost:3000/diamol/diamol.git
    git add .\ch15\exercises\Jenkinsfile.windows # Ou Jenkinsfile
    git commit -m "Update PWD domain for CD"
    git push ch15 # Use creds Gogs (diamol/sua_senha_gogs)
    ```
4.  **Iniciar Build Jenkins:** VÃ¡ para `http://localhost:8080/job/diamol/` e clique "Build Now".

---

ğŸ“„ PÃ¡gina 314

ğŸ” Resumo dos principais pontos:

*   Figura 15.13: Mostra pipeline Jenkins pausado no estÃ¡gio "Await Approval" apÃ³s deploy UAT.
*   Pipeline: CI (Get Code, Build, Test, Push Registry) -> CD UAT -> Pause -> CD Prod.
*   Pausa para aprovaÃ§Ã£o manual Ã© opcional; bom para comeÃ§ar com CD.
*   **ExercÃ­cio Try-it-now:** Verificar deploy UAT no PWD e aprovar no Jenkins.
    *   1. No PWD (`node1`), verificar container UAT: `docker container ls`, `docker container logs timecheck-uat_timecheck_1`.
    *   2. No Jenkins UI, clicar na caixa azul "Await Approval".
    *   3. Na janela popup, clicar "Do It!" (ou "Proceed"). Pipeline continua para deploy Prod.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina descreve o estado do pipeline apÃ³s a conclusÃ£o bem-sucedida dos estÃ¡gios de CI e do primeiro estÃ¡gio de CD (Deploy UAT). O pipeline agora estÃ¡ **pausado**, esperando uma intervenÃ§Ã£o manual no estÃ¡gio "Await Approval" (Figura 15.13). Essa pausa Ã© uma feature comum em pipelines CD para permitir uma verificaÃ§Ã£o final ou aprovaÃ§Ã£o formal antes de implantar em produÃ§Ã£o.
*   O exercÃ­cio instrui a fazer essa verificaÃ§Ã£o e aprovaÃ§Ã£o:
    1.  Conectar-se ao ambiente de UAT (o nÃ³ PWD) e usar comandos `docker` para confirmar que a aplicaÃ§Ã£o (`timecheck-uat`) foi implantada corretamente e estÃ¡ funcionando.
    2.  Voltar Ã  interface do Jenkins e clicar na etapa pausada para dar a aprovaÃ§Ã£o, permitindo que o pipeline prossiga para o Ãºltimo estÃ¡gio: Deploy em ProduÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Deploy UAT no PWD:**
    *   VÃ¡ para a aba do navegador com seu PWD (`node1`).
    *   Execute no terminal do PWD:
        ```bash
        docker container ls
        # Procure por um container com nome iniciando 'timecheck-uat_timecheck_...'
        docker container logs <nome_container_uat> # Verifique se estÃ¡ logando
        ```
2.  **Aprovar no Jenkins:**
    *   VÃ¡ para a aba do navegador com o Jenkins (`http://localhost:8080/job/diamol/`).
    *   Clique no estÃ¡gio "Await Approval" que estÃ¡ piscando ou destacado.
    *   Clique no botÃ£o "Proceed" (ou similar) na janela que aparece.
*   O pipeline agora continuarÃ¡ e executarÃ¡ o estÃ¡gio "Prod".

---

ğŸ“„ PÃ¡gina 315

ğŸ” Resumo dos principais pontos:

*   Figura 15.14: Mostra verificaÃ§Ã£o no PWD e aprovaÃ§Ã£o no Jenkins.
*   EstÃ¡gios CD sÃ£o simples: chamam script que roda `docker-compose` (ou `docker stack deploy`).
*   Script recebe creds (certs TLS) e host remoto via variÃ¡veis de ambiente.
*   Separar lÃ³gica Docker/Compose (nos scripts) da orquestraÃ§Ã£o do pipeline (Jenkinsfile) -> portabilidade.
*   **Listagem 15.3:** Trecho do `Jenkinsfile` e do script `04-uat.bat` para deploy UAT.
    *   **Jenkinsfile (EstÃ¡gio 'UAT'):**
        *   `withCredentials([...])`: Bloco Jenkins que torna credenciais (certs) disponÃ­veis como variÃ¡veis de ambiente (`$ca`, `$cert`, `$key`) *dentro* do bloco.
        *   `dir(...)`: Muda diretÃ³rio.
        *   `sh '...'`: Executa script `04-uat.bat`.
    *   **Script `04-uat.bat`:**
        *   Usa `docker-compose`.
        *   Passa host remoto (`--host tcp://$UAT_ENGINE`), flags TLS (`--tlsverify`, `--tlscacert $ca`, etc.) usando variÃ¡veis providas pelo `withCredentials`.
        *   Passa nome do projeto (`-p timecheck-uat`) e arquivos Compose (`-f ...`).
        *   Executa `up -d`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica a implementaÃ§Ã£o dos estÃ¡gios de CD no pipeline Jenkins. A lÃ³gica segue o mesmo padrÃ£o dos estÃ¡gios CI: o `Jenkinsfile` orquestra a execuÃ§Ã£o de scripts externos.
*   A **Listagem 15.3** mostra como isso funciona para o deploy em UAT:
    *   **`Jenkinsfile`:** Usa a funcionalidade `withCredentials` do Jenkins. Isso busca as credenciais dos certificados (que foram carregadas de arquivos locais na inicializaÃ§Ã£o do Jenkins) e as expÃµe temporariamente como variÃ¡veis de ambiente (`$ca`, `$cert`, `$key`) apenas para os comandos dentro do bloco `withCredentials`. Em seguida, ele executa o script `04-uat.bat`.
    *   **`04-uat.bat`:** Este script contÃ©m o comando `docker-compose` real. Crucialmente, ele usa:
        *   A variÃ¡vel `$UAT_ENGINE` (definida na seÃ§Ã£o `environment` do Jenkinsfile, apontando para o PWD) na flag `--host`.
        *   As variÃ¡veis `$ca`, `$cert`, `$key` (vindas do `withCredentials`) nas flags `--tlscacert`, `--tlscert`, `--tlskey`.
        *   Flags `-p` e `-f` para definir o nome do projeto e os arquivos Compose a serem usados para o ambiente UAT.
        *   O comando `up -d` para iniciar/atualizar a aplicaÃ§Ã£o.
*   Essa abordagem mantÃ©m os scripts (`.bat`) genÃ©ricos, recebendo os detalhes de conexÃ£o e credenciais do ambiente do pipeline, o que facilita a adaptaÃ§Ã£o para outras ferramentas de CI/CD.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Explorar Arquivos:**
    *   Abra `ch15/exercises/Jenkinsfile.windows` (ou `Jenkinsfile`). Localize o `stage('UAT')` e o bloco `withCredentials`.
    *   Abra `ch15/exercises/ci/04-uat.bat`. Veja o comando `docker-compose` completo. Note como ele usa as variÃ¡veis de ambiente (`%UAT_ENGINE%`, `%ca%`, etc. na sintaxe Batch/Windows, ou `$VAR` na sintaxe sh/Linux).

---

ğŸ“„ PÃ¡gina 316

ğŸ” Resumo dos principais pontos:

*   EstÃ¡gio Prod Ã© quase idÃªntico ao UAT, sÃ³ muda arquivos Compose.
*   ApÃ³s pipeline completar, ambos deploys (UAT e Prod) estarÃ£o rodando no PWD.
*   **ExercÃ­cio Try-it-now:** Verificar deploys no PWD apÃ³s pipeline concluir.
    *   Comandos PWD: `docker container ls`, `docker container logs timecheck-prod_timecheck_1`.
*   Figura 15.15: Mostra `ls` e `logs` no PWD, confirmando containers `uat` e `prod` rodando.
*   Sucesso: Pipeline CI/CD completo, deployando para "remoto" (PWD) a partir de infra CI local (containers).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que o estÃ¡gio de deploy em ProduÃ§Ã£o no pipeline segue exatamente o mesmo padrÃ£o do UAT, apenas usando um conjunto diferente de arquivos Compose (provavelmente `docker-compose-prod.yml`) e um nome de projeto diferente (`timecheck-prod`).
*   O exercÃ­cio final instrui a verificar o resultado no PWD apÃ³s a conclusÃ£o de todo o pipeline Jenkins. VocÃª deve encontrar contÃªineres rodando para *ambos* os ambientes, UAT e ProduÃ§Ã£o (embora, neste exemplo simulado, eles estejam rodando no mesmo Docker Engine do PWD).
*   Isso conclui a demonstraÃ§Ã£o de um pipeline CI/CD funcional que utiliza acesso remoto seguro para implantaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Pipeline Jenkins:** Volte para `http://localhost:8080/job/diamol/`. O Ãºltimo build deve estar verde (Success) e todos os estÃ¡gios (incluindo UAT, Await Approval, Prod) devem ter sido concluÃ­dos.
2.  **Verificar Deploys no PWD:**
    *   VÃ¡ para a aba do PWD (`node1`).
    *   Execute no terminal:
        ```bash
        docker container ls
        # Deve listar containers com nomes como 'timecheck-uat_...' e 'timecheck-prod_...'
        docker container logs timecheck-prod_timecheck_1 # Verifique logs da instÃ¢ncia 'prod'
        docker container logs timecheck-uat_timecheck_1 # Verifique logs da instÃ¢ncia 'uat'
        ```

---

ğŸ“„ PÃ¡gina 317

ğŸ” Resumo dos principais pontos:

*   Figura 15.15: Confirma containers UAT e Prod rodando no PWD.
*   Pipeline CI/CD poderoso, requer apenas Docker server para infra e para targets.
*   **Alerta Final:** Mesmo com acesso seguro (TLS/SSH), modelo de acesso Docker ainda Ã© "Tudo ou Nada".
*   **15.5 Understanding the access model for Docker** (Entendendo o modelo de acesso para Docker)
    *   NÃ£o precisa de seÃ§Ã£o inteira, Ã© simples, mas importante.
    *   SeguranÃ§a (TLS/SSH) cobre **Criptografia** e **AutenticaÃ§Ã£o**.
    *   **NÃƒO** cobre **AutorizaÃ§Ã£o** (permissÃµes granulares) nem **Auditoria** (quem fez o quÃª).
    *   Modelo Docker CE/EE bÃ¡sico: Conectou? Pode fazer tudo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina celebra o sucesso do pipeline CI/CD, mas imediatamente introduz uma ressalva de seguranÃ§a importante na seÃ§Ã£o 15.5: o **modelo de acesso "tudo ou nada"** do Docker Engine padrÃ£o (Community Edition ou mesmo Enterprise Edition bÃ¡sico sem UCP).
*   Mesmo usando conexÃµes seguras com TLS ou SSH, que garantem que apenas usuÃ¡rios autenticados possam se conectar e que a comunicaÃ§Ã£o seja criptografada, uma vez que um usuÃ¡rio estÃ¡ conectado Ã  API Docker, ele tem **permissÃµes totais** para realizar qualquer operaÃ§Ã£o Docker (criar, parar, remover contÃªineres, volumes, redes, secrets, configs, imagens, etc.).
*   NÃ£o existe um mecanismo nativo no Docker Engine bÃ¡sico para dizer "UsuÃ¡rio Alice sÃ³ pode listar contÃªineres no ambiente de teste" ou "Pipeline CI sÃ³ pode fazer deploy, nÃ£o pode remover volumes".
*   Isso significa que a seguranÃ§a do acesso remoto depende fortemente de *limitar* quem tem as credenciais (certs TLS, chaves SSH, senhas) para se conectar aos Docker Engines, especialmente os de produÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Com as credenciais TLS do PWD que vocÃª usou, vocÃª poderia executar `docker --host ... network rm <alguma_rede>` ou `docker --host ... volume prune` no PWD, mesmo que sua intenÃ§Ã£o fosse apenas fazer deploy. NÃ£o hÃ¡ restriÃ§Ã£o de permissÃ£o na API.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Acesso Docker BÃ¡sico: Ã‰ como ter uma chave mestra super segura (TLS/SSH) para entrar no prÃ©dio. A chave garante que sÃ³ pessoas autorizadas entram (autenticaÃ§Ã£o) e que ninguÃ©m ouve suas conversas lÃ¡ dentro (criptografia). Mas, uma vez lÃ¡ dentro, a chave abre *todas* as portas (sem autorizaÃ§Ã£o granular).

---

ğŸ“„ PÃ¡gina 318

ğŸ” Resumo dos principais pontos:

*   Modelo "Tudo ou Nada" pode ser assustador dependendo do contexto (seguranÃ§a, infra).
*   Defesa em profundidade ajuda (rede manager separada, acesso IP restrito, rotaÃ§Ã£o de CA), mas risco interno existe.
*   **Alternativas com AutorizaÃ§Ã£o/Auditoria:**
    *   **Kubernetes RBAC (Role-Based Access Control):** K8s tem modelo de permissÃ£o granular.
    *   **Docker Enterprise (UCP):** VersÃ£o comercial do Docker inclui RBAC.
    *   **GitOps:** Abordagem diferente (pull-based), cluster busca config/deploy do Git, sem acesso externo direto ao cluster. (Figura 15.16).
*   Diagrama (Figura 15.16): Ilustra GitOps. Pipeline CI/CD faz push para Git (imagem + config/manifesto). Agente no cluster detecta mudanÃ§a no Git e aplica o deploy *de dentro* do cluster.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina discute as implicaÃ§Ãµes do modelo de acesso "tudo ou nada" e menciona que, embora medidas de seguranÃ§a de infraestrutura (defesa em profundidade) ajudem, elas nÃ£o eliminam completamente o risco, especialmente o risco interno.
*   Apresenta alternativas que *oferecem* controle de acesso mais granular (AutorizaÃ§Ã£o) e/ou auditoria:
    1.  **Kubernetes RBAC:** O Kubernetes possui um sistema sofisticado de Role-Based Access Control que permite definir papÃ©is (roles) com permissÃµes especÃ­ficas (ex: sÃ³ pode listar pods, pode criar deployments) e associar esses papÃ©is a usuÃ¡rios ou contas de serviÃ§o.
    2.  **Docker Enterprise (com UCP):** A plataforma comercial da Docker (agora Mirantis) inclui o Universal Control Plane (UCP), que adiciona RBAC e outras funcionalidades de gerenciamento sobre o Swarm ou K8s.
    3.  **GitOps:** Uma abordagem arquitetural e operacional fundamentalmente diferente. Em vez do pipeline de CI/CD "empurrar" (push) o deploy para o cluster, o fluxo Ã© invertido:
        *   O pipeline faz push dos artefatos (imagem Docker) e da *definiÃ§Ã£o do estado desejado* (manifestos K8s, ou arquivos Compose Swarm) para um repositÃ³rio **Git**.
        *   Um **agente** rodando *dentro* do cluster monitora esse repositÃ³rio Git.
        *   Quando o agente detecta uma mudanÃ§a no Git (ex: nova versÃ£o do manifesto), ele "puxa" (pull) essa definiÃ§Ã£o e a aplica ao cluster *localmente*.
        *   **BenefÃ­cio:** O pipeline de CI/CD nÃ£o precisa mais de credenciais para acessar o cluster diretamente. O Git se torna a Ãºnica fonte da verdade, e o acesso ao cluster Ã© muito mais restrito. A Figura 15.16 ilustra esse fluxo pull-based.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **GitOps:** Ferramentas populares para implementar GitOps com Kubernetes incluem Argo CD e Flux. Elas rodam no cluster K8s e sincronizam com repositÃ³rios Git. Para Swarm, as opÃ§Ãµes sÃ£o menos padronizadas, mas o conceito pode ser adaptado com scripts customizados ou ferramentas emergentes.

---

ğŸ“„ PÃ¡gina 319

ğŸ” Resumo dos principais pontos:

*   GitOps Ã© abordagem interessante (repetÃ­vel, versionado, auditÃ¡vel, single source of truth no Git).
*   Requer tempo para implementar, mas pode evoluir gradualmente a partir de pipelines CI/CD simples.
*   **15.6 Lab** (LaboratÃ³rio)
    *   **Contexto:** Deploy no exercÃ­cio 15.4 "trapaceou", usou imagem do Docker Hub em vez do registro local, pois PWD nÃ£o acessa registro local.
    *   **Objetivo:** Corrigir isso. Fazer pipeline CI/CD completo que builda, testa, push para **Docker Hub**, e depois deploya (UAT/Prod no PWD) usando a imagem do **Docker Hub**.
*   **Tarefa:**
    1.  Modificar pipeline (EstÃ¡gio Push? Adicionar novo estÃ¡gio?) para **taggear** imagem com sua conta Docker Hub (ex: `<seu_id>/ch15-timecheck:3.0`) alÃ©m da tag do registro local.
    2.  Modificar pipeline para **fazer push** para Docker Hub (requer login seguro no Hub).
    3.  Modificar arquivos Compose de deploy (UAT/Prod) para usar a imagem do **Docker Hub** (com sua ID) em vez da imagem `diamol/...` usada no exercÃ­cio.
*   **Dicas:**
    *   Usar credencial Jenkins tipo "Username/password" para login Docker Hub. Disponibilizar via `withCredentials`.
    *   PWD pode parar de ouvir na porta exposta; talvez precise reiniciar sessÃ£o PWD (novo domÃ­nio -> editar Jenkinsfile de novo).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre GitOps e introduz o **Lab do CapÃ­tulo 15**.
*   O lab corrige uma "trapaÃ§a" do exercÃ­cio anterior: o deploy no PWD usou uma imagem `diamol/...` prÃ©-publicada no Docker Hub, porque o PWD (sendo um ambiente remoto) nÃ£o conseguiria acessar o contÃªiner do registro Docker rodando na *sua mÃ¡quina local*.
*   A tarefa agora Ã© criar um fluxo mais realista:
    1.  O pipeline CI ainda builda a imagem.
    2.  O estÃ¡gio Push agora deve:
        *   Fazer push para o registro local (`registry.local:5000/...`) como antes (para testes locais talvez).
        *   **E tambÃ©m** fazer login no **Docker Hub** (de forma segura usando credenciais armazenadas no Jenkins).
        *   **Re-taggear** a imagem construÃ­da com o nome correto para o seu repositÃ³rio no Docker Hub (ex: `alexandre/ch15-timecheck:3.0`).
        *   **Fazer push** dessa imagem para o Docker Hub.
    3.  Os estÃ¡gios de deploy CD (UAT e Prod) devem ser modificados para usar a imagem que acabou de ser enviada para o **Docker Hub** (ex: `alexandre/ch15-timecheck:3.0`) em vez da imagem `diamol/...`. O PWD *consegue* acessar o Docker Hub.
*   As dicas mencionam como lidar com as credenciais do Docker Hub no Jenkins (usando tipo Username/password e `withCredentials`) e alertam sobre a natureza efÃªmera das sessÃµes PWD.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 15:**
    1.  **Credenciais Docker Hub no Jenkins:**
        *   Acesse Jenkins (`http://localhost:8080`) > Manage Jenkins > Manage Credentials > Jenkins > Global credentials > Add Credentials.
        *   Kind: "Username with password".
        *   Scope: Global.
        *   Username: Seu Docker ID.
        *   Password: Sua senha Docker Hub (ou melhor: um Access Token gerado no Hub).
        *   ID: `dockerhub-creds` (ou outro nome fÃ¡cil de referenciar).
    2.  **Modificar `Jenkinsfile`:**
        *   No estÃ¡gio `Push`: Adicionar bloco `withCredentials([usernamePassword(credentialsId: 'dockerhub-creds', usernameVariable: 'DOCKER_USER', passwordVariable: 'DOCKER_PASS')]) { ... }`.
        *   Dentro do bloco, adicionar comandos no script `03-push.bat` (ou similar):
            *   `docker login -u $DOCKER_USER -p $DOCKER_PASS` (ou usar `echo "$DOCKER_PASS" | docker login ... --password-stdin`).
            *   `docker image tag registry.local:5000/diamol/ch15-timecheck:...? <seu_id>/ch15-timecheck:3.0` (adaptar tag local).
            *   `docker image push <seu_id>/ch15-timecheck:3.0`.
    3.  **Modificar Arquivos Compose de Deploy (UAT/Prod):** Nos arquivos `.yml` usados pelos estÃ¡gios UAT e Prod (ex: `docker-compose-uat.yml`), altere a linha `image:` do serviÃ§o `timecheck` para usar a imagem do seu Docker Hub: `image: <seu_id>/ch15-timecheck:3.0`.
    4.  **Commit & Push para Gogs.**
    5.  **Rodar Pipeline Jenkins.** Verificar se push para Hub ocorre e se deploys UAT/Prod no PWD usam a imagem correta (pode verificar com `docker service inspect <stack>_<service>` no PWD).

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch15/lab/README.md

---

ğŸ“„ PÃ¡gina 320

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 15 no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Fornece o link para a soluÃ§Ã£o completa do Lab 15 no repositÃ³rio do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 15, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch15/lab/README.md

---

ğŸ“„ PÃ¡gina 321

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 16: Building Docker images that run anywhere: Linux, Windows, Intel, and Arm** (Construindo imagens Docker que rodam em qualquer lugar...)
*   RevisÃ£o: ExercÃ­cios do livro funcionam em Mac, Windows, Linux, R-Pi.
*   Motivo: Imagens do livro (`diamol/...`) sÃ£o **multi-arquitetura (multi-arch)**.
*   **Imagem Multi-Arch:**
    *   ConstruÃ­da/Enviada para registro com **mÃºltiplas variantes**.
    *   Cada variante para um OS ou CPU diferente.
    *   Todas usam o **mesmo nome/tag**.
*   **Como funciona para o usuÃ¡rio:**
    *   `docker pull` ou `docker run <imagem_multi_arch>`: Docker Engine detecta OS/CPU local e automaticamente baixa a variante *compatÃ­vel* da imagem.
    *   Ex: Rodar `diamol/base` no Windows/Intel baixa variante Windows/amd64; rodar no R-Pi/Arm baixa Linux/arm. App funciona igual.
*   FÃ¡cil para usuÃ¡rio, exige esforÃ§o do publicador da imagem.
*   Foco Cap. 16: Como produzir builds multi-arch. Aviso para nÃ£o pular, mesmo se nÃ£o usar Win/Arm agora.
*   **16.1 Why multi-architecture images are important** (Por que imagens multi-arquitetura sÃ£o importantes)
    *   Exemplo: AWS oferece VMs Intel/AMD e **Arm** (A1 instances). Arm Ã© quase **50% mais barato**.
    *   Outras nuvens devem seguir. Grande economia potencial.
    *   Desafio: Apps construÃ­dos para Intel (`x64`/`amd64`) **nÃ£o rodam** em Arm.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 16, focado em um conceito avanÃ§ado mas cada vez mais importante: **imagens multi-arquitetura**.
*   Explica que a razÃ£o pela qual os exemplos do livro funcionam em diversas plataformas Ã© que as imagens `diamol/...` fornecidas sÃ£o, na verdade, "manifestos" que apontam para diferentes versÃµes da imagem, cada uma construÃ­da para uma combinaÃ§Ã£o especÃ­fica de sistema operacional (Linux, Windows) e arquitetura de CPU (Intel/AMD x64, Arm32, Arm64).
*   Quando vocÃª executa `docker pull` ou `docker run`, o seu Docker Engine local informa ao registro qual Ã© o seu OS e CPU, e o registro (se a imagem for multi-arch) envia de volta a variante correta. Para o usuÃ¡rio, parece que Ã© uma Ãºnica imagem que "magicamente" funciona em todo lugar.
*   A seÃ§Ã£o 16.1 justifica a importÃ¢ncia disso, principalmente com o crescimento do uso de processadores **Arm** (comuns em celulares, IoT, Raspberry Pi, e agora tambÃ©m em servidores na nuvem como os da AWS) por serem mais eficientes energeticamente e, consequentemente, mais baratos. O problema Ã© a incompatibilidade: software compilado para Intel/AMD nÃ£o roda em Arm e vice-versa.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª jÃ¡ usou imagens multi-arch sem saber (as `diamol/...`).
*   O Docker Hub mostra se uma imagem Ã© multi-arch. VÃ¡ para `https://hub.docker.com/_/python`, clique em uma tag (ex: `3.10`), e veja a seÃ§Ã£o "Supported Architectures" listando `linux/amd64`, `linux/arm/v7`, `linux/arm64`, etc.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Imagem Multi-Arch: Ã‰ como pedir um manual de instruÃ§Ãµes para "Python 3.10". A editora (registro Docker) te pergunta "Qual idioma e para qual tipo de dispositivo vocÃª quer?" (seu OS/CPU). VocÃª responde "PortuguÃªs para Tablet" (Linux/Arm), e ela te envia a versÃ£o correta do manual. Se seu amigo pedir o mesmo manual ("Python 3.10") mas disser "InglÃªs para Computador" (Windows/Intel), ele recebe uma versÃ£o diferente, mas o conteÃºdo ensina a mesma coisa.

---

ğŸ“„ PÃ¡gina 322

ğŸ” Resumo dos principais pontos:

*   IoT tambÃ©m usa Arm (eficiÃªncia energÃ©tica).
*   Desafio Arm: Precisa de plataforma app que rode em Arm + construir app *em* mÃ¡quina Arm (ou via emulaÃ§Ã£o).
*   Docker resolve isso:
    *   **Docker Desktop (Win/Mac):** Suporta **emulaÃ§Ã£o** para construir e rodar imagens Arm em mÃ¡quinas Intel.
*   **ExercÃ­cio Try-it-now:** Usar emulaÃ§Ã£o no Docker Desktop para build Arm. (NÃ£o funciona em Docker Engine puro ou PWD).
    *   1. Habilitar **Experimental Features** nas Settings do Docker Desktop (Figura 16.1).
*   Figura 16.1: Mostra opÃ§Ã£o "Enable experimental features" nas Settings (pode estar em General ou Daemon/Features in development).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina continua a motivaÃ§Ã£o para multi-arch, mencionando IoT e Edge computing onde Arm Ã© dominante.
*   Destaca a soluÃ§Ã£o Docker para o desafio de construir para Arm sem ter hardware Arm: **emulaÃ§Ã£o** disponÃ­vel no **Docker Desktop** (Windows e Mac). O Docker Desktop pode usar tecnologias como QEMU por baixo dos panos para simular um ambiente Arm e permitir que vocÃª construa (`docker build`) e atÃ© rode (`docker run`) imagens Arm, mesmo estando em uma mÃ¡quina Intel/AMD.
*   O exercÃ­cio instrui a habilitar as "Experimental Features" no Docker Desktop, pois a funcionalidade de build multi-plataforma (`docker build --platform ...`) e a emulaÃ§Ã£o associada ainda podem estar sob essa flag (embora em versÃµes recentes, partes dela, como `buildx`, possam estar habilitadas por padrÃ£o).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Habilitar Experimental:**
    *   Clique direito no Ã­cone Docker > Settings.
    *   Procure pela seÃ§Ã£o "General" ou "Features in development" ou "Docker Engine".
    *   Encontre e marque a caixa "Enable experimental features".
    *   Clique "Apply & Restart". Aguarde reiniciar.
2.  **(Verificar se buildx estÃ¡ disponÃ­vel - comando pode jÃ¡ funcionar mesmo sem experimental em versÃµes recentes):**
    ```powershell
    docker buildx version
    ```
    Se retornar uma versÃ£o, vocÃª estÃ¡ pronto para builds multiplataforma.

---

ğŸ“„ PÃ¡gina 323

ğŸ” Resumo dos principais pontos:

*   **ContinuaÃ§Ã£o Ex. Build Arm:** (ApÃ³s habilitar experimental)
    *   2. Navegar para `ch16/exercises`.
    *   3. Construir imagem para Arm 64-bit usando flag `--platform`:
        *   `docker build -t diamol/ch16-whoami:linux-arm64 --platform linux/arm64 ./whoami`.
    *   4. Verificar arquitetura da imagem criada:
        *   `docker image inspect diamol/ch16-whoami:linux-arm64 -f '{{.Os}}/{{.Architecture}}'`. Deve mostrar `linux/arm64`.
    *   5. Verificar arquitetura nativa do seu Docker Engine:
        *   `docker info -f '{{.OSType}}/{{.Architecture}}'`. Deve mostrar `linux/amd64` (se modo Linux) ou `windows/amd64`.
*   Resultado: Imagem Arm construÃ­da em mÃ¡quina Intel via emulaÃ§Ã£o.
*   Imagem pode ser enviada (`push`) e rodada em mÃ¡quina Arm real (R-Pi, AWS A1).
*   Figura 16.2: Mostra saÃ­da dos comandos, confirmando build cross-platform.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com as features experimentais (ou `buildx`) ativas, o exercÃ­cio demonstra o build **cross-platform**.
*   O comando `docker build` agora usa a flag `--platform linux/arm64`. Isso instrui o Docker a construir a imagem como se estivesse em uma mÃ¡quina Linux com CPU Arm de 64 bits. O Docker Desktop usarÃ¡ emulaÃ§Ã£o (QEMU) para executar os passos do Dockerfile que precisam rodar no ambiente de destino (ex: comandos `RUN`).
*   O `Dockerfile` na pasta `./whoami` provavelmente usa imagens base (.NET Core SDK/runtime) que *sÃ£o* multi-arch (incluem variantes `linux/arm64`). O build compila o app .NET Core *para* Arm64.
*   Os comandos `docker image inspect` e `docker info` sÃ£o usados para confirmar que a imagem resultante foi de fato criada para `linux/arm64`, enquanto a mÃ¡quina local que fez o build Ã© `linux/amd64` (ou `windows/amd64`).
*   **Importante:** Builds com emulaÃ§Ã£o sÃ£o significativamente **mais lentos** do que builds nativos. SÃ£o Ã³timos para desenvolvimento e CI, mas para produÃ§Ã£o em larga escala, ter nÃ³s de build Arm reais Ã© preferÃ­vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch16\exercises
    ```
2.  **Build Cross-Platform:**
    ```powershell
    docker build -t diamol/ch16-whoami:linux-arm64 --platform linux/arm64 ./whoami
    # Este build serÃ¡ BEM mais lento que um build nativo. Aguarde.
    ```
3.  **Verificar Arquitetura da Imagem:**
    ```powershell
    docker image inspect diamol/ch16-whoami:linux-arm64 -f '{{.Os}}/{{.Architecture}}'
    # Deve retornar linux/arm64
    ```
4.  **Verificar Arquitetura Local:**
    ```powershell
    docker info -f '{{.OSType}}/{{.Architecture}}'
    # Deve retornar linux/amd64 (ou windows/amd64 se modo Windows)
    ```

---

ğŸ“„ PÃ¡gina 324

ğŸ” Resumo dos principais pontos:

*   Docker sabe OS/Arch do host e usa para match ao baixar (`pull`) imagens.
*   OtimizaÃ§Ã£o de `pull` (descompressÃ£o, etc.) sÃ³ funciona se imagem for compatÃ­vel.
*   Se nÃ£o houver variante compatÃ­vel no registro, `pull` falha com erro.
*   **ExercÃ­cio Try-it-now:** Tentar baixar imagem Windows em host Linux (ou vice-versa).
    *   Comando (em host Linux/Mac/Win-LinuxMode): `docker image pull mcr.microsoft.com/windows/nanoserver:1809`.
*   Figura 16.3: Mostra erro "no matching manifest for linux/amd64 in the manifest list entries".
*   **Manifest List:** Metadado no registro que lista as variantes (OS/Arch) disponÃ­veis para uma tag de imagem.
*   Erro significa: registro foi consultado, manifest list da imagem foi encontrada, mas nenhuma entrada nela correspondia ao OS/Arch do Docker Engine que fez o pedido.
*   Imagem `nanoserver` nÃ£o Ã© multi-arch (sÃ³ Windows).
*   PrincÃ­pio: Arch da imagem deve bater com Arch do Engine.
    *   NuanÃ§as: Linux pode baixar arch CPU diferente, mas falha no `run` ("exec format error"). Windows com LCOW (experimental) pode rodar Linux, mas complexo/instÃ¡vel.
    *   Melhor prÃ¡tica: Usar arquitetura correspondente. Imagens multi-arch facilitam isso.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica por que vocÃª nÃ£o pode simplesmente rodar qualquer imagem em qualquer Docker Engine: hÃ¡ uma verificaÃ§Ã£o de compatibilidade de **Sistema Operacional (OS)** e **Arquitetura de CPU (Arch)**.
*   Quando vocÃª faz `docker pull` ou `docker run`, seu Docker Engine informa ao registro: "Eu sou um Linux/amd64 (ou Windows/amd64), me dÃª a imagem X".
*   O registro verifica a **Manifest List** da imagem X. Esse manifesto Ã© um pequeno Ã­ndice que lista todas as variantes da imagem X disponÃ­veis e para qual combinaÃ§Ã£o de OS/Arch cada variante se destina.
*   Se o registro encontrar uma entrada no manifesto que corresponde ao OS/Arch do seu Engine, ele envia essa variante especÃ­fica.
*   Se **nenhuma** entrada corresponder, o registro informa ao Engine, e o comando `pull`/`run` falha com o erro "no matching manifest..." (como na Figura 16.3).
*   O exercÃ­cio demonstra isso tentando baixar uma imagem exclusivamente Windows (`nanoserver`) em um Engine Linux. A falha Ã© esperada.
*   A pÃ¡gina menciona algumas exceÃ§Ãµes (Linux tentando rodar CPU errada, LCOW), mas conclui que a melhor prÃ¡tica Ã© sempre rodar imagens na arquitetura correspondente, e que imagens multi-arch sÃ£o a forma de fazer isso de maneira transparente para o usuÃ¡rio.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Testar Falha de Pull (Modo Linux):** (Certifique-se que Docker Desktop estÃ¡ no modo Linux)
    ```powershell
    docker image pull mcr.microsoft.com/windows/nanoserver:1809
    # Deve falhar com erro "no matching manifest for linux/amd64..."
    ```
2.  **Testar Falha de Pull (Modo Windows):** (Troque Docker Desktop para modo Windows)
    ```powershell
    docker image pull python:3.10-alpine
    # Deve falhar com erro "no matching manifest for windows/amd64..." (pois alpine Ã© Linux)
    # Volte para o modo Linux apÃ³s o teste.
    ```

---

ğŸ“„ PÃ¡gina 325

ğŸ” Resumo dos principais pontos:

*   **16.2 Building multi-arch images from one or more Dockerfiles** (Construindo imagens multi-arch a partir de um ou mais Dockerfiles)
*   **Abordagem 1: Dockerfile Ãšnico (Cross-Platform).**
    *   Como no exemplo `whoami`.
    *   Escrever Dockerfile multi-estÃ¡gio.
    *   Usar imagens base (SDK, runtime) que *jÃ¡ sÃ£o* multi-arch e suportam todas as suas targets.
    *   Construir o *mesmo* Dockerfile em mÃ¡quinas (ou emulaÃ§Ã£o) de arquiteturas diferentes.
    *   BenefÃ­cio: ManutenÃ§Ã£o de 1 Dockerfile sÃ³.
*   Diagrama (Figura 16.4): Ilustra abordagem com Dockerfile Ãºnico para .NET Core SDK (base `mcr.microsoft.com/dotnet/sdk` Ã© multi-arch).
*   **Abordagem 2: Dockerfiles MÃºltiplos (EspecÃ­ficos por Arch).**
    *   Usar quando imagem base *nÃ£o* Ã© multi-arch, ou nÃ£o suporta *todas* as targets, ou precisa de lÃ³gica *diferente* por OS/Arch.
    *   Ex: Um `Dockerfile.linux-amd64`, outro `Dockerfile.windows-amd64`, outro `Dockerfile.linux-arm`.
    *   Desvantagem: Mais arquivos para manter.
    *   Vantagem: Mais flexibilidade para adaptar a cada target.
*   Diagrama (Figura 16.5): Ilustra abordagem com Dockerfiles mÃºltiplos para Maven (talvez imagem base Maven oficial nÃ£o tenha Windows, ou precise de passos diferentes).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o apresenta as duas estratÃ©gias principais para *criar* o conteÃºdo das diferentes variantes de uma imagem multi-arquitetura:
    1.  **Dockerfile Ãšnico:** Se a sua aplicaÃ§Ã£o e as imagens base que vocÃª usa (`FROM ...`) sÃ£o inerentemente multiplataforma (como .NET Core, Go moderno, Python, Node.js rodando sobre bases multi-arch como Debian, Alpine, Ubuntu, ou as oficiais da Microsoft/linguagens), vocÃª pode ter um Ãºnico `Dockerfile`. VocÃª entÃ£o executa `docker build --platform <target>` (usando mÃ¡quinas reais ou emulaÃ§Ã£o) para cada plataforma desejada, usando sempre o *mesmo* Dockerfile. O resultado de cada build Ã© uma variante da imagem para aquela plataforma especÃ­fica. Esta Ã© a abordagem preferida pela simplicidade de manutenÃ§Ã£o. (Figura 16.4)
    2.  **Dockerfiles MÃºltiplos:** Se vocÃª precisa de passos de construÃ§Ã£o diferentes para cada plataforma (ex: instalar dependÃªncias de formas diferentes no Linux e Windows), ou se as imagens base que vocÃª precisa usar sÃ³ existem para plataformas especÃ­ficas, entÃ£o vocÃª cria Dockerfiles separados para cada target (ex: `Dockerfile.windows`, `Dockerfile.linux`). Cada arquivo contÃ©m as instruÃ§Ãµes otimizadas ou necessÃ¡rias para aquela plataforma especÃ­fica. VocÃª constrÃ³i cada Dockerfile na plataforma correspondente (ou via emulaÃ§Ã£o). (Figura 16.5)
*   Ambas as abordagens resultam em mÃºltiplas imagens variantes (ex: `myapp:linux-amd64`, `myapp:windows-amd64`, `myapp:linux-arm64`). O prÃ³ximo passo (SeÃ§Ã£o 16.3) serÃ¡ como "unir" essas variantes sob um Ãºnico nome multi-arch (ex: `myapp:latest`) usando um manifesto.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O exemplo `whoami` (p.323) usou a Abordagem 1.
*   O prÃ³ximo exercÃ­cio (`folder-list`) usarÃ¡ a Abordagem 2, com Dockerfiles separados.

---

ğŸ“„ PÃ¡gina 326

ğŸ” Resumo dos principais pontos:

*   Exemplo `folder-list`: App simples (imprime info runtime, lista pasta). Usa Abordagem 2 (Dockerfiles mÃºltiplos).
*   4 Dockerfiles: Win/Intel, Linux/Intel, Linux/Arm32, Linux/Arm64.
*   Pode-se usar Docker Desktop (Linux mode + emulaÃ§Ã£o) para construir 3 variantes Linux.
*   **ExercÃ­cio Try-it-now:** Construir variantes Linux do `folder-list`.
    *   Navegar para `ch16/exercises/folder-list`.
    *   Build `linux-amd64` (nativo): `docker build -t ...:linux-amd64 -f ./Dockerfile.linux-amd64 .`
    *   Build `linux-arm64` (emulaÃ§Ã£o): `docker build -t ...:linux-arm64 -f ./Dockerfile.linux-arm64 --platform linux/arm64 .`
    *   Build `linux-arm` (emulaÃ§Ã£o): `docker build -t ...:linux-arm -f ./Dockerfile.linux-arm --platform linux/arm .`
    *   Rodar contÃªineres de cada variante para verificar.
        *   `docker run ...:linux-amd64`
        *   `docker run ...:linux-arm64`
        *   `docker run ...:linux-arm`

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz um novo exemplo, `folder-list`, que serÃ¡ construÃ­do usando a **Abordagem 2 (Dockerfiles MÃºltiplos)**. Existem arquivos separados (`Dockerfile.linux-amd64`, `Dockerfile.linux-arm64`, `Dockerfile.linux-arm`, e provavelmente um `Dockerfile.windows-amd64` nÃ£o usado neste exercÃ­cio) para cada plataforma alvo.
*   O exercÃ­cio instrui a construir as trÃªs variantes Linux usando o Docker Desktop (que deve estar no modo Linux).
    *   O build para `linux-amd64` serÃ¡ nativo e rÃ¡pido, pois corresponde Ã  arquitetura do host (Intel/AMD).
    *   Os builds para `linux-arm64` e `linux-arm` usarÃ£o a flag `--platform` e rodarÃ£o via **emulaÃ§Ã£o**, sendo visivelmente mais lentos.
*   ApÃ³s construir as trÃªs imagens variantes (cada uma com uma tag especÃ­fica indicando a plataforma), o exercÃ­cio pede para rodar um contÃªiner de cada uma (`docker run ...`). GraÃ§as Ã  emulaÃ§Ã£o do Docker Desktop, mesmo as imagens Arm devem conseguir rodar na sua mÃ¡quina Intel/AMD (embora possam rodar mais lentamente e ter limitaÃ§Ãµes). O objetivo Ã© verificar se cada variante se identifica corretamente na saÃ­da.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch16\exercises\folder-list
    ```
2.  **Construir Variantes Linux:** (Certifique-se que Docker Desktop estÃ¡ em modo Linux e experimental/buildx ativo)
    ```powershell
    # Nativo amd64
    docker image build -t diamol/ch16-folder-list:linux-amd64 -f ./Dockerfile.linux-amd64 .
    # Emulado arm64 (lento)
    docker image build -t diamol/ch16-folder-list:linux-arm64 -f ./Dockerfile.linux-arm64 --platform linux/arm64 .
    # Emulado arm (lento)
    docker image build -t diamol/ch16-folder-list:linux-arm -f ./Dockerfile.linux-arm --platform linux/arm .
    ```
3.  **Rodar e Verificar:**
    ```powershell
    docker container run diamol/ch16-folder-list:linux-amd64
    # Deve imprimir "Built as: linux/amd64" e info x86_64

    docker container run diamol/ch16-folder-list:linux-arm64
    # Deve imprimir "Built as: linux/arm64" e info aarch64

    docker container run diamol/ch16-folder-list:linux-arm
    # Deve imprimir "Built as: linux/arm" e info armv7l
    ```

---

ğŸ“„ PÃ¡gina 327

ğŸ” Resumo dos principais pontos:

*   Figura 16.6: Mostra saÃ­da dos `docker run` das 3 variantes Linux, confirmando que rodam (via emulaÃ§Ã£o) e se identificam corretamente.
*   Dockerfiles Linux sÃ£o similares (sÃ³ muda string hardcoded).
*   Dockerfile Windows tem mesma funcionalidade, mas usa comandos Windows diferentes (`echo %VAR%`, `dir`). Vantagem de Dockerfiles separados.
*   **Listagem 16.1:** Compara Dockerfile `linux-arm64` e `windows-amd64`.
    *   `FROM` diferente (base especÃ­fica).
    *   Windows usa `escape=` para permitir `\` em paths.
    *   Comandos `CMD` sÃ£o diferentes (`uname` vs `echo %PROC...%`, `ls` vs `dir`).
*   Caso de uso para Dockerfiles mÃºltiplos: Suportar OS diferente (Win/Linux), ou base multi-arch nÃ£o existe/serve. Ex: Golden images Prometheus/Grafana do autor (oficiais sÃ³ Linux -> Windows precisa de Dockerfile separado).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa o resultado do exercÃ­cio anterior (Figura 16.6), confirmando que a emulaÃ§Ã£o do Docker Desktop permite rodar as imagens Arm e que cada uma reporta a arquitetura para a qual foi construÃ­da.
*   Compara (Listagem 16.1) o `Dockerfile.linux-arm64` com o `Dockerfile.windows-amd64` (que nÃ£o foi construÃ­do no exercÃ­cio anterior, mas existe nos fontes). A comparaÃ§Ã£o destaca por que Dockerfiles separados sÃ£o Ã s vezes necessÃ¡rios:
    *   Usam imagens `FROM` diferentes e especÃ­ficas da plataforma.
    *   Usam sintaxe e comandos especÃ­ficos do Sistema Operacional (`uname`/`ls` vs `echo %VAR%`/`dir`).
    *   O Dockerfile Windows usa a diretiva `escape=\`` para facilitar o uso de barras invertidas nos comandos `CMD`.
*   Reafirma os cenÃ¡rios onde mÃºltiplos Dockerfiles sÃ£o a abordagem correta: suporte a OS fundamentalmente diferentes (Linux vs. Windows) ou quando uma imagem base multi-arch adequada nÃ£o estÃ¡ disponÃ­vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Construir Variante Windows (Opcional):**
    1.  Troque Docker Desktop para o modo Windows Containers.
    2.  Navegue para `ch16\exercises\folder-list`.
    3.  Construa: `docker image build -t diamol/ch16-folder-list:windows-amd64 -f ./Dockerfile.windows-amd64 .`
    4.  Rode: `docker container run diamol/ch16-folder-list:windows-amd64` (Deve mostrar "Built as: windows/amd64" e info de CPU Windows).
    5.  Volte para o modo Linux Containers.

---

ğŸ“„ PÃ¡gina 328

ğŸ” Resumo dos principais pontos:

*   Para apps prÃ³prios, tentar ter Dockerfile Ãºnico Ã© mais fÃ¡cil (mas cuidado com comandos OS-specific nÃ£o portÃ¡veis).
*   Risco: `RUN` falha no build se comando invÃ¡lido; `CMD` nÃ£o Ã© verificado, pode falhar sÃ³ no `run`.
*   **ExercÃ­cio Try-it-now:** Demonstrar falha de Dockerfile "multi-arch" mal escrito.
    *   Dockerfile padrÃ£o (`.` ou sem `-f`) na pasta `folder-list` tenta ser multi-arch mas mistura comandos Linux/Windows.
    *   1. Buildar imagem com Dockerfile padrÃ£o: `docker image build -t diamol/ch16-folder-list .` (Implicitamente usa `Dockerfile`). Build deve *passar*.
    *   2. Tentar rodar a imagem: `docker container run diamol/ch16-folder-list`. Deve *falhar*.
*   Figura 16.7: Mostra que build passa, mas `run` falha (erro no `CMD`).
*   Importante testar imagens em todas as targets.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina adverte sobre os riscos ao tentar criar um **Dockerfile Ãºnico** que funcione para mÃºltiplas plataformas (especialmente Linux e Windows): Ã© fÃ¡cil incluir acidentalmente um comando em uma instruÃ§Ã£o `RUN` ou `CMD` que sÃ³ funciona em um dos sistemas operacionais.
*   Comandos `RUN` invÃ¡lidos geralmente causam falha durante o `docker build`. No entanto, comandos dentro de `CMD` ou `ENTRYPOINT` nÃ£o sÃ£o executados durante o build; eles sÃ³ rodam quando um contÃªiner Ã© iniciado (`docker run`). Portanto, um `CMD` com comandos incompatÃ­veis pode resultar em uma imagem que parece construir com sucesso, mas que **falha imediatamente** ao tentar rodar um contÃªiner.
*   O exercÃ­cio demonstra isso usando o `Dockerfile` padrÃ£o na pasta `folder-list`, que foi intencionalmente escrito para falhar. O build provavelmente passarÃ¡, mas o `docker run` falharÃ¡ porque o `CMD` mistura comandos Linux e Windows.
*   A liÃ§Ã£o Ã©: sempre **teste** suas imagens construÃ­das em **todas as plataformas de destino** para garantir que elas nÃ£o apenas construam, mas tambÃ©m rodem corretamente.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar (se necessÃ¡rio):**
    ```powershell
    cd C:\DockerLabs\diamol\ch16\exercises\folder-list
    ```
2.  **Construir Imagem "Ruim":**
    ```powershell
    docker image build -t diamol/ch16-folder-list . # Usa 'Dockerfile' por padrÃ£o
    # Build deve completar sem erros.
    ```
3.  **Tentar Rodar (Deve Falhar):**
    ```powershell
    docker container run diamol/ch16-folder-list
    # Deve sair com erro, mostrando a falha ao executar o CMD misturado.
    ```

---

ğŸ“„ PÃ¡gina 329

ğŸ” Resumo dos principais pontos:

*   Figura 16.7: Confirma build OK, run falha.
*   **Arquiteturas Docker e Nomes:**
    *   Tabela 16.1: Mapeia OS, CPU, Word Length, Nome CPU Docker, Aliases.
        *   Windows / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Arm / 64-bit -> `arm64` (ou `aarch64`, `armv8`)
        *   Linux / Arm / 32-bit -> `arm` (ou `arm32v7`, `armv7`, `armhf`)
*   Docker suporta mais (x86 32-bit, Mainframes IBM Z/POWER/PowerPC com Linux

**You**

ğŸ§  Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), pÃ¡gina por pÃ¡gina, a apartir da pÃ¡gina 329.

Para cada pÃ¡gina, siga este modelo de resposta:

ğŸ“„ PÃ¡gina X

ğŸ” Resumo dos principais pontos:

\[bullet points com os tÃ³picos abordados na pÃ¡gina\]

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

\[explique os conceitos em portuguÃªs claro, com boa didÃ¡tica, como um professor\]

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

\[comandos ou trechos de cÃ³digo aplicÃ¡veis no Windows, usando Docker Desktop e Visual Studio Code\]

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

\[comparaÃ§Ãµes simples ou observaÃ§Ãµes para reforÃ§ar o entendimento\]

âœ… Importante: percorra automaticamente todas as pÃ¡ginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "prÃ³xima".

Quando terminar todas as pÃ¡ginas, diga:

"âœ… Fim da leitura. Todas as pÃ¡ginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Ok, Alexandre, continuando a anÃ¡lise a partir da pÃ¡gina 329.

---

ğŸ“„ PÃ¡gina 329

ğŸ” Resumo dos principais pontos:

*   Figura 16.7: Confirma build OK, run falha.
*   **Arquiteturas Docker e Nomes:**
    *   Tabela 16.1: Mapeia OS, CPU, Word Length, Nome CPU Docker, Aliases comuns.
        *   Windows / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Arm / 64-bit -> `arm64` (ou `aarch64`, `armv8`)
        *   Linux / Arm / 32-bit -> `arm` (ou `arm32v7`, `armv7`, `armhf`)
*   Docker suporta mais arquiteturas (ex: Intel x86 32-bit, Mainframes IBM Z/POWER/PowerPC com Linux).
*   `amd64`: PadrÃ£o para desktops/laptops/servidores Intel/AMD.
*   `arm`/`arm64`: Comuns em celulares, IoT, R-Pi (Pi4 Ã© 64-bit).
*   **16.3 Pushing multi-arch images to registries with manifests** (Enviando imagens multi-arch para registros com manifestos)
    *   Builds (nativos ou emulados) criam variantes separadas (ex: `:linux-amd64`, `:linux-arm64`).
    *   Para uni-las sob um nome Ãºnico multi-arch (ex: `:latest` ou `:2.0`), usa-se um **Manifest** (ou Manifest List).
    *   Manifesto: Metadado no registro que aponta para as diferentes variantes da imagem.
    *   Criado com CLI `docker` e enviado (`push`) ao registro.
    *   ContÃ©m lista das variantes; variantes precisam existir no registro *antes* de criar/enviar manifesto.
    *   Fluxo: Criar/Push variantes -> Criar/Push manifesto.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina finaliza a discussÃ£o sobre Dockerfiles multi-plataforma apresentando uma tabela Ãºtil (Tabela 16.1) que resume os nomes e aliases comuns usados pelo Docker para identificar as principais combinaÃ§Ãµes de Sistema Operacional e Arquitetura de CPU (como `amd64` para Intel/AMD 64-bit, `arm64` para Arm 64-bit, etc.). Conhecer esses nomes Ã© importante ao usar a flag `--platform` ou ao inspecionar imagens.
*   A seÃ§Ã£o 16.3 introduz o passo final para criar uma imagem verdadeiramente multi-arquitetura: o **Manifesto** (ou, mais precisamente, a **Lista de Manifestos**).
*   AtÃ© agora, vocÃª construiu imagens *separadas* para cada plataforma (ex: `minha-app:linux-amd64`, `minha-app:linux-arm64`). O objetivo Ã© ter um Ãºnico nome, como `minha-app:1.0`, que funcione em qualquer plataforma.
*   O Manifesto Ã© a "cola" que faz isso acontecer. Ã‰ um pequeno arquivo de metadados armazenado no registro Docker que diz: "A imagem `minha-app:1.0` na verdade consiste nestas variantes: a imagem X Ã© para linux/amd64, a imagem Y Ã© para linux/arm64, a imagem Z Ã© para windows/amd64, etc.".
*   A ferramenta de linha de comando `docker` (com features experimentais ou `buildx`) permite criar e enviar (`push`) esse manifesto para o registro.
*   **Fluxo Essencial:**
    1.  Construa cada variante da imagem (ex: `meu-app:linux-amd64`, `meu-app:linux-arm64`).
    2.  Envie (`push`) *cada uma* dessas variantes para o registro.
    3.  Use o comando `docker manifest create ...` para criar um manifesto local que lista todas essas variantes sob o nome multi-arch desejado (ex: `meu-app:1.0`).
    4.  Use o comando `docker manifest push ...` para enviar o manifesto para o registro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª jÃ¡ construiu e tagueou as variantes Linux (`:linux-amd64`, `:linux-arm64`, `:linux-arm`) para `diamol/ch16-folder-list` (e talvez a `:windows-amd64`).
*   O prÃ³ximo passo serÃ¡ enviar essas variantes para o Docker Hub e depois criar/enviar o manifesto.
*   Os comandos serÃ£o `docker manifest create` e `docker manifest push`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Manifesto Multi-Arch: Ã‰ o Ã­ndice ou sumÃ¡rio de um manual de instruÃ§Ãµes multilÃ­ngue. O nome na capa Ã© "Manual do Produto X" (`minha-app:1.0`). O Ã­ndice (manifesto) diz: "Para InglÃªs/PC, veja pÃ¡gina 1 (imagem linux/amd64). Para PortuguÃªs/Tablet, veja pÃ¡gina 50 (imagem linux/arm). Para JaponÃªs/Celular, veja pÃ¡gina 100 (imagem windows/amd64)". Quando vocÃª pede o manual, a editora (registro) consulta o Ã­ndice e te envia apenas a seÃ§Ã£o relevante para vocÃª.

---

ğŸ“„ PÃ¡gina 330

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Enviar variantes `folder-list` para Docker Hub.
    *   1. Taguear variantes com seu Docker ID (se ainda nÃ£o feito).
        *   `docker image tag diamol/...:linux-amd64 "$dockerId/ch16-folder-list:linux-amd64"` (e para arm64, arm).
    *   2. Fazer push de todas as tags para seu repositÃ³rio no Hub.
        *   `docker image push "$dockerId/ch16-folder-list"` (push sem tag especÃ­fica envia todas as tags locais para aquele repo).
        *   OU `docker image push --all-tags "$dockerId/ch16-folder-list"`.
*   Registros sÃ£o agnÃ³sticos Ã  arquitetura; armazenam e servem qualquer tipo. Sabem arch da imagem para match no pull.
*   Figura 16.8: Mostra saÃ­da do `push`, indicando arquitetura de cada imagem enviada.
*   **Comandos `docker manifest`:**
    *   Feature nova/experimental da CLI. Precisa habilitar "experimental" na CLI.
    *   Pode ser via Settings Docker Desktop (Command Line) ou editando `~/.docker/config.json`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O primeiro passo para publicar a imagem multi-arch `folder-list` Ã© enviar (`push`) todas as variantes individuais (que vocÃª construiu nas pÃ¡ginas anteriores e tagueou com seu `$dockerId`) para o Docker Hub.
*   O exercÃ­cio mostra como re-taguear as imagens `diamol/...` locais para incluir seu Docker ID e, em seguida, usar `docker image push "$dockerId/ch16-folder-list"`. Quando `docker push` Ã© usado com um nome de repositÃ³rio base (sem tag especÃ­fica), ele tenta enviar todas as tags locais associadas Ã quele repositÃ³rio para o registro remoto. Alternativamente, `--all-tags` faz isso explicitamente.
*   ApÃ³s o push, todas as variantes (`:linux-amd64`, `:linux-arm64`, `:linux-arm`) existirÃ£o no seu repositÃ³rio no Docker Hub, mas ainda como imagens separadas.
*   Para usar os comandos `docker manifest` (necessÃ¡rios para criar a lista de manifestos multi-arch), a pÃ¡gina informa que pode ser necessÃ¡rio habilitar as features experimentais na **CLI Docker**, nÃ£o apenas no Engine (como foi feito para emulaÃ§Ã£o). Isso pode ser feito nas configuraÃ§Ãµes do Docker Desktop ou editando o arquivo de configuraÃ§Ã£o da CLI (`config.json`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Taguear com Seu ID:** (Se `$dockerId` estÃ¡ definido)
    ```powershell
    docker image tag diamol/ch16-folder-list:linux-amd64 "$dockerId/ch16-folder-list:linux-amd64"
    docker image tag diamol/ch16-folder-list:linux-arm64 "$dockerId/ch16-folder-list:linux-arm64"
    docker image tag diamol/ch16-folder-list:linux-arm "$dockerId/ch16-folder-list:linux-arm"
    # Se construiu a Windows:
    # docker image tag diamol/ch16-folder-list:windows-amd64 "$dockerId/ch16-folder-list:windows-amd64"
    ```
2.  **Fazer Login (se necessÃ¡rio):** `docker login -u $dockerId`
3.  **Enviar Variantes:**
    ```powershell
    docker image push --all-tags "$dockerId/ch16-folder-list"
    ```
    Observe a saÃ­da enviando as diferentes tags.
4.  **Habilitar Experimental CLI (se necessÃ¡rio):**
    *   **Docker Desktop:** Settings > Command Line > Marque "Enable experimental features" > Apply & Restart (se pedir).
    *   **(Alternativa) Editar `config.json`:**
        *   Encontre o arquivo: `$HOME\.docker\config.json` (geralmente `C:\Users\<seu_user>\.docker\config.json`).
        *   Abra no VS Code. Adicione ou modifique para ter `"experimental": "enabled"`. Exemplo:
          ```json
          {
            "auths": { ... },
            "credsStore": "desktop.exe",
            "experimental": "enabled"
          }
          ```
        *   Salve o arquivo. (Reiniciar Docker nÃ£o deve ser necessÃ¡rio para config da CLI).

---

ğŸ“„ PÃ¡gina 331

ğŸ” Resumo dos principais pontos:

*   Figura 16.9: Mostra opÃ§Ã£o "Enable Experimental Features" na CLI do Docker Desktop.
*   Se usar Docker Engine (nÃ£o Desktop), editar `~/.docker/config.json` para adicionar `"experimental":"enabled"`.
*   Com CLI experimental ativa, comandos `docker manifest ...` ficam disponÃ­veis.
*   `docker manifest inspect <imagem>:<tag>`: LÃª metadados do manifesto (ou lista de manifestos) do registro. NÃ£o precisa ter imagem local.
*   **ExercÃ­cio Try-it-now:** Inspecionar manifesto da imagem `diamol/base` (que Ã© multi-arch).
    *   Comando: `docker manifest inspect diamol/base`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina detalha como habilitar as features experimentais especificamente para a **CLI Docker**, o que Ã© necessÃ¡rio para usar os comandos `docker manifest`. Mostra a opÃ§Ã£o na interface do Docker Desktop (Figura 16.9) e como fazer editando o arquivo `config.json` manualmente para quem nÃ£o usa o Desktop.
*   Uma vez habilitado, o comando `docker manifest inspect <imagem_no_registro>` torna-se disponÃ­vel. Este comando Ã© muito Ãºtil, pois ele consulta diretamente o registro Docker (sem baixar a imagem) e exibe o conteÃºdo do manifesto (ou da lista de manifestos) para a imagem e tag especificadas. Para uma imagem multi-arch, isso mostrarÃ¡ as diferentes variantes (OS/Arch) que ela suporta.
*   O exercÃ­cio usa `docker manifest inspect diamol/base` para exemplificar. Como `diamol/base` Ã© multi-arch, a saÃ­da mostrarÃ¡ vÃ¡rias entradas, cada uma descrevendo uma variante diferente (ex: linux/amd64, linux/arm64, windows/amd64).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Experimental CLI:** Verifique se habilitou na pÃ¡gina anterior.
2.  **Inspecionar Manifesto:**
    ```powershell
    docker manifest inspect diamol/base
    ```
3.  **Analisar SaÃ­da:** VocÃª verÃ¡ uma estrutura JSON. O campo principal Ã© `"manifests"`, que Ã© uma lista `[]`. Dentro da lista, haverÃ¡ vÃ¡rios objetos `{}`. Cada objeto representa uma variante e terÃ¡ um campo `"platform"` com `"os"` e `"architecture"`. Procure pelas variantes `linux/amd64`, `linux/arm`, `linux/arm64`, `windows/amd64`. (A prÃ³xima figura mostra isso).

---

ğŸ“„ PÃ¡gina 332

ğŸ” Resumo dos principais pontos:

*   Figura 16.10: Mostra saÃ­da (filtrada com `jq`) do `manifest inspect diamol/base`. Confirma mÃºltiplas entradas no array `manifests`, cada uma com `digest`, `platform.os`, `platform.architecture`.
*   `manifest inspect` nÃ£o tem filtro; retorna tudo.
*   **Criando Manifesto (Lista):**
    *   Comando `docker manifest create`. Une imagens variantes sob tag Ãºnica multi-arch.
    *   Manifesto Ã© criado localmente primeiro, depois enviado (`push`).
    *   **Manifest List:** Nome tÃ©cnico para o manifesto multi-arch. ContÃ©m lista de manifestos individuais das imagens variantes.
*   Diagrama (Figura 16.11): RelaÃ§Ã£o Imagem <-> Manifesto <-> Lista de Manifestos no Registro.
    *   Cada imagem/variante (ex: `:linux-amd64`) tem seu prÃ³prio manifesto individual no registro.
    *   A Lista de Manifestos (ex: `:latest`) aponta para os manifestos individuais das variantes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 16.10 confirma a saÃ­da esperada do `manifest inspect` para uma imagem multi-arch como `diamol/base`, mostrando as diferentes plataformas suportadas.
*   A pÃ¡gina entÃ£o explica o processo de **criaÃ§Ã£o** da Lista de Manifestos usando `docker manifest create`.
*   **Conceito:** VocÃª cria um novo objeto (a Lista de Manifestos) que atuarÃ¡ como o "Ã­ndice" para a sua imagem multi-arch. Este Ã­ndice conterÃ¡ referÃªncias Ã s imagens variantes individuais (que jÃ¡ devem existir no registro).
*   A Figura 16.11 ajuda a visualizar: cada imagem concreta (ex: a versÃ£o linux/amd64, a versÃ£o linux/arm64) tem seu prÃ³prio manifesto descrevendo suas camadas. A Lista de Manifestos Ã© um nÃ­vel acima: ela nÃ£o descreve camadas, apenas aponta para os manifestos das variantes que compÃµem a imagem multi-arch.
*   O nome que vocÃª dÃ¡ Ã  Lista de Manifestos (`docker manifest create <nome_lista> ...`) torna-se o nome/tag da sua imagem multi-arch no registro (ex: `meu-app:1.0`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio executarÃ¡ `docker manifest create` usando as variantes `folder-list` que vocÃª enviou para o Hub.

---

ğŸ“„ PÃ¡gina 333

ğŸ” Resumo dos principais pontos:

*   Nome da manifest list vira a tag multi-arch.
*   Pode-se usar tag `latest` (padrÃ£o se omitido no `create`) ou tag de versÃ£o (ex: `:1.0`).
*   **ExercÃ­cio Try-it-now:** Criar e enviar manifesto para `folder-list`.
    *   1. Criar manifesto local (nome `$dockerId/ch16-folder-list`, tag `latest` implÃ­cita), listando as 3 variantes Linux enviadas anteriormente.
        ```bash
        docker manifest create "$dockerId/ch16-folder-list" \
          "$dockerId/ch16-folder-list:linux-amd64" \
          "$dockerId/ch16-folder-list:linux-arm64" \
          "$dockerId/ch16-folder-list:linux-arm"
        ```
    *   2. Enviar manifesto para Docker Hub:
        ```bash
        docker manifest push "$dockerId/ch16-folder-list"
        ```
    *   3. Verificar no Docker Hub (navegar para sua pÃ¡gina).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina mostra como usar os comandos `docker manifest create` e `docker manifest push`.
*   **`docker manifest create <nome_manifest_list> <imagem_variante_1> <imagem_variante_2> ...`**:
    *   `<nome_manifest_list>`: O nome completo (incluindo registro/usuÃ¡rio e tag opcional) que vocÃª quer dar Ã  sua imagem multi-arch (ex: `alexandre/ch16-folder-list:latest`).
    *   `<imagem_variante_...>`: Os nomes completos das imagens variantes individuais (que jÃ¡ devem estar no registro) que farÃ£o parte desta imagem multi-arch.
    *   Este comando cria a lista de manifestos *localmente* (em memÃ³ria ou cache, nÃ£o como um arquivo visÃ­vel).
*   **`docker manifest push <nome_manifest_list>`**:
    *   Envia a lista de manifestos criada localmente para o registro Docker.
*   O exercÃ­cio cria um manifesto chamado `$dockerId/ch16-folder-list` (tag `:latest` implÃ­cita) que agrupa as trÃªs variantes Linux (`:linux-amd64`, `:linux-arm64`, `:linux-arm`) previamente enviadas. Em seguida, envia esse manifesto para o Docker Hub.
*   O resultado Ã© que agora existe uma imagem `$dockerId/ch16-folder-list:latest` no Docker Hub que Ã© multi-arch para essas trÃªs plataformas Linux.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar Manifesto Local (PowerShell):** (Use seu `$dockerId`)
    ```powershell
    # Use backticks ` para quebrar linha no PowerShell
    docker manifest create "$dockerId/ch16-folder-list" `
      "$dockerId/ch16-folder-list:linux-amd64" `
      "$dockerId/ch16-folder-list:linux-arm64" `
      "$dockerId/ch16-folder-list:linux-arm"
    # (Se vocÃª tambÃ©m enviou :windows-amd64, adicione-o Ã  lista)
    ```
2.  **Enviar Manifesto para o Hub:**
    ```powershell
    docker manifest push "$dockerId/ch16-folder-list"
    ```
3.  **Verificar no Hub:**
    *   Use o comando `echo "https://hub.docker.com/r/$dockerId/ch16-folder-list/tags"` para obter o URL.
    *   Acesse no navegador. VocÃª deve ver a tag `latest` e, ao clicar nela ou inspecionÃ¡-la, verÃ¡ as diferentes arquiteturas (linux/amd64, linux/arm64, linux/arm) associadas a ela.

---

ğŸ“„ PÃ¡gina 334

ğŸ” Resumo dos principais pontos:

*   Figura 16.12: Mostra a tag `latest` no Docker Hub com mÃºltiplas arquiteturas (OS/ARCH).
*   Qualquer Docker Engine Linux pode agora rodar `$dockerId/ch16-folder-list`, baixando a variante correta (amd64, arm64, arm).
*   Builds Arm via emulaÃ§Ã£o sÃ£o lentos e nem sempre 100% precisos.
*   Para builds rÃ¡pidos/precisos: **Build Farm** (mÃ¡quinas reais com diferentes OS/Arch).
    *   Exemplo do autor: R-Pis + outros SBCs para buildar imagens do livro. Pipeline CI conecta a cada mÃ¡quina, builda variante, push, depois cria/push manifesto.
*   **16.4 Building multi-arch images with Docker Buildx** (Construindo imagens multi-arch com Docker Buildx)
    *   Alternativa mais eficiente e fÃ¡cil para build farm: **Buildx**.
    *   ExtensÃ£o dos comandos `docker build`. Usa novo engine de build otimizado.
    *   Mesma entrada (`Dockerfile`), mesma saÃ­da (imagem).
    *   Brilha em builds cross-platform: integra com Contexts Docker, distribui builds em mÃºltiplos servers com 1 comando.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o resultado no Docker Hub (Figura 16.12): uma Ãºnica tag (`latest`) agora representa mÃºltiplas arquiteturas.
*   Discute a limitaÃ§Ã£o da **emulaÃ§Ã£o** (usada para construir as variantes Arm no Docker Desktop): Ã© lenta e pode nÃ£o ser perfeita. Para builds frequentes, rÃ¡pidos e confiÃ¡veis em produÃ§Ã£o, a soluÃ§Ã£o ideal Ã© ter uma **Build Farm** com mÃ¡quinas fÃ­sicas (ou VMs) reais para cada arquitetura alvo (ex: um servidor Linux/Intel, um R-Pi para Linux/Arm, um servidor Windows). O pipeline de CI/CD entÃ£o orquestraria os builds em cada nÃ³ da farm.
*   A seÃ§Ã£o 16.4 introduz o **Docker Buildx** como uma ferramenta mais moderna e integrada para gerenciar esse processo de build multi-arquitetura, simplificando a necessidade de uma build farm complexa.
*   **Buildx:**
    *   Ã‰ um plugin da CLI Docker (`docker buildx build ...`) que substitui/estende o `docker build`.
    *   Usa um backend de build mais avanÃ§ado (BuildKit) com melhor performance e caching.
    *   Seu grande trunfo Ã© a capacidade de orquestrar builds para **mÃºltiplas plataformas simultaneamente** usando diferentes "builders" (que podem ser Docker Engines locais ou remotos definidos em Docker Contexts) com um Ãºnico comando.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O Docker Desktop moderno jÃ¡ inclui o Buildx. VocÃª pode verificar com `docker buildx version`.
*   Os prÃ³ximos exercÃ­cios usarÃ£o `docker buildx` no PWD para simular uma build farm distribuÃ­da.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Build Farm Tradicional: Ter vÃ¡rias cozinhas separadas (mÃ¡quinas de build), uma para cada tipo de culinÃ¡ria (arquitetura). O chef (pipeline CI) precisa ir a cada cozinha para fazer a parte correspondente do banquete (imagem multi-arch).
*   Buildx: Um chef super moderno que tem assistentes remotos em cada cozinha especializada. O chef envia a mesma receita base (Dockerfile) para todos os assistentes simultaneamente (`docker buildx build --platform ...`), eles cozinham suas partes, enviam de volta, e o chef monta o banquete final (imagem multi-arch com manifesto) automaticamente.

---

ğŸ“„ PÃ¡gina 335

ğŸ” Resumo dos principais pontos:

*   **LimitaÃ§Ãµes Buildx (atuais):**
    *   NÃ£o funciona com Windows Containers (ainda).
    *   SÃ³ suporta build de Dockerfile *Ãºnico* (nÃ£o a abordagem de mÃºltiplos Dockerfiles por arch).
*   OK se target for sÃ³ variantes Linux CPU.
*   **Exemplo Buildx com PWD (Build Farm Simulada):**
    *   UsarÃ¡ PWD para simular mÃºltiplos nÃ³s de build.
    *   Passo 1: Criar Docker Contexts para cada nÃ³ da "farm".
*   **ExercÃ­cio Try-it-now:** Configurar PWD e Contexts para Buildx.
    *   1. PWD: Criar 2 instÃ¢ncias (`node1`, `node2`).
    *   2. No `node1`: Salvar IP do `node2`, testar SSH (`ssh $node2ip`, `exit`).
    *   3. No `node1`: Criar contexto para `node1` (local): `docker context create node1 --docker "host=unix..."`.
    *   4. No `node1`: Criar contexto para `node2` (remoto via SSH): `docker context create node2 --docker "host=ssh://root@$node2ip"`.
    *   5. Listar contextos: `docker context ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina menciona as limitaÃ§Ãµes atuais do Buildx: ele Ã© focado em Linux e na abordagem de Dockerfile Ãºnico multiplataforma. Se vocÃª precisa construir para Windows ou usar Dockerfiles separados por arquitetura, o mÃ©todo manual com `docker build` + `docker manifest` (SeÃ§Ãµes 16.2 e 16.3) ainda Ã© necessÃ¡rio.
*   O restante da seÃ§Ã£o mostrarÃ¡ como usar o Buildx para construir para mÃºltiplas arquiteturas de CPU Linux (amd64 e 386/i386 - 32-bit Intel) usando um Swarm de 2 nÃ³s no PWD como uma build farm simulada.
*   O primeiro passo Ã© configurar **Docker Contexts** no nÃ³ que atuarÃ¡ como "cliente" do Buildx (`node1`) para que ele saiba como se conectar aos nÃ³s da "farm" (`node1` mesmo e `node2`).
*   O exercÃ­cio guia vocÃª na criaÃ§Ã£o desses dois contextos no `node1` do PWD:
    *   `node1`: Aponta para o Docker Engine local do `node1` via socket Unix.
    *   `node2`: Aponta para o Docker Engine do `node2` via SSH.
*   Com esses contextos definidos, o Buildx poderÃ¡ usÃ¡-los para distribuir os trabalhos de build.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Executar no PWD:**
    1.  Acesse PWD, inicie 2 instÃ¢ncias (`node1`, `node2`).
    2.  Selecione `node1`, anote o IP de `node2`.
    3.  Execute os comandos do exercÃ­cio no terminal do `node1`:
        ```bash
        node2ip=<IP_DO_NODE2>
        echo $node2ip
        ssh root@$node2ip # Testar SSH
        exit
        docker context create node1 --docker "host=unix:///var/run/docker.sock"
        docker context create node2 --docker "host=ssh://root@$node2ip"
        docker context ls
        ```
        VocÃª deve ver os contextos `default`, `node1`, e `node2` listados. `default` ou `node1` estarÃ¡ ativo.

---

ğŸ“„ PÃ¡gina 336

ğŸ” Resumo dos principais pontos:

*   Figura 16.13: Mostra saÃ­da da criaÃ§Ã£o e listagem dos contextos no PWD.
*   Contextos facilitam setup Buildx. Em CI real, criar contextos para cada nÃ³ da farm. NÃ³s podem ser standalone (nÃ£o precisam de Swarm/K8s).
*   **Instalando e Configurando Buildx:**
    *   Buildx Ã© plugin CLI; jÃ¡ vem no Docker Desktop/CE recentes. PWD nÃ£o tem.
*   **ExercÃ­cio Try-it-now:** Instalar Buildx no PWD e criar "builder".
    *   1. Baixar binÃ¡rio Buildx e colocar na pasta de plugins: `wget ...`, `chmod ...`.
    *   2. Criar builder multi-nÃ³ chamado `ch16`, usando `node1` para `linux/amd64`:
        *   `docker buildx create --use --name ch16 --platform linux/amd64 node1`. (`--use` ativa o novo builder).
    *   3. Adicionar `node2` ao mesmo builder `ch16` para `linux/386`:
        *   `docker buildx create --append --name ch16 --platform linux/386 node2`.
    *   4. Listar builders: `docker buildx ls`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com os contextos prontos, o prÃ³ximo passo Ã© configurar o **Buildx**.
*   Como o PWD usa uma versÃ£o do Docker que pode nÃ£o incluir o plugin Buildx, o exercÃ­cio primeiro mostra como baixÃ¡-lo do GitHub e instalÃ¡-lo manualmente no diretÃ³rio de plugins da CLI Docker (`~/.docker/cli-plugins/`).
*   Uma vez instalado, o comando `docker buildx create` Ã© usado para definir um **builder**. Um builder Ã© um agrupamento lÃ³gico de nÃ³s (definidos por contextos) que o Buildx pode usar para realizar builds.
*   O exercÃ­cio cria um builder chamado `ch16`:
    *   Primeiro, ele Ã© criado usando o contexto `node1` e especificando que este nÃ³ deve ser usado para builds `linux/amd64`. A flag `--use` torna este builder o padrÃ£o para comandos `buildx`.
    *   Depois, o contexto `node2` Ã© *adicionado* (`--append`) ao mesmo builder `ch16`, especificando que ele deve ser usado para builds `linux/386` (Intel 32-bit).
*   O comando `docker buildx ls` lista os builders configurados e os nÃ³s/plataformas associados a cada um.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Executar no PWD (continuando no `node1`):**
    1.  **Instalar Buildx:**
        ```bash
        mkdir -p ~/.docker/cli-plugins
        wget -O ~/.docker/cli-plugins/docker-buildx https://github.com/docker/buildx/releases/download/v0.10.4/buildx-v0.10.4.linux-amd64 # Use uma versÃ£o recente, ex v0.10.4
        chmod a+x ~/.docker/cli-plugins/docker-buildx
        docker buildx version # Verificar se instalou
        ```
    2.  **Criar Builder:**
        ```bash
        docker buildx create --use --name ch16 --platform linux/amd64 node1
        ```
    3.  **Adicionar NÃ³ ao Builder:**
        ```bash
        docker buildx create --append --name ch16 --platform linux/386 node2
        ```
    4.  **Listar Builders:**
        ```bash
        docker buildx ls
        ```
        VocÃª deve ver o builder `ch16` listado, com `node1` (linux/amd64) e `node2` (linux/386) como nÃ³s, e um asterisco indicando que ele Ã© o builder ativo.

---

ğŸ“„ PÃ¡gina 337

ğŸ” Resumo dos principais pontos:

*   Figura 16.14: Mostra saÃ­da do `buildx ls`, confirmando builder `ch16` com 2 nÃ³s/plataformas.
*   Buildx Ã© flexÃ­vel: pode detectar plataformas dos nÃ³s ou pode-se especificar manualmente.
*   Build farm `ch16` agora pode construir para Linux Intel 64-bit e 32-bit, *se* Dockerfile usar bases multi-arch que suportem ambos.
*   Buildx distribui build (Dockerfile + contexto) para nÃ³s concorrentemente.
*   **ExercÃ­cio Try-it-now:** Clonar repo, buildar e enviar imagem multi-arch com Buildx.
    *   1. Clonar repo `diamol` no PWD (`node1`).
    *   2. Navegar para `diamol/ch16/exercises/folder-list-2/` (contÃ©m Dockerfile multi-arch).
    *   3. Salvar Docker ID, fazer `docker login`.
    *   4. Usar `docker buildx build` para construir *ambas* as variantes e fazer push *automaticamente*.
        ```bash
        docker buildx build -t "$dockerId/ch16-folder-list-2" \
          --platform linux/amd64,linux/386 --push .
        ```
        *   `-t ...`: Nome da imagem multi-arch final.
        *   `--platform ...`: Lista de plataformas a construir (separadas por vÃ­rgula). Buildx usa os nÃ³s configurados no builder ativo para cada plataforma.
        *   `--push`: Diz ao Buildx para, apÃ³s construir as variantes, fazer push delas E criar/fazer push do manifesto multi-arch automaticamente.
        *   `.`: Contexto de build.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com o builder `ch16` configurado, a "build farm" de 2 nÃ³s estÃ¡ pronta. O Buildx agora sabe que pode usar `node1` para builds `linux/amd64` e `node2` para `linux/386`.
*   O exercÃ­cio final demonstra o poder do `docker buildx build`:
    1.  Clona-se o repositÃ³rio para ter acesso ao `Dockerfile` (em `folder-list-2`, que deve ser um Dockerfile Ãºnico multiplataforma).
    2.  Faz-se login no Docker Hub para permitir o push.
    3.  Executa-se `docker buildx build` com parÃ¢metros chave:
        *   `-t "$dockerId/..."`: Define o nome final da imagem multi-arch que serÃ¡ criada no registro.
        *   `--platform linux/amd64,linux/386`: Especifica as duas plataformas alvo. O Buildx automaticamente direcionarÃ¡ o build amd64 para `node1` e o build 386 para `node2`, executando-os em paralelo.
        *   `--push`: Esta flag Ã© crucial. Ela combina mÃºltiplos passos: constrÃ³i todas as variantes especificadas, envia cada variante construÃ­da para o registro, cria a lista de manifestos multi-arch referenciando essas variantes, e envia a lista de manifestos para o registro sob a tag especificada em `-t`. Tudo em um comando!
        *   `.`: O diretÃ³rio de contexto contendo o Dockerfile.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Executar no PWD (continuando no `node1`):**
    1.  **Clonar Repo:**
        ```bash
        git clone https://github.com/sixeyed/diamol.git
        ```
    2.  **Navegar:**
        ```bash
        cd diamol/ch16/exercises/folder-list-2/
        ```
    3.  **Login Docker Hub:** (Substitua pelo seu ID)
        ```bash
        dockerId=<your-docker-id>
        docker login -u $dockerId # Digite a senha
        ```
    4.  **Build e Push Multi-Arch com Buildx:**
        ```bash
        docker buildx build -t "$dockerId/ch16-folder-list-2" --platform linux/amd64,linux/386 --push .
        ```
        Observe a saÃ­da detalhada e colorida do Buildx, mostrando o progresso em ambos os nÃ³s (targets). Aguarde a conclusÃ£o (pode levar alguns minutos).

---

ğŸ“„ PÃ¡gina 338

ğŸ” Resumo dos principais pontos:

*   Figura 16.15: Mostra final da saÃ­da do `buildx build --push` e a imagem resultante no Docker Hub (tag `latest` com 2 arquiteturas: amd64, 386).
*   Buildx simplifica builds multi-arch:
    *   Combina build das variantes, push das variantes, criaÃ§Ã£o do manifesto, push do manifesto em 1 comando (`--push`).
    *   Distribui trabalho entre nÃ³s da farm.
*   DiferenÃ§a no Hub: Buildx (com `--push`) cria sÃ³ a tag multi-arch (ex: `latest`), nÃ£o as tags individuais por variante (ex: `:linux-amd64`). Mais limpo para usuÃ¡rios.
*   ConclusÃ£o: Se nÃ£o precisar de Windows, Buildx Ã© a melhor forma atual para multi-arch Linux.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa o resultado do comando `buildx build --push` (Figura 16.15). A ferramenta orquestrou todo o processo: enviou o contexto e o Dockerfile para `node1` (para build amd64) e `node2` (para build 386), executou os builds em paralelo, coletou os resultados, enviou as imagens variantes resultantes para o Docker Hub e, finalmente, criou e enviou a lista de manifestos sob a tag `$dockerId/ch16-folder-list-2:latest`.
*   Uma diferenÃ§a notÃ¡vel em relaÃ§Ã£o ao mÃ©todo manual (build + push variantes + manifest create + manifest push) Ã© que o Buildx, por padrÃ£o com `--push`, nÃ£o cria as tags intermediÃ¡rias por arquitetura (como `:linux-amd64`) no registro, apenas a tag final multi-arch (`:latest` ou a especificada em `-t`). Isso torna a visualizaÃ§Ã£o no Docker Hub mais limpa para os consumidores finais da imagem.
*   A conclusÃ£o Ã© que, para cenÃ¡rios focados em mÃºltiplas arquiteturas de CPU **Linux**, o Buildx Ã© a ferramenta preferida devido Ã  sua simplicidade e eficiÃªncia, superando o mÃ©todo manual com `docker manifest`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar no Hub:**
    *   `echo "https://hub.docker.com/r/$dockerId/ch16-folder-list-2/tags"`
    *   Acesse o URL. VocÃª deve ver a tag `latest` com as arquiteturas `linux/amd64` e `linux/386`.
2.  **(Opcional) Remover Builder/Contexts/NÃ³s PWD:**
    *   No `node1` do PWD:
        ```bash
        docker buildx rm ch16 # Remove o builder
        docker context rm node1 node2 # Remove os contexts
        # Feche a sessÃ£o PWD quando terminar.
        ```

---

ğŸ“„ PÃ¡gina 339

ğŸ” Resumo dos principais pontos:

*   **16.5 Understanding where multi-arch images fit in your roadmap** (Entendendo onde imagens multi-arch se encaixam no seu roadmap)
*   Talvez nÃ£o precise agora, mas vale a pena conhecer. Pode se tornar necessÃ¡rio (IoT, custo nuvem Arm, suporte Windows).
*   Diagrama (Figura 16.16): EvoluÃ§Ã£o de um projeto suportando mais arquiteturas ao longo do tempo (comeÃ§a Linux/Intel -> adiciona Arm -> adiciona Windows).
*   **Como se preparar / Future-proofing:**
    1.  **Regra 1:** Sempre usar imagens base (`FROM`) que *jÃ¡ sÃ£o* multi-arch nos seus Dockerfiles.
    2.  **Regra 2:** Evitar comandos especÃ­ficos de OS em `RUN`/`CMD`. Se precisar de lÃ³gica complexa, criar utilitÃ¡rio na mesma linguagem do app (portÃ¡vel).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o discute a importÃ¢ncia estratÃ©gica de considerar o suporte multi-arquitetura, mesmo que nÃ£o seja uma necessidade imediata. O cenÃ¡rio de TI estÃ¡ mudando, com Arm ganhando espaÃ§o em servidores e a necessidade de suportar Windows Containers persistindo em muitos lugares. Estar preparado para adicionar suporte a novas plataformas no futuro Ã© uma boa prÃ¡tica.
*   A Figura 16.16 ilustra como um projeto pode comeÃ§ar suportando apenas Linux/Intel e, posteriormente, adicionar suporte para Arm e/ou Windows conforme os requisitos mudam.
*   Para facilitar essa transiÃ§Ã£o futura, a pÃ¡gina sugere duas regras simples ao escrever *todos* os seus Dockerfiles *desde jÃ¡*:
    1.  **Use Bases Multi-Arch:** Sempre que possÃ­vel, escolha imagens na sua instruÃ§Ã£o `FROM` que jÃ¡ sejam multi-arch e suportem as plataformas que vocÃª *pode* vir a precisar no futuro (pelo menos Linux/Intel, Linux/Arm64, Windows/Intel). A maioria das imagens oficiais populares jÃ¡ sÃ£o assim.
    2.  **Evite Comandos EspecÃ­ficos:** Tente nÃ£o usar comandos nos seus `RUN` ou `CMD` que sÃ³ funcionem em um sistema operacional (como `apt-get` vs. `choco`, ou `uname` vs. `ver`). Se precisar de lÃ³gica complexa, encapsule-a em um script ou pequeno utilitÃ¡rio escrito na mesma linguagem da sua aplicaÃ§Ã£o (que geralmente Ã© multiplataforma), em vez de usar scripts shell especÃ­ficos do OS.
*   Seguir essas regras tornarÃ¡ muito mais fÃ¡cil adicionar suporte a uma nova arquitetura posteriormente, muitas vezes apenas exigindo a configuraÃ§Ã£o do pipeline de CI para construir tambÃ©m para a nova plataforma, sem grandes mudanÃ§as no Dockerfile.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Verificar Base:** Ao escolher uma imagem `FROM`, verifique sua pÃ¡gina no Docker Hub para ver as "Supported Architectures". Prefira imagens que listem `amd64`, `arm64`, `windows/amd64` (se relevante).
*   **Exemplo Comando PortÃ¡vel vs. NÃ£o-PortÃ¡vel:**
    *   NÃ£o-PortÃ¡vel: `RUN apt-get install -y curl` (SÃ³ Linux Debian/Ubuntu)
    *   Mais PortÃ¡vel (se app for .NET): `RUN dotnet tool install --global portable.runtime.tool` (Ferramenta .NET roda onde .NET roda)
    *   PortÃ¡vel (se usar base multi-arch com ferramenta): `RUN apk add --no-cache curl` (Alpine Linux, pode existir em base multi-arch Alpine)

---

ğŸ“„ PÃ¡gina 340

ğŸ” Resumo dos principais pontos:

*   Imagens oficiais no Hub sÃ£o geralmente multi-arch -> boas bases (diretas ou para golden images).
*   Golden images do livro (`diamol/...`) sÃ£o multi-arch. CÃ³digo fonte em `images/` como exemplo.
*   Plataformas de app modernas (Go, Node.js, .NET Core, Java) sÃ£o multi-arch. BD: Postgres Ã© boa opÃ§Ã£o multi-arch.
*   **Build Multi-Arch em ServiÃ§os CI Gerenciados:**
    *   Limitado atualmente. Alguns suportam Linux + Windows.
    *   Para suporte Arm, geralmente precisa de build farm prÃ³pria (VMs na nuvem ou hardware fÃ­sico).
    *   OpÃ§Ãµes: AWS (VMs Linux/Win/Arm), Azure DevOps/GitHub Actions (Linux/Win, talvez Arm no futuro?).
*   Importante: NÃ£o assumir que nunca precisarÃ¡. Seguir boas prÃ¡ticas facilita adicionar suporte depois.
*   **16.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Corrigir Dockerfile para ser multi-arch e otimizado.
    *   Ponto de Partida: Dockerfile (`ch16/lab`) que funciona mas usa base especÃ­fica de arch e comandos OS-specific. Imagem resultante Ã© grande.
*   **Tarefa:** Refatorar Dockerfile para:
    1.  Construir variantes para Linux/Intel, Linux/Arm, Windows/Intel.
    2.  Ser otimizado (provavelmente multi-estÃ¡gio).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a a disponibilidade de imagens base multi-arch (oficiais e as `diamol/` do livro) e plataformas de aplicaÃ§Ã£o que facilitam a criaÃ§Ã£o de aplicaÃ§Ãµes multiplataforma. Menciona Postgres como um bom exemplo de banco de dados multi-arch.
*   Discute a realidade atual dos **serviÃ§os de build gerenciados na nuvem**: o suporte nativo para build multi-arquitetura completo (incluindo Arm) ainda Ã© limitado. Embora muitos suportem Linux e Windows, construir para Arm frequentemente exige que vocÃª configure sua prÃ³pria "build farm" (conjunto de mÃ¡quinas de build) com hardware Arm real (ex: VMs Arm na AWS).
*   O **Lab do CapÃ­tulo 16** foca em refatorar um Dockerfile existente que *nÃ£o* foi escrito pensando em multiplataforma. O Dockerfile original provavelmente usa uma imagem `FROM` especÃ­fica de uma arquitetura e comandos `RUN`/`CMD` que sÃ³ funcionam nela.
*   Sua tarefa Ã© reescrever este Dockerfile para que ele possa ser usado (talvez com pequenas variaÃ§Ãµes ou na abordagem de Dockerfile Ãºnico com build cross-platform) para gerar imagens funcionais e otimizadas para Linux (Intel/Arm) e Windows (Intel). Isso provavelmente envolverÃ¡:
    *   Mudar para uma imagem `FROM` que seja multi-arch.
    *   Substituir comandos OS-specific por alternativas portÃ¡veis ou usar build multi-estÃ¡gio para isolar passos especÃ­ficos.
    *   Aplicar otimizaÃ§Ãµes de tamanho/cache.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 16:**
    1.  Navegue para `ch16\lab`. Examine o `Dockerfile` atual. Identifique a imagem `FROM` e os comandos `RUN`/`CMD`.
    2.  **Escolha a Abordagem:**
        *   **Dockerfile Ãšnico:** Encontre uma imagem base multi-arch que substitua a atual e funcione em Linux/Win/Arm (se possÃ­vel). Reescreva os comandos `RUN`/`CMD` para serem portÃ¡veis (talvez usando um script ou utilitÃ¡rio se a lÃ³gica for complexa). Use build multi-estÃ¡gio para otimizar. Construa com `docker buildx build --platform linux/amd64,linux/arm,windows/amd64 ...` (ou `docker build --platform ...` repetido).
        *   **Dockerfiles MÃºltiplos:** Crie `Dockerfile.linux`, `Dockerfile.windows` (e talvez `Dockerfile.arm`). Cada um usa a melhor base para sua plataforma. Adapte os comandos `RUN`/`CMD` para cada OS. Use build multi-estÃ¡gio em cada um. Construa cada um separadamente (`docker build -f ...`). Depois crie/push o manifesto manualmente (`docker manifest create/push`).
    3.  Verifique se as imagens resultantes funcionam e sÃ£o otimizadas em tamanho.

---

ğŸ“„ PÃ¡gina 341

ğŸ” Resumo dos principais pontos:

*   **Dicas Lab 16:**
    *   InstruÃ§Ãµes Dockerfile (ex: `COPY`, `WORKDIR`) sÃ£o cross-platform. Comandos OS em `RUN` podem nÃ£o ser.
    *   Alguns comandos Windows sÃ£o iguais a Linux. Golden images `diamol/base` podem ter aliases para outros.
*   Link para soluÃ§Ã£o no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Fornece dicas para o Lab 16, sugerindo focar em substituir comandos `RUN` especÃ­ficos do OS por instruÃ§Ãµes Dockerfile (que sÃ£o inerentemente multiplataforma) ou por comandos que funcionem em ambos os ambientes. Menciona que as imagens base `diamol/` podem incluir alguns truques (aliases) para fazer comandos Linux comuns funcionarem no Windows.
*   Oferece o link para a soluÃ§Ã£o de referÃªncia.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Exemplo de SubstituiÃ§Ã£o:**
    *   Ruim (Linux): `RUN mkdir /app/data`
    *   Ruim (Windows): `RUN mkdir C:\app\data`
    *   Bom (Cross-platform Dockerfile): `WORKDIR /app`, `RUN mkdir data` (relativo ao WORKDIR) - Ou atÃ© melhor, se possÃ­vel, usar `VOLUME /app/data` se for para dados persistentes.
*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch16/lab/README.md

---

ğŸ“„ PÃ¡gina 342

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, separando a Parte 3 da Parte 4.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 343

ğŸ” Resumo dos principais pontos:

*   **InÃ­cio da Parte 4: Getting your containers ready for production** (Preparando seus contÃªineres para produÃ§Ã£o)
*   Quase no fim; Ãºltimos tÃ³picos importantes antes de ir para produÃ§Ã£o.
*   Foco Parte 4: Boas prÃ¡ticas essenciais para produÃ§Ã£o.
    *   OtimizaÃ§Ã£o de imagens Docker (tamanho, velocidade, seguranÃ§a).
    *   IntegraÃ§Ã£o com plataforma Docker: ler config, escrever logs.
    *   Abordagens arquiteturais Ãºteis com Docker:
        *   Reverse Proxy.
        *   Message Queue.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina introduz a Parte 4 e final do livro, que se concentra em refinar as imagens e aplicaÃ§Ãµes Docker para estarem verdadeiramente prontas para um ambiente de produÃ§Ã£o.
*   Os tÃ³picos cobrirÃ£o otimizaÃ§Ãµes finais nas imagens, como garantir que as aplicaÃ§Ãµes se integrem bem com os mecanismos de configuraÃ§Ã£o e logging da plataforma Docker, e a introduÃ§Ã£o de dois padrÃµes de arquitetura muito comuns e poderosos no mundo dos contÃªineres:
    *   **Proxy Reverso:** Para gerenciar trÃ¡fego de entrada, seguranÃ§a (SSL) e performance (cache).
    *   **Fila de Mensagens (Message Queue):** Para comunicaÃ§Ã£o assÃ­ncrona, desacoplamento e escalabilidade.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª revisitarÃ¡ a otimizaÃ§Ã£o de imagens (Cap. 17).
*   VocÃª aprenderÃ¡ padrÃµes de configuraÃ§Ã£o e logging mais avanÃ§ados (Caps. 18, 19).
*   VocÃª rodarÃ¡ e configurarÃ¡ proxies reversos como Nginx ou Traefik (Cap. 20).
*   VocÃª rodarÃ¡ e usarÃ¡ filas de mensagens como Redis ou NATS (Cap. 21).

---

ğŸ“„ PÃ¡gina 344

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, separando a introduÃ§Ã£o da Parte 4 do inÃ­cio do CapÃ­tulo 17.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 345

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 17: Optimizing your Docker images for size, speed, and security** (Otimizando suas imagens Docker para tamanho, velocidade e seguranÃ§a)
*   ApÃ³s containerizar e rodar em cluster, otimizaÃ§Ã£o Ã© passo importante antes de ir para produÃ§Ã£o.
*   Por quÃª otimizar? Builds/deploys rÃ¡pidos, seguranÃ§a, evitar problemas (ex: disco cheio).
*   Dockerfile parece simples, mas esconde complexidade -> entender para otimizar.
*   CapÃ­tulo expande Cap. 3 (imagens = mÃºltiplas camadas).
*   **17.1 How you optimize Docker images** (Como otimizar imagens Docker)
    *   Formato Docker jÃ¡ Ã© otimizado (compartilhamento de camadas -> reduz build, rede, disco).
    *   Mas Docker *nÃ£o* remove imagens/camadas antigas automaticamente. Limpeza (`prune`) Ã© manual.
    *   Disco pode encher com camadas nÃ£o usadas (especialmente em dev/teste com updates frequentes).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 17, focado na **otimizaÃ§Ã£o de imagens Docker**. Argumenta que, mesmo apÃ³s ter a aplicaÃ§Ã£o rodando em contÃªineres, hÃ¡ um trabalho de refinamento importante a ser feito na *imagem* para tornÃ¡-la eficiente e segura para produÃ§Ã£o.
*   Os objetivos da otimizaÃ§Ã£o sÃ£o:
    *   **Tamanho:** Imagens menores sÃ£o mais rÃ¡pidas de baixar, enviar e ocupam menos espaÃ§o em disco nos nÃ³s do cluster e nos registros.
    *   **Velocidade (Build):** Otimizar o uso do cache de build acelera o pipeline de CI/CD.
    *   **SeguranÃ§a:** Imagens menores geralmente contÃªm menos software, reduzindo a "superfÃ­cie de ataque" para vulnerabilidades.
*   A seÃ§Ã£o 17.1 comeÃ§a relembrando que o sistema de camadas do Docker jÃ¡ oferece otimizaÃ§Ãµes, mas o acÃºmulo de camadas nÃ£o utilizadas ainda Ã© um problema comum se a limpeza nÃ£o for feita.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O comando `docker system prune` (e `docker image prune`, `docker container prune`, `docker volume prune`) serÃ¡ seu amigo para limpeza.
*   O capÃ­tulo focarÃ¡ em como *construir* imagens que sejam inerentemente menores e mais rÃ¡pidas de construir/transferir.

---

ğŸ“„ PÃ¡gina 346

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Verificar uso de disco com `docker system df`.
*   Figura 17.1: Exemplo de saÃ­da `system df`, mostrando muitos GBs usados por imagens inativas.
*   Servidores antigos podem desperdiÃ§ar centenas de GBs. `docker system prune` ajuda, mas otimizar imagens Ã© melhor.
*   **OtimizaÃ§Ã£o 1: NÃ£o incluir arquivos desnecessÃ¡rios.**
    *   Comum copiar pastas inteiras (`COPY . .`) que incluem docs, imagens, binÃ¡rios nÃ£o usados em runtime.
    *   Seja explÃ­cito no `COPY`.
*   **Listagem 17.1:** ComparaÃ§Ã£o de Dockerfiles.
    *   `v1`: `COPY . .` (copia tudo, incluindo pasta `docs`).
    *   `v2`: `COPY . .` seguido por `RUN rm -rf docs` (tenta remover depois).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio com `docker system df` serve para conscientizar sobre quanto espaÃ§o imagens e outros artefatos Docker podem ocupar se nÃ£o forem gerenciados. A limpeza periÃ³dica com `prune` Ã© recomendada.
*   A primeira tÃ©cnica de otimizaÃ§Ã£o abordada Ã© a mais bÃ¡sica: **evitar incluir lixo na imagem**. Ã‰ muito comum usar `COPY . .` no Dockerfile por conveniÃªncia, mas isso copia *tudo* do diretÃ³rio de contexto, incluindo arquivos que nÃ£o sÃ£o necessÃ¡rios para a aplicaÃ§Ã£o rodar (cÃ³digo fonte apÃ³s compilaÃ§Ã£o, arquivos de teste, documentaÃ§Ã£o, etc.).
*   A Listagem 17.1 apresenta um cenÃ¡rio problemÃ¡tico:
    *   `v1` copia tudo.
    *   `v2` copia tudo e depois tenta *remover* os arquivos desnecessÃ¡rios (`docs`) em um passo `RUN` subsequente.
*   A intuiÃ§Ã£o poderia dizer que `v2` seria menor, mas como veremos, devido ao funcionamento das camadas, isso nÃ£o acontece.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Uso de Disco:**
    ```powershell
    docker system df
    # Anote o tamanho das Imagens (SIZE e RECLAIMABLE)
    ```
2.  **(Opcional) Limpeza:** Se o espaÃ§o recuperÃ¡vel (RECLAIMABLE) for grande, vocÃª pode limpar:
    ```powershell
    docker image prune -a # Remove imagens nÃ£o usadas (cuidado!)
    docker system prune # Remove containers parados, redes/caches nÃ£o usados
    docker system df # Verifique novamente
    ```
3.  **Preparar para PrÃ³ximo ExercÃ­cio:** VocÃª precisarÃ¡ dos Dockerfiles da Listagem 17.1 e do conteÃºdo da pasta `ch17/exercises/build-context` (que deve ter subpastas `app` e `docs`).

---

ğŸ“„ PÃ¡gina 347

ğŸ” Resumo dos principais pontos:

*   Tentar remover em `v2` nÃ£o funciona: `rm` em camada posterior sÃ³ *oculta* arquivos da camada `COPY`, nÃ£o reduz tamanho total.
*   **ExercÃ­cio Try-it-now:** Construir v1 e v2, comparar tamanhos.
    *   Navegar para `ch17/exercises/build-context`.
    *   `docker build -t ...:v1 .`
    *   `docker build -t ...:v2 -f ./Dockerfile.v2 .`
    *   `docker image ls -f reference=diamol/ch17*`.
*   Resultado esperado (Figura 17.2): v1 e v2 tÃªm **exatamente o mesmo tamanho**. `rm` nÃ£o adiantou.
*   ExplicaÃ§Ã£o: Camadas sÃ£o unidas. Arquivos de `COPY` ainda existem na camada 1, sÃ³ estÃ£o escondidos pela camada 2 (`RUN rm`).
*   **ExercÃ­cio Try-it-now:** Provar que arquivos "deletados" ainda existem em camada anterior.
    *   1. Rodar container da imagem final v2: `docker run ...:v2`. (Deve mostrar sÃ³ `app`, nÃ£o `docs`).
    *   2. Ver histÃ³rico de v2: `docker history ...:v2`. Anotar ID da camada *anterior* ao `RUN rm`.
    *   3. Rodar container a partir da *camada anterior*: `docker run <id_camada_anterior>`. (Deve mostrar `app` *e* `docs`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina revela a "pegadinha" da Listagem 17.1: tentar remover arquivos em uma instruÃ§Ã£o `RUN` *depois* de terem sido copiados em uma instruÃ§Ã£o `COPY` anterior **nÃ£o reduz o tamanho final da imagem**.
*   **Por quÃª?** Cada instruÃ§Ã£o (`COPY`, `RUN`) cria uma camada. A imagem final Ã© a uniÃ£o de todas as camadas. A camada criada pelo `COPY . .` contÃ©m as pastas `app` e `docs`. A camada criada pelo `RUN rm -rf docs` contÃ©m uma informaÃ§Ã£o que diz "oculte a pasta `docs` da camada anterior". O resultado visÃ­vel no contÃªiner final nÃ£o mostra `docs`, mas os arquivos fÃ­sicos de `docs` *ainda estÃ£o presentes* na camada do `COPY`, ocupando espaÃ§o.
*   O primeiro exercÃ­cio comprova isso mostrando que as imagens `v1` e `v2` tÃªm o mesmo tamanho.
*   O segundo exercÃ­cio demonstra isso de forma mais concreta:
    *   Executa-se um contÃªiner da imagem `v2` final; ele nÃ£o mostra a pasta `docs`.
    *   Usa-se `docker history` para encontrar o ID da camada criada *antes* da execuÃ§Ã£o do `RUN rm`.
    *   Executa-se um contÃªiner *diretamente* a partir daquele ID de camada anterior. Como este contÃªiner sÃ³ "vÃª" as camadas atÃ© aquele ponto, a instruÃ§Ã£o `rm` ainda nÃ£o foi aplicada, e a pasta `docs` estÃ¡ visÃ­vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\build-context
    ```
2.  **Construir e Comparar Tamanhos:**
    ```powershell
    docker image build -t diamol/ch17-build-context:v1 .
    docker image build -t diamol/ch17-build-context:v2 -f ./Dockerfile.v2 .
    docker image ls -f reference='diamol/ch17-build-context:*'
    # Confirme que v1 e v2 tÃªm o mesmo SIZE.
    ```
3.  **Verificar Camadas:**
    ```powershell
    # Rodar v2 final (nÃ£o mostra 'docs')
    docker container run diamol/ch17-build-context:v2

    # Obter ID da camada ANTES do RUN rm (geralmente a 2Âª linha de cima para baixo)
    docker history diamol/ch17-build-context:v2
    # Anote o IMAGE ID da linha abaixo da que contÃ©m 'rm -rf docs'

    # Rodar container da camada anterior (mostra 'docs')
    docker container run <ID_DA_CAMADA_ANTERIOR>
    ```

---

ğŸ“„ PÃ¡gina 348

ğŸ” Resumo dos principais pontos:

*   Figura 17.3: Mostra saÃ­da do `docker run <camada_anterior>`, confirmando que `docs` ainda estava lÃ¡.
*   **LiÃ§Ã£o 1: Otimizar cada camada.** NÃ£o copiar lixo esperando limpar depois.
*   Melhor: Ser preciso no `COPY`.
*   **Listagem 17.2:** Dockerfile `v3` otimizado. SÃ³ copia pasta `app`.
    ```dockerfile
    FROM diamol/base
    CMD echo app- && ls app && echo docs- && ls docs
    COPY ./app ./app # Copia apenas o necessÃ¡rio
    ```
*   **OtimizaÃ§Ã£o 2: Contexto de Build (`.dockerignore`).**
    *   `docker build` primeiro comprime e envia o **contexto de build** (pasta do build) para o Docker Engine.
    *   Contexto pode conter arquivos desnecessÃ¡rios para o build (ex: `.git`, logs, binÃ¡rios locais). Envio consome tempo/banda.
    *   SoluÃ§Ã£o: Arquivo `.dockerignore` na raiz do contexto. Lista arquivos/pastas a serem **excluÃ­dos** do envio para o Engine.
*   **ExercÃ­cio Try-it-now:** Construir v3 com e sem `.dockerignore`.
    *   1. Build v3 (sem `.dockerignore`): `docker build -t ...:v3 -f ./Dockerfile.v3 .`. Observar tamanho do contexto enviado (ex: 2MB).
    *   2. Renomear arquivo preparado: `mv rename.dockerignore .dockerignore`.
    *   3. Ver conteÃºdo do `.dockerignore` (`cat .dockerignore`). Ex: `docs/`, `Dockerfile*`.
    *   4. Build v3 novamente (com `.dockerignore`): `docker build -t ...:v3 -f ./Dockerfile.v3 .`. Observar tamanho do contexto (ex: 4KB).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a liÃ§Ã£o sobre limpeza de camadas: a Ãºnica forma eficaz Ã© **nÃ£o adicionar o lixo em primeiro lugar**. Use instruÃ§Ãµes `COPY` mais especÃ­ficas em vez de `COPY . .` sempre que possÃ­vel. A Listagem 17.2 mostra a versÃ£o `v3` corrigida, que sÃ³ copia a pasta `app`, resultando em uma imagem menor.
*   Introduz a segunda grande otimizaÃ§Ã£o relacionada ao build: o **Contexto de Build** e o arquivo `.dockerignore`.
*   Quando vocÃª executa `docker build ... .`, o `.` diz ao Docker para usar o diretÃ³rio atual como contexto. A *primeira coisa* que a CLI Docker faz Ã© empacotar *todo* o conteÃºdo desse diretÃ³rio (e subdiretÃ³rios) e enviÃ¡-lo para o Docker Engine (que pode estar na mesma mÃ¡quina ou remotamente). O Engine entÃ£o usa esse contexto para executar o build.
*   Se o diretÃ³rio de contexto contiver muitos arquivos grandes que nÃ£o sÃ£o realmente necessÃ¡rios para construir a imagem (como a pasta `.git`, dependÃªncias baixadas localmente, arquivos de log, outras imagens Docker exportadas, etc.), esse envio inicial pode ser muito lento e consumir muita rede/disco.
*   O arquivo `.dockerignore` (colocado na raiz do contexto, com sintaxe similar ao `.gitignore`) resolve isso. Ele instrui a CLI Docker a **ignorar** certos arquivos e pastas ao criar o pacote de contexto a ser enviado para o Engine. Arquivos ignorados simplesmente nÃ£o fazem parte do contexto que o Engine recebe.
*   O exercÃ­cio demonstra o impacto disso:
    *   O primeiro build `v3` envia um contexto grande (2MB no exemplo, por causa de um arquivo de imagem na pasta `docs`).
    *   ApÃ³s criar/renomear `.dockerignore` para excluir `docs/` e os prÃ³prios `Dockerfile`s, o segundo build `v3` envia um contexto minÃºsculo (4KB), tornando o inÃ­cio do build muito mais rÃ¡pido.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar (se necessÃ¡rio):** `cd C:\DockerLabs\diamol\ch17\exercises\build-context`
2.  **Construir v3 (Sem Ignore):**
    ```powershell
    docker image build -t diamol/ch17-build-context:v3 -f ./Dockerfile.v3 .
    # Observe a linha "Sending build context to Docker daemon X.XMB"
    ```
3.  **Criar/Renomear `.dockerignore`:**
    ```powershell
    Rename-Item -Path ./rename.dockerignore -NewName .dockerignore
    # Verificar conteÃºdo:
    Get-Content .dockerignore
    # Deve listar 'docs/' e 'Dockerfile*'
    ```
4.  **Construir v3 (Com Ignore):**
    ```powershell
    docker image build -t diamol/ch17-build-context:v3 -f ./Dockerfile.v3 .
    # Observe a linha "Sending build context...". O tamanho deve ser muito menor (kB).
    # O tamanho final da imagem (docker image ls) tambÃ©m serÃ¡ menor que v1/v2.
    ```

---

ğŸ“„ PÃ¡gina 349

ğŸ” Resumo dos principais pontos:

*   Figura 17.4: Mostra saÃ­da dos builds v3, destacando reduÃ§Ã£o drÃ¡stica no tamanho do contexto enviado com `.dockerignore`.
*   `.dockerignore` economiza tempo (envio do contexto) e pode economizar espaÃ§o na imagem (evita cÃ³pia acidental).
*   Formato similar a `.gitignore`. Usar templates GitHub como base Ã© bom (incluir `.git/` se Dockerfile na raiz).
*   PrÃ³ximo passo na otimizaÃ§Ã£o: Escolher a imagem base correta.
*   **17.2 Choosing the right base images** (Escolhendo as imagens base corretas)
    *   Escolha da base impacta **seguranÃ§a**, tamanho e tempo de transferÃªncia.
    *   Base OS grande (ex: Ubuntu completo, Windows Server Core) pode ter ferramentas extras -> brechas de seguranÃ§a em container.
    *   Ex: Se imagem base tem `curl`, atacante que invadir container pode usÃ¡-lo para baixar malware ou exfiltrar dados.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre `.dockerignore`, ressaltando seus benefÃ­cios duplos: acelera o inÃ­cio do build (menos dados para enviar) e ajuda a prevenir que arquivos indesejados sejam incluÃ­dos na imagem acidentalmente por um `COPY` abrangente.
*   A seÃ§Ã£o 17.2 move o foco da otimizaÃ§Ã£o para a **escolha da imagem base** (`FROM ...`). Esta Ã© uma das decisÃµes mais impactantes.
*   O argumento principal Ã© que imagens base "grandes" (como uma imagem completa de servidor Ubuntu ou Windows Server Core) trazem muitos utilitÃ¡rios e bibliotecas que sÃ£o Ãºteis em um servidor normal, mas desnecessÃ¡rios e potencialmente **perigosos** dentro de um contÃªiner focado em rodar uma Ãºnica aplicaÃ§Ã£o.
*   Cada ferramenta extra presente na imagem base aumenta a **superfÃ­cie de ataque**. O exemplo dado Ã© o `curl`: se sua aplicaÃ§Ã£o web nÃ£o precisa do `curl` para funcionar, mas ele estÃ¡ presente na imagem base, um invasor que consiga executar comandos no seu contÃªiner comprometido pode usar o `curl` para baixar mais ferramentas de ataque ou enviar dados roubados para fora. O mesmo vale para compiladores, shells avanÃ§ados, editores de texto, etc.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Pense nas imagens base que vocÃª jÃ¡ usou:
    *   `diamol/base`: Provavelmente mÃ­nima.
    *   `diamol/node`, `diamol/openjdk`, `diamol/golang`: ContÃªm runtimes/SDKs, mas idealmente sobre uma base OS mÃ­nima.
    *   `debian:stretch-slim`: Exemplo de base Linux mÃ­nima (Debian).
    *   `mcr.microsoft.com/windows/nanoserver`: Base Windows mÃ­nima.
    *   `ubuntu`, `mcr.microsoft.com/windows/servercore`: Bases maiores.
*   A recomendaÃ§Ã£o serÃ¡ usar as bases *menores* possÃ­veis que ainda permitam sua aplicaÃ§Ã£o funcionar.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Escolher Imagem Base: Ã‰ como escolher o carro para uma viagem especÃ­fica.
    *   Base Grande (Ubuntu/Server Core): Um caminhÃ£o com todas as ferramentas possÃ­veis. Ã“timo se vocÃª precisar consertar tudo na estrada, mas pesado, lento e chama atenÃ§Ã£o.
    *   Base Pequena (Alpine/Nano): Um carro compacto e eficiente. Leva vocÃª ao destino, Ã© rÃ¡pido, gasta pouco, discreto, mas nÃ£o tem ferramentas extras se algo muito inesperado acontecer. Para contÃªineres (que devem ser substituÃ­dos, nÃ£o consertados), o compacto Ã© geralmente melhor.

---

ğŸ“„ PÃ¡gina 350

ğŸ” Resumo dos principais pontos:

*   Vale tambÃ©m para imagens base de plataforma de app (ex: Java JDK vs JRE).
*   Tabela 17.1: Compara tamanhos de imagens OpenJDK 11 (JDK vs JRE vs Slim/Nano).
    *   Linux: JDK=296MB, JRE=103MB, JRE-Slim=69MB.
    *   Windows: JDK=2.4GB, JRE=2.2GB, JRE-Nano=277MB.
*   DiferenÃ§as enormes! Usar variantes `*-slim` (Linux/Debian), `*-alpine` (Linux/Alpine), `*-nanoserver` (Windows) reduz muito tamanho e superfÃ­cie de ataque.
*   Nem todo app funciona nas variantes mÃ­nimas, mas vale a pena testar.
*   Risco de ter SDK na imagem final: Atacante pode compilar/executar cÃ³digo arbitrÃ¡rio dentro do container comprometido.
*   **ExercÃ­cio Try-it-now:** Demonstrar risco do SDK na imagem final.
    *   App: API Java simples (`diamol/ch17-truth-app`) construÃ­da com base JDK (`:11-jdk`).
    *   1. Buildar imagem: `docker build -t diamol/ch17-truth-app .`
    *   2. Rodar container: `docker run -d -p 8010:80 --name truth ...`
    *   3. Testar API: `curl http://localhost:8010/truth` (deve retornar `true`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina estende o argumento sobre bases mÃ­nimas para as imagens que fornecem **runtimes ou SDKs** de linguagens (Java, .NET, Python, etc.). Muitas vezes, essas imagens vÃªm em mÃºltiplas variantes.
*   A Tabela 17.1 Ã© um exemplo chocante com OpenJDK 11:
    *   A imagem com o JDK completo (necessÃ¡rio para *compilar* Java) Ã© muito maior que a imagem apenas com o JRE (necessÃ¡rio para *rodar* Java compilado).
    *   Existem variantes ainda menores (`-slim` no Linux, `-nanoserver` no Windows) que usam um OS base mÃ­nimo e um JRE enxuto.
*   A recomendaÃ§Ã£o Ã© clara: para a imagem *final* da sua aplicaÃ§Ã£o (o Ãºltimo estÃ¡gio do build multi-estÃ¡gio), use a imagem base **mais minimalista possÃ­vel** que contenha apenas o **runtime** necessÃ¡rio, nÃ£o o SDK completo. As bases recomendadas sÃ£o Alpine ou Debian Slim para Linux, e Nano Server para Windows.
*   AlÃ©m do tamanho, o risco de **seguranÃ§a** de incluir o SDK na imagem de produÃ§Ã£o Ã© destacado: se um invasor ganhar acesso ao contÃªiner, ele pode usar o compilador (JDK) presente na imagem para criar e executar seu prÃ³prio cÃ³digo malicioso dentro do seu ambiente.
*   O exercÃ­cio prepara a demonstraÃ§Ã£o desse risco: constrÃ³i e roda uma API Java usando uma imagem base que inclui o JDK completo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\truth-app
    ```
2.  **Construir Imagem (Base JDK):**
    ```powershell
    docker image build -t diamol/ch17-truth-app .
    # Observe o Dockerfile, ele usa 'FROM diamol/openjdk' que provavelmente Ã© baseado no JDK.
    # Verifique o tamanho da imagem: docker image ls diamol/ch17-truth-app
    ```
3.  **Rodar Container:**
    ```powershell
    docker container run -d -p 8010:80 --name truth diamol/ch17-truth-app
    ```
4.  **Testar API:**
    ```powershell
    Invoke-WebRequest http://localhost:8010/truth
    # Ou curl http://localhost:8010/truth
    # A saÃ­da (Content) deve ser 'true'.
    ```

---

ğŸ“„ PÃ¡gina 351

ğŸ” Resumo dos principais pontos:

*   Container `truth` tem API Java rodando, mas tambÃ©m ferramentas para compilar outros apps Java.
*   SuposiÃ§Ã£o: Invasor consegue acesso shell ao container. Imagem "acidentalmente" inclui arquivo de cÃ³digo fonte de teste (`FileUpdateTest.java`).
*   Invasor pode usar JDK (presente na imagem) para compilar e rodar esse cÃ³digo, alterando comportamento do app.
*   **ExercÃ­cio Try-it-now:** Simular invasÃ£o e execuÃ§Ã£o de cÃ³digo.
    *   1. Conectar ao shell do container `truth`: `docker exec -it truth sh` (Linux) ou `cmd` (Windows).
    *   2. Dentro do container, compilar e rodar o cÃ³digo Java de teste: `javac FileUpdateTest.java`, `java FileUpdateTest`.
    *   3. Sair do container: `exit`.
    *   4. Testar API novamente: `curl http://localhost:8010/truth`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina descreve o cenÃ¡rio de ataque: um invasor consegue executar comandos dentro do contÃªiner `truth`. Como a imagem foi construÃ­da sobre uma base com o JDK completo, o invasor encontra as ferramentas `javac` (compilador) e `java` (runtime) disponÃ­veis.
*   O cenÃ¡rio Ã© facilitado por um arquivo de cÃ³digo fonte (`FileUpdateTest.java`) "esquecido" na imagem. O invasor compila esse arquivo usando `javac` e o executa usando `java`. Esse cÃ³digo de teste maliciosamente (ou acidentalmente) altera algum estado ou configuraÃ§Ã£o da aplicaÃ§Ã£o principal que estÃ¡ rodando no mesmo processo ou ambiente.
*   O exercÃ­cio simula exatamente isso:
    1.  Usa `docker exec -it` para obter um shell dentro do contÃªiner `truth` que jÃ¡ estÃ¡ rodando.
    2.  Dentro do shell do contÃªiner, executa os comandos `javac` e `java` para compilar e rodar o cÃ³digo malicioso/teste.
    3.  Sai do contÃªiner.
    4.  Acessa a API `/truth` novamente pelo host. A expectativa Ã© que a resposta tenha mudado (de `true` para `false`), demonstrando que o cÃ³digo executado pelo "invasor" afetou a aplicaÃ§Ã£o principal.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Conectar ao Container:** (Assumindo imagem base Linux)
    ```powershell
    docker container exec -it truth sh
    ```
2.  **Dentro do Container (`/app #`):**
    ```bash
    ls # VocÃª deve ver FileUpdateTest.java e talvez outros arquivos .class/.jar
    javac FileUpdateTest.java # Compila
    ls # Agora deve ter FileUpdateTest.class
    java FileUpdateTest # Executa (pode imprimir algo ou nÃ£o)
    exit
    ```
3.  **Testar API Novamente:**
    ```powershell
    Invoke-WebRequest http://localhost:8010/truth
    # Ou curl http://localhost:8010/truth
    # A saÃ­da (Content) agora deve ser 'false'.
    ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Deixar SDK na imagem de produÃ§Ã£o: Ã‰ como deixar a caixa de ferramentas completa do mecÃ¢nico dentro do carro depois que ele terminou o conserto. Se um ladrÃ£o arrombar o carro, ele pode usar as ferramentas para desmontar o carro por dentro ou fazer outras modificaÃ§Ãµes indesejadas. Leve apenas o estepe e o macaco (o runtime), nÃ£o a oficina inteira (o SDK).

---

ğŸ“„ PÃ¡gina 352

ğŸ” Resumo dos principais pontos:

*   Figura 17.5: Mostra saÃ­da antes (`true`) e depois (`false`) da "invasÃ£o".
*   Exemplo Ã© simplificado, mas ilustra risco real. Ataque funcionaria mesmo se container tivesse rede restrita.
*   LiÃ§Ã£o: Imagem base deve ter sÃ³ o necessÃ¡rio para rodar app, sem ferramentas de build (exceto linguagens interpretadas onde build tools = runtime tools).
*   **Golden Images:** Revisitadas como soluÃ§Ã£o.
    *   Time dedicado escolhe base mÃ­nima oficial, adiciona configs/ferramentas *essenciais* da empresa.
    *   Imagem `diamol/openjdk` do autor segue essa prÃ¡tica (multi-arch, base mÃ­nima).
    *   Pode-se integrar **varredura de seguranÃ§a** (ex: Anchore) no build da golden image.
*   **Anchore:** Ferramenta open source para anÃ¡lise de imagem Docker.
    *   Roda em containers (mas nÃ£o multi-arch atualmente).
    *   ExercÃ­cio requer Linux/Intel (Docker Desktop ou PWD).
*   **ExercÃ­cio Try-it-now:** Usar Anchore para analisar golden image `diamol/openjdk`.
    *   Navegar para `ch17/exercises/anchore`.
    *   1. Iniciar stack Anchore: `docker-compose up -d`.
    *   2. Esperar download do BD de vulnerabilidades (pode levar 15 min): `docker exec ... anchore-cli system wait`.
    *   3. Copiar Dockerfile da golden image para container Anchore API: `docker cp ...`.
    *   4. Adicionar imagem + Dockerfile para anÃ¡lise: `docker exec ... anchore-cli image add diamol/openjdk --dockerfile /Dockerfile`.
    *   5. Esperar anÃ¡lise completar: `docker exec ... anchore-cli image wait diamol/openjdk`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a demonstraÃ§Ã£o do risco do SDK e reforÃ§a a importÃ¢ncia de usar imagens base mÃ­nimas contendo apenas o runtime.
*   Reintroduz as **Golden Images** (Cap. 5) como a melhor prÃ¡tica para gerenciar imagens base em uma organizaÃ§Ã£o. Uma equipe centralizada pode criar e manter essas golden images, garantindo que usem bases oficiais mÃ­nimas, adicionem apenas o necessÃ¡rio e, crucialmente, passem por **varreduras de seguranÃ§a** regulares.
*   Introduz o **Anchore Engine** como uma ferramenta open source popular para realizar essas varreduras de seguranÃ§a em imagens Docker. O Anchore analisa todas as camadas da imagem, identifica o software instalado (pacotes do SO, bibliotecas de linguagem) e compara com bancos de dados de vulnerabilidades conhecidas (CVEs).
*   O exercÃ­cio instrui a iniciar o Anchore (que roda como mÃºltiplos contÃªineres definidos em um `docker-compose.yml`) e usÃ¡-lo para analisar a golden image `diamol/openjdk` do autor. O processo envolve:
    1.  Iniciar os contÃªineres do Anchore.
    2.  Esperar que ele baixe seu banco de dados de vulnerabilidades (demorado na primeira vez).
    3.  Adicionar a imagem a ser analisada (`diamol/openjdk`) ao Anchore, opcionalmente fornecendo seu Dockerfile para anÃ¡lise extra.
    4.  Esperar que a anÃ¡lise seja concluÃ­da.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Importante:** Anchore Engine (nesta versÃ£o do livro) nÃ£o roda bem em Windows Containers ou Mac. Use **Docker Desktop no modo Linux** ou **Play With Docker (PWD)**.
1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\anchore
    ```
2.  **Iniciar Anchore:**
    ```powershell
    docker-compose up -d
    ```
3.  **Esperar BD (em outro terminal):** Pode levar bastante tempo!
    ```powershell
    docker exec anchore_engine-api_1 anchore-cli system wait
    # Mantenha este terminal esperando. Continue os prÃ³ximos passos em outro.
    ```
4.  **Copiar Dockerfile (em outro terminal):**
    ```powershell
    # O path pode precisar de ajuste dependendo de onde estÃ¡ a pasta 'images'
    docker container cp "$(pwd)/../../../images/openjdk/Dockerfile" anchore_engine-api_1:/Dockerfile
    ```
5.  **Adicionar Imagem para AnÃ¡lise:**
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image add diamol/openjdk --dockerfile /Dockerfile
    ```
6.  **Esperar AnÃ¡lise:**
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image wait diamol/openjdk
    ```
    Aguarde este comando retornar antes de prosseguir para a prÃ³xima pÃ¡gina.

---

ğŸ“„ PÃ¡gina 353

ğŸ” Resumo dos principais pontos:

*   Figura 17.6: Mostra comando `image add` e `image wait` do Anchore.
*   ApÃ³s anÃ¡lise, Anchore conhece muito sobre a imagem:
    *   LicenÃ§as open source usadas.
    *   Detalhes do OS e plataforma de app.
    *   Vulnerabilidades de seguranÃ§a (CVEs) nos binÃ¡rios.
*   Resultados podem ser usados como quality gate para aceitar/rejeitar update de imagem base.
    *   Ex: Falhar build se licenÃ§a proibida ou CVE crÃ­tica encontrada.
*   Anchore tem plugins CI/CD (Jenkins) para automaÃ§Ã£o de polÃ­ticas.
*   Pode-se consultar resultados via API Anchore.
*   **ExercÃ­cio Try-it-now:** Consultar resultados da anÃ¡lise Anchore.
    *   1. Checar componentes Java encontrados: `docker exec ... anchore-cli image content diamol/openjdk java`.
    *   2. Checar vulnerabilidades conhecidas: `docker exec ... anchore-cli image vuln diamol/openjdk all`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Com a anÃ¡lise do Anchore concluÃ­da, a pÃ¡gina explica os tipos de informaÃ§Ãµes que ele extrai da imagem Docker: detalhes sobre o software instalado, suas licenÃ§as e, mais importante, quaisquer **vulnerabilidades de seguranÃ§a conhecidas** associadas a esses componentes.
*   Essas informaÃ§Ãµes sÃ£o valiosas para tomar decisÃµes informadas sobre quais imagens base usar e quando atualizÃ¡-las. Uma organizaÃ§Ã£o pode definir polÃ­ticas (ex: "nÃ£o usar imagens com vulnerabilidades crÃ­ticas ou altas nÃ£o corrigidas", "nÃ£o usar software com licenÃ§a X") e usar o Anchore (integrado ao pipeline de CI/CD) para **automatizar a verificaÃ§Ã£o** dessas polÃ­ticas. Builds de imagens que violarem as polÃ­ticas podem ser automaticamente reprovados.
*   O exercÃ­cio mostra como consultar manualmente alguns desses resultados usando a CLI do Anchore (executada dentro do contÃªiner da API do Anchore):
    *   `image content ... java`: Lista os pacotes/componentes relacionados a Java encontrados na imagem.
    *   `image vuln ... all`: Lista todas as vulnerabilidades (CVEs) encontradas em *qualquer* componente da imagem.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Consultar ConteÃºdo Java:** (ApÃ³s `image wait` completar na p.352)
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image content diamol/openjdk java
    ```
2.  **Consultar Vulnerabilidades:**
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image vuln diamol/openjdk all
    ```
    Observe a saÃ­da. Pode haver uma lista de CVEs. Preste atenÃ§Ã£o na coluna `Severity` (Negligible, Low, Medium, High, Critical).

---

ğŸ“„ PÃ¡gina 354

ğŸ” Resumo dos principais pontos:

*   Figura 17.7: Mostra saÃ­da (parcial) do `image content` e `image vuln`.
*   Resultados para `diamol/openjdk`: detalhes Java, lista de CVEs (maioria `Negligible`). SaÃ­da inclui link para detalhes da CVE.
*   Resultados aceitÃ¡veis pois usou base OpenJDK *mÃ­nima*.
*   Se analisasse `openjdk:11-jdk` (completo), veria mais CVEs, incluindo `Low` em lib SSL. Poderia ser inaceitÃ¡vel.
*   Scan reforÃ§a valor de usar bases mÃ­nimas e golden images.
*   Outras ferramentas de scan: Clair (open source), Aqua, Twistlock (comerciais, integram com registro).
*   Ferramentas ajudam a entender seguranÃ§a e dÃ£o confianÃ§a nas golden images.
*   Pode-se escanear imagens de app tambÃ©m; polÃ­tica pode exigir que `FROM` seja golden image.
*   **17.3 Minimizing image layer count and layer size** (Minimizando contagem e tamanho das camadas da imagem)
    *   ApÃ³s escolher base segura/mÃ­nima, prÃ³ximo passo: otimizar o que *vocÃª* adiciona.
    *   Foco: Instalar software. Processos de instalaÃ§Ã£o podem deixar "resÃ­duos" (listas de pacotes cacheadas, dependÃªncias recomendadas desnecessÃ¡rias).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa os resultados do scan do Anchore para a imagem `diamol/openjdk` (Figura 17.7). Como esperado de uma imagem baseada em um JRE mÃ­nimo, as vulnerabilidades encontradas sÃ£o poucas e de baixa severidade ("Negligible"). A pÃ¡gina contrasta isso com o que seria encontrado na imagem JDK completa, que teria mais software e, consequentemente, mais vulnerabilidades potenciais. Isso reforÃ§a a recomendaÃ§Ã£o de usar bases mÃ­nimas e o valor das golden images (que podem ser escaneadas e aprovadas).
*   Menciona outras ferramentas de mercado (open source e comerciais) que fazem anÃ¡lise de seguranÃ§a similar. O importante Ã© integrar *alguma* forma de varredura no seu processo. Escanear as imagens finais das aplicaÃ§Ãµes (e nÃ£o apenas as golden images) tambÃ©m Ã© recomendado, e pode-se criar polÃ­ticas para garantir que elas usem as golden images aprovadas como base.
*   A seÃ§Ã£o 17.3 volta o foco para a otimizaÃ§Ã£o do **seu prÃ³prio Dockerfile**, especificamente em como vocÃª **adiciona software** (ex: usando gerenciadores de pacote como `apt-get`, `apk`, `yum`) ou arquivos Ã s suas imagens. A meta Ã© garantir que cada camada adicione o mÃ­nimo necessÃ¡rio, evitando "resÃ­duos" que aumentam o tamanho.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **(Opcional) Parar Anchore:** Se nÃ£o for usar mais:
    ```powershell
    # Estando em ch17\exercises\anchore
    docker-compose down -v # -v remove os volumes com o BD do Anchore
    ```
*   A prÃ³xima seÃ§Ã£o mostrarÃ¡ otimizaÃ§Ãµes para `apt-get` (Linux). Os princÃ­pios se aplicam a outros gerenciadores (ex: `choco` no Windows, `apk` no Alpine).

---

ğŸ“„ PÃ¡gina 355

ğŸ” Resumo dos principais pontos:

*   **OtimizaÃ§Ã£o com Gerenciadores de Pacote (Exemplo: `apt-get` Debian/Linux):**
*   **ExercÃ­cio Try-it-now:** Comparar install nÃ£o otimizado vs. otimizado. (Requer host Linux ou Docker Desktop modo Linux).
    *   Navegar para `ch17/exercises/socat`.
    *   `v1`: Instala `curl` e `socat` com comandos `apt-get` padrÃ£o em `RUN`s separados.
        *   `docker build -t ...:v1 .`
    *   `v2`: Usa tweaks (`--no-install-recommends`, `rm` cache `apt`) em `RUN` Ãºnico.
        *   `docker build -t ...:v2 -f Dockerfile.v2 .`
    *   Comparar tamanhos: `docker image ls -f reference=diamol/ch17-socat`.
*   Resultado esperado (Figura 17.8): Imagem `v2` significativamente menor (ex: 20MB a menos, >20% de reduÃ§Ã£o).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina foca em como otimizar a instalaÃ§Ã£o de pacotes usando gerenciadores como o `apt-get` (comum em Debian/Ubuntu). O exercÃ­cio compara duas abordagens:
    *   **NÃ£o Otimizada (`v1`):** Usa comandos `apt-get update` e `apt-get install` separados em mÃºltiplas instruÃ§Ãµes `RUN`. Isso Ã© ineficiente porque:
        1.  `apt-get update` baixa listas de pacotes que ficam cacheadas em uma camada.
        2.  Cada `apt-get install` pode baixar o pacote e suas dependÃªncias (incluindo as "recomendadas", nem sempre necessÃ¡rias) em camadas separadas.
        3.  Os arquivos de cache do `apt` permanecem nas camadas intermediÃ¡rias, mesmo que nÃ£o sejam necessÃ¡rios na imagem final.
    *   **Otimizada (`v2`):** Aplica vÃ¡rias tÃ©cnicas em uma **Ãºnica instruÃ§Ã£o `RUN`** (para evitar camadas intermediÃ¡rias desnecessÃ¡rias):
        1.  Chama `apt-get update` no inÃ­cio.
        2.  Usa `apt-get install -y --no-install-recommends ...` para instalar os pacotes desejados (`curl`, `socat`) *sem* puxar dependÃªncias opcionais/recomendadas.
        3.  No final do *mesmo* `RUN`, executa `rm -rf /var/lib/apt/lists/*` para limpar o cache de listas de pacotes baixado pelo `update`.
*   Como tudo acontece em uma Ãºnica camada `RUN`, a limpeza do cache efetivamente remove esses arquivos do resultado final, e a opÃ§Ã£o `--no-install-recommends` evita baixar pacotes extras. O resultado Ã© uma imagem consideravelmente menor.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Requer Docker Desktop MODO LINUX.**
1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\socat
    ```
2.  **Construir v1:**
    ```powershell
    docker image build -t diamol/ch17-socat:v1 .
    ```
3.  **Construir v2:**
    ```powershell
    docker image build -t diamol/ch17-socat:v2 -f Dockerfile.v2 .
    ```
4.  **Comparar Tamanhos:**
    ```powershell
    docker image ls -f reference=diamol/ch17-socat
    # Observe a diferenÃ§a significativa no SIZE entre v1 e v2.
    ```

---

ğŸ“„ PÃ¡gina 356

ğŸ” Resumo dos principais pontos:

*   Figura 17.8: Confirma reduÃ§Ã£o de tamanho de ~20MB com otimizaÃ§Ãµes `apt-get`.
*   **Listagem 17.3:** Compara Dockerfiles `v1` (nÃ£o otimizado) e `v2` (otimizado) do `socat`. Destaca `RUN` Ãºnico, `--no-install-recommends`, `rm` do cache apt.
*   **BenefÃ­cio `RUN` Ãšnico:** Cria 1 camada sÃ³. Reduz contagem total (mÃ¡ximo ~127), mas principal vantagem Ã© **clareza/atomicidade** e permitir limpeza eficaz (como `rm` do cache apt na mesma camada da instalaÃ§Ã£o).
*   **Erro Comum:** Colocar `rm` em `RUN` separado -> nÃ£o reduz tamanho (sÃ³ oculta).
*   **OtimizaÃ§Ã£o 3: Download + ExtraÃ§Ã£o + Limpeza em Camada Ãšnica.**
    *   CenÃ¡rio comum: Baixar arquivo comprimido (zip, tar.gz), extrair, deletar arquivo original.
    *   Temptador: Fazer em `RUN`s separados durante dev (para cache do download).
    *   OtimizaÃ§Ã£o Final: Combinar download, extraÃ§Ã£o e `rm` do arquivo baixado em um **Ãºnico `RUN`**.
*   **ExercÃ­cio Try-it-now:** Exemplo com dataset ML (grande).
    *   Navegar para `ch17/exercises/ml-dataset`.
    *   `v1`: `RUN wget ...`, `RUN tar ...`, `RUN rm ...` (separados).
        *   `docker build -t ...:v1 .`
    *   `v2`: Otimizado (download, extraÃ§Ã£o *seletiva*, `rm` em `RUN` Ãºnico).
        *   `docker build -t ...:v2 -f Dockerfile.v2 .`
    *   Comparar tamanhos: `docker image ls -f reference=diamol/ch17-ml-dataset`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Listagem 17.3 deixa clara a diferenÃ§a entre as abordagens `apt-get`. A chave da otimizaÃ§Ã£o `v2` Ã© realizar o `update`, `install` (com `--no-install-recommends`) e a limpeza (`rm -rf /var/lib/apt/lists/*`) **dentro da mesma instruÃ§Ã£o `RUN`**. Isso garante que os arquivos temporÃ¡rios do cache do `apt` sejam removidos *antes* da camada ser finalizada, efetivamente reduzindo o tamanho.
*   A pÃ¡gina entÃ£o generaliza esse princÃ­pio para outro cenÃ¡rio comum: baixar um arquivo compactado, extrair seu conteÃºdo e remover o arquivo compactado original. Se esses passos forem feitos em instruÃ§Ãµes `RUN` separadas, o arquivo compactado original (potencialmente grande) permanecerÃ¡ em uma camada intermediÃ¡ria, inflando o tamanho final da imagem, mesmo que tenha sido removido depois.
*   A **otimizaÃ§Ã£o** Ã© combinar tudo em um Ãºnico `RUN`: `RUN wget ... && tar ... && rm ...`. O `&&` garante que os comandos rodem em sequÃªncia na mesma camada.
*   O exercÃ­cio compara uma versÃ£o nÃ£o otimizada (`v1`) com uma otimizada (`v2`) para baixar e extrair (parcialmente) um dataset de Machine Learning. A versÃ£o `v2` nÃ£o sÃ³ combina os passos, mas tambÃ©m usa uma opÃ§Ã£o do `tar` para extrair *apenas* o arquivo necessÃ¡rio, evitando extrair GBs de dados desnecessÃ¡rios para depois deletÃ¡-los (o que tambÃ©m nÃ£o funcionaria bem).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\ml-dataset
    ```
2.  **Construir v1 (Lento/Grande):**
    ```powershell
    docker image build -t diamol/ch17-ml-dataset:v1 .
    # Este pode demorar e ocupar bastante espaÃ§o temporÃ¡rio durante o build.
    ```
3.  **Construir v2 (Otimizado):**
    ```powershell
    docker image build -t diamol/ch17-ml-dataset:v2 -f Dockerfile.v2 .
    ```
4.  **Comparar Tamanhos:**
    ```powershell
    docker image ls -f reference=diamol/ch17-ml-dataset
    # A diferenÃ§a de SIZE deve ser GIGANTESCA.
    ```

---

ğŸ“„ PÃ¡gina 357

ğŸ” Resumo dos principais pontos:

*   Figura 17.9: Mostra comparaÃ§Ã£o de tamanho das imagens ML dataset (v1 ~2.5GB, v2 ~24MB). DiferenÃ§a enorme.
*   NÃ£o Ã© exemplo tÃ£o artificial; comum separar `RUN`s durante dev para debug/cache. Precisa otimizar (combinar) depois.
*   **Listagem 17.4:** Dockerfile otimizado (`v2`) do ML dataset. Combina `wget`, `tar` (extraÃ§Ã£o seletiva), `rm` em `RUN` Ãºnico.
    *   ExtraÃ§Ã£o seletiva (`tar -xf ... <arquivo_especifico>`) tambÃ©m crucial para evitar inflar camada.
*   **Alternativa Melhor: Multi-Stage Builds para OtimizaÃ§Ã£o.**
    *   Revisita multi-stage (Cap. 4). Ideal para otimizar imagem final.
*   **Listagem 17.5:** RefatoraÃ§Ã£o do ML dataset usando multi-stage (`v3`).
    ```dockerfile
    # EstÃ¡gio 1: Download
    FROM diamol/base AS download
    ARG DATASET_URL=...
    RUN wget -O dataset.tar.gz ${DATASET_URL}

    # EstÃ¡gio 2: ExtraÃ§Ã£o Completa (em ambiente temporÃ¡rio)
    FROM diamol/base AS expand
    COPY --from=download dataset.tar.gz .
    RUN tar xvzf dataset.tar.gz

    # EstÃ¡gio 3: Final (copia SÃ“ o arquivo necessÃ¡rio)
    FROM diamol/base
    WORKDIR /dataset/url_svmlight
    COPY --from=expand url_svmlight/Day1.svm . # Copia sÃ³ Day1.svm
    ```
*   BenefÃ­cios multi-stage aqui:
    *   Dockerfile mais legÃ­vel (cada estÃ¡gio tem propÃ³sito claro).
    *   NÃ£o precisa de otimizaÃ§Ãµes complexas nos comandos (`tar` pode extrair tudo no estÃ¡gio `expand`).
    *   Imagem final sÃ³ contÃ©m o que foi explicitamente copiado (`Day1.svm`).
    *   Tamanho final de `v3` serÃ¡ pequeno como `v2` otimizado.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma a drÃ¡stica reduÃ§Ã£o de tamanho obtida com a otimizaÃ§Ã£o do `RUN` Ãºnico e extraÃ§Ã£o seletiva (Figura 17.9, Listagem 17.4).
*   Introduz uma abordagem **ainda melhor** e mais legÃ­vel para cenÃ¡rios como este: usar **builds multi-estÃ¡gio**.
*   A Listagem 17.5 reescreve o processo de obtenÃ§Ã£o do arquivo `Day1.svm` usando trÃªs estÃ¡gios:
    1.  **`download`:** Usa `wget` para baixar o arquivo `.tar.gz` completo.
    2.  **`expand`:** Copia o `.tar.gz` do estÃ¡gio `download` e usa `tar` para extrair **todo** o conteÃºdo do arquivo compactado (sem se preocupar com tamanho aqui, pois este estÃ¡gio Ã© temporÃ¡rio).
    3.  **`final` (sem nome):** Usa uma imagem base limpa, define o `WORKDIR` e copia **apenas** o arquivo especÃ­fico (`Day1.svm`) que realmente interessa, a partir do estÃ¡gio `expand`.
*   **Vantagens:**
    *   **Legibilidade:** Cada estÃ¡gio tem uma responsabilidade clara.
    *   **Simplicidade:** Os comandos dentro de cada estÃ¡gio podem ser mais diretos (ex: nÃ£o precisa da extraÃ§Ã£o seletiva do `tar`).
    *   **OtimizaÃ§Ã£o AutomÃ¡tica:** A imagem final sÃ³ terÃ¡ o `Day1.svm` porque apenas ele foi copiado. Todo o "lixo" (arquivo `.tar.gz`, outros arquivos extraÃ­dos) dos estÃ¡gios `download` e `expand` Ã© descartado automaticamente. O resultado Ã© uma imagem final pequena, obtida de forma mais clara.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª criarÃ¡ um `Dockerfile.v3` com o conteÃºdo da Listagem 17.5 (na pasta `ml-dataset`).
*   O prÃ³ximo exercÃ­cio construirÃ¡ essa versÃ£o `v3` e suas sub-partes.

---

ğŸ“„ PÃ¡gina 358

ğŸ” Resumo dos principais pontos:

*   Build multi-stage `v3` serÃ¡ pequeno como `v2`, mas mais fÃ¡cil de debugar.
*   **Build por EstÃ¡gio (Debugging):** Pode-se construir atÃ© um estÃ¡gio especÃ­fico usando `docker build --target <nome_estagio>`.
*   **ExercÃ­cio Try-it-now:** Construir `v3` completo e tambÃ©m atÃ© estÃ¡gios intermediÃ¡rios.
    *   Navegar para `ml-dataset`.
    *   1. Build completo `v3`: `docker build -t ...:v3 -f Dockerfile.v3 .`
    *   2. Build atÃ© `download`: `docker build -t ...:v3-download -f Dockerfile.v3 --target download .`
    *   3. Build atÃ© `expand`: `docker build -t ...:v3-expand -f Dockerfile.v3 --target expand .`
    *   4. Comparar tamanhos: `docker image ls -f reference='diamol/ch17-ml-dataset:v3*'`.
*   Figura 17.10: Mostra tamanhos: `v3` (final) pequeno (24MB), `v3-download` mÃ©dio (200MB+), `v3-expand` enorme (2GB+).
*   Confirma que multi-stage otimiza tamanho final e permite inspecionar estÃ¡gios intermediÃ¡rios para debug.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz a flag `--target <nome_estagio>` para o comando `docker build`. Isso permite interromper um build multi-estÃ¡gio em um estÃ¡gio intermediÃ¡rio especÃ­fico (usando o nome `AS <nome_estagio>` definido no Dockerfile). O resultado Ã© uma imagem que contÃ©m apenas as camadas atÃ© aquele estÃ¡gio.
*   Isso Ã© extremamente Ãºtil para **debugging**. Se um passo em um estÃ¡gio posterior estÃ¡ falhando, vocÃª pode construir a imagem atÃ© o estÃ¡gio *anterior*, rodar um contÃªiner interativo a partir dela (`docker run -it <imagem_do_estagio> sh`), e inspecionar o filesystem ou tentar rodar o comando problemÃ¡tico manualmente para entender o erro.
*   O exercÃ­cio demonstra isso construindo a imagem `v3` completa e tambÃ©m versÃµes paradas nos estÃ¡gios `download` e `expand`.
*   A comparaÃ§Ã£o dos tamanhos (Figura 17.10) ilustra perfeitamente o processo:
    *   `v3-download`: ContÃ©m a base + o arquivo `.tar.gz` baixado (~200MB).
    *   `v3-expand`: ContÃ©m a base + *todo* o conteÃºdo extraÃ­do (~2GB).
    *   `v3` (final): ContÃ©m a base + *apenas* o `Day1.svm` copiado (~24MB).
*   Isso confirma que multi-stage Ã© a melhor abordagem: legÃ­vel, fÃ¡cil de depurar (com `--target`) e otimizado automaticamente para o tamanho final.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:** `cd C:\DockerLabs\diamol\ch17\exercises\ml-dataset`
2.  **Criar `Dockerfile.v3`:** Salve o conteÃºdo da Listagem 17.5 como `Dockerfile.v3`.
3.  **Construir EstÃ¡gios:**
    ```powershell
    docker image build -t diamol/ch17-ml-dataset:v3 -f Dockerfile.v3 .
    docker image build -t diamol/ch17-ml-dataset:v3-download -f Dockerfile.v3 --target download .
    docker image build -t diamol/ch17-ml-dataset:v3-expand -f Dockerfile.v3 --target expand .
    ```
4.  **Comparar Tamanhos:**
    ```powershell
    docker image ls -f reference='diamol/ch17-ml-dataset:v3*'
    # Confirme os tamanhos dramaticamente diferentes.
    ```
5.  **(Opcional) Debugar EstÃ¡gio:**
    ```powershell
    # Rodar container do estÃ¡gio 'expand' para ver todos os arquivos extraÃ­dos
    docker run -it diamol/ch17-ml-dataset:v3-expand sh
    # Dentro do container: ls -R / (ou onde o WORKDIR estava)
    # exit
    ```

---

ğŸ“„ PÃ¡gina 359

ğŸ” Resumo dos principais pontos:

*   **Vantagem Final Multi-Stage: Cache por EstÃ¡gio.**
    *   Cada *estÃ¡gio* tem seu prÃ³prio cache independente.
    *   Se mudar algo no estÃ¡gio `expand`, o estÃ¡gio `download` (anterior) ainda virÃ¡ do cache.
*   **OtimizaÃ§Ã£o 4: Uso do Cache (Velocidade de Build).**
    *   Revisita caching (maximizar reuso).
    *   Regra: Ordenar instruÃ§Ãµes no Dockerfile da menos frequente para a mais frequente.
    *   Static setup (`EXPOSE`, `ENV`, `ENTRYPOINT`) no inÃ­cio.
    *   BinÃ¡rios/config da app (`COPY`) no final.
    *   Pode exigir iteraÃ§Ã£o para acertar ordem.
*   **ExercÃ­cio Try-it-now:** Demonstrar otimizaÃ§Ã£o de cache com multi-stage e ordem.
    *   App: InstalaÃ§Ã£o Jenkins mÃ­nima (sÃ³ baixa JAR, copia config).
    *   Navegar para `ch17/exercises/jenkins`.
    *   `v1` (`Dockerfile`): Ordem ruim - `COPY config` *antes* de `RUN wget` (download grande).
    *   `v2` (`Dockerfile.v2`): Ordem boa + multi-stage - `RUN wget` em estÃ¡gio inicial, `COPY config` no estÃ¡gio final.
    *   1. Build v1 e v2: `docker build -t ...:v1 .`, `docker build -t ...:v2 -f ...v2 .`
    *   2. Modificar arquivo de config: `echo 2.0 > jenkins.install.UpgradeWizard.state`.
    *   3. Re-build v1 e v2. Comparar tempos de build.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina revela outra vantagem crucial do multi-estÃ¡gio: o **cache de build funciona por estÃ¡gio**. Se vocÃª tem estÃ¡gios `A -> B -> C` e faz uma mudanÃ§a apenas no estÃ¡gio `C`, os estÃ¡gios `A` e `B` virÃ£o inteiramente do cache quando vocÃª reconstruir, tornando o build muito mais rÃ¡pido.
*   Isso se conecta Ã  otimizaÃ§Ã£o final discutida: maximizar o uso do cache para **acelerar os builds**. A regra geral (jÃ¡ vista no Cap. 3) Ã© ordenar as instruÃ§Ãµes dentro de cada estÃ¡gio (ou no Dockerfile inteiro, se nÃ£o for multi-estÃ¡gio) da **menos volÃ¡til para a mais volÃ¡til**.
*   O exercÃ­cio compara duas versÃµes de um Dockerfile para instalar o Jenkins:
    *   `v1` (nÃ£o otimizado): Copia um arquivo de configuraÃ§Ã£o local (`COPY ...config...`) *antes* de baixar o JAR grande do Jenkins (`RUN wget ...`).
    *   `v2` (otimizado): Usa multi-estÃ¡gio. O download (`RUN wget`) acontece em um estÃ¡gio inicial. O estÃ¡gio final copia o JAR baixado do estÃ¡gio anterior e *depois* copia o arquivo de configuraÃ§Ã£o local (`COPY ...config...`).
*   O teste envolve construir ambas as versÃµes, depois modificar o arquivo de configuraÃ§Ã£o local e reconstruir ambas. A expectativa Ã©:
    *   Re-build `v1`: Como o `COPY config` veio antes do `wget`, a mudanÃ§a no config quebra o cache *antes* do download. O `wget` (lento) terÃ¡ que ser executado novamente. Build lento.
    *   Re-build `v2`: A mudanÃ§a no config sÃ³ afeta o Ãºltimo `COPY` no estÃ¡gio final. O estÃ¡gio inicial (com o `wget` lento) virÃ¡ inteiramente do cache. Build rÃ¡pido.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\jenkins
    ```
2.  **Build Inicial v1 e v2:**
    ```powershell
    docker image build -t diamol/ch17-jenkins:v1 .
    docker image build -t diamol/ch17-jenkins:v2 -f Dockerfile.v2 .
    ```
3.  **Modificar Config:**
    ```powershell
    Set-Content -Path ./jenkins.install.UpgradeWizard.state -Value "2.0"
    # Ou edite no VS Code e salve
    ```
4.  **Re-Build v1 (Medir Tempo):**
    ```powershell
    Measure-Command { docker image build -t diamol/ch17-jenkins:v1 . }
    # Observe a saÃ­da do build (deve mostrar o wget rodando) e o tempo total.
    ```
5.  **Re-Build v2 (Medir Tempo):**
    ```powershell
    Measure-Command { docker image build -t diamol/ch17-jenkins:v2 -f Dockerfile.v2 . }
    # Observe a saÃ­da (deve usar cache para o download) e o tempo total (deve ser bem menor).
    ```

---

ğŸ“„ PÃ¡gina 360

ğŸ” Resumo dos principais pontos:

*   Figura 17.11: Mostra comparaÃ§Ã£o de tempos de re-build (v1 lento >10s, v2 rÃ¡pido <1s) confirmando benefÃ­cio da otimizaÃ§Ã£o de ordem/multi-stage para cache.
*   Uso bom de cache -> builds CI/CD rÃ¡pidos.
*   **Cuidado: Over-caching.**
    *   Se `RUN` instala/baixa software (ex: `apt-get install pacote` *sem* versÃ£o), cache pode impedir pegar updates.
    *   `RUN` Ã© cacheado atÃ© instruÃ§Ã£o mudar *ou* cache quebrar antes.
*   **SoluÃ§Ã£o:** Sempre usar **versÃµes explÃ­citas** ao instalar/baixar software em `RUN`.
    *   Ex: `apt-get install curl=7.52.1...` (Exemplo socat, Listagem 17.3).
    *   Ex: Usar `ARG VERSAO=...` e `wget .../$VERSAO/...` (Exemplo Jenkins).
    *   Permite usar cache atÃ© *vocÃª* decidir atualizar a versÃ£o no Dockerfile/build args.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 17.11 comprova experimentalmente a enorme diferenÃ§a no tempo de build ao otimizar a ordem das instruÃ§Ãµes e usar multi-estÃ¡gio para aproveitar o cache. Builds rÃ¡pidos sÃ£o essenciais para pipelines CI/CD Ã¡geis.
*   A pÃ¡gina, no entanto, alerta sobre um risco: o **excesso de cache (over-caching)**. Isso acontece quando uma instruÃ§Ã£o `RUN` que baixa ou instala software (ex: `apt-get install meu-pacote`) Ã© cacheada. Se vocÃª nÃ£o especificou uma versÃ£o exata, o cache farÃ¡ com que builds futuros continuem usando a versÃ£o antiga que foi baixada na primeira vez, mesmo que uma versÃ£o mais nova (com correÃ§Ãµes de seguranÃ§a!) esteja disponÃ­vel nos repositÃ³rios. A camada cacheada sÃ³ serÃ¡ invalidada se a prÃ³pria linha `RUN` no Dockerfile mudar ou se uma instruÃ§Ã£o anterior quebrar o cache.
*   A **soluÃ§Ã£o** para evitar over-caching e garantir controle sobre as dependÃªncias Ã© sempre especificar **versÃµes explÃ­citas** ao instalar software via `RUN`:
    *   Para gerenciadores de pacote: `apt-get install <pacote>=<versao>`, `apk add <pacote>=<versao>`, `npm install <pacote>@<versao>`, etc.
    *   Para downloads diretos (wget/curl): Inclua a versÃ£o na URL ou passe-a via `ARG`, como no exemplo do Jenkins.
*   Isso garante que vocÃª obtenha os benefÃ­cios do cache, mas apenas atÃ© o momento em que *vocÃª* decidir atualizar a versÃ£o no Dockerfile (ou no argumento de build), garantindo builds reprodutÃ­veis e controlados.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Revise a Listagem 17.3 (p.356): Note que `apt-get install` usava versÃµes explÃ­citas (`curl=7.52.1...`, `socat=1.7.3...`). Isso Ã© bom.
*   Revise o Dockerfile do Jenkins (`ch17\exercises\jenkins\Dockerfile.v2`). Ele usa `ARG JENKINS_VERSION=...` e `ARG URL=.../${JENKINS_VERSION}/jenkins.war`. Isso tambÃ©m Ã© bom, pois a versÃ£o Ã© controlada externamente ou pelo default no `ARG`.

---

ğŸ“„ PÃ¡gina 361

ğŸ” Resumo dos principais pontos:

*   **17.5 Understanding why optimization counts** (Entendendo por que a otimizaÃ§Ã£o conta)
*   Boas prÃ¡ticas resumidas:
    1.  Escolher base correta (idealmente golden images curadas).
    2.  Usar multi-stage (exceto para apps mais simples).
    3.  NÃ£o adicionar lixo (foco no tamanho da camada).
    4.  Ordenar instruÃ§Ãµes por frequÃªncia de mudanÃ§a (maximizar cache).
*   OtimizaÃ§Ã£o impacta todo ciclo de vida.
*   Diagrama (Figura 17.12): Impactos da otimizaÃ§Ã£o no ciclo de vida da imagem.
    *   **Build:** Cache rÃ¡pido, contexto pequeno -> CI rÃ¡pido.
    *   **Push:** Camadas menores, mais reuso -> Push rÃ¡pido, menos banda/storage no registro.
    *   **Pull:** Camadas menores, mais reuso -> Pull rÃ¡pido nos nÃ³s de produÃ§Ã£o/teste.
    *   **Scan:** Menos software -> Scan rÃ¡pido, menos vulnerabilidades.
    *   **Run:** Menos software -> Menor superfÃ­cie de ataque.
*   **17.6 Lab** (LaboratÃ³rio)

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A seÃ§Ã£o 17.5 resume as quatro principais prÃ¡ticas de otimizaÃ§Ã£o de imagem Docker abordadas no capÃ­tulo: escolher a base certa, usar multi-stage, minimizar o conteÃºdo de cada camada e ordenar instruÃ§Ãµes para o cache.
*   A Figura 17.12 Ã© um excelente sumÃ¡rio visual dos **benefÃ­cios** da otimizaÃ§Ã£o em **todo o ciclo de vida** de uma imagem Docker:
    *   **Build:** Imagens otimizadas constroem mais rÃ¡pido (cache, contexto menor).
    *   **DistribuiÃ§Ã£o (Push/Pull):** Imagens menores e com mais camadas compartilhadas sÃ£o mais rÃ¡pidas de enviar para registros e de baixar nos servidores, economizando banda e espaÃ§o.
    *   **SeguranÃ§a (Scan/Run):** Imagens menores contÃªm menos software, o que significa menos vulnerabilidades potenciais para serem encontradas em varreduras e menor superfÃ­cie de ataque se o contÃªiner for comprometido em tempo de execuÃ§Ã£o.
*   Esses benefÃ­cios combinados justificam o esforÃ§o investido na otimizaÃ§Ã£o das suas imagens.
*   Prepara para o Lab do CapÃ­tulo 17.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. Ã‰ uma revisÃ£o conceitual dos benefÃ­cios da otimizaÃ§Ã£o.

---

ğŸ“„ PÃ¡gina 362

ğŸ” Resumo dos principais pontos:

*   Figura 17.12: Diagrama dos benefÃ­cios da otimizaÃ§Ã£o.
*   **ContinuaÃ§Ã£o do Lab 17:**
    *   Objetivo: Otimizar imagem que instala CLI `docker`.
    *   Ponto de Partida: Dockerfiles (Linux/Windows) em `ch17/lab` que funcionam mas geram imagens grandes.
*   **Tarefa:** Refatorar Dockerfile(s) para atingir metas:
    1.  Tamanho Otimizado: `< 80MB` (Linux), `< 330MB` (Windows).
    2.  Cache Otimizado: Re-builds devem ser `< 1s`.
    3.  Funcionalidade: `docker run <imagem> docker version` deve funcionar (mostrar versÃ£o CLI, erro no Server Ã© OK).
*   Dicas: Pensar criativamente, talvez nÃ£o sÃ³ otimizar instruÃ§Ãµes existentes, mas repensar o processo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina detalha o **Lab do CapÃ­tulo 17**. O desafio Ã© pegar Dockerfiles funcionais, mas nÃ£o otimizados, que instalam a CLI do Docker e refatorÃ¡-los para atender a metas especÃ­ficas de tamanho e velocidade de build, mantendo a funcionalidade principal.
*   As metas sÃ£o:
    1.  **Tamanho:** Reduzir drasticamente o tamanho final da imagem. Isso provavelmente exigirÃ¡ usar uma base mÃ­nima e tÃ©cnicas para evitar camadas infladas durante a instalaÃ§Ã£o da CLI (talvez baixando o binÃ¡rio diretamente em vez de usar um gerenciador de pacotes que traga muitas dependÃªncias, ou usando multi-stage para limpar arquivos temporÃ¡rios).
    2.  **Velocidade (Cache):** Garantir que re-builds (sem mudanÃ§as reais) sejam quase instantÃ¢neos, indicando bom uso do cache. Isso requer ordenar as instruÃ§Ãµes corretamente.
    3.  **Funcionalidade:** A imagem resultante ainda deve ser capaz de executar o comando `docker version`.
*   A dica sugere que pode ser necessÃ¡rio mais do que apenas reordenar ou combinar `RUN`s; talvez uma abordagem completamente diferente para obter a CLI Docker seja necessÃ¡ria para atingir as metas de tamanho.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 17:**
    1.  Navegue para `ch17\lab`. Examine o `Dockerfile` (Linux) e/ou `Dockerfile.windows`.
    2.  Construa a versÃ£o original (`docker build -t lab17-v1 ...`) e anote o tamanho.
    3.  Analise os passos. Provavelmente estÃ¡ usando `apt-get` ou `curl/tar` de forma nÃ£o otimizada, ou uma base grande.
    4.  **RefatoraÃ§Ã£o:**
        *   **Base:** Troque `FROM` para uma imagem mÃ­nima (ex: `alpine` para Linux, `mcr.microsoft.com/windows/nanoserver` para Windows).
        *   **Obter CLI Docker:** Em vez de `apt-get install docker-ce-cli`, talvez seja melhor usar `curl` ou `wget` para baixar o binÃ¡rio estÃ¡tico da CLI diretamente de `download.docker.com` em um estÃ¡gio multi-stage inicial. Verifique se a base mÃ­nima tem `curl` ou `wget`, ou adicione/remova em um `RUN` Ãºnico nesse estÃ¡gio.
        *   **EstÃ¡gio Final:** Copie *apenas* o binÃ¡rio `docker` baixado do estÃ¡gio anterior para a imagem base final mÃ­nima (ex: em `/usr/local/bin/`). Defina `ENTRYPOINT ["docker"]` ou `CMD ["docker"]`.
    5.  Construa a versÃ£o otimizada (`docker build -t lab17-v2 ...`) e compare tamanho.
    6.  Teste o cache reconstruindo `v2`.
    7.  Teste a funcionalidade: `docker run --rm lab17-v2 version`.

---

ğŸ“„ PÃ¡gina 363

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 17 no GitHub.
*   **CapÃ­tulo 18: Application configuration management in containers** (Gerenciamento de configuraÃ§Ã£o de aplicaÃ§Ã£o em contÃªineres)
*   Apps precisam carregar config do ambiente (env vars, arquivos).
*   Docker cria ambiente (seta env vars, constrÃ³i filesystem de vÃ¡rias fontes - imagem, volumes, configs, secrets).
*   PeÃ§as existem para config flexÃ­vel (mesma imagem, configs diferentes por ambiente).
*   Precisa de trabalho para unir peÃ§as: app precisa mesclar config de mÃºltiplas fontes.
*   Foco Cap. 18: Abordagem recomendada (+ alternativas) com exemplos .NET Core, Java, Go, Node.js. Requer colaboraÃ§Ã£o Dev/Ops.
*   **18.1 A multi-tiered approach to app configuration** (Uma abordagem multi-camadas para configuraÃ§Ã£o de app)
*   Modelo deve refletir estrutura dos dados de config:
    1.  **NÃ­vel Release:** Igual para todos ambientes de uma versÃ£o (ex: nome do app).
    2.  **NÃ­vel Ambiente:** Diferente por ambiente (ex: string conexÃ£o BD, nÃ­vel log).
    3.  **NÃ­vel Feature:** Para ligar/desligar features (feature flags).
*   Fontes podem ser estÃ¡ticas (imagem) ou dinÃ¢micas (env var, arquivos montados).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 18, focado em como as **aplicaÃ§Ãµes dentro dos contÃªineres** devem lidar com suas **configuraÃ§Ãµes**.
*   Relembra que o Docker fornece vÃ¡rios mecanismos para injetar configuraÃ§Ã£o no ambiente do contÃªiner (variÃ¡veis de ambiente, montagem de arquivos via volumes/bind mounts/configs/secrets).
*   O desafio Ã© fazer com que a *aplicaÃ§Ã£o* utilize esses mecanismos de forma flexÃ­vel, permitindo que a mesma imagem se adapte a diferentes ambientes. Isso geralmente requer que a aplicaÃ§Ã£o seja capaz de **ler e mesclar configuraÃ§Ãµes de mÃºltiplas fontes**, com uma ordem de precedÃªncia definida (ex: variÃ¡vel de ambiente sobrescreve arquivo, que sobrescreve default da imagem).
*   A seÃ§Ã£o 18.1 propÃµe uma **abordagem multi-camadas** para pensar sobre a configuraÃ§Ã£o:
    *   **Defaults (NÃ­vel Release):** ConfiguraÃ§Ãµes bÃ¡sicas que raramente mudam e podem vir embutidas na imagem Docker.
    *   **Ambiente:** ConfiguraÃ§Ãµes especÃ­ficas do ambiente (Dev, Test, Prod) que devem ser injetadas externamente (via arquivos montados - Configs/Secrets/Volumes - ou `env_file`).
    *   **Overrides/Features:** ConfiguraÃ§Ãµes individuais que precisam ser facilmente alteradas (ex: para debugging, A/B testing, feature flags) e sÃ£o ideais para serem passadas via variÃ¡veis de ambiente (`environment:` no Compose ou `-e` no `docker run`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O capÃ­tulo usarÃ¡ bibliotecas de configuraÃ§Ã£o populares em diferentes linguagens (node-config, .NET Core config, Viper Go) para demonstrar como implementar esse carregamento multi-fonte.
*   VocÃª usarÃ¡ `docker run -e VAR=valor` e `docker run -v /path/host:/path/container` (ou equivalentes Compose) para injetar as configuraÃ§Ãµes.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Config Multi-Camadas: Ã‰ como configurar seu telefone.
    *   Defaults (Imagem): ConfiguraÃ§Ãµes de fÃ¡brica.
    *   Ambiente (Arquivo Montado): ConfiguraÃ§Ãµes que vocÃª define para "Casa" vs "Trabalho" (Wi-Fi, NÃ£o Perturbe).
    *   Overrides (Env Var): Ajustes rÃ¡pidos que vocÃª faz na hora (Brilho da tela, Volume).

---

ğŸ“„ PÃ¡gina 364

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 18.1): Ilustra hierarquia de config: Defaults (Imagem) <- Arquivos (Filesystem) <- VariÃ¡veis de Ambiente (Env Vars). Env Vars tÃªm maior precedÃªncia.
*   **Exemplo Node.js:** Usa biblioteca `node-config`.
    *   LÃª config de mÃºltiplas fontes hierÃ¡rquicas + overrides de env vars.
    *   App `access-log` configurado para ler de:
        1.  `/app/config` (padrÃµes, empacotado na imagem).
        2.  `/app/config-override` (vazio na imagem; pode ser montado de volume/config/secret).
*   **ExercÃ­cio Try-it-now:** Rodar `access-log` com default e com override.
    *   1. Rodar com default: `docker run -d -p 8080:80 diamol/ch18-access-log`.
    *   2. Rodar com override (bind mount de pasta `config/dev` local para `/app/config-override`): `docker run -d -p 8081:80 -v "$(pwd)/config/dev:/app/config-override" diamol/ch18-access-log`.
    *   3. Verificar config via API (`/config`) em ambos: `curl :8080/config`, `curl :8081/config`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 18.1 estabelece a **ordem de precedÃªncia** tÃ­pica e recomendada para configuraÃ§Ã£o: VariÃ¡veis de Ambiente (maior prioridade) > Arquivos de ConfiguraÃ§Ã£o Montados > Arquivos de ConfiguraÃ§Ã£o PadrÃ£o (na imagem). Isso permite flexibilidade mÃ¡xima.
*   O primeiro exemplo usa Node.js com a biblioteca `node-config`. Essa biblioteca Ã© configurada para procurar arquivos de configuraÃ§Ã£o em uma ordem especÃ­fica de diretÃ³rios e, por fim, permite que variÃ¡veis de ambiente sobrescrevam tudo.
*   O app `access-log` (uma nova versÃ£o para este capÃ­tulo) procura por:
    *   Arquivos em `/app/config` (contÃ©m `default.json` na imagem).
    *   Arquivos em `/app/config-override` (estÃ¡ vazio na imagem, mas Ã© o local onde montaremos arquivos especÃ­ficos do ambiente).
*   O exercÃ­cio demonstra isso:
    *   O primeiro contÃªiner usa apenas o `default.json` da imagem.
    *   O segundo contÃªiner usa um **bind mount** para mapear a pasta `config/dev` do host (que contÃ©m um arquivo como `local.json` com settings de dev) para `/app/config-override` dentro do contÃªiner. O `node-config` detecta e carrega esse arquivo, sobrescrevendo ou adicionando configuraÃ§Ãµes ao `default.json`.
    *   A API `/config` (exposta pela aplicaÃ§Ã£o de exemplo) permite verificar quais configuraÃ§Ãµes estÃ£o ativas em cada contÃªiner.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\access-log
    # Verifique a pasta 'config\dev' e o arquivo dentro dela (ex: local.json)
    ```
2.  **Rodar Container Default:**
    ```powershell
    docker container run -d -p 8080:80 --name accesslog-default diamol/ch18-access-log
    ```
3.  **Rodar Container com Override:** (Ajuste $TargetPath se imagem for Windows)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override' # Para imagem Linux
    docker container run -d -p 8081:80 --name accesslog-dev -v "$SourcePath`:$TargetPath" diamol/ch18-access-log
    ```
4.  **Verificar Configs:**
    ```powershell
    curl http://localhost:8080/config ; echo "" # Default
    curl http://localhost:8081/config ; echo "" # Com override Dev
    ```
    Compare as saÃ­das. A segunda deve mostrar `environment: DEV` e talvez `metrics.enabled: false` (dependendo do `local.json`). A primeira mostrarÃ¡ `environment: UNKNOWN`.

---

ğŸ“„ PÃ¡gina 365

ğŸ” Resumo dos principais pontos:

*   Figura 18.2: Mostra saÃ­da da API `/config` para os dois contÃªineres, confirmando que o override de arquivo foi aplicado no segundo.
*   Fonte do override (volume, config obj, secret) Ã© transparente para o app, desde que montado no path esperado (`/app/config-override`).
*   NuanÃ§a: Montar diretÃ³rio Ã© mais flexÃ­vel que arquivo Ãºnico (montagem de arquivo Ãºnico nÃ£o funciona em Win Container). Nomes de arquivos na origem devem bater com os esperados pelo app.
*   `node-config` tambÃ©m suporta override via **variÃ¡veis de ambiente** (maior precedÃªncia).
    *   Formato incomum: Precisa passar string JSON na variÃ¡vel `NODE_CONFIG`.
*   **ExercÃ­cio Try-it-now:** Rodar 3Âª versÃ£o com override de arquivo + override de env var.
    *   Objetivo: Usar config `dev` (metrics=false), mas ligar mÃ©tricas via env var.
    *   Comando: `docker run -d -p 8082:80 -v ...:/app/config-override -e NODE_CONFIG='{\"metrics\": {\"enabled\":\"true\"}}' diamol/ch18-access-log`.
    *   Verificar config: `curl http://localhost:8082/config`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o sucesso do override via arquivo montado e generaliza que a *origem* desse arquivo (bind mount local, Volume Docker, Config Object Swarm, Secret Swarm) nÃ£o importa para a aplicaÃ§Ã£o, desde que o arquivo apareÃ§a no caminho que ela espera (`/app/config-override`). Menciona a preferÃªncia por montar diretÃ³rios em vez de arquivos Ãºnicos por portabilidade (especialmente devido Ã  limitaÃ§Ã£o do Windows).
*   Introduz a camada final de override para `node-config`: variÃ¡veis de ambiente. A biblioteca `node-config` tem uma maneira especÃ­fica (e um pouco estranha) de fazer isso: vocÃª nÃ£o define variÃ¡veis individuais (como `METRICS_ENABLED=true`), mas sim uma Ãºnica variÃ¡vel chamada `NODE_CONFIG` contendo uma *string JSON* que representa as configuraÃ§Ãµes a serem sobrescritas.
*   O exercÃ­cio demonstra isso:
    *   Inicia um contÃªiner montando a pasta `config/dev` (que define `metrics.enabled: false`).
    *   **E tambÃ©m** define a variÃ¡vel de ambiente `-e NODE_CONFIG='{\"metrics\": {\"enabled\":\"true\"}}'`. (Note as barras invertidas `\` para escapar as aspas duplas dentro da string JSON no comando shell).
    *   Como a variÃ¡vel de ambiente tem a maior precedÃªncia no `node-config`, a configuraÃ§Ã£o final para `metrics.enabled` deve ser `true`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar Container com Override Duplo:** (Ajuste $TargetPath se necessÃ¡rio)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override'
    # No PowerShell, aspas simples preservam a string JSON literal.
    # Se usar aspas duplas, pode precisar escapar as aspas internas com ` ou \.
    $JsonOverride = '{\"metrics\": {\"enabled\":\"true\"}}'
    docker container run -d -p 8082:80 --name accesslog-env -v "$SourcePath`:$TargetPath" -e NODE_CONFIG=$JsonOverride diamol/ch18-access-log
    ```
2.  **Verificar Config:**
    ```powershell
    curl http://localhost:8082/config ; echo ""
    ```
    Verifique se `environment` Ã© `DEV` (do arquivo) mas `metrics.enabled` Ã© `true` (do env var).

---

ğŸ“„ PÃ¡gina 366

ğŸ” Resumo dos principais pontos:

*   Figura 18.3: Mostra saÃ­da do 3Âº container, confirmando override da env var sobre o arquivo (`metrics.enabled: true`).
*   PadrÃ£o Multi-Tiered (Imagem -> Arquivo -> EnvVar) Ã© recomendado.
*   FlexÃ­vel para dev (ex: ligar/desligar mÃ©tricas) e ops (deploy em ambientes diferentes).
*   Detalhes de implementaÃ§Ã£o (paths, formato env var) importam e podem gerar erros se nÃ£o comunicados/documentados.
*   **Alternativa:** Empacotar config de *todos* os ambientes *dentro* da imagem.
*   **18.2 Packaging config for every environment** (Empacotando config para cada ambiente)
    *   Frameworks suportam carregar `config.{Environment}.ext` baseado em variÃ¡vel `ENVIRONMENT_NAME`.
    *   Ex: .NET Core: `appsettings.json` (default) + `appsettings.{Environment}.json` (override) + Env Vars (final override).
*   App To-Do (`diamol/ch18-todo-list`) usa essa abordagem .NET Core.
    *   Imagem contÃ©m `appsettings.json`, `appsettings.Development.json`, `appsettings.Test.json`, `appsettings.Production.json`.
    *   VariÃ¡vel `DOTNET_ENVIRONMENT` (ou `ASPNETCORE_ENVIRONMENT`) define qual arquivo `.Environment.` carregar.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a demonstraÃ§Ã£o do padrÃ£o multi-camadas (padrÃ£o na imagem, arquivo externo, variÃ¡vel de ambiente) como sendo flexÃ­vel e recomendado, mas alerta para a necessidade de boa documentaÃ§Ã£o e comunicaÃ§Ã£o sobre como a configuraÃ§Ã£o funciona (paths esperados, formato das variÃ¡veis, etc.) para evitar erros.
*   A seÃ§Ã£o 18.2 introduz uma **abordagem alternativa**: em vez de injetar arquivos de configuraÃ§Ã£o especÃ­ficos do ambiente em tempo de execuÃ§Ã£o, **empacotar os arquivos de configuraÃ§Ã£o para *todos* os ambientes *dentro* da prÃ³pria imagem Docker**.
*   Muitos frameworks de aplicaÃ§Ã£o modernos suportam isso nativamente. O exemplo dado Ã© o **.NET Core**:
    1.  Ele sempre carrega um arquivo base (ex: `appsettings.json`).
    2.  Ele verifica uma variÃ¡vel de ambiente (geralmente `ASPNETCORE_ENVIRONMENT` ou `DOTNET_ENVIRONMENT`) para saber em qual ambiente estÃ¡ rodando (ex: "Development", "Test", "Production").
    3.  Ele entÃ£o carrega um arquivo especÃ­fico para aquele ambiente (ex: `appsettings.Production.json`), cujos valores sobrescrevem os do arquivo base.
    4.  Finalmente, ele ainda permite que variÃ¡veis de ambiente individuais sobrescrevam qualquer configuraÃ§Ã£o vinda dos arquivos.
*   A vantagem dessa abordagem Ã© que a imagem jÃ¡ contÃ©m "tudo" que precisa para rodar em qualquer ambiente; vocÃª sÃ³ precisa definir *uma* variÃ¡vel de ambiente (`DOTNET_ENVIRONMENT`) para selecionar o conjunto correto de configuraÃ§Ãµes. A desvantagem serÃ¡ discutida depois (seguranÃ§a, gerenciamento de config).
*   A nova versÃ£o do app To-Do (`diamol/ch18-todo-list`) implementa esse padrÃ£o .NET Core.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio rodarÃ¡ essa imagem To-Do, alternando o ambiente via variÃ¡vel de ambiente.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Empacotar Todas Configs na Imagem: Ã‰ como um manual de instruÃ§Ãµes que jÃ¡ vem com seÃ§Ãµes separadas para "Montagem RÃ¡pida", "Montagem Completa", "Montagem Profissional". VocÃª sÃ³ precisa dizer qual seÃ§Ã£o seguir (definir `DOTNET_ENVIRONMENT`), e todas as instruÃ§Ãµes jÃ¡ estÃ£o lÃ¡ dentro do manual (imagem).

---

ğŸ“„ PÃ¡gina 367

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar To-Do com config default (Development) e Test.
    *   1. Rodar default: `docker run -d -p 8083:80 diamol/ch18-todo-list`. (Dockerfile seta `DOTNET_ENVIRONMENT=Development` como padrÃ£o).
    *   2. Rodar Test: `docker run -d -p 8084:80 -e DOTNET_ENVIRONMENT=Test diamol/ch18-todo-list`.
*   Containers rodam mesma imagem, mas carregam arquivos `.json` diferentes de dentro da imagem (`appsettings.Development.json` vs `appsettings.Test.json`).
*   Sem necessidade de montar volumes externos para mudar ambiente.
*   Verificar: Acessar `/diagnostics` endpoint em ambos (`:8083` e `:8084`).
*   Figura 18.4: Mostra saÃ­da da API `/diagnostics`, confirmando `EnvironmentName` diferente (Development vs Test) e outras configs (ex: `Database:Provider`) que mudam entre os arquivos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra a abordagem de "configuraÃ§Ã£o por ambiente empacotada".
*   O primeiro contÃªiner Ã© iniciado sem especificar a variÃ¡vel `DOTNET_ENVIRONMENT`. Como o Dockerfile desta imagem define `Development` como o valor padrÃ£o para essa variÃ¡vel, a aplicaÃ§Ã£o carrega `appsettings.Development.json` para sobrescrever os defaults de `appsettings.json`.
*   O segundo contÃªiner Ã© iniciado com `-e DOTNET_ENVIRONMENT=Test`. Isso sobrescreve o padrÃ£o do Dockerfile. A aplicaÃ§Ã£o agora carrega `appsettings.Test.json` para sobrescrever os defaults.
*   Ambos os contÃªineres usam a **mesma imagem**, mas exibem configuraÃ§Ãµes diferentes porque selecionaram arquivos diferentes que jÃ¡ estavam *dentro* da imagem. NÃ£o foi necessÃ¡rio montar nenhum arquivo externo.
*   Acessar o endpoint `/diagnostics` (especÃ­fico desta aplicaÃ§Ã£o de exemplo) permite visualizar a configuraÃ§Ã£o ativa em cada instÃ¢ncia. A Figura 18.4 mostra que o ambiente e outras configuraÃ§Ãµes (como o provedor de banco de dados) sÃ£o de fato diferentes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar Default (Dev):**
    ```powershell
    docker container run -d -p 8083:80 --name todo-dev-packed diamol/ch18-todo-list
    ```
2.  **Rodar Test:**
    ```powershell
    docker container run -d -p 8084:80 --name todo-test-packed -e DOTNET_ENVIRONMENT=Test diamol/ch18-todo-list
    ```
3.  **Verificar Configs:**
    *   Acesse `http://localhost:8083/diagnostics` no navegador. Observe `EnvironmentName: Development` e outras settings.
    *   Acesse `http://localhost:8084/diagnostics`. Observe `EnvironmentName: Test` e settings diferentes (ex: `Database:Provider: Postgres`).

---

ğŸ“„ PÃ¡gina 368

ğŸ” Resumo dos principais pontos:

*   Figura 18.4: Confirma configs diferentes para Dev e Test.
*   Abordagem funciona bem se sistema de gerenciamento de config for separado do source control. Pipeline CI/CD pode injetar os arquivos de config corretos na imagem durante build.
*   **Desvantagem:** **SeguranÃ§a!** NÃ£o se pode empacotar *tudo*. Dados confidenciais (senhas, chaves API) NÃƒO devem ir na imagem Docker.
*   Assumir que registro pode ser comprometido; nÃ£o expor segredos em texto plano em arquivos na imagem.
*   Mesmo com essa abordagem, ainda precisa permitir **overrides via arquivos externos** (para segredos) e **via variÃ¡veis de ambiente** (para flexibilidade).
*   App To-Do ainda suporta isso: carrega arquivos de `/app/config-overrides` (se existir) e lÃª env vars (padrÃ£o .NET Core).
*   Permite cenÃ¡rios como rodar "produÃ§Ã£o" localmente, mas sobrescrevendo para usar BD de arquivo (SQLite).
*   **ExercÃ­cio Try-it-now:** Rodar em modo `Production` com override de arquivo para usar SQLite.
    *   Modo Production (sem override) falharia (espera BD remoto).
    *   Navegar para `ch18/exercises/todo-list`.
    *   Comando: `docker run -d -p 8085:80 -e DOTNET_ENVIRONMENT=Production -v "$(pwd)/config/prod-local:/app/config-override" diamol/ch18-todo-list`.
    *   Monta arquivo local (`prod-local/local.json`?) que provavelmente define `Database:Provider=SQLite`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina discute as implicaÃ§Ãµes da abordagem de "empacotar todas as configs". Pode funcionar se houver um processo externo (talvez manual ou via pipeline) que insere os arquivos de configuraÃ§Ã£o corretos na imagem durante o build, mantendo a separaÃ§Ã£o entre cÃ³digo e config.
*   No entanto, a principal **desvantagem Ã© a seguranÃ§a**. Ã‰ inaceitÃ¡vel colocar dados sensÃ­veis (senhas, chaves) diretamente em arquivos dentro de uma imagem Docker, pois qualquer pessoa que obtiver a imagem (ex: de um registro comprometido ou por acesso indevido) poderÃ¡ extrair esses segredos.
*   Portanto, mesmo que se use essa abordagem de empacotar arquivos por ambiente, ainda Ã© **necessÃ¡rio** ter um mecanismo para injetar configuraÃ§Ãµes **externamente** em tempo de execuÃ§Ã£o, pelo menos para os dados sensÃ­veis (usando Secrets) e, idealmente, tambÃ©m para permitir overrides rÃ¡pidos (usando variÃ¡veis de ambiente).
*   Felizmente, o app To-Do (`diamol/ch18-todo-list`) foi construÃ­do para suportar isso: alÃ©m de carregar `appsettings.{Environment}.json`, ele tambÃ©m procura por arquivos em `/app/config-override` (onde podemos montar secrets ou configs externos) e tambÃ©m lÃª variÃ¡veis de ambiente (comportamento padrÃ£o do .NET Core).
*   O exercÃ­cio demonstra isso rodando o app no modo `Production` (que normalmente esperaria um BD Postgres remoto), mas usando um bind mount para injetar um arquivo de override que o forÃ§a a usar SQLite, permitindo rodar localmente sem o BD externo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\todo-list
    # Verifique a pasta config\prod-local e o arquivo dentro (ex: local.json)
    # Ele deve conter algo como { "Database": { "Provider": "Sqlite" } }
    ```
2.  **Rodar Prod com Override Local:** (Ajuste target se imagem for Linux)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\prod-local"
    $TargetPath = 'C:\app\config-override' # Para imagem Windows
    # $TargetPath = '/app/config-override' # Para imagem Linux
    docker container run -d -p 8085:80 --name todo-prod-local -e DOTNET_ENVIRONMENT=Production -v "$SourcePath`:$TargetPath" diamol/ch18-todo-list
    ```
3.  **Verificar:** Acesse `http://localhost:8085/diagnostics`. Deve mostrar `EnvironmentName: Production` mas `Database:Provider: Sqlite`. Acesse `http://localhost:8085/` e tente adicionar um item; deve funcionar.

---

ğŸ“„ PÃ¡gina 369

ğŸ” Resumo dos principais pontos:

*   Figura 18.5: Mostra UI /diagnostics confirmando `Environment: Production` mas `Database: Sqlite` devido ao override.
*   App ainda suporta override final via variÃ¡veis de ambiente individuais (padrÃ£o .NET Core).
*   **ExercÃ­cio Try-it-now:** Rodar Prod local (com override de arquivo) + override de env var.
    *   Objetivo: Mudar nome do release via env var.
    *   Comando: `docker run -d -p 8086:80 -e DOTNET_ENVIRONMENT=Production -e release=CUSTOM -v ...:/app/config-override diamol/ch18-todo-list`.
    *   `-e release=CUSTOM`: Sobrescreve chave `release` vinda dos arquivos JSON.
*   Verificar: Acessar `http://localhost:8086/diagnostics`. Ver release customizado.
*   Figura 18.6: Mostra UI /diagnostics com `release: CUSTOM`.
*   **CrÃ­tica do Autor:** NÃ£o gosta da abordagem de empacotar mÃºltiplos arquivos de config na imagem.
    *   Risco de incluir dados semi-sensÃ­veis (nomes de servidor, URLs, etc.).
    *   SeparaÃ§Ã£o confusa entre config no source control vs. config em sistema externo.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina demonstra a camada final de precedÃªncia na configuraÃ§Ã£o do .NET Core: variÃ¡veis de ambiente individuais sobrescrevem configuraÃ§Ãµes vindas de *todos* os arquivos JSON carregados (base, ambiente, overrides externos).
*   O exercÃ­cio define a variÃ¡vel de ambiente `release=CUSTOM`. O framework .NET Core automaticamente mapeia essa variÃ¡vel para a chave de configuraÃ§Ã£o `release` e sobrescreve o valor que veio dos arquivos `appsettings...json`.
*   Acessar `/diagnostics` confirma que a variÃ¡vel de ambiente teve a maior prioridade.
*   O autor entÃ£o expressa sua **opiniÃ£o pessoal contrÃ¡ria** Ã  prÃ¡tica de empacotar arquivos de configuraÃ§Ã£o especÃ­ficos de ambiente dentro da imagem Docker. Ele argumenta que Ã© difÃ­cil garantir que nenhum dado, mesmo que nÃ£o seja uma senha direta (como nomes de servidores internos ou URLs de APIs privadas), que possa ser Ãºtil a um atacante seja incluÃ­do acidentalmente. AlÃ©m disso, cria uma gestÃ£o dividida da configuraÃ§Ã£o (parte no cÃ³digo/imagem, parte em sistemas externos). Ele implicitamente prefere o modelo anterior (padrÃµes na imagem, todo o resto injetado externamente).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar Prod Local com Override Env Var:** (Use $SourcePath, $TargetPath de antes)
    ```powershell
    docker container run -d -p 8086:80 --name todo-prod-custom -e DOTNET_ENVIRONMENT=Production -e release=CUSTOM -v "$SourcePath`:$TargetPath" diamol/ch18-todo-list
    ```
2.  **Verificar:** Acesse `http://localhost:8086/diagnostics`. Confirme que `release` mostra `CUSTOM`.

---

ğŸ“„ PÃ¡gina 370

ğŸ” Resumo dos principais pontos:

*   Figura 18.6: Confirma override da env var `release=CUSTOM`.
*   OpiniÃ£o autor: Prefere padrÃ£o mais consistente (nÃ£o empacotar configs de ambiente na imagem).
*   ContÃªineres permitem escolher padrÃ£o preferido.
*   Problema: MÃºltiplas stacks tecnolÃ³gicas -> MÃºltiplas abordagens de config.
*   **Exemplo Go:** Biblioteca `Viper`.
    *   Similar a .NET Core / node-config.
    *   Especifica paths de config, lÃª env vars para override.
*   App Image Gallery (`diamol/ch18-image-gallery`) usa Viper. Hierarquia:
    1.  `/app/config` (padrÃµes da imagem).
    2.  `/app/config-override` (vazio na imagem, para montar).
    3.  VariÃ¡veis de Ambiente (override final).
*   **Formato TOML:** Viper suporta JSON, YAML, mas TOML Ã© popular em Go. FÃ¡cil de ler/escrever, mapeia bem para cÃ³digo.
*   **Listagem 18.1:** Exemplo de arquivo config `config.toml`.
    ```toml
    release = "19.12"
    environment = "UNKNOWN"

    [metrics]
    enabled = true

    [apis.image]
    url = "http://iotd/image"

    [apis.access]
    url = "http://accesslog/access-log"
    ```

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a crÃ­tica Ã  abordagem de empacotar configuraÃ§Ãµes e transiciona para o prÃ³ximo exemplo usando Go, destacando que a diversidade de tecnologias muitas vezes leva a abordagens de configuraÃ§Ã£o ligeiramente diferentes, mesmo que o padrÃ£o geral (mÃºltiplas fontes com precedÃªncia) seja o mesmo.
*   Introduz a biblioteca `Viper` como uma soluÃ§Ã£o popular de configuraÃ§Ã£o em Go, que oferece funcionalidades semelhantes Ã s vistas para Node.js e .NET Core (ler mÃºltiplos arquivos, sobrescrever com variÃ¡veis de ambiente).
*   A versÃ£o `diamol/ch18-image-gallery` usa Viper com a mesma estrutura de diretÃ³rios (`/app/config` para defaults da imagem, `/app/config-override` para montagens externas).
*   Apresenta o formato **TOML** (Tom's Obvious, Minimal Language) como uma alternativa legÃ­vel a JSON e YAML, frequentemente usada na comunidade Go. A Listagem 18.1 mostra um exemplo de arquivo TOML com seÃ§Ãµes (`[metrics]`, `[apis]`) e subseÃ§Ãµes (`[apis.image]`) que sÃ£o fÃ¡ceis de entender.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ a imagem `diamol/ch18-image-gallery` e injetarÃ¡ configuraÃ§Ã£o via bind mount (com arquivo TOML) e variÃ¡veis de ambiente nos prÃ³ximos exercÃ­cios.
*   Abra `ch18\exercises\image-gallery\config\default\config.toml` no VS Code para ver o arquivo TOML padrÃ£o da imagem.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   TOML: Outro formato para escrever sua lista de compras ou configuraÃ§Ãµes. Pode ser mais agradÃ¡vel de ler e escrever para humanos do que JSON (muitas chaves e aspas) ou YAML (sensÃ­vel Ã  indentaÃ§Ã£o), especialmente para configuraÃ§Ãµes mais simples.

---

ğŸ“„ PÃ¡gina 371

ğŸ” Resumo dos principais pontos:

*   TOML Ã© fÃ¡cil de ler/debugar/comparar.
*   **ExercÃ­cio Try-it-now:** Rodar app Go com config default (da imagem).
    *   Comando: `docker run -d -p 8086:80 diamol/ch18-image-gallery`. (Nota: Usa porta 8086, que jÃ¡ pode estar em uso pelo ex. anterior. Usarei 8087).
    *   Verificar config via API: `curl http://localhost:8087/config`.
*   Figura 18.7: Mostra config default (release, environment UNKNOWN, URLs API).
*   **API `/config`:** Ãštil para debug, mas precisa ser **segura**.
    *   NÃ£o expor segredos.
    *   Proteger endpoint (autenticaÃ§Ã£o/autorizaÃ§Ã£o).
    *   Tornar a prÃ³pria API `/config` opcional (via config).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio instrui a rodar a aplicaÃ§Ã£o Go (`image-gallery`) usando apenas a configuraÃ§Ã£o padrÃ£o empacotada na imagem (o `config.toml` dentro de `/app/config`).
*   Assim como os outros exemplos, este app expÃµe um endpoint `/config` que retorna a configuraÃ§Ã£o ativa no momento em formato JSON. AcessÃ¡-lo permite verificar quais configuraÃ§Ãµes estÃ£o sendo usadas.
*   A pÃ¡gina aproveita para dar conselhos importantes sobre **seguranÃ§a** ao implementar um endpoint como `/config` que expÃµe a configuraÃ§Ã£o da aplicaÃ§Ã£o:
    1.  **Nunca inclua segredos:** A API nÃ£o deve retornar senhas, chaves API, etc.
    2.  **Proteja o Acesso:** O endpoint deve exigir autenticaÃ§Ã£o e autorizaÃ§Ã£o, nÃ£o ser pÃºblico.
    3.  **Torne-o Opcional:** Idealmente, a exposiÃ§Ã£o desse endpoint deve ser controlada por uma configuraÃ§Ã£o (ex: habilitado apenas em ambiente de desenvolvimento ou teste).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Limpar Porta Anterior (se usou 8086):**
    ```powershell
    docker rm -f todo-prod-custom # Ou o nome que deu
    ```
2.  **Rodar Go App (Default Config) na porta 8087:**
    ```powershell
    docker container run -d -p 8087:80 --name gallery-default diamol/ch18-image-gallery
    ```
3.  **Verificar Config Default:**
    ```powershell
    curl http://localhost:8087/config ; echo ""
    # Verifique os valores padrÃ£o (environment=UNKNOWN, etc.)
    ```

---

ğŸ“„ PÃ¡gina 372

ğŸ” Resumo dos principais pontos:

*   Figura 18.7: Confirma config default.
*   Abordagem Image Gallery: Defaults na imagem, mas *espera* override por ambiente.
*   **ExercÃ­cio Try-it-now:** Rodar app Go com override de arquivo (Dev).
    *   Navegar para `ch18/exercises/image-gallery`.
    *   Comando: `docker run -d -p 8087:80 -v "$(pwd)/config/dev:/app/config-override" diamol/ch18-image-gallery`. (Nota: Reusa porta 8087, precisa parar anterior. Usarei 8088).
    *   Verificar config: `curl http://localhost:8088/config`.
*   Figura 18.8: Mostra config mesclada (environment=DEV).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que o design da configuraÃ§Ã£o da imagem Go `image-gallery` Ã© ligeiramente diferente dos anteriores: ela fornece defaults, mas *assume* que um arquivo de override especÃ­fico do ambiente serÃ¡ fornecido em `/app/config-override`.
*   O exercÃ­cio demonstra isso rodando o contÃªiner novamente, mas desta vez usando um bind mount para mapear a pasta local `config/dev` (que contÃ©m um `config.toml` com `environment = "DEV"`) para `/app/config-override` dentro do contÃªiner.
*   O Viper (biblioteca Go) detecta e carrega este arquivo de override, mesclando-o com o default da imagem. Acessar a API `/config` agora deve mostrar a configuraÃ§Ã£o combinada, incluindo `environment = "DEV"`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Parar/Remover Container Anterior:**
    ```powershell
    docker rm -f gallery-default
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\image-gallery
    # Verifique pasta config\dev e arquivo config.toml dentro dela.
    ```
3.  **Rodar Go App com Override de Arquivo (porta 8088):** (Ajuste target se imagem for Win)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override' # Para imagem Linux
    docker container run -d -p 8088:80 --name gallery-dev -v "$SourcePath`:$TargetPath" diamol/ch18-image-gallery
    ```
4.  **Verificar Config Mesclada:**
    ```powershell
    curl http://localhost:8088/config ; echo ""
    # Verifique se environment=DEV e outros valores estÃ£o corretos.
    ```

---

ğŸ“„ PÃ¡gina 373

ğŸ” Resumo dos principais pontos:

*   Figura 18.8: Confirma config DEV mesclada.
*   VariaÃ§Ãµes nos modelos de config entre apps/linguagens sÃ£o comuns. DifÃ­cil padronizar 100%.
*   Exemplo Go: Override final via Env Vars usa convenÃ§Ã£o diferente.
*   **ExercÃ­cio Try-it-now:** Rodar Go com override de arquivo + override de env var.
    *   Viper espera prefixo + separador `_` + nome config em MAIÃšSCULAS (pode variar). App usa prefixo `IG`.
    *   Comando: `docker run -d -p 8088:80 -v ...:/app/config-override -e IG_METRICS.ENABLED=TRUE diamol/ch18-image-gallery`. (Nota: Reusa porta 8088, precisa parar anterior. Usarei 8089. Nome da var pode precisar ser `IG_METRICS_ENABLED`).
    *   Verificar config: `curl http://localhost:8089/config`.
*   Figura 18.9: Mostra config final com `metrics.enabled: true` vindo da env var.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reconhece que, apesar do padrÃ£o geral ser similar (mÃºltiplas fontes com precedÃªncia), os **detalhes** de como as bibliotecas de configuraÃ§Ã£o em diferentes linguagens lidam com overrides (especialmente via variÃ¡veis de ambiente) podem variar. Isso reforÃ§a a necessidade de documentaÃ§Ã£o clara (idealmente nos arquivos Compose/manifestos).
*   No caso do Viper (Go), uma convenÃ§Ã£o comum (e configurÃ¡vel) Ã© que as variÃ¡veis de ambiente precisam ter um **prefixo** especÃ­fico (definido pelo app, aqui `IG`), seguido por `_`, e o nome da chave de configuraÃ§Ã£o (Ã s vezes em maiÃºsculas, com `.` substituÃ­do por `_`). Ex: Para sobrescrever `metrics.enabled`, a variÃ¡vel pode ser `IG_METRICS_ENABLED`.
*   O exercÃ­cio tenta sobrescrever a configuraÃ§Ã£o de mÃ©tricas (que veio do arquivo) usando uma variÃ¡vel de ambiente.
    *   **CorreÃ§Ã£o ProvÃ¡vel:** A variÃ¡vel no exercÃ­cio `-e IG_METRICS.ENABLED=TRUE` provavelmente deveria ser `-e IG_METRICS_ENABLED=TRUE` para seguir a convenÃ§Ã£o de substituir `.` por `_` e usar maiÃºsculas, se o Viper estiver configurado assim. Assumirei a versÃ£o com `_`.
*   O comando roda o contÃªiner com o bind mount do arquivo `dev` E a variÃ¡vel de ambiente. A API `/config` deve mostrar o resultado final, com a variÃ¡vel de ambiente tendo a maior precedÃªncia.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Parar/Remover Container Anterior:**
    ```powershell
    docker rm -f gallery-dev
    ```
2.  **Rodar Go App com Override Duplo (porta 8089):**
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override'
    # Usando a provÃ¡vel variÃ¡vel correta para Viper
    docker container run -d -p 8089:80 --name gallery-env -v "$SourcePath`:$TargetPath" -e IG_METRICS_ENABLED=true diamol/ch18-image-gallery
    ```
3.  **Verificar Config Final:**
    ```powershell
    curl http://localhost:8089/config ; echo ""
    # Verifique se environment=DEV e metrics.enabled=true.
    ```

---

ğŸ“„ PÃ¡gina 374

ğŸ” Resumo dos principais pontos:

*   Figura 18.9: Confirma override da env var (`metrics.enabled: true`).
*   RecapitulaÃ§Ã£o: 3 apps, 3 stacks, mesmo padrÃ£o config (multi-tier), mas detalhes diferentes. GerenciÃ¡vel via documentaÃ§Ã£o nos manifestos.
*   **Ãšltimo Exemplo: AplicaÃ§Ãµes Legadas.**
*   **18.4 Configuring legacy apps in the same way as new apps** (Configurando apps legados da mesma forma que apps novos)
    *   Apps legados tÃªm ideias prÃ³prias sobre config (ex: XML em paths fixos, sem ler env vars).
    *   Pode-se aplicar mesmo padrÃ£o multi-tier, mas requer **trabalho extra** no Dockerfile/imagem.
    *   **Abordagem:** Empacotar utilitÃ¡rio/script na imagem que roda *na inicializaÃ§Ã£o* do container, *antes* do app legado.
        *   UtilitÃ¡rio lÃª config das fontes "modernas" (arquivos montados, env vars).
        *   Mescla/transforma essas configs.
        *   Escreve o resultado final no formato e local que o app legado espera (ex: gera o XML).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina resume os exemplos anteriores, destacando a consistÃªncia do padrÃ£o multi-camadas apesar das variaÃ§Ãµes de implementaÃ§Ã£o entre linguagens/bibliotecas.
*   A seÃ§Ã£o 18.4 aborda um desafio comum: como aplicar esses padrÃµes modernos de configuraÃ§Ã£o a **aplicaÃ§Ãµes legadas** que nÃ£o foram projetadas para ler variÃ¡veis de ambiente ou arquivos de configuraÃ§Ã£o em locais flexÃ­veis?
*   A soluÃ§Ã£o proposta Ã© inteligente: em vez de modificar o cÃ³digo legado (o que pode ser impossÃ­vel ou arriscado), modifica-se a **imagem Docker** e o **processo de inicializaÃ§Ã£o** do contÃªiner.
*   **Passos:**
    1.  Cria-se um pequeno **utilitÃ¡rio ou script** (na linguagem que for mais conveniente para manipular a configuraÃ§Ã£o) que *entende* o padrÃ£o moderno (ler de `/config-override`, ler env vars com prefixo `LEGACYAPP_...`, etc.).
    2.  Esse utilitÃ¡rio Ã© incluÃ­do na imagem Docker junto com a aplicaÃ§Ã£o legada.
    3.  O `CMD` ou `ENTRYPOINT` do Dockerfile Ã© modificado para **primeiro executar o utilitÃ¡rio de configuraÃ§Ã£o** e **depois executar a aplicaÃ§Ã£o legada**.
    4.  O utilitÃ¡rio lÃª as configuraÃ§Ãµes modernas injetadas no contÃªiner (via volumes, secrets, env vars), processa-as e **gera o arquivo de configuraÃ§Ã£o no formato exato (ex: XML) e no local exato** que a aplicaÃ§Ã£o legada espera encontrar.
*   Dessa forma, a aplicaÃ§Ã£o legada roda sem modificaÃ§Ãµes, lendo sua configuraÃ§Ã£o da maneira antiga, mas essa configuraÃ§Ã£o foi dinamicamente gerada na inicializaÃ§Ã£o com base nos padrÃµes modernos injetados pelo Docker/Compose/Swarm.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio usarÃ¡ uma versÃ£o "legada" (simulada) da API Java IOTD, junto com um utilitÃ¡rio Java `ConfigLoader` que farÃ¡ essa transformaÃ§Ã£o na inicializaÃ§Ã£o.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Config para App Legado: Ã‰ como ter um aparelho antigo que sÃ³ entende fitas K7. VocÃª quer tocar um MP3 moderno nele. A soluÃ§Ã£o Ã© usar um "adaptador" (o utilitÃ¡rio) que lÃª o MP3 (config moderna) e o grava na fita K7 (formato legado) na hora, para que o aparelho antigo possa tocar.

---

ğŸ“„ PÃ¡gina 375

ğŸ” Resumo dos principais pontos:

*   Exemplo "Legado": API Java IOTD (`diamol/ch18-image-of-the-day`), imagem construÃ­da com padrÃ£o legado (requer utilitÃ¡rio `ConfigLoader` para gerar config final).
*   UtilitÃ¡rio lÃª override de arquivo (path via env var `CONFIG_SOURCE_PATH`) e env vars (prefixo `IOTD_`). Escreve config final em path fixo (via `CONFIG_TARGET_PATH`).
*   **ExercÃ­cio Try-it-now:** Rodar app "legado" com default e com override de arquivo.
    *   1. Rodar default: `docker run -d -p 8089:80 diamol/ch18-image-of-the-day`.
    *   2. Rodar com override: `docker run -d -p 8090:80 -v "$(pwd)/config/dev:/config-override" -e CONFIG_SOURCE_PATH="/config-override/application.properties" diamol/ch18-image-of-the-day`.
    *   3. Verificar configs via API `/config`: `curl :8089/config`, `curl :8090/config`.
*   ExperiÃªncia do *usuÃ¡rio* Ã© similar (monta volume, seta env var), mesmo que mecanismo interno seja diferente. Ãšnica diferenÃ§a: precisa setar `CONFIG_SOURCE_PATH`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina aplica a abordagem "legada" Ã  API Java `image-of-the-day`. A imagem `diamol/ch18-image-of-the-day` contÃ©m a API e um utilitÃ¡rio `ConfigLoader`. O `CMD` desta imagem primeiro executa `ConfigLoader` e depois a API.
*   O `ConfigLoader` foi programado para:
    *   Ler um arquivo de propriedades de override de um caminho especificado pela variÃ¡vel de ambiente `CONFIG_SOURCE_PATH`.
    *   Ler variÃ¡veis de ambiente que comeÃ§am com `IOTD_`.
    *   Mesclar essas configuraÃ§Ãµes com alguma base (talvez interna ou de outro arquivo).
    *   Escrever o resultado final em um arquivo no caminho especificado por `CONFIG_TARGET_PATH` (que a API Java principal espera ler).
*   O exercÃ­cio demonstra o uso:
    *   O primeiro contÃªiner roda sem overrides externos. O `ConfigLoader` roda, mas nÃ£o encontra fontes externas, e provavelmente gera um arquivo de configuraÃ§Ã£o padrÃ£o para a API.
    *   O segundo contÃªiner:
        *   Monta a pasta `config/dev` local em `/config-override`.
        *   Define `CONFIG_SOURCE_PATH` para apontar para o arquivo `application.properties` dentro dessa pasta montada.
        *   O `ConfigLoader` agora lÃª esse arquivo de override, mescla e gera a configuraÃ§Ã£o final para a API usar.
*   O ponto principal Ã© que, do ponto de vista de quem *opera* o contÃªiner (definindo volumes e env vars), a experiÃªncia Ã© muito parecida com a dos apps modernos, apesar da complexidade extra na inicializaÃ§Ã£o interna. A Ãºnica diferenÃ§a notÃ¡vel Ã© a necessidade de fornecer o `CONFIG_SOURCE_PATH`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\image-of-the-day
    # Verifique a pasta config\dev e o arquivo application.properties dentro.
    ```
2.  **Rodar Default (Porta 8089):**
    ```powershell
    docker container run -d -p 8089:80 --name iotd-legacy-default diamol/ch18-image-of-the-day
    ```
3.  **Rodar com Override de Arquivo (Porta 8090):** (Ajuste target se imagem for Win)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/config-override' # Para imagem Linux
    $SourceFileInContainer = '/config-override/application.properties'
    docker container run -d -p 8090:80 --name iotd-legacy-dev -v "$SourcePath`:$TargetPath" -e CONFIG_SOURCE_PATH=$SourceFileInContainer diamol/ch18-image-of-the-day
    ```
4.  **Verificar Configs:**
    ```powershell
    curl http://localhost:8089/config ; echo "" # Default
    curl http://localhost:8090/config ; echo "" # Com override Dev
    ```
    Compare as saÃ­das (ex: `environment`).

---

ğŸ“„ PÃ¡gina 376

ğŸ” Resumo dos principais pontos:

*   Figura 18.10: Mostra saÃ­da das APIs `/config`, confirmando que override de arquivo funcionou para app "legado".
*   **UtilitÃ¡rio de Config:**
    *   Acontece na inicializaÃ§Ã£o (atrasa um pouco app ficar pronto, adiciona ponto de falha -> usar health checks!).
    *   Permite estender imagem para fazer app antigo se comportar como novo.
    *   Controla lÃ³gica de inicializaÃ§Ã£o (`CMD`/`ENTRYPOINT`).
*   `ConfigLoader` tambÃ©m suporta override via Env Vars (com prefixo `IOTD_`).
*   **ExercÃ­cio Try-it-now:** Rodar "legado" com override de arquivo + override de env var.
    *   Objetivo: Mudar `environment` via env var `IOTD_ENVIRONMENT`.
    *   Comando: `docker run -d -p 8091:80 -v ...:/config-override -e CONFIG_SOURCE_PATH=... -e IOTD_ENVIRONMENT="custom" diamol/ch18-image-of-the-day`.
    *   Verificar config: `curl http://localhost:8091/config`.
*   Figura 18.11: Mostra `environment: custom` vindo da env var.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que o override de arquivo funcionou para a aplicaÃ§Ã£o "legada" simulada. Reitera que adicionar essa lÃ³gica de inicializaÃ§Ã£o customizada (o utilitÃ¡rio `ConfigLoader`) tem pequenas desvantagens (atraso no startup, mais um ponto de falha) que devem ser mitigadas com health checks apropriados.
*   Explica que o utilitÃ¡rio `ConfigLoader` tambÃ©m foi programado para seguir o padrÃ£o completo, permitindo que **variÃ¡veis de ambiente** (com prefixo `IOTD_`) sobrescrevam as configuraÃ§Ãµes vindas dos arquivos.
*   O exercÃ­cio demonstra essa camada final de override:
    *   Roda o contÃªiner com o bind mount do arquivo `dev` (que define `environment=DEV`).
    *   Mas tambÃ©m define a variÃ¡vel de ambiente `-e IOTD_ENVIRONMENT="custom"`.
    *   O `ConfigLoader`, ao rodar, detecta a variÃ¡vel `IOTD_ENVIRONMENT` e usa seu valor ("custom") com precedÃªncia sobre o valor ("DEV") vindo do arquivo `application.properties`.
*   O resultado (Figura 18.11) mostra que a configuraÃ§Ã£o final reflete o valor da variÃ¡vel de ambiente, completando o padrÃ£o multi-camadas mesmo para a aplicaÃ§Ã£o "legada".

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Parar/Remover Anterior (se usou 8090):**
    ```powershell
    docker rm -f iotd-legacy-dev
    ```
2.  **Rodar com Override Duplo (Porta 8091):**
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/config-override'
    $SourceFileInContainer = '/config-override/application.properties'
    docker container run -d -p 8091:80 --name iotd-legacy-env -v "$SourcePath`:$TargetPath" -e CONFIG_SOURCE_PATH=$SourceFileInContainer -e IOTD_ENVIRONMENT="custom" diamol/ch18-image-of-the-day
    ```
3.  **Verificar Config:**
    ```powershell
    curl http://localhost:8091/config ; echo ""
    # Verifique se environment=custom.
    ```

---

ğŸ“„ PÃ¡gina 377

ğŸ” Resumo dos principais pontos:

*   Figura 18.11: Confirma `environment: custom`.
*   ConclusÃ£o: App "legado" agora usa mesmo padrÃ£o config multi-tier dos outros. Transparente para usuÃ¡rio/operador.
*   **DocumentaÃ§Ã£o de Config:** Fazer via arquivos de manifesto (Compose).
*   **Exemplo Compose Completo (Image Gallery):** ReÃºne todos os componentes com config Dev + mÃ©tricas habilitadas via env var.
*   **Listagem 18.3:** Trechos de `docker-compose.yml` configurando os 3 apps.
    *   `accesslog`: Usa `-e NODE_CONFIG=...` e `secrets:` (com `file:` apontando para `dev/local.json`).
    *   `iotd`: Usa `-e IOTD_MANAGEMENT...` e `secrets:` (com `file:` apontando para `dev/application.properties`).
    *   `image-gallery`: Usa `-e IG_METRICS.ENABLED...` e `secrets:` (com `file:` apontando para `dev/config.toml`).
    *   SeÃ§Ã£o raiz `secrets:` define as fontes `file:` para cada secret.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui o exemplo do app legado, mostrando que a abordagem do utilitÃ¡rio permite um comportamento de configuraÃ§Ã£o externo consistente com apps modernos.
*   Enfatiza que a melhor forma de **documentar** as configuraÃ§Ãµes necessÃ¡rias (e as variaÃ§Ãµes entre ambientes) Ã© no prÃ³prio **arquivo de manifesto** da aplicaÃ§Ã£o (o `docker-compose.yml` para Compose/Swarm, ou manifestos K8s).
*   A Listagem 18.3 exemplifica isso reunindo as configuraÃ§Ãµes para rodar a aplicaÃ§Ã£o Image Gallery completa (accesslog, iotd, image-gallery) em um modo "Dev com mÃ©tricas". O arquivo Compose mostra claramente para cada serviÃ§o:
    *   Quais variÃ¡veis de ambiente (`environment:`) sÃ£o necessÃ¡rias (e seus formatos especÃ­ficos, como a string JSON para `NODE_CONFIG` ou o prefixo `IOTD_` ou `IG_`).
    *   Quais arquivos de configuraÃ§Ã£o externos (`secrets:` com `source:`) sÃ£o esperados e onde serÃ£o montados (`target:`).
    *   A seÃ§Ã£o `secrets:` raiz define a origem (`file:`) desses arquivos de configuraÃ§Ã£o externos no host para este ambiente especÃ­fico (Dev).
*   Este arquivo Compose serve tanto para **executar** a configuraÃ§Ã£o quanto para **documentÃ¡-la**.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ um `docker-compose.yml` similar Ã  Listagem 18.3 (provavelmente em `ch18/exercises/docker-compose.yml`) para rodar a stack completa no prÃ³ximo exercÃ­cio.
*   Note o uso de `secrets:` aqui para carregar os arquivos de config `dev`. Isso funciona bem no Compose e prepara para usar secrets reais no Swarm/K8s para configuraÃ§Ãµes mais sensÃ­veis.

---

ğŸ“„ PÃ¡gina 378

ğŸ” Resumo dos principais pontos:

*   AnÃ¡lise Listagem 18.3: PadrÃ£o similar, detalhes diferentes (formato env var, tipo arquivo secret). Uso de `secrets` funciona como volumes/configs.
*   **ExperiÃªncia UsuÃ¡rio/Ops:** Boa (fÃ¡cil mudar ambiente/features), mas Ruim (precisa saber quirks de cada app).
    *   Mitigado por ter overrides Compose prÃ©-feitos para cada config comum.
*   **ExercÃ­cio Try-it-now:** Rodar app completo com config fixa do Compose file.
    *   1. Limpar containers.
    *   2. Navegar para `ch18/exercises`.
    *   3. Rodar `docker-compose up -d` (usarÃ¡ `docker-compose.yml` padrÃ£o com config da Listagem 18.3).
    *   4. Verificar configs via APIs `/config` de cada serviÃ§o (ports 8010, 8020, 8030 - *Nota: verificar ports reais no .yml*).
*   Figura 18.12: Mostra saÃ­da das 3 APIs `/config`, confirmando `environment: DEV`, `release: 19.12`, `metrics: enabled: true` em todos.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a anÃ¡lise da Listagem 18.3, reforÃ§ando a consistÃªncia do padrÃ£o, mas tambÃ©m as inevitÃ¡veis diferenÃ§as de implementaÃ§Ã£o. A recomendaÃ§Ã£o Ã© encapsular essas configuraÃ§Ãµes em arquivos Compose de override bem definidos para cada cenÃ¡rio comum.
*   O exercÃ­cio final do capÃ­tulo instrui a rodar a aplicaÃ§Ã£o Image Gallery completa usando o `docker-compose.yml` que implementa a configuraÃ§Ã£o "Dev com MÃ©tricas" da Listagem 18.3.
*   O objetivo Ã© verificar se todos os trÃªs componentes (`access-log`, `iotd`, `image-gallery`) carregaram corretamente a combinaÃ§Ã£o de configuraÃ§Ãµes:
    *   `release: 19.12` (do `default.json/.properties/.toml` na imagem).
    *   `environment: DEV` (do arquivo `dev/...` carregado via `secrets:`).
    *   `metrics.enabled: true` (da variÃ¡vel de ambiente definida em `environment:` no Compose).
*   A verificaÃ§Ã£o Ã© feita acessando a API `/config` de cada serviÃ§o (nas portas publicadas por eles no `docker-compose.yml`). A Figura 18.12 mostra o resultado esperado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Opcional: docker volume prune -f
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises
    ```
3.  **Verificar `docker-compose.yml`:** Abra o arquivo. Confirme que ele define os 3 serviÃ§os, a rede, os secrets (com `file:` apontando para subpastas `dev`), e as variÃ¡veis `environment:` para ligar mÃ©tricas. Anote as portas publicadas para cada serviÃ§o (ex: 8010, 8020, 8030).
4.  **Iniciar Stack:**
    ```powershell
    docker-compose up -d
    ```
5.  **Verificar Configs:** (Use as portas corretas do seu `.yml`)
    ```powershell
    curl http://localhost:8010/config ; echo "" # Gallery (Go)
    curl http://localhost:8020/config ; echo "" # IOTD (Java)
    curl http://localhost:8030/config ; echo "" # AccessLog (Node)
    ```
    Confirme que todos mostram `DEV`, `19.12`, e `metrics enabled true` (ou equivalente).
6.  **Testar App/MÃ©tricas (Opcional):** Acesse `http://localhost:8010`. Acesse os endpoints `/metrics` de cada serviÃ§o (ex: `:8010/metrics`, `:8020/actuator/prometheus`, `:8030/metrics`).

---

ğŸ“„ PÃ¡gina 379

ğŸ” Resumo dos principais pontos:

*   Figura 18.12: Confirma configs aplicadas via Compose.
*   Fim da parte prÃ¡tica de config. PrÃ³ximo: entender benefÃ­cios do modelo flexÃ­vel.
*   **18.5 Understanding why a flexible configuration model pays off** (Entendendo por que um modelo de configuraÃ§Ã£o flexÃ­vel compensa)
*   Revisita CI/CD (Caps 11, 15): Builda imagem Ãºnica, promove entre ambientes.
*   Modelo config multi-tier suporta isso: mesma imagem, comportamento diferente por ambiente.
*   BenefÃ­cios de usar env vars para overrides finais (feature flags):
    *   Reagir rÃ¡pido a problemas prod (desligar log verboso, desligar feature bugada).
    *   Replicar prod localmente (usar config prod, mas sobrescrever BD com env var para usar SQLite).
*   **Payback:** Habilidade de rodar *exatamente a mesma imagem* em qualquer ambiente.
*   Diagrama (Figura 18.13): Ciclo de vida da imagem: CI/CD -> Smoke -> UAT -> Prod -> Dev (debug). Mesma imagem, configs diferentes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A seÃ§Ã£o 18.5 resume o **valor estratÃ©gico** de investir tempo em um modelo de configuraÃ§Ã£o flexÃ­vel e multi-camadas (PadrÃ£o na Imagem -> Arquivo Externo -> VariÃ¡vel de Ambiente) para suas aplicaÃ§Ãµes conteinerizadas.
*   O benefÃ­cio principal Ã© habilitar o fluxo de trabalho ideal de CI/CD: **construir a imagem Docker uma Ãºnica vez e promover essa mesma imagem imutÃ¡vel atravÃ©s de todos os ambientes**. A flexibilidade na configuraÃ§Ã£o permite que essa imagem Ãºnica se adapte Ã s necessidades de cada ambiente (conectar ao BD de teste, usar chaves de API de produÃ§Ã£o, ter logging diferente, etc.) sem precisar reconstruir a imagem.
*   O uso de variÃ¡veis de ambiente como a camada final de override Ã© particularmente poderoso para:
    *   **OperaÃ§Ãµes:** Permitir ajustes rÃ¡pidos em produÃ§Ã£o (ex: diminuir nÃ­vel de log se estiver causando sobrecarga) sem precisar de um novo deploy completo da aplicaÃ§Ã£o.
    *   **Feature Flags:** Ligar ou desligar funcionalidades especÃ­ficas da aplicaÃ§Ã£o dinamicamente via variÃ¡veis de ambiente.
    *   **Debugging:** Facilitar a reproduÃ§Ã£o de um problema de produÃ§Ã£o no ambiente de desenvolvimento, usando a configuraÃ§Ã£o de produÃ§Ã£o mas sobrescrevendo apenas o necessÃ¡rio (como a conexÃ£o do banco de dados) via variÃ¡veis de ambiente.
*   A Figura 18.13 ilustra esse fluxo virtuoso da imagem Ãºnica atravÃ©s dos diferentes ambientes, cada um aplicando sua prÃ³pria configuraÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando novo. Ã‰ a justificativa final para os padrÃµes mostrados no capÃ­tulo. Pense em como vocÃª gerenciaria diferentes configuraÃ§Ãµes para seus prÃ³prios projetos usando essas camadas.

---

ğŸ“„ PÃ¡gina 380

ğŸ” Resumo dos principais pontos:

*   Figura 18.13: Diagrama do ciclo de vida da imagem.
*   Modelo flexÃ­vel Ã© **future-proof**:
    *   Runtimes de contÃªiner (Compose, Swarm, K8s) suportam carregar arquivos (configs/secrets) e setar env vars.
    *   Mesma imagem/modelo funciona em todos.
    *   TambÃ©m funciona em PaaS e Serverless (usam mesmos mecanismos de config).
*   **18.6 Lab** (LaboratÃ³rio)
    *   Objetivo: Praticar configuraÃ§Ã£o de app com mÃºltiplas fontes.
    *   App: Image Gallery novamente.
    *   Ponto de Partida: `docker-compose.yml` em `ch18/lab` com serviÃ§os definidos, mas *sem* configuraÃ§Ã£o aplicada.
*   **Tarefa:** Modificar `docker-compose.yml` para configurar *todos* os componentes para:
    1.  Usar **Volumes** (Docker Volumes nomeados) para carregar arquivos de override.
    2.  Carregar os overrides do ambiente **Test**.
    3.  Sobrescrever o ciclo de release para ser `"20.01"` (em vez de "19.12").

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina finaliza a seÃ§Ã£o de benefÃ­cios, destacando que o modelo de configuraÃ§Ã£o multi-camadas baseado em arquivos e variÃ¡veis de ambiente Ã© **portÃ¡til** nÃ£o apenas entre diferentes orquestradores de contÃªiner (Compose, Swarm, K8s), mas tambÃ©m para outras plataformas modernas como PaaS (Platform-as-a-Service) e FaaS (Functions-as-a-Service / Serverless), que geralmente usam os mesmos mecanismos. Investir nesse modelo agora facilita migraÃ§Ãµes futuras.
*   O **Lab do CapÃ­tulo 18** pede para vocÃª aplicar esse modelo completo ao configurar a aplicaÃ§Ã£o Image Gallery. VocÃª recebe um `docker-compose.yml` bÃ¡sico e precisa modificÃ¡-lo para:
    1.  **Usar Volumes:** Em vez de bind mounts (como nos exercÃ­cios) ou secrets (como no exemplo final), usar **Volumes Docker Nomeados** para fornecer os arquivos de configuraÃ§Ã£o de override para cada serviÃ§o. Isso envolverÃ¡:
        *   Definir volumes nomeados na seÃ§Ã£o raiz `volumes:`.
        *   Montar cada volume no `target` apropriado (`/app/config-override`) dentro de cada serviÃ§o (`access-log`, `iotd`, `image-gallery`).
        *   *PrÃ©-popular* esses volumes com os arquivos de configuraÃ§Ã£o do ambiente de **Teste** (provavelmente copiando os arquivos de `ch18/exercises/.../config/test` para o local dos volumes no host antes de iniciar, ou usando um container temporÃ¡rio para copiar os arquivos para o volume - esta parte pode ser tricky). *Alternativa mais simples (talvez o que o lab espera):* Usar bind mounts para os arquivos de teste, mas **declarar** volumes nomeados no YAML (mesmo que nÃ£o usados diretamente para config) como parte do exercÃ­cio de sintaxe. **VERIFIQUE A SOLUÃ‡ÃƒO DO LAB se esta parte for confusa.** O mais provÃ¡vel Ã© usar bind mount para os arquivos de teste, mas a instruÃ§Ã£o pede Volumes. Talvez o objetivo seja sÃ³ declarar o volume e ainda usar bind mount para os arquivos, ou usar um container inicializador. Vamos assumir bind mount por simplicidade na estratÃ©gia.
    2.  **Carregar Config Test:** Usar os arquivos das pastas `config/test` de cada app como `source` para os bind mounts (ou para popular os volumes).
    3.  **Sobrescrever Release:** Usar a seÃ§Ã£o `environment:` em cada serviÃ§o para definir a variÃ¡vel de ambiente apropriada (`release=20.01`, `NODE_CONFIG='{"release":"20.01"}'`, `IG_RELEASE="20.01"`) para sobrescrever o ciclo de release para "20.01".

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 18 (Usando Bind Mounts para simplicidade, apesar da instruÃ§Ã£o ambÃ­gua sobre Volumes):**
    1.  Navegue para `ch18\lab`. Copie o `docker-compose.yml` inicial.
    2.  Adicione seÃ§Ã£o `networks:` (ex: `app-net:`). Conecte todos os serviÃ§os a ela.
    3.  Para cada serviÃ§o (`access-log`, `iotd`, `image-gallery`):
        *   Adicione `ports:` se necessÃ¡rio para acessar APIs `/config`.
        *   Adicione seÃ§Ã£o `environment:` e defina a variÃ¡vel para setar `release` para `"20.01"` (formato varia por app!).
        *   Adicione seÃ§Ã£o `volumes:` (ou `--mount`) para fazer bind mount da pasta de config de **Teste** apropriada (ex: `../exercises/access-log/config/test`) para `/app/config-override` (ou path correto). Use paths relativos ou absolutos corretos.
    4.  **Rodar:** `docker-compose up -d`.
    5.  **Verificar:** Acesse as APIs `/config` de cada serviÃ§o e confirme que `environment` Ã© `TEST` e `release` Ã© `20.01`.

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch18/lab/README.md (Verifique como eles lidaram com a questÃ£o Volume vs. Bind Mount para os arquivos de config).

---

ğŸ“„ PÃ¡gina 381

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 18 no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Fornece o link para a soluÃ§Ã£o completa do Lab 18 no repositÃ³rio do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 18, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch18/lab/README.md

---

ğŸ“„ PÃ¡gina 382

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco, separando o Lab 18 do inÃ­cio do CapÃ­tulo 19.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 383

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 384

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡gina 385

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 19: Writing and managing application logs with Docker** (Escrevendo e gerenciando logs de aplicaÃ§Ã£o com Docker)
*   Logging: Geralmente chato, mas nÃ£o com Docker.
*   **PrincÃ­pio BÃ¡sico:** App precisa escrever logs para **Standard Output (stdout)** ou **Standard Error (stderr)**. Docker coleta dessas streams.
*   CapÃ­tulo cobre como fazer isso e o que fazer *depois*.
*   **Docker Logging Framework:** Ã‰ plugÃ¡vel. Uma vez que logs saem do container (via stdout/stderr), Docker pode enviÃ¡-los para diferentes destinos (drivers de log).
*   Permite modelo poderoso: logs de todos os containers centralizados em log store com UI de busca (ex: EFK stack - Elasticsearch, Fluentd, Kibana). Tudo open source, rodando em containers.
*   **19.1 Welcome to stderr and stdout!** (Bem-vindo ao stderr e stdout!)
    *   Imagem Docker = Filesystem + Metadado (qual processo iniciar).
    *   Processo roda em foreground, como comando no terminal.
    *   SaÃ­da do processo (logs, erros) vai para stdout e stderr.
    *   Docker *assiste* stdout/stderr e coleta essa saÃ­da como "logs do contÃªiner".

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 19, focado em **logging** no contexto Docker.
*   O ponto fundamental e mais importante Ã©: para que o Docker possa gerenciar os logs da sua aplicaÃ§Ã£o, a aplicaÃ§Ã£o precisa ser configurada para escrever suas mensagens de log nas **saÃ­das padrÃ£o (standard output - stdout)** ou de **erro padrÃ£o (standard error - stderr)**. Estas sÃ£o as mesmas streams que um programa de linha de comando usa para imprimir texto no seu terminal.
*   Quando um contÃªiner estÃ¡ rodando, o Docker monitora essas duas streams do processo principal. Tudo que Ã© escrito nelas Ã© capturado pelo Docker e se torna o que chamamos de "logs do contÃªiner" (acessÃ­veis via `docker logs`).
*   A grande vantagem disso Ã© que, uma vez que os logs estÃ£o sendo capturados pelo Docker, ele pode usar **drivers de log** diferentes para enviÃ¡-los a diversos destinos. O padrÃ£o Ã© salvar em um arquivo JSON no host, mas pode-se configurar para enviar para Fluentd, Splunk, Syslog, AWS CloudWatch, etc., permitindo a centralizaÃ§Ã£o e anÃ¡lise de logs de mÃºltiplos contÃªineres.
*   A seÃ§Ã£o 19.1 revisita como o processo principal do contÃªiner se relaciona com stdout/stderr.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Qualquer comando que vocÃª roda no PowerShell/CMD que imprime texto na tela estÃ¡ usando stdout/stderr.
*   AplicaÃ§Ãµes .NET Core, Java (com Logback/Log4j configurados para ConsoleAppender), Node.js (com `console.log`), Python (`print`) geralmente escrevem para stdout/stderr por padrÃ£o ou podem ser facilmente configuradas para isso.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   stdout/stderr: Pense neles como os dois "megafones" que o processo dentro do contÃªiner pode usar para falar. stdout Ã© para mensagens normais, stderr Ã© para erros. O Docker fica "ouvindo" esses megafones.
*   Logging Drivers: SÃ£o como "serviÃ§os postais" diferentes que o Docker pode usar para pegar as mensagens dos megafones e enviÃ¡-las para destinos diferentes (arquivo local, servidor de log central, nuvem, etc.).

---

ğŸ“„ PÃ¡gina 386

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar `timecheck` (Cap 15) em foreground para ver logs em stdout.
    *   Comando: `docker run diamol/ch15-timecheck:3.0` (sem `-d`).
    *   Terminal fica preso, mostrando logs da app (timestamps) periodicamente.
    *   Sair com `Ctrl+C`.
*   Figura 19.1: Mostra terminal com logs do `timecheck` rodando em foreground.
*   Este Ã© o modelo operacional padrÃ£o: processo foreground, logs via stdout/stderr, Docker coleta.
*   Todos os apps do livro seguem este padrÃ£o.
*   Diagrama (Figura 19.2): Ilustra App -> stdout/stderr -> Docker coletando.
*   **Armazenamento PadrÃ£o:** Docker salva logs coletados em arquivos **JSON** no host.
    *   Logs persistem para containers detached ou `Exited`.
    *   Arquivos JSON tÃªm mesmo ciclo de vida do container (removidos com `docker rm`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra o comportamento padrÃ£o: rodar um contÃªiner em **primeiro plano (foreground)** (sem `-d`). O terminal que iniciou o contÃªiner fica vinculado Ã s streams stdout/stderr do processo dentro do contÃªiner, exibindo diretamente qualquer saÃ­da que ele produza. Isso Ã© Ãºtil para depuraÃ§Ã£o rÃ¡pida, mas nÃ£o para serviÃ§os que precisam rodar continuamente.
*   A Figura 19.2 reforÃ§a visualmente: a aplicaÃ§Ã£o escreve para stdout/stderr, e o Docker Engine captura essa saÃ­da.
*   Explica como o Docker armazena esses logs por padrÃ£o quando *nÃ£o* estÃ¡ rodando em foreground: ele usa um **driver de log** chamado `json-file`. Esse driver escreve cada linha de log (junto com metadados como timestamp e stream) como um objeto JSON em um arquivo especÃ­fico para aquele contÃªiner no disco do host Docker.
*   **Importante:** Esses arquivos JSON sÃ£o gerenciados pelo Docker e sÃ£o **removidos** quando o contÃªiner correspondente Ã© removido com `docker rm`. Se vocÃª precisar de retenÃ§Ã£o de logs a longo prazo, nÃ£o pode depender apenas desses arquivos locais; precisarÃ¡ usar um driver de log que envie os dados para um sistema externo.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar em Foreground:**
    ```powershell
    # Use uma imagem simples que imprime e sai, ou a timecheck
    docker container run diamol/ch15-timecheck:3.0
    # Observe os logs aparecerem. Pressione Ctrl+C para parar.
    ```
2.  **LocalizaÃ§Ã£o dos Arquivos JSON (Informativo):**
    *   No Docker Desktop, os arquivos estÃ£o dentro da VM Linux ou nos dados do serviÃ§o Windows, e nÃ£o sÃ£o facilmente acessÃ­veis diretamente do host Windows.
    *   Em um servidor Linux Docker nativo, estariam tipicamente em `/var/lib/docker/containers/<container-id>/<container-id>-json.log`.
    *   Em um servidor Windows com Windows Containers, estariam em `C:\ProgramData\docker\containers\<container-id>\...`.
    *   VocÃª normalmente interage com os logs via `docker logs`, nÃ£o acessando esses arquivos diretamente.

---

ğŸ“„ PÃ¡gina 387

ğŸ” Resumo dos principais pontos:

*   Figura 19.2: Diagrama App -> stdout/stderr -> Docker.
*   **ExercÃ­cio Try-it-now:** Rodar `timecheck` em background (`-d`), verificar logs, parar container, verificar logs de novo, inspecionar path do log file.
    *   `docker run -d --name timecheck ...`
    *   `docker logs --tail 1 timecheck` (ver Ãºltimo log rodando)
    *   `docker stop timecheck`
    *   `docker logs --tail 1 timecheck` (ver Ãºltimo log mesmo parado)
    *   `docker inspect --format='{{.LogPath}}' timecheck` (ver caminho do arquivo JSON no host/VM)
*   Figura 19.3: Mostra saÃ­da dos comandos e exemplo de path do log file no Windows.
*   Acessar log file diretamente nÃ£o Ã© comum/fÃ¡cil (especialmente no Docker Desktop). `docker logs` Ã© a interface padrÃ£o.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra que os logs capturados pelo Docker (usando o driver `json-file` padrÃ£o) persistem mesmo quando o contÃªiner estÃ¡ rodando em background (`-d`) ou quando estÃ¡ parado (`Exited` apÃ³s `docker stop`). O comando `docker logs` ainda consegue ler o conteÃºdo do arquivo JSON associado ao contÃªiner.
*   O comando `docker inspect --format='{{.LogPath}}'` revela o caminho exato no sistema de arquivos do host (ou da VM do Docker Desktop) onde o arquivo `...-json.log` para aquele contÃªiner especÃ­fico estÃ¡ armazenado.
*   A pÃ¡gina reitera que, embora seja bom saber que os logs sÃ£o armazenados em arquivos, a maneira normal e recomendada de acessÃ¡-los Ã© atravÃ©s do comando `docker logs`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar em Background e Ver Logs:**
    ```powershell
    # Limpar anterior se existir
    docker rm -f timecheck timecheck2 timecheck3 timecheck4 timecheck5

    # Rodar timecheck v3 em background
    docker container run -d --name timecheck diamol/ch15-timecheck:3.0

    # Esperar um pouco e ver Ãºltimo log
    Start-Sleep -Seconds 5
    docker container logs --tail 1 timecheck

    # Parar o container
    docker container stop timecheck

    # Verificar que log ainda existe
    docker container logs --tail 1 timecheck

    # Ver path do log file (informativo)
    docker container inspect --format='{{.LogPath}}' timecheck
    # A saÃ­da serÃ¡ um path dentro da VM do Docker Desktop (ex: /var/lib/docker/...)
    ```

---

ğŸ“„ PÃ¡gina 388

ğŸ” Resumo dos principais pontos:

*   Formato JSON bruto no arquivo de log: `{"log":"...", "stream":"stdout/stderr", "time":"..."}`.
*   Listagem 19.1: Exemplo do JSON bruto.
*   `docker logs` lÃª e formata esse JSON.
*   **Gerenciamento Tamanho/RotaÃ§Ã£o de Log:**
    *   PadrÃ£o: 1 arquivo JSON por container, cresce indefinidamente (risco de encher disco).
    *   SoluÃ§Ã£o: Configurar Docker para usar **rolling files** com tamanho mÃ¡ximo.
        *   Quando arquivo atinge `max-size`, Docker comeÃ§a novo arquivo (atÃ© `max-file` arquivos).
        *   Quando todos cheios, sobrescreve o mais antigo.
    *   Pode configurar globalmente (Engine `daemon.json`) ou por container (`docker run --log-opt ...`).
    *   Configurar por container Ã© bom para ter controle fino (ex: logs pequenos/rotacionados para app verboso, logs completos para outros).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra o formato JSON real (Listagem 19.1) que o driver `json-file` usa para armazenar cada entrada de log. O comando `docker logs` simplesmente lÃª esse arquivo e extrai o campo `"log"` para exibiÃ§Ã£o (opcionalmente adicionando timestamp ou outros detalhes com flags).
*   Aborda um problema prÃ¡tico importante do driver padrÃ£o: o arquivo de log pode crescer sem limite. Para evitar que logs de contÃªineres muito "falantes" consumam todo o espaÃ§o em disco, o Docker permite configurar a **rotaÃ§Ã£o de logs**.
*   As opÃ§Ãµes principais (para o driver `json-file`) sÃ£o:
    *   `max-size`: O tamanho mÃ¡ximo que um Ãºnico arquivo de log pode atingir (ex: `10m` para 10MB, `5k` para 5KB).
    *   `max-file`: O nÃºmero mÃ¡ximo de arquivos de log rotacionados a serem mantidos.
*   Quando um arquivo atinge `max-size`, o Docker o renomeia (ex: `log.1`) e comeÃ§a um novo `log`. Isso continua atÃ© atingir `max-file` arquivos (`log`, `log.1`, ..., `log.N-1`). Quando o `log` atual enche novamente, o arquivo mais antigo (`log.N-1`) Ã© excluÃ­do, os outros sÃ£o renomeados (`log.N-2` -> `log.N-1`, etc.), e um novo `log` Ã© iniciado.
*   Essas opÃ§Ãµes podem ser definidas globalmente para todos os contÃªineres no `daemon.json` ou individualmente para um contÃªiner especÃ­fico usando a flag `--log-opt` no comando `docker run` (ou `logging: options:` no Docker Compose).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio usarÃ¡ `--log-opt` no `docker run`.
*   **Exemplo de ConfiguraÃ§Ã£o Global (`daemon.json`):**
    ```json
    {
      "log-driver": "json-file",
      "log-opts": {
        "max-size": "10m",
        "max-file": "3"
      }
      // ... outras configs ...
    }
    ```
    *(Lembre-se de reiniciar o Docker Engine apÃ³s mudar `daemon.json`)*.

---

ğŸ“„ PÃ¡gina 389

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Rodar `timecheck` com opÃ§Ãµes de log por container.
    *   Comando: `docker run -d --name timecheck2 --log-opt max-size=5k --log-opt max-file=3 -e Timer__IntervalSeconds=1 diamol/ch15-timecheck:3.0`.
    *   `--log-opt max-size=5k`: Limita cada arquivo a 5 KB.
    *   `--log-opt max-file=3`: MantÃ©m no mÃ¡ximo 3 arquivos.
    *   `-e Timer__IntervalSeconds=1`: Faz app logar mais rÃ¡pido (a cada 1s).
    *   Esperar alguns minutos para logs acumularem/rotacionarem.
    *   Inspecionar log path: `docker inspect --format='{{.LogPath}}' timecheck2`.
*   SaÃ­da do `inspect` ainda mostra path de 1 arquivo, mas Docker gerencia rotaÃ§Ã£o com sufixos (`.1`, `.2`).
*   Se host permite acesso (Win Container, Linux CE), pode-se listar diretÃ³rio e ver arquivos rotacionados.
*   Figura 19.4: Mostra exemplo de arquivos rotacionados (`...-json.log`, `...-json.log.1`, `...-json.log.2`) no host.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra a configuraÃ§Ã£o de rotaÃ§Ã£o de logs *por contÃªiner* usando as flags `--log-opt`.
*   Um novo contÃªiner (`timecheck2`) Ã© iniciado com limites pequenos (`max-size=5k`, `max-file=3`) e configurado para logar mais frequentemente (`-e Timer__IntervalSeconds=1`) para acelerar o processo de rotaÃ§Ã£o.
*   ApÃ³s esperar um tempo, o comando `docker inspect` ainda mostrarÃ¡ o caminho para o arquivo de log "principal" (sem sufixo). No entanto, por baixo dos panos, o Docker Engine estÃ¡ gerenciando os arquivos rotacionados (adicionando `.1`, `.2`, etc.) naquele mesmo diretÃ³rio e respeitando os limites definidos.
*   A Figura 19.4 ilustra como esses arquivos apareceriam no sistema de arquivos do host (se diretamente acessÃ­vel).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar com RotaÃ§Ã£o:**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck timecheck2

    docker container run -d --name timecheck2 --log-opt max-size=5k --log-opt max-file=3 -e Timer__IntervalSeconds=1 diamol/ch15-timecheck:3.0
    ```
2.  **Esperar e Inspecionar:**
    ```powershell
    Write-Host "Aguardando logs rotacionarem..."
    Start-Sleep -Seconds 60 # Espere 1 minuto ou mais

    docker container inspect --format='{{.LogPath}}' timecheck2
    # Anote o caminho (informativo)

    # Verificar logs via Docker (deve mostrar apenas os logs mais recentes, <= 15KB)
    docker logs timecheck2
    ```
    *Como vocÃª estÃ¡ no Docker Desktop, nÃ£o conseguirÃ¡ ver os arquivos `.1`, `.2` diretamente no seu filesystem Windows.*

---

ğŸ“„ PÃ¡gina 390

ğŸ” Resumo dos principais pontos:

*   Docker tem estÃ¡gio de coleta/processamento para logs stdout/stderr -> onde opÃ§Ãµes (`--log-opt`) e drivers (`--log-driver`) atuam.
*   Para aproveitar, app *precisa* enviar logs para stdout/stderr.
*   **Problema:** Nem todo app faz isso.
    *   Apps background (Windows Service, Linux daemon): Processo principal do container nÃ£o Ã© o app -> sem logs.
    *   Apps com logging frameworks antigos/diferentes: Escrevem para arquivos, Syslog (Linux), Event Log (Windows) -> Docker nÃ£o vÃª.
*   **19.2 Relaying logs from other sinks to stdout** (Retransmitindo logs de outros coletores/sinks para stdout)
*   **ExercÃ­cio Try-it-now:** Demonstrar app que loga em arquivo, nÃ£o stdout.
    *   Imagem: `diamol/ch19-timecheck:4.0`.
    *   1. Rodar container: `docker run -d --name timecheck3 ...:4.0`.
    *   2. Verificar `docker logs timecheck3` (vazio).
    *   3. Conectar ao container (`docker exec -it ... sh/cmd`).
    *   4. Ler arquivo de log interno: `cat /logs/timecheck.log`. Logs estÃ£o lÃ¡.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina resume que as funcionalidades avanÃ§adas de logging do Docker (drivers, opÃ§Ãµes) dependem fundamentalmente de os logs da aplicaÃ§Ã£o serem direcionados para **stdout/stderr**.
*   Introduce o problema comum: muitas aplicaÃ§Ãµes, especialmente as mais antigas ou aquelas projetadas para rodar como serviÃ§os de background, **nÃ£o** logam para stdout/stderr por padrÃ£o. Elas podem:
    *   Rodar como um processo de background, enquanto o processo principal do contÃªiner Ã© apenas um script de inicializaÃ§Ã£o que termina.
    *   Usar frameworks de logging configurados para escrever em arquivos (`app.log`), no Syslog do sistema (Linux) ou no Event Log (Windows).
*   Em todos esses casos, o Docker Engine nÃ£o captura os logs da aplicaÃ§Ã£o atravÃ©s do monitoramento de stdout/stderr do processo principal.
*   A seÃ§Ã£o 19.2 introduz a soluÃ§Ã£o para isso.
*   O exercÃ­cio demonstra o problema usando uma versÃ£o modificada do `timecheck` (`:4.0`) que escreve seus logs para um arquivo `/logs/timecheck.log` *dentro* do contÃªiner. O resultado esperado Ã© que `docker logs timecheck3` nÃ£o mostrarÃ¡ nada, mas se vocÃª entrar no contÃªiner (`docker exec`) e ler o arquivo, os logs estarÃ£o lÃ¡.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar App v4 (Log em Arquivo):**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck3

    docker container run -d --name timecheck3 diamol/ch19-timecheck:4.0
    ```
2.  **Verificar Docker Logs (Vazio):**
    ```powershell
    Start-Sleep -Seconds 5
    docker logs timecheck3 # Deve retornar vazio ou quase vazio
    ```
3.  **Verificar Arquivo Interno:** (Assumindo imagem Linux)
    ```powershell
    docker container exec -it timecheck3 sh
    # Dentro do container:
    ls /logs # Deve mostrar timecheck.log
    cat /logs/timecheck.log # Deve mostrar os logs reais da aplicaÃ§Ã£o
    exit
    ```

---

ğŸ“„ PÃ¡gina 391

ğŸ” Resumo dos principais pontos:

*   Figura 19.5: Mostra `docker logs` vazio e `cat /logs/timecheck.log` com conteÃºdo.
*   Docker sÃ³ lÃª stdout/stderr, nÃ£o outros sinks (arquivos, syslog, etc.).
*   **PadrÃ£o "Sidecar" ou Processo Duplo para Relaying:**
    *   Rodar um **segundo processo** no comando de inicializaÃ§Ã£o do container.
    *   Esse segundo processo lÃª logs do sink original (ex: arquivo, syslog) e os **escreve para stdout**.
    *   Esse relay deve ser o *Ãºltimo* processo iniciado (para ser o processo foreground que Docker monitora).
*   Diagrama (Figura 19.6): Ilustra PadrÃ£o Relay.
    *   App escreve em `app.log`.
    *   Relay Utility (rodando no mesmo container) lÃª `app.log`.
    *   Relay Utility escreve para stdout.
    *   Docker coleta de stdout.
*   **Desvantagens/Cuidados:**
    *   Relay precisa ser robusto (se ele falhar, container sai, mesmo se app OK).
    *   Se app falhar mas relay continuar, container fica Up (precisa de health check no app).
    *   Ineficiente em disco (logs duplicados: no sink original + no JSON do Docker).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina apresenta a soluÃ§Ã£o padrÃ£o para capturar logs de aplicaÃ§Ãµes que nÃ£o escrevem em stdout/stderr: adicionar um **processo "relay"** (retransmissor) dentro do mesmo contÃªiner.
*   **Como funciona:**
    1.  A aplicaÃ§Ã£o principal roda (muitas vezes em background).
    2.  Um segundo processo (um script ou um pequeno utilitÃ¡rio) Ã© iniciado.
    3.  Esse segundo processo Ã© configurado para "seguir" ou ler continuamente o local onde a aplicaÃ§Ã£o principal estÃ¡ escrevendo seus logs (ex: o arquivo `app.log`).
    4.  Tudo que o relay lÃª, ele imediatamente escreve para seu prÃ³prio **stdout**.
    5.  O comando de inicializaÃ§Ã£o do contÃªiner (`CMD` ou `ENTRYPOINT`) Ã© configurado para iniciar a aplicaÃ§Ã£o principal em background e depois iniciar o processo relay em **foreground**.
    6.  Como o relay estÃ¡ em foreground, o Docker captura o stdout *dele*, que agora contÃ©m os logs originais da aplicaÃ§Ã£o. (Figura 19.6 ilustra isso).
*   Essa tÃ©cnica Ã© Ã s vezes chamada de padrÃ£o "sidecar" (embora mais comumente sidecar se refira a um contÃªiner separado; aqui sÃ£o dois processos no mesmo contÃªiner).
*   A pÃ¡gina alerta para as **desvantagens**:
    *   Complexidade e fragilidade: O relay precisa ser confiÃ¡vel.
    *   Mascaramento de falhas: Falha do app pode nÃ£o parar o relay (e vice-versa). Health checks sÃ£o essenciais.
    *   DuplicaÃ§Ã£o de armazenamento: Os logs existem no local original *e* no arquivo JSON do Docker.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo Dockerfile (`v5`) implementarÃ¡ esse padrÃ£o com um utilitÃ¡rio `.NET` chamado `Tail.dll`.
*   Se fosse Linux puro, poderia usar `CMD meu_app & tail -F /var/log/meu_app.log`. O `tail -F` seguiria o arquivo e escreveria para stdout, enquanto `meu_app` rodaria em background (`&`).

---

ğŸ“„ PÃ¡gina 392

ğŸ” Resumo dos principais pontos:

*   Ainda assim, relay Ã© padrÃ£o Ãºtil (especialmente se app roda em background). Melhor ter logs duplicados do que logs invisÃ­veis para Docker.
*   **Exemplo `timecheck:5.0`:** Adiciona utilitÃ¡rio `Tail.dll` para relay.
*   **Listagem 19.2:** Trechos do Dockerfile `v5` multi-stage.
    *   EstÃ¡gio `builder` compila app principal.
    *   EstÃ¡gio `utility-builder` compila `Tail.dll`.
    *   EstÃ¡gio final `app` copia ambos.
    *   **`CMD`s diferentes para Windows/Linux:**
        *   Windows: `CMD start /B dotnet TimeCheck.dll && dotnet Tail.dll /logs timecheck.log`. (`start /B` roda app em background).
        *   Linux: `CMD dotnet TimeCheck.dll & dotnet Tail.dll /logs timecheck.log`. (`&` roda app em background).
    *   Em ambos, `Tail.dll` roda em foreground, lendo o log e escrevendo para stdout.
*   **ExercÃ­cio Try-it-now:** Rodar v5 e verificar logs via `docker logs` e arquivo interno.
    *   `docker run -d --name timecheck4 diamol/ch19-timecheck:5.0`.
    *   `docker logs timecheck4` (deve mostrar logs).
    *   `docker exec ... cat /logs/timecheck.log` (logs ainda existem no arquivo tambÃ©m).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina argumenta que, apesar das desvantagens, o padrÃ£o relay Ã© frequentemente a melhor (ou Ãºnica) soluÃ§Ã£o para integrar aplicaÃ§Ãµes "nÃ£o-stdout" com o ecossistema de logging do Docker.
*   A Listagem 19.2 mostra como a imagem `timecheck:5.0` implementa isso:
    *   Usa build multi-estÃ¡gio para compilar tanto a aplicaÃ§Ã£o `TimeCheck.dll` quanto o utilitÃ¡rio `Tail.dll`.
    *   O estÃ¡gio final copia ambos.
    *   O `CMD` Ã© crucial: ele primeiro inicia a aplicaÃ§Ã£o principal `TimeCheck.dll` **em background** (usando `start /B` no Windows ou `&` no Linux) e depois inicia o utilitÃ¡rio `Tail.dll` **em foreground**, instruindo-o a monitorar o arquivo `/logs/timecheck.log`.
    *   Como `Tail.dll` estÃ¡ em foreground, o Docker monitora o stdout *dele*. O `Tail.dll` lÃª as linhas do arquivo de log e as imprime em seu stdout, efetivamente retransmitindo os logs para o Docker.
*   O exercÃ­cio instrui a rodar esta imagem `v5`. A expectativa Ã© que `docker logs timecheck4` *agora funcione*, mostrando os logs da aplicaÃ§Ã£o, mas que os logs *tambÃ©m continuem sendo escritos* no arquivo `/logs/timecheck.log` dentro do contÃªiner (confirmando a duplicaÃ§Ã£o).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar v5 com Relay:**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck4

    docker container run -d --name timecheck4 diamol/ch19-timecheck:5.0
    ```
2.  **Verificar Docker Logs (Funciona!):**
    ```powershell
    Start-Sleep -Seconds 5
    docker logs timecheck4
    ```
3.  **Verificar Arquivo Interno (Duplicado):** (Assumindo imagem Linux)
    ```powershell
    docker container exec -it timecheck4 sh
    # Dentro do container:
    cat /logs/timecheck.log
    # Logs estÃ£o aqui tambÃ©m.
    exit
    ```

---

ğŸ“„ PÃ¡gina 393

ğŸ” Resumo dos principais pontos:

*   Figura 19.7: Mostra `docker logs` funcionando para v5, e `cat` mostrando logs duplicados no arquivo.
*   Usou utilitÃ¡rio customizado (`Tail.dll`) para cross-platform; `tail -F` Linux nÃ£o tem equivalente Windows direto.
*   UtilitÃ¡rio customizado Ã© mais flexÃ­vel (pode ler de qualquer sink). Cobre qualquer cenÃ¡rio onde logs nÃ£o vÃ£o para stdout.
*   Com todos os apps escrevendo container logs (direto ou via relay), pode-se usar logging plugÃ¡vel do Docker.
*   **19.3 Collecting and forwarding container logs** (Coletando e encaminhando logs de contÃªiner)
    *   Docker adiciona camada de gerenciamento consistente (start, stop, inspect, logs).
    *   Ãštil com **logging centralizado**.
    *   Exemplo: **Fluentd**.
        *   Camada de logging unificada, open source, projeto CNCF, maduro, flexÃ­vel.
        *   Ingere logs de vÃ¡rias fontes.
        *   Filtra/enriquece logs.
        *   Encaminha para vÃ¡rios destinos.
        *   Roda em container.
    *   **Fluxo:**
        1.  Rodar Fluentd container (ouve por logs).
        2.  Rodar app containers usando **driver de log `fluentd`** do Docker (`--log-driver=fluentd`).
        3.  Logs do app (stdout/stderr) sÃ£o enviados pelo Docker Engine para Fluentd (em vez do `json-file`).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a discussÃ£o sobre o padrÃ£o relay, justificando o uso do utilitÃ¡rio customizado pela portabilidade e flexibilidade. O objetivo principal Ã© garantir que **todos** os seus contÃªineres, independentemente de como logavam originalmente, agora emitam seus logs de uma forma que o Docker possa capturar (via stdout/stderr do processo foreground).
*   A seÃ§Ã£o 19.3 introduz o **prÃ³ximo nÃ­vel** de gerenciamento de logs: usar o sistema plugÃ¡vel de **logging drivers** do Docker para enviar os logs capturados para um **sistema de logging centralizado**.
*   **Fluentd** Ã© apresentado como uma soluÃ§Ã£o popular e poderosa para isso:
    *   Ã‰ um agregador/processador/encaminhador de logs muito versÃ¡til.
    *   Roda como um contÃªiner.
    *   O Docker Engine tem um `log-driver` embutido chamado `fluentd`. Quando vocÃª inicia um contÃªiner com `--log-driver=fluentd`, o Docker Engine, em vez de escrever os logs em um arquivo JSON, os envia (geralmente via rede UDP ou TCP) para um endereÃ§o onde o serviÃ§o Fluentd estÃ¡ ouvindo.
    *   O Fluentd recebe esses logs, pode processÃ¡-los (adicionar metadados, filtrar, transformar) e entÃ£o enviÃ¡-los para um ou mais destinos finais (Elasticsearch, Splunk, S3, etc.).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker run --log-driver=fluentd ...` para configurar contÃªineres a enviarem logs para o Fluentd.
*   VocÃª rodarÃ¡ o Fluentd em um contÃªiner, configurando-o (via volume mount de arquivo de config) para receber logs e fazer algo com eles (inicialmente, apenas imprimir no prÃ³prio stdout do Fluentd).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Log Driver `fluentd`: Ã‰ como dizer ao Docker: "Em vez de arquivar os logs deste contÃªiner localmente, por favor, envie-os para este endereÃ§o postal especial onde o carteiro Fluentd os recolherÃ¡".
*   Fluentd: O carteiro/centro de distribuiÃ§Ã£o de logs. Ele recebe cartas (logs) de vÃ¡rios remetentes (containers), pode carimbar informaÃ§Ãµes extras (enriquecer), separar por CEP (filtrar) e entregar em diferentes destinos finais (Elasticsearch, S3, etc.).

---

ğŸ“„ PÃ¡gina 394

ğŸ” Resumo dos principais pontos:

*   Fluentd usa arquivo de config para processar logs.
*   **ExercÃ­cio Try-it-now:** Rodar Fluentd (config simples: ecoar logs para stdout) e app enviando logs para ele.
    *   Navegar para `ch19/exercises/fluentd`.
    *   1. Rodar Fluentd: `docker run -d -p 24224:24224 --name fluentd -v "$(pwd)/conf:/fluentd/etc" -e FLUENTD_CONF=stdout.conf diamol/fluentd`.
        *   `-p 24224`: Porta padrÃ£o que Fluentd ouve.
        *   `-v ...`: Monta pasta `conf` local (com `stdout.conf`) para config do Fluentd.
        *   `-e FLUENTD_CONF`: Diz ao Fluentd qual arquivo de config usar.
    *   2. Rodar `timecheck` usando driver `fluentd`: `docker run -d --log-driver=fluentd --name timecheck5 diamol/ch19-timecheck:5.0`.
    *   3. Verificar logs `timecheck5`: `docker logs timecheck5` (deve dar erro ou vazio).
    *   4. Verificar logs `fluentd`: `docker logs --tail 1 fluentd` (deve mostrar log do timecheck).
*   ObservaÃ§Ã£o: `docker logs` nÃ£o funciona com driver `fluentd` (logs nÃ£o estÃ£o mais no host local). Precisa ver no destino (stdout do Fluentd, neste caso).
*   Figura 19.8: Mostra erro do `logs timecheck5` e log do timecheck aparecendo nos logs do `fluentd`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra a configuraÃ§Ã£o bÃ¡sica do driver `fluentd`.
    1.  Inicia-se um contÃªiner `fluentd`. Ele Ã© configurado (via volume mount `-v` e variÃ¡vel de ambiente `-e`) para usar um arquivo `stdout.conf`. Este arquivo de configuraÃ§Ã£o instrui o Fluentd a simplesmente pegar qualquer log que ele receba na porta 24224 e imprimi-lo em seu prÃ³prio stdout.
    2.  Inicia-se um contÃªiner `timecheck5` com a opÃ§Ã£o `--log-driver=fluentd`. O Docker Engine agora enviarÃ¡ os logs (stdout/stderr) deste contÃªiner para o endereÃ§o padrÃ£o do Fluentd (que o driver conhece, geralmente `localhost:24224` do ponto de vista do Engine).
    3.  Tentar usar `docker logs timecheck5` **nÃ£o funciona** (ou retorna vazio/erro), porque o driver `json-file` nÃ£o estÃ¡ mais sendo usado para este contÃªiner; os logs nÃ£o estÃ£o sendo armazenados localmente associados a ele.
    4.  Em vez disso, ao verificar `docker logs fluentd`, vocÃª verÃ¡ os logs do `timecheck5` aparecendo lÃ¡, porque a configuraÃ§Ã£o `stdout.conf` do Fluentd os estÃ¡ ecoando.
*   Isso prova que o redirecionamento de logs via driver estÃ¡ funcionando.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch19\exercises\fluentd
    # Verifique a pasta 'conf' e o arquivo 'stdout.conf'
    ```
2.  **Rodar Fluentd (stdout config):** (Ajuste path se necessÃ¡rio)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "conf"
    # No Docker Desktop Linux mode, o path do host Ã© montado diretamente.
    docker container run -d -p 24224:24224 --name fluentd -v "$SourcePath`:/fluentd/etc" -e FLUENTD_CONF=stdout.conf diamol/fluentd
    ```
3.  **Rodar Timecheck com Driver Fluentd:**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck5
    docker container run -d --log-driver=fluentd --name timecheck5 diamol/ch19-timecheck:5.0
    ```
4.  **Verificar Logs:**
    ```powershell
    Start-Sleep -Seconds 5
    docker logs timecheck5 # Deve dar erro ou vazio
    docker logs --tail 1 fluentd # Deve mostrar um log do timecheck5
    ```

---

ğŸ“„ PÃ¡gina 395

ğŸ” Resumo dos principais pontos:

*   Figura 19.8: Confirma erro em `logs timecheck5` e logs aparecendo em `logs fluentd`.
*   Fluentd adiciona metadados (container ID/name) aos logs para identificaÃ§Ã£o.
*   Usar stdout no Fluentd Ã© sÃ³ para demo. Uso real: encaminhar para datastore central.
*   **Pilha EFK:** Elasticsearch + Fluentd + Kibana.
    *   **Elasticsearch:** Banco de dados NoSQL de documentos, bom para logs. Roda em container.
    *   **Kibana:** Interface web de busca e visualizaÃ§Ã£o para Elasticsearch. Roda em container.
*   Diagrama (Figura 19.9): Arquitetura EFK.
    *   App Containers -> (Docker Engine com driver fluentd) -> Fluentd Container -> Elasticsearch Container(s).
    *   Browser -> Kibana Container -> Elasticsearch Container(s).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que o Fluentd automaticamente enriquece os logs recebidos com metadados Ãºteis, como o ID e o nome do contÃªiner de origem, o que Ã© essencial quando se centraliza logs de mÃºltiplas fontes.
*   Introduz a **Pilha EFK** (Elasticsearch, Fluentd, Kibana) como uma soluÃ§Ã£o open source muito popular para logging centralizado com Docker:
    *   **Fluentd:** Coleta os logs dos contÃªineres (via driver Docker) e os envia para o Elasticsearch.
    *   **Elasticsearch:** Armazena os logs de forma eficiente e permite buscas rÃ¡pidas e complexas. Ã‰ um banco de dados de documentos distribuÃ­do e escalÃ¡vel.
    *   **Kibana:** Fornece a interface web para pesquisar, visualizar e criar dashboards a partir dos logs armazenados no Elasticsearch.
*   A Figura 19.9 ilustra como esses trÃªs componentes (cada um rodando em seu(s) prÃ³prio(s) contÃªiner(es)) interagem: Fluentd recebe de todos, envia para Elasticsearch; Kibana lÃª do Elasticsearch para exibir ao usuÃ¡rio.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O prÃ³ximo exercÃ­cio usarÃ¡ um `docker-compose.yml` para iniciar toda a pilha EFK (Fluentd, Elasticsearch, Kibana) como contÃªineres interconectados.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Pilha EFK:
    *   Fluentd: O carteiro que coleta logs de todos os contÃªineres.
    *   Elasticsearch: O grande arquivo central/biblioteca onde o carteiro armazena todos os logs de forma organizada e indexada.
    *   Kibana: O bibliotecÃ¡rio com um computador super-rÃ¡pido que permite a vocÃª pesquisar e encontrar qualquer log especÃ­fico nesse arquivo gigantesco, e atÃ© criar grÃ¡ficos sobre os tipos de logs recebidos.

---

ğŸ“„ PÃ¡gina 396

ğŸ” Resumo dos principais pontos:

*   Arquitetura EFK parece complexa, mas fÃ¡cil de iniciar com Docker Compose.
*   Infra de log em containers -> Usa-se driver `fluentd` para apps que querem log centralizado.
*   **ExercÃ­cio Try-it-now:** Rodar stack EFK e app enviando logs.
    *   1. Limpar containers.
    *   2. Navegar para `ch19/exercises`.
    *   3. Iniciar stack EFK: `docker-compose -f fluentd/docker-compose.yml up -d`. (Define serviÃ§os fluentd, elasticsearch, kibana).
    *   4. Rodar `timecheck` com driver `fluentd`: `docker run -d --log-driver=fluentd diamol/ch19-timecheck:5.0`.
*   **ConfiguraÃ§Ã£o Inicial Kibana:**
    *   Esperar Elasticsearch iniciar (~2 min).
    *   Acessar Kibana: `http://localhost:5601`.
    *   Clicar "Discover".
    *   PedirÃ¡ "index pattern". Digitar `fluentd*`. (Figura 19.10).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra como iniciar a pilha EFK completa usando um arquivo `docker-compose.yml` fornecido (`fluentd/docker-compose.yml`). Esse arquivo define os trÃªs serviÃ§os (Fluentd, Elasticsearch, Kibana) e como eles se conectam.
*   ApÃ³s iniciar a pilha EFK, qualquer contÃªiner iniciado com `--log-driver=fluentd` terÃ¡ seus logs automaticamente coletados pelo Fluentd e enviados para o Elasticsearch. O exercÃ­cio inicia o `timecheck` novamente dessa forma.
*   Como Ã© a primeira vez que se acessa o Kibana (`http://localhost:5601`) apÃ³s ele comeÃ§ar a receber dados do Elasticsearch, Ã© necessÃ¡rio fazer uma configuraÃ§Ã£o inicial mÃ­nima: dizer ao Kibana qual(is) "Ã­ndice(s)" (coleÃ§Ãµes de documentos) no Elasticsearch ele deve monitorar.
    *   O Fluentd, por padrÃ£o (ou configuraÃ§Ã£o comum), cria Ã­ndices diÃ¡rios no Elasticsearch com nomes como `fluentd-YYYY.MM.DD`.
    *   Para que o Kibana encontre todos esses Ã­ndices, usa-se um padrÃ£o de nome (index pattern) com curinga: `fluentd*`. Isso farÃ¡ o Kibana procurar por todos os Ã­ndices que comeÃ§am com "fluentd".

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Opcional: docker volume prune -f
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch19\exercises
    ```
3.  **Iniciar EFK Stack:**
    ```powershell
    docker-compose -f fluentd/docker-compose.yml up -d
    ```
    *Isso pode levar algum tempo, especialmente na primeira vez, para baixar as imagens de Elasticsearch e Kibana, que sÃ£o grandes.*
4.  **Verificar Stack:**
    ```powershell
    docker-compose -f fluentd/docker-compose.yml ps
    # Deve mostrar fluentd, elasticsearch, kibana Up.
    ```
5.  **Rodar App com Driver Fluentd:**
    ```powershell
    docker container run -d --log-driver=fluentd diamol/ch19-timecheck:5.0
    ```
6.  **Configurar Kibana:**
    *   Aguarde 1-2 minutos para Elasticsearch iniciar completamente.
    *   Abra o navegador em `http://localhost:5601`.
    *   Pode levar um tempo para carregar. Se pedir login/setup, procure pela opÃ§Ã£o de ir para o Discover ou Management > Index Patterns.
    *   Clique em "Discover" (Ã­cone de bÃºssola no menu esquerdo).
    *   Na tela "Define index pattern", digite `fluentd*` no campo "Index pattern name".
    *   Clique "Next step".

---

ğŸ“„ PÃ¡gina 397

ğŸ” Resumo dos principais pontos:

*   Figura 19.10: Tela do Kibana pedindo Index Pattern (`fluentd*`).
*   **Config Kibana (cont.):**
    *   PrÃ³xima tela: escolher campo de filtro de tempo. Selecionar `@timestamp`. (Figura 19.11).
    *   Clicar "Create index pattern".
*   Setup inicial Kibana pode ser automatizado (mas bom ver manualmente).
*   Cada log do Fluentd vira documento no Elasticsearch (Ã­ndice `fluentd-{date}`).
*   Kibana Discover:
    *   Mostra histograma de logs ao longo do tempo.
    *   Lista documentos (logs) individuais.
*   Figura 19.12: Tela Discover do Kibana mostrando logs do `timecheck`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina continua a configuraÃ§Ã£o inicial do Kibana:
    *   ApÃ³s definir o padrÃ£o `fluentd*`, o Kibana precisa saber qual campo nos documentos contÃ©m a informaÃ§Ã£o de data/hora para poder filtrar e ordenar por tempo. O Fluentd geralmente adiciona um campo chamado `@timestamp` para isso. Seleciona-se esse campo.
    *   Clica-se em "Create index pattern".
*   Com isso, o Kibana estÃ¡ pronto para visualizar os logs. A tela principal "Discover" mostra:
    *   Um grÃ¡fico de barras no topo (histograma) indicando quantos logs foram recebidos em diferentes intervalos de tempo.
    *   Abaixo, uma lista dos logs individuais mais recentes, com seus campos (incluindo a mensagem original do log, o `@timestamp`, e metadados adicionados pelo Fluentd/Docker como `container_name`, `container_id`).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Continuar Config Kibana:** (ApÃ³s digitar `fluentd*` e clicar Next)
    *   Na lista suspensa "Time Filter field name", selecione `@timestamp`.
    *   Clique em "Create index pattern".
2.  **Explorar Logs:**
    *   VocÃª serÃ¡ levado para a tela "Discover".
    *   Ajuste o intervalo de tempo no canto superior direito (ex: "Last 15 minutes") se necessÃ¡rio.
    *   VocÃª deverÃ¡ ver barras no histograma e, abaixo, os logs do contÃªiner `timecheck` que vocÃª iniciou, com campos como `log`, `container_name`, `@timestamp`.

---

ğŸ“„ PÃ¡gina 398

ğŸ” Resumo dos principais pontos:

*   Figura 19.12: Confirma visualizaÃ§Ã£o dos logs no Kibana Discover.
*   Kibana permite busca full-text e filtros por campos/data. Tem dashboards (como Grafana) para visualizaÃ§Ãµes (contagem logs/app, erros, etc.).
*   Elasticsearch Ã© escalÃ¡vel para produÃ§Ã£o. EFK Ã© mais gerenciÃ¡vel que logs em console.
*   **Logando App Multi-ServiÃ§o (Image Gallery):**
*   **ExercÃ­cio Try-it-now:** Rodar Image Gallery com driver `fluentd` para todos os componentes.
    *   Usar `docker-compose -f image-gallery/docker-compose.yml up -d`. (Arquivo jÃ¡ configurado com driver e tags).
    *   Gerar trÃ¡fego (`http://localhost:8010`).
*   Config Fluentd para Image Gallery adiciona **tag** ao log (identifica componente). `log_name` no Kibana.
*   Figura 19.13: Kibana filtrado para mostrar sÃ³ logs de `access-log`, usando campo `log_name`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina destaca os recursos do Kibana alÃ©m da simples visualizaÃ§Ã£o: busca poderosa (como Google, mas nos seus logs) e capacidade de criar dashboards. Confirma que a pilha EFK Ã© adequada para produÃ§Ã£o.
*   O prÃ³ximo exercÃ­cio aplica o logging centralizado Ã  aplicaÃ§Ã£o Image Gallery (3 serviÃ§os). Um arquivo `docker-compose.yml` (`image-gallery/docker-compose.yml`) jÃ¡ estÃ¡ preparado para iniciar os 3 serviÃ§os configurados para usar o driver `fluentd`.
*   Uma configuraÃ§Ã£o importante nesse arquivo (mostrada na prÃ³xima pÃ¡gina) Ã© o uso da opÃ§Ã£o `tag` no `log-driver`. Isso permite adicionar informaÃ§Ãµes estruturadas (como nome do app, nome do serviÃ§o) Ã  tag que o Docker envia para o Fluentd. O Fluentd pode entÃ£o usar essa tag para filtrar ou enriquecer os logs.
*   ApÃ³s iniciar o Image Gallery e gerar trÃ¡fego, os logs de todos os 3 serviÃ§os fluirÃ£o para o Elasticsearch. A Figura 19.13 mostra como, no Kibana, pode-se entÃ£o filtrar facilmente para ver logs de apenas um componente especÃ­fico (ex: `access-log`) usando o campo derivado da tag (chamado `log_name` no exemplo).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Iniciar Image Gallery (com Logging Fluentd):** (Estando em `ch19/exercises`)
    ```powershell
    # Parar/Remover Timecheck (opcional)
    # docker rm -f <id_ou_nome_timecheck>

    # Iniciar Image Gallery
    docker-compose -f image-gallery/docker-compose.yml up -d
    ```
2.  **Verificar ServiÃ§os:**
    ```powershell
    docker-compose -f image-gallery/docker-compose.yml ps
    # Deve mostrar accesslog, iotd, image-gallery Up
    ```
3.  **Gerar TrÃ¡fego:** Abra `http://localhost:8010` e atualize algumas vezes.
4.  **Explorar no Kibana:**
    *   Volte para `http://localhost:5601` (Discover).
    *   Atualize o intervalo de tempo se necessÃ¡rio. VocÃª deverÃ¡ ver logs dos 3 serviÃ§os intercalados.
    *   Encontre um log do `access-log`. Clique para expandir. Procure por um campo como `log_name` ou `tag` que identifique a origem como `gallery.access-log...`.
    *   Use a barra de filtro no topo para filtrar por esse campo. Ex: `log_name : "gallery.access-log*"` (sintaxe pode variar). Agora sÃ³ devem aparecer logs do `access-log`.

---

ğŸ“„ PÃ¡gina 399

ğŸ” Resumo dos principais pontos:

*   Campo `log_name` no Kibana veio da opÃ§Ã£o `tag` do driver Fluentd.
*   Pode usar valores fixos ou **template strings** (ex: `{{.ImageName}}`, `{{.ContainerName}}`) na tag.
*   Exemplo usa `gallery.<componente>.{{.ImageName}}`. Identifica app, componente, versÃ£o da imagem.
*   **Listagem 19.3:** Trechos do `image-gallery/docker-compose.yml` mostrando seÃ§Ã£o `logging:` para cada serviÃ§o.
    ```yaml
    services:
      accesslog:
        image: diamol/ch18-access-log # Imagem do Cap 18
        logging: # ConfiguraÃ§Ã£o do driver de log
          driver: "fluentd" # Usa driver fluentd
          options: # OpÃ§Ãµes para o driver
            tag: "gallery.access-log.{{.ImageName}}" # Define a tag
      iotd:
        image: diamol/ch18-image-of-the-day
        logging:
          driver: "fluentd"
          options:
            tag: "gallery.iotd.{{.ImageName}}"
      image-gallery:
        image: diamol/ch18-image-gallery
        logging:
          driver: "fluentd"
          options:
            tag: "gallery.image-gallery.{{.ImageName}}"
    # ...
    ```
*   Modelo EFK Ã© recomendado para produÃ§Ã£o. Outros drivers/ferramentas existem (Graylog, Splunk).
*   Configurar driver/opts no `daemon.json` (global) vs. no Compose (por app/ambiente). Compose dÃ¡ mais clareza por ambiente.
*   Fluentd Ã© boa opÃ§Ã£o inicial (fÃ¡cil, escala, flexÃ­vel - pode enriquecer/filtrar).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como as tags foram definidas no Docker Compose para permitir a filtragem vista no Kibana. A seÃ§Ã£o `logging:` dentro da definiÃ§Ã£o de cada serviÃ§o no `docker-compose.yml` permite configurar o driver de log e suas opÃ§Ãµes.
*   **Listagem 19.3** mostra que para cada serviÃ§o:
    *   `driver: "fluentd"` foi definido.
    *   `options: tag: "..."` foi usado para definir um formato de tag.
*   A tag usa uma combinaÃ§Ã£o de texto fixo (`gallery.access-log.`) e **template strings** fornecidas pelo Docker (`{{.ImageName}}`). O Docker substitui esses templates pelos valores reais do contÃªiner no momento em que envia o log para o Fluentd. `{{.ImageName}}` serÃ¡ substituÃ­do pelo nome completo da imagem (ex: `diamol/ch18-access-log:latest`). Outros templates Ãºteis incluem `{{.ContainerName}}`, `{{.ContainerID}}`.
*   Essa tag estruturada Ã© entÃ£o usada pelo Fluentd (com configuraÃ§Ã£o apropriada, vista depois) para extrair informaÃ§Ãµes e/ou rotear os logs.
*   A pÃ¡gina reitera a recomendaÃ§Ã£o da pilha EFK (ou similar) para produÃ§Ã£o e menciona a flexibilidade de configurar logging drivers globalmente (`daemon.json`) ou por aplicaÃ§Ã£o (Compose), preferindo a segunda por clareza ambiental. Destaca a flexibilidade do Fluentd para processamento avanÃ§ado de logs.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Abra o arquivo `ch19\exercises\image-gallery\docker-compose.yml` no VS Code. Localize a seÃ§Ã£o `logging:` em cada serviÃ§o e veja as tags definidas.
*   No Kibana, expanda um log e procure pelo campo `tag` (pode estar aninhado). Veja como ele corresponde ao formato definido no Compose, com `{{.ImageName}}` substituÃ­do.

---

ğŸ“„ PÃ¡gina 400

ğŸ” Resumo dos principais pontos:

*   **19.4 Managing your log output and collection** (Gerenciando sua saÃ­da e coleta de logs)
*   Balancear info Ãºtil vs. volume de dados.
*   Flexibilidade Docker: App pode logar verbosamente (stdout), mas **Fluentd pode filtrar** antes de armazenar.
*   Permite ligar/desligar logs detalhados mudando config do *Fluentd* (nÃ£o do app -> sem restart do app).
*   **ConfiguraÃ§Ã£o Fluentd:** Usa `match` para rotear logs baseado na `tag`.
*   **Listagem 19.4:** Config Fluentd (`elasticsearch-filtered.conf`?) que filtra logs de `access-log`.
    ```aconf
    # Bloco para logs com tag iniciando 'gallery.access-log.'
    <match gallery.access-log.**>
      @type copy
      <store>
        @type stdout # Envia SÃ“ para stdout do Fluentd
      </store>
    </match>

    # Bloco para OUTROS logs com tag iniciando 'gallery.'
    <match gallery.**>
      @type copy
      <store>
        @type elasticsearch # Envia para Elasticsearch
        # ... (config elasticsearch) ...
      </store>
    </match>
    ```
*   ExplicaÃ§Ã£o:
    *   Primeiro `<match>` captura logs de `access-log` (tag `gallery.access-log.**`). `@type copy` + `@type stdout` os direciona para a saÃ­da padrÃ£o do Fluentd.
    *   Segundo `<match>` captura *outros* logs `gallery.**` (iotd, image-gallery). `@type copy` + `@type elasticsearch` os envia para o Elasticsearch.
    *   Ordem importa; primeiro match vence.
*   Config tambÃ©m enriquece logs (split da tag em campos app, service, image).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o aborda o gerenciamento do **volume** de logs. Uma estratÃ©gia Ãºtil Ã© configurar suas aplicaÃ§Ãµes para logar em um nÃ­vel relativamente detalhado (ex: Debug ou Information) por padrÃ£o, mas usar a camada de coleta (Fluentd) para **filtrar** os logs antes de enviÃ¡-los para o armazenamento de longo prazo (Elasticsearch).
*   **Vantagem:** Se vocÃª precisar ver os logs detalhados temporariamente para diagnosticar um problema, vocÃª pode reconfigurar e reiniciar *apenas* o Fluentd para parar de filtrar, sem precisar reiniciar os contÃªineres da sua aplicaÃ§Ã£o (o que poderia fazer o problema desaparecer).
*   A **Listagem 19.4** mostra um trecho de um arquivo de configuraÃ§Ã£o do Fluentd que implementa esse filtro:
    *   Usa diretivas `<match ...>` para direcionar logs com base em padrÃµes na `tag`. O `**` Ã© um curinga.
    *   O primeiro `<match gallery.access-log.**>` intercepta todos os logs cujo tag comeÃ§a com `gallery.access-log.`. Dentro dele, `@type stdout` direciona esses logs para a saÃ­da padrÃ£o do prÃ³prio contÃªiner Fluentd (efetivamente descartando-os do armazenamento principal).
    *   O segundo `<match gallery.**>` captura todos os *outros* logs cuja tag comeÃ§a com `gallery.` (ou seja, `iotd` e `image-gallery`). Dentro dele, `@type elasticsearch` os encaminha para o Elasticsearch.
*   A ordem dos blocos `<match>` Ã© importante; o primeiro que corresponder serÃ¡ usado.
*   A pÃ¡gina tambÃ©m menciona que esta configuraÃ§Ã£o do Fluentd (provavelmente no mesmo arquivo) tambÃ©m adiciona lÃ³gica para "parsear" a tag e criar campos separados como `app_name`, `service_name`, `image_name` no documento final do Elasticsearch, facilitando ainda mais a filtragem e agregaÃ§Ã£o no Kibana.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª usarÃ¡ `docker-compose` com um arquivo override para instruir o serviÃ§o `fluentd` a usar este novo arquivo de configuraÃ§Ã£o com filtro.
*   VocÃª tambÃ©m usarÃ¡ outro override para (talvez) reconfigurar o app Image Gallery para logar mais verbosamente, para ter algo a filtrar.

---

ğŸ“„ PÃ¡gina 401

ğŸ” Resumo dos principais pontos:

*   **ExercÃ­cio Try-it-now:** Aplicar filtro Fluentd e aumentar log do app.
    *   1. Atualizar Fluentd para usar config filtrada: `docker-compose -f fluentd/... -f fluentd/override-gallery-filtered.yml up -d`. (Override aponta para novo .conf).
    *   2. Atualizar App para logar mais: `docker-compose -f image-gallery/... -f image-gallery/override-logging.yml up -d`. (Override provavelmente seta env var para nÃ­vel Debug).
*   Resultado: Logs de `access-log` nÃ£o devem mais aparecer no Kibana, sÃ³ nos logs do container `fluentd`. Logs de `iotd`/`image-gallery` devem aparecer no Kibana, agora enriquecidos com novos campos (app, service, image).
*   Figura 19.14: Kibana mostrando logs enriquecidos (campos extras), sem logs de `access-log`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio aplica a configuraÃ§Ã£o de filtragem do Fluentd e tambÃ©m aumenta o nÃ­vel de log da aplicaÃ§Ã£o Image Gallery (provavelmente definindo uma variÃ¡vel de ambiente via `override-logging.yml` para que ela gere logs mais detalhados).
*   O comando para Fluentd usa um arquivo override (`override-gallery-filtered.yml`) que deve instruir o serviÃ§o `fluentd` a montar ou usar um arquivo de configuraÃ§Ã£o diferente (aquele com os `<match>` da Listagem 19.4). O contÃªiner Fluentd Ã© recriado com a nova configuraÃ§Ã£o.
*   O comando para o Image Gallery usa outro override (`override-logging.yml`) para recriar os contÃªineres da aplicaÃ§Ã£o com uma configuraÃ§Ã£o de logging mais verbosa.
*   O resultado esperado Ã©:
    *   No Kibana: Os logs detalhados de `iotd` e `image-gallery` aparecerÃ£o, agora com campos extras como `app_name`, `service_name`, `image_name` (extraÃ­dos da tag pelo Fluentd). Os logs de `access-log` **desaparecerÃ£o** do Kibana.
    *   Nos logs do contÃªiner `fluentd` (`docker logs fluentd_...`): Os logs de `access-log` (que foram filtrados) **aparecerÃ£o** aqui, pois foram direcionados para `@type stdout`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar (se necessÃ¡rio):** `cd C:\DockerLabs\diamol\ch19\exercises`
2.  **Aplicar Overrides:**
    ```powershell
    # Atualizar Fluentd com filtro
    docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery-filtered.yml up -d
    # Atualizar App com logging verbose
    docker-compose -f image-gallery/docker-compose.yml -f image-gallery/override-logging.yml up -d
    ```
3.  **Verificar Kibana:**
    *   Gere trÃ¡fego (`http://localhost:8010`).
    *   Acesse Kibana (`http://localhost:5601`). Atualize a busca.
    *   VocÃª deve ver logs de `iotd` e `image-gallery` (talvez mais detalhados). Expanda um log e procure pelos novos campos (`app_name`, etc.).
    *   Confirme que **nÃ£o** hÃ¡ logs recentes de `access-log` (filtrando por `log_name: "gallery.access-log*"` nÃ£o deve retornar nada novo).

---

ğŸ“„ PÃ¡gina 402

ğŸ” Resumo dos principais pontos:

*   Logs de `access-log` ainda disponÃ­veis no stdout do container Fluentd.
*   **ExercÃ­cio Try-it-now:** Verificar logs `access-log` no stdout do Fluentd.
    *   Comando: `docker container logs --tail 1 fluentd_fluentd_1`.
*   Figura 19.15: Mostra log de `access-log` no output do `logs fluentd...`, confirmando que foi capturado mas redirecionado. Log ainda estÃ¡ enriquecido.
*   **Simulando Debug em Prod:** Remover filtro para ver logs `access-log` no Kibana novamente.
    *   Problema surgiu -> quer ver logs detalhados -> reconfigura *Fluentd* (nÃ£o o app).
*   **ExercÃ­cio Try-it-now:** Reverter filtro Fluentd.
    *   Comando: `docker-compose -f fluentd/... -f fluentd/override-gallery.yml up -d`. (Usa override que aponta para config *sem* filtro para `access-log`).
*   Fluentd reinicia com nova config.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina primeiro confirma (com exercÃ­cio e Figura 19.15) que os logs de `access-log`, embora filtrados do Elasticsearch, ainda estÃ£o sendo recebidos pelo Fluentd e direcionados para seu stdout, provando que o app ainda estÃ¡ logando e o Fluentd ainda estÃ¡ coletando.
*   Em seguida, demonstra o benefÃ­cio da filtragem na camada de coleta: simula-se um cenÃ¡rio onde se precisa reativar a visualizaÃ§Ã£o dos logs detalhados de `access-log` no Kibana (talvez para investigar um problema).
*   Em vez de reconfigurar e reiniciar os contÃªineres do `access-log` (o que poderia alterar o estado e dificultar a depuraÃ§Ã£o), reinicia-se *apenas* o contÃªiner do Fluentd, fornecendo um arquivo de configuraÃ§Ã£o diferente (`override-gallery.yml`) que agora instrui o Fluentd a enviar *todos* os logs `gallery.**` (incluindo `access-log`) para o Elasticsearch.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Logs Fluentd:**
    ```powershell
    docker logs --tail 1 fluentd_fluentd_1 # (Nome pode variar ligeiramente)
    # Deve mostrar um log recente do access-log
    ```
2.  **Reverter Filtro Fluentd:**
    ```powershell
    docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery.yml up -d
    # Isso recria o container fluentd com a config original (sem filtro especÃ­fico para access-log)
    ```

---

ğŸ“„ PÃ¡gina 403

ğŸ” Resumo dos principais pontos:

*   Deploy do Fluentd (sem filtro) -> logs `access-log` voltam a ir para Elasticsearch.
*   Figura 19.16: Kibana agora mostrando logs recentes de `iotd` e `access-log`.
*   **Alerta:** Risco de perda de logs durante restart do Fluentd.
    *   Se containers app enviam logs enquanto Fluentd estÃ¡ reiniciando, logs podem ser perdidos (driver `fluentd` nÃ£o faz buffer por padrÃ£o).
    *   AceitÃ¡vel em cluster prod (Fluentd pode rodar com HA), mas preferÃ­vel a reiniciar app (que pode perder estado do problema).
*   **19.5 Understanding the container logging model** (Entendendo o modelo de logging de contÃªiner)
    *   FlexÃ­vel, *se* app logar para stdout/stderr (direto ou via relay).
    *   EFK stack Ã© exemplo; muitos drivers/backends possÃ­veis (Graylog, Splunk, CloudWatch...).
    *   EFK roda bem em dev e escala para prod.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma que, apÃ³s reiniciar o Fluentd com a configuraÃ§Ã£o sem filtro, os logs de `access-log` voltam a aparecer no Kibana (Figura 19.16), tudo sem tocar nos contÃªineres da aplicaÃ§Ã£o.
*   Faz um alerta importante: o driver `fluentd` do Docker, por padrÃ£o, opera de forma "nÃ£o bloqueante". Se ele nÃ£o conseguir enviar um log para o Fluentd (ex: porque o Fluentd estÃ¡ temporariamente indisponÃ­vel durante um reinÃ­cio), ele pode **descartar** esse log para nÃ£o bloquear a aplicaÃ§Ã£o. Isso significa que pode haver perda de logs durante a reconfiguraÃ§Ã£o do Fluentd. Em ambientes de produÃ§Ã£o com mÃºltiplos nÃ³s e Fluentd rodando com alta disponibilidade, esse risco Ã© menor. Mesmo com o risco, geralmente Ã© preferÃ­vel perder alguns logs transitÃ³rios do que reiniciar a aplicaÃ§Ã£o que vocÃª estÃ¡ tentando depurar.
*   A seÃ§Ã£o 19.5 resume o modelo de logging em contÃªineres:
    1.  FaÃ§a sua aplicaÃ§Ã£o logar para stdout/stderr (diretamente ou via relay).
    2.  Configure o Docker Engine (globalmente ou por contÃªiner) para usar um **logging driver** que envie esses logs para seu sistema de logging centralizado preferido.
    3.  Use as ferramentas desse sistema (ex: Kibana para EFK) para buscar, visualizar e alertar sobre os logs.
*   A pilha EFK Ã© destacada como uma opÃ§Ã£o popular e escalÃ¡vel.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Verificar Kibana Novamente:** Gere trÃ¡fego (`http://localhost:8010`). Acesse Kibana (`http://localhost:5601`). Confirme que logs de `access-log` estÃ£o aparecendo novamente.
2.  **Explorar Outros Drivers (Conceitual):** Pesquise por "docker logging drivers" na documentaÃ§Ã£o oficial para ver as outras opÃ§Ãµes suportadas nativamente (syslog, journald, gelf, splunk, awslogs, gcplogs, etc.) e como configurÃ¡-las.

---

ğŸ“„ PÃ¡gina 404

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 19.17): Arquitetura EFK em produÃ§Ã£o.
    *   Fluentd rodando como `daemonset` (ou `global service` Swarm) - 1 instÃ¢ncia *por nÃ³*.
    *   Cada Fluentd coleta logs dos containers *no seu nÃ³*.
    *   Envia logs para cluster Elasticsearch (tambÃ©m em containers, escalado).
    *   Kibana conecta ao cluster ES.
*   **Nota de Cautela (Autor):** Alguns preferem que app logue *diretamente* no backend (ex: app envia direto para Elasticsearch API).
    *   Autor **nÃ£o gosta** dessa abordagem.
    *   PrÃ³s (aparentes): Menos camadas/processamento.
    *   Contras (reais): Menos flexibilidade (acopla app ao backend de log), mais complexo para app (precisa lidar com falhas de envio), difÃ­cil mudar backend depois.
*   **RecomendaÃ§Ã£o Autor:** Manter simples/flexÃ­vel -> App loga para stdout, plataforma (Docker + driver + Fluentd/etc) cuida do resto.
*   **19.6 Lab** (LaboratÃ³rio)

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 19.17 ilustra como a pilha EFK normalmente Ã© implantada em um **ambiente de produÃ§Ã£o clusterizado (Swarm/K8s)** para escalabilidade e resiliÃªncia:
    *   O **Fluentd** nÃ£o roda como um Ãºnico contÃªiner, mas sim como um **DaemonSet (K8s)** ou **Global Service (Swarm)**. Isso garante que *cada nÃ³* do cluster tenha sua prÃ³pria instÃ¢ncia do Fluentd rodando.
    *   Cada instÃ¢ncia do Fluentd Ã© configurada para coletar logs *apenas* dos contÃªineres que estÃ£o rodando *no mesmo nÃ³* que ela (geralmente acessando os arquivos JSON do Docker naquele nÃ³).
    *   Todas as instÃ¢ncias do Fluentd enviam os logs coletados para um **cluster Elasticsearch centralizado** (que tambÃ©m roda em contÃªineres, possivelmente com mÃºltiplas instÃ¢ncias para escalabilidade e redundÃ¢ncia de dados).
    *   O Kibana se conecta a esse cluster Elasticsearch.
*   A pÃ¡gina conclui com uma **opiniÃ£o forte do autor**: ele desaconselha fazer com que a aplicaÃ§Ã£o envie logs *diretamente* para o sistema de armazenamento final (como chamar a API do Elasticsearch de dentro do cÃ³digo da aplicaÃ§Ã£o). Embora possa parecer que elimina uma camada (Fluentd), na prÃ¡tica isso acopla fortemente a aplicaÃ§Ã£o ao sistema de logging especÃ­fico, dificulta mudanÃ§as futuras e adiciona complexidade/responsabilidade Ã  aplicaÃ§Ã£o (tratar falhas de envio, buffering, etc.). A recomendaÃ§Ã£o Ã© manter a aplicaÃ§Ã£o simples (logar para stdout) e deixar a plataforma cuidar do transporte e processamento dos logs.
*   Prepara para o Lab do CapÃ­tulo 19.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Implantar Fluentd como DaemonSet/Global Service estÃ¡ fora do escopo do Compose single-host, mas o princÃ­pio Ã© importante para produÃ§Ã£o.
*   O Lab focarÃ¡ na configuraÃ§Ã£o do Fluentd em si.

---

ğŸ“„ PÃ¡gina 405

ğŸ” Resumo dos principais pontos:

*   **Lab 19:** Foco em configurar Fluentd.
*   Ponto de Partida: Compose file para app Random Number (`ch19/lab/numbers.yml`?) e Compose file para EFK stack (`ch19/lab/efk.yml`?).
*   Problemas Iniciais: App nÃ£o usa driver `fluentd`; config Fluentd nÃ£o faz enriquecimento.
*   **Tarefa:**
    1.  Modificar Compose do app (`numbers.yml`?) para que *todos* os componentes (`numbers-api`, `numbers-web`) usem driver `fluentd` e definam `tag` estruturada (app, service, image).
    2.  Modificar config Fluentd (`elasticsearch.conf`) para:
        *   Parsear a `tag` vinda do app `numbers` e criar campos `app_name`, `service_name`, `image_name`.
        *   Adicionar bloco `<match>` "catch-all" no final para encaminhar logs de *outras* fontes (nÃ£o `numbers`) para `stdout` (como fallback/debug).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O **Lab do CapÃ­tulo 19** foca na configuraÃ§Ã£o prÃ¡tica do **Fluentd** e na integraÃ§Ã£o dos logs da aplicaÃ§Ã£o com ele. VocÃª receberÃ¡ arquivos Compose bÃ¡sicos e precisarÃ¡ modificÃ¡-los.
*   **Passos:**
    1.  **Configurar AplicaÃ§Ã£o:** Edite o `docker-compose.yml` da aplicaÃ§Ã£o Random Number (`numbers-api`, `numbers-web`). Para cada serviÃ§o, adicione a seÃ§Ã£o `logging:` especificando `driver: fluentd` e a opÃ§Ã£o `tag:` com um formato estruturado, como `"numbers.{{.ServiceName}}.{{.ImageName}}"` (ou similar).
    2.  **Configurar Fluentd (Enriquecimento):** Edite o arquivo de configuraÃ§Ã£o do Fluentd (`elasticsearch.conf`, que provavelmente serÃ¡ montado no contÃªiner `fluentd` definido no Compose da EFK). VocÃª precisarÃ¡ adicionar um bloco `<filter ...>` ou modificar o `<match>` existente que envia para o Elasticsearch. Dentro desse bloco, use plugins do Fluentd (como `record_transformer` ou parsers especÃ­ficos) para extrair as partes da `tag` (separada por '.') e criar os novos campos (`app_name`, `service_name`, `image_name`) no registro de log antes de enviÃ¡-lo ao Elasticsearch.
    3.  **Configurar Fluentd (Fallback):** Adicione um bloco `<match **>` *no final* do arquivo de configuraÃ§Ã£o do Fluentd. Este match captura *todos* os logs que nÃ£o foram capturados pelos matches anteriores (como os de `gallery.**` ou `numbers.**`). Dentro deste bloco, use `@type stdout` para que logs inesperados ou de outras fontes sejam impressos nos logs do prÃ³prio Fluentd, facilitando a depuraÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 19:**
    1.  Navegue para `ch19\lab`. Examine os arquivos `.yml` e `.conf` fornecidos.
    2.  **Edite `numbers.yml`:** Adicione a seÃ§Ã£o `logging:` a ambos os serviÃ§os (`numbers-api`, `numbers-web`) com `driver: fluentd` e `options: tag: "numbers.{{.ServiceName}}.{{.ImageName}}"`.
    3.  **Edite `elasticsearch.conf`:**
        *   Localize o `<match ...>` que envia para `@type elasticsearch`.
        *   *Antes* dele (ou dentro dele, dependendo da estrutura exata), adicione um bloco `<filter numbers.**>` (para aplicar sÃ³ aos logs do app numbers).
        *   Dentro do `<filter>`, use `<record_transformer>` para adicionar os novos campos baseados na tag. A sintaxe exata para extrair da tag pode exigir consulta Ã  documentaÃ§Ã£o do Fluentd ou da soluÃ§Ã£o do lab. Exemplo conceitual:
          ```aconf
          <filter numbers.**>
            @type record_transformer
            enable_ruby true
            <record>
              app_name ${tag_parts[0]}
              service_name ${tag_parts[1]}
              image_name ${tag_parts[2]}
            </record>
          </filter>
          ```
        *   *Depois* de todos os matches especÃ­ficos (como o para Elasticsearch), adicione o fallback:
          ```aconf
          <match **>
            @type stdout
          </match>
          ```
    4.  **Rodar:** Use `docker-compose -f efk.yml -f numbers.yml up -d`.
    5.  **Verificar:** Gere logs do app Numbers. Verifique no Kibana se os logs aparecem e se os novos campos (`app_name`, etc.) estÃ£o presentes. Tente rodar outro app (ex: `timecheck`) com driver `fluentd` mas tag *diferente*; seus logs devem aparecer nos logs do `fluentd` container (devido ao match `**`), nÃ£o no Kibana.

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md

---

ğŸ“„ PÃ¡gina 406

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 19 no GitHub.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Fornece o link para a soluÃ§Ã£o completa do Lab 19 no repositÃ³rio do livro.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 19, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md

---

ğŸ“„ PÃ¡gina 407

ğŸ” Resumo dos principais pontos:

*   **CapÃ­tulo 20: Controlling HTTP traffic to containers with a reverse proxy** (Controlando trÃ¡fego HTTP para contÃªineres com um proxy reverso)
*   Docker roteia trÃ¡fego externo (via Ingress/Publish), mas sÃ³ 1 container por porta host.
*   Problema: Em produÃ§Ã£o, quer-se mÃºltiplos apps (sites, APIs) acessÃ­veis nas portas padrÃ£o HTTP (80) e HTTPS (443).
*   SoluÃ§Ã£o: **Reverse Proxy**.
*   Reverse Proxy: Componente de rede que atua como **gateway** para mÃºltiplas aplicaÃ§Ãµes web/HTTP.
*   Foco Cap. 20: Features e padrÃµes habilitados por reverse proxy. Exemplos com Nginx e Traefik (populares, rodam em containers).
*   **20.1 What is a reverse proxy?** (O que Ã© um proxy reverso?)
    *   Proxy "normal" (forward proxy): Intercepta trÃ¡fego *saindo* da sua rede (cliente -> proxy -> internet). Usado para controle, cache, seguranÃ§a na ponta do cliente.
    *   **Reverse Proxy:** Intercepta trÃ¡fego *entrando* para seus servidores (internet -> reverse proxy -> app server). Fica na frente dos servidores de aplicaÃ§Ã£o.
*   Diagrama (Figura 20.1): Ilustra Reverse Proxy em container.
    *   TrÃ¡fego externo chega SÃ“ no container do Reverse Proxy (ex: na porta 80/443).
    *   Reverse Proxy decide para qual container de aplicaÃ§Ã£o (web1, web2, api1) encaminhar a requisiÃ§Ã£o (baseado em hostname, path, etc.).
    *   Containers de aplicaÃ§Ã£o ficam "internos", sem portas publicadas diretamente no host.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 20, focado no **Reverse Proxy**, um componente essencial em arquiteturas de contÃªineres (e web em geral).
*   O problema que ele resolve Ã©: como expor **mÃºltiplas aplicaÃ§Ãµes web ou APIs** que rodam em contÃªineres diferentes, mas que precisam ser acessadas atravÃ©s das portas padrÃ£o **80 (HTTP)** e **443 (HTTPS)**, sendo que o Docker sÃ³ permite que um contÃªiner escute em uma porta especÃ­fica do host por vez?
*   A soluÃ§Ã£o Ã© o **Reverse Proxy**:
    *   Ã‰ um servidor especializado (como Nginx, Traefik, HAProxy, Caddy) que roda em um contÃªiner.
    *   *Apenas* o contÃªiner do reverse proxy tem as portas 80 e 443 publicadas no host (`-p 80:80 -p 443:443`).
    *   Todo o trÃ¡fego HTTP/S externo chega primeiro a ele.
    *   O reverse proxy examina a requisiÃ§Ã£o (geralmente o nome do host/domÃ­nio ou o caminho URL) e, com base em regras de configuraÃ§Ã£o, decide para qual contÃªiner de aplicaÃ§Ã£o *interno* ele deve encaminhar ("proxy") essa requisiÃ§Ã£o.
    *   Os contÃªineres das aplicaÃ§Ãµes rodam *sem* publicar portas no host, comunicando-se com o proxy atravÃ©s da rede interna do Docker.
*   A Figura 20.1 ilustra essa arquitetura de gateway.
*   O capÃ­tulo usarÃ¡ **Nginx** (um clÃ¡ssico poderoso) e **Traefik** (um proxy moderno "cloud-native") como exemplos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª rodarÃ¡ Nginx e Traefik em contÃªineres.
*   VocÃª configurarÃ¡ regras de roteamento (via arquivos de config para Nginx, ou via labels Docker para Traefik).
*   VocÃª rodarÃ¡ contÃªineres de aplicaÃ§Ã£o *sem* a flag `-p`, e os acessarÃ¡ *atravÃ©s* do proxy na porta 80/443 usando nomes de host (que vocÃª configurarÃ¡ no seu arquivo `hosts` local para testes).

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Reverse Proxy: Ã‰ como o recepcionista/telefonista de um grande prÃ©dio comercial. Todos os visitantes/ligaÃ§Ãµes (trÃ¡fego externo) chegam primeiro na recepÃ§Ã£o (proxy na porta 80/443). O recepcionista pergunta para qual empresa/departamento (hostname/path) o visitante/ligaÃ§Ã£o se destina e o direciona para o andar/ramal correto (contÃªiner da aplicaÃ§Ã£o). Os escritÃ³rios das empresas (contÃªineres de app) nÃ£o tÃªm acesso direto da rua (sem portas publicadas no host).

---

ğŸ“„ PÃ¡gina 408

ğŸ” Resumo dos principais pontos:

*   Figura 20.1: Diagrama do Reverse Proxy.
*   BenefÃ­cios do Reverse Proxy:
    *   Apps ficam internos -> mais fÃ¡cil escalar, atualizar, segurar.
*   Reverse proxies nÃ£o sÃ£o novos, mas "shift left" com containers (rodam em dev/teste tambÃ©m, nÃ£o sÃ³ prod).
*   Exemplo com **Nginx**:
    *   Popular (>30% internet), leve, rÃ¡pido, poderoso servidor HTTP (serve estÃ¡ticos + proxy).
*   **ExercÃ­cio Try-it-now:** Rodar Nginx e ver pÃ¡gina default.
    *   1. Criar rede `ch20` (Linux/Windows).
    *   2. Navegar para `ch20/exercises`.
    *   3. Rodar Nginx com Compose (usa override linux/windows para montar pasta `sites-enabled` local): `docker-compose -f nginx/... -f nginx/override-... up -d`.
    *   4. Acessar `http://localhost`.
*   Nginx usa arquivos de config por site. Pasta `sites-enabled` montada estÃ¡ vazia -> Nginx serve pÃ¡gina default.
*   Figura 20.2: Mostra pÃ¡gina "Welcome to nginx!".

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a os benefÃ­cios de colocar as aplicaÃ§Ãµes atrÃ¡s de um reverse proxy (melhor gerenciamento, seguranÃ§a). Menciona que, com contÃªineres, Ã© fÃ¡cil usar o mesmo proxy em todos os ambientes (dev, teste, prod), o que nÃ£o acontecia antigamente ("shift left").
*   ComeÃ§a o exemplo prÃ¡tico usando **Nginx**.
*   O exercÃ­cio instrui a:
    1.  Criar uma rede Docker (`ch20`) para os contÃªineres deste capÃ­tulo.
    2.  Usar `docker-compose` para iniciar um contÃªiner Nginx. O Compose file (`nginx/docker-compose.yml` com override) faz duas coisas importantes:
        *   Publica a porta 80 do contÃªiner Nginx na porta 80 do host (`-p 80:80`).
        *   Usa um **bind mount** para mapear uma pasta local chamada `nginx/sites-enabled` para o diretÃ³rio `/etc/nginx/conf.d` (ou similar) dentro do contÃªiner Nginx. Ã‰ neste diretÃ³rio que o Nginx procura por arquivos de configuraÃ§Ã£o (`.conf`) que definem os sites/proxies que ele deve servir.
    3.  Acessar `http://localhost`. Como a pasta `sites-enabled` local estÃ¡ vazia no inÃ­cio, o Nginx nÃ£o encontra nenhuma configuraÃ§Ã£o customizada e serve sua pÃ¡gina de boas-vindas padrÃ£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar Rede:**
    ```powershell
    # Se usando Linux mode no Desktop
    docker network create ch20
    # Se usando Windows Containers mode
    # docker network create --driver=nat ch20
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch20\exercises
    ```
3.  **Rodar Nginx:** (Usando override apropriado)
    ```powershell
    # Se Linux mode
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml up -d
    # Se Windows Containers mode (verifique se override-windows.yml existe)
    # docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-windows.yml up -d
    ```
4.  **Verificar Nginx:**
    ```powershell
    docker ps # Ver container nginx rodando
    ```
5.  **Acessar PÃ¡gina Default:** Abra navegador em `http://localhost`. VocÃª deve ver a pÃ¡gina "Welcome to nginx!".

*   **Estrutura Nginx Config:** Nginx geralmente tem pastas como `sites-available` (onde vocÃª cria todas as configs) e `sites-enabled` (onde vocÃª cria links simbÃ³licos para as configs de `sites-available` que quer ativar). O bind mount direto para `sites-enabled` simplifica o exemplo.

---

ğŸ“„ PÃ¡gina 409

ğŸ” Resumo dos principais pontos:

*   Figura 20.2: PÃ¡gina default Nginx.
*   Configurar Nginx como proxy: Adicionar arquivo `.conf` na pasta `sites-enabled`.
*   **Roteamento por Hostname:** Para hospedar mÃºltiplos apps na mesma porta (80), Nginx usa header `Host:` da requisiÃ§Ã£o HTTP para escolher qual config/app servir.
*   Teste Local: Adicionar domÃ­nios ao arquivo `hosts` local para simular diferentes hostnames apontando para `127.0.0.1`.
*   **ExercÃ­cio Try-it-now:** Configurar proxy para app `whoami`.
    *   1. Adicionar `whoami.local` ao `hosts` (apontando para `127.0.0.1`). Comandos Linux/Win fornecidos.
    *   2. Iniciar container `whoami` (Compose file em `whoami/`) *sem* publicar portas, mas na rede `ch20`.
        *   `docker-compose -f whoami/docker-compose.yml up -d`.
    *   3. Copiar config Nginx `whoami.local.conf` da pasta `sites-available` para `sites-enabled` (que estÃ¡ montada no container Nginx).
        *   `cp ./nginx/sites-available/whoami.local ./nginx/sites-enabled/`
    *   4. Reiniciar Nginx para carregar nova config: `docker-compose -f nginx/... restart nginx`.
    *   5. Acessar `http://whoami.local` no navegador.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como configurar o Nginx para atuar como um reverse proxy para uma aplicaÃ§Ã£o especÃ­fica. Isso Ã© feito criando um arquivo de configuraÃ§Ã£o (`.conf`) para essa aplicaÃ§Ã£o e colocando-o no diretÃ³rio que o Nginx monitora (`sites-enabled`, que mapeamos para uma pasta local).
*   Como queremos que o Nginx sirva mÃºltiplas aplicaÃ§Ãµes na mesma porta 80, precisamos de uma forma de diferenciar as requisiÃ§Ãµes. O mÃ©todo padrÃ£o Ã© usar o **nome do host** (domÃ­nio) que o cliente (navegador) solicita. O navegador envia esse nome no cabeÃ§alho HTTP `Host:`. O Nginx lÃª esse cabeÃ§alho e o usa para encontrar o arquivo de configuraÃ§Ã£o correspondente (que terÃ¡ uma diretiva `server_name` com aquele host).
*   Para **testar isso localmente**, usamos o truque do arquivo `hosts` novamente (como na p.93), adicionando entradas como `127.0.0.1 whoami.local`. Isso faz com que, ao digitar `http://whoami.local` no navegador, a requisiÃ§Ã£o seja enviada para `127.0.0.1` (sua mÃ¡quina local), mas com o header `Host: whoami.local`.
*   O exercÃ­cio entÃ£o executa os passos para configurar o Nginx para servir o app `whoami` (um app simples que mostra info do container):
    1.  Modifica o `hosts` local.
    2.  Inicia o container `whoami` **sem publicar portas**, mas conectando-o Ã  rede `ch20` (para que o Nginx possa encontrÃ¡-lo pelo nome `whoami` via DNS Docker).
    3.  Copia o arquivo `whoami.local.conf` (que contÃ©m as regras de proxy para `whoami.local`) para a pasta `sites-enabled` local. Como essa pasta estÃ¡ montada no Nginx via bind mount, o Nginx agora "vÃª" o novo arquivo de configuraÃ§Ã£o.
    4.  Reinicia o serviÃ§o `nginx` (usando `docker-compose restart`) para que ele releia a configuraÃ§Ã£o e ative o novo site.
    5.  Acessa `http://whoami.local`. O Nginx deve receber, rotear para o container `whoami`, obter a resposta e enviÃ¡-la de volta.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 whoami.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Iniciar App `whoami`:** (Estando em `ch20/exercises`)
    ```powershell
    # Verifique o whoami/docker-compose.yml. Ele deve definir o serviÃ§o 'whoami' e conectÃ¡-lo Ã  rede 'ch20' (que deve ser definida como externa ou criada neste file).
    docker-compose -f .\whoami\docker-compose.yml up -d
    ```
3.  **Copiar Config Nginx:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\whoami.local -Destination .\nginx\sites-enabled\
    ```
4.  **Reiniciar Nginx:** (Use o comando que iniciou o Nginx)
    ```powershell
    # Se Linux mode
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx
    # Se Windows Containers mode
    # docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-windows.yml restart nginx
    ```
5.  **Acessar no Navegador:** Abra `http://whoami.local`. VocÃª deve ver a saÃ­da do aplicativo whoami (hostname do container, IPs, etc.).

---

ğŸ“„ PÃ¡gina 410

ğŸ” Resumo dos principais pontos:

*   Figura 20.3: Mostra a saÃ­da do app `whoami` sendo servida atravÃ©s do proxy em `whoami.local`.
*   Config Nginx para proxy Ã© simples. **Listagem 20.1:** Config `whoami.local.conf`.
    ```nginx
    server {
        server_name whoami.local; # Responde a este hostname
        location / { # Para qualquer path sob o domÃ­nio
            # Encaminha para o serviÃ§o 'whoami' na rede Docker (porta 80 http)
            proxy_pass http://whoami;
            # Passa o Host header original do cliente para o app backend
            proxy_set_header Host $host;
            # Adiciona header extra na resposta para debug (mostra qual Nginx serviu)
            add_header X-Host $hostname;
        }
    }
    ```
*   **ExplicaÃ§Ã£o Config:**
    *   `server { ... }`: Define um servidor virtual.
    *   `server_name whoami.local;`: Faz este bloco responder a requisiÃ§Ãµes com `Host: whoami.local`.
    *   `location / { ... }`: Aplica regras a todas as requisiÃ§Ãµes (`/`).
    *   `proxy_pass http://whoami;`: A diretiva principal. Diz ao Nginx para encaminhar a requisiÃ§Ã£o para o endereÃ§o `http://whoami`. O Nginx usa o DNS Docker para resolver `whoami` para o IP do container `whoami` na rede `ch20` e faz uma requisiÃ§Ã£o HTTP para ele.
    *   `proxy_set_header Host $host;`: Reenvia o cabeÃ§alho `Host` original do cliente para a aplicaÃ§Ã£o backend (`whoami`). Importante para apps que dependem desse header. `$host` Ã© variÃ¡vel Nginx.
    *   `add_header X-Host $hostname;`: Adiciona um cabeÃ§alho customizado (`X-Host`) Ã  *resposta* que volta para o cliente, contendo o hostname do *contÃªiner Nginx* que processou a requisiÃ§Ã£o. Ãštil para debug.
*   Config simples atua como "pass-through": sÃ³ encaminha request/response. Se app backend falhar, Nginx repassa falha.
*   **ExercÃ­cio Try-it-now:** Proxy para API `numbers` (que falha).
    *   1. Adicionar `api.numbers.local` ao `hosts`.
    *   2. Rodar API `numbers` (Compose file em `numbers/`).
    *   3. Copiar config Nginx `api.numbers.local.conf`.
    *   4. Reiniciar Nginx.
    *   5. Acessar `http://api.numbers.local/rng`. Refrescar atÃ© quebrar (ver erro 500 do Nginx, vindo da API).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a configuraÃ§Ã£o bÃ¡sica do Nginx para atuar como reverse proxy (Listagem 20.1). A diretiva chave Ã© `proxy_pass`, que informa ao Nginx para onde encaminhar a requisiÃ§Ã£o recebida. Ele usa o nome do serviÃ§o Docker (`whoami`) como hostname, confiando no DNS interno do Docker para resoluÃ§Ã£o. `proxy_set_header Host` tambÃ©m Ã© importante para que a aplicaÃ§Ã£o backend receba o nome de host que o usuÃ¡rio originalmente digitou.
*   Explica que essa configuraÃ§Ã£o bÃ¡sica apenas repassa a comunicaÃ§Ã£o. Se a aplicaÃ§Ã£o por trÃ¡s do proxy (`whoami` ou `numbers-api`) retornar um erro (como o erro 500 da API `numbers` apÃ³s algumas chamadas), o Nginx simplesmente enviarÃ¡ essa resposta de erro de volta ao cliente.
*   O exercÃ­cio aplica a mesma tÃ©cnica de proxy para a API `numbers-api`, tornando-a acessÃ­vel via `http://api.numbers.local/rng`. Ao testar, espera-se ver o Nginx retornando o erro 500 apÃ³s algumas atualizaÃ§Ãµes, demonstrando o comportamento pass-through.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 api.numbers.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Rodar API `numbers`:** (Estando em `ch20/exercises`)
    ```powershell


**You**

ğŸ§  Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), pÃ¡gina por pÃ¡gina, a apartir da pÃ¡gina 410.

Para cada pÃ¡gina, siga este modelo de resposta:

ğŸ“„ PÃ¡gina X

ğŸ” Resumo dos principais pontos:

\[bullet points com os tÃ³picos abordados na pÃ¡gina\]

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

\[explique os conceitos em portuguÃªs claro, com boa didÃ¡tica, como um professor\]

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

\[comandos ou trechos de cÃ³digo aplicÃ¡veis no Windows, usando Docker Desktop e Visual Studio Code\]

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

\[comparaÃ§Ãµes simples ou observaÃ§Ãµes para reforÃ§ar o entendimento\]

âœ… Importante: percorra automaticamente todas as pÃ¡ginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "prÃ³xima".

Quando terminar todas as pÃ¡ginas, diga:

"âœ… Fim da leitura. Todas as pÃ¡ginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Okay, Alexandre, continuando a anÃ¡lise a partir da pÃ¡gina 410.

---

ğŸ“„ PÃ¡gina 410

ğŸ” Resumo dos principais pontos:

*   Figura 20.3: Mostra a saÃ­da do app `whoami` sendo servida atravÃ©s do proxy em `whoami.local`.
*   Config Nginx para proxy Ã© simples. **Listagem 20.1:** Config `whoami.local.conf`.
    ```nginx
    server {
        server_name whoami.local; # Responde a este hostname
        location / { # Para qualquer path sob o domÃ­nio
            # Encaminha para o serviÃ§o 'whoami' na rede Docker (porta 80 http)
            proxy_pass http://whoami;
            # Passa o Host header original do cliente para o app backend
            proxy_set_header Host $host;
            # Adiciona header extra na resposta para debug (mostra qual Nginx serviu)
            add_header X-Host $hostname;
        }
    }
    ```
*   **ExplicaÃ§Ã£o Config:**
    *   `server { ... }`: Define um servidor virtual.
    *   `server_name whoami.local;`: Faz este bloco responder a requisiÃ§Ãµes com `Host: whoami.local`.
    *   `location / { ... }`: Aplica regras a todas as requisiÃ§Ãµes (`/`).
    *   `proxy_pass http://whoami;`: A diretiva principal. Diz ao Nginx para encaminhar a requisiÃ§Ã£o para o endereÃ§o `http://whoami`. O Nginx usa o DNS Docker para resolver `whoami` para o IP do container `whoami` na rede `ch20` e faz uma requisiÃ§Ã£o HTTP para ele.
    *   `proxy_set_header Host $host;`: Reenvia o cabeÃ§alho `Host` original do cliente para a aplicaÃ§Ã£o backend (`whoami`). Importante para apps que dependem desse header. `$host` Ã© variÃ¡vel Nginx.
    *   `add_header X-Host $hostname;`: Adiciona um cabeÃ§alho customizado (`X-Host`) Ã  *resposta* que volta para o cliente, contendo o hostname do *contÃªiner Nginx* que processou a requisiÃ§Ã£o. Ãštil para debug.
*   Config simples atua como "pass-through": sÃ³ encaminha request/response. Se app backend falhar, Nginx repassa falha.
*   **ExercÃ­cio Try-it-now:** Proxy para API `numbers` (que falha).
    *   1. Adicionar `api.numbers.local` ao `hosts`.
    *   2. Rodar API `numbers` (Compose file em `numbers/`).
    *   3. Copiar config Nginx `api.numbers.local.conf`.
    *   4. Reiniciar Nginx.
    *   5. Acessar `http://api.numbers.local/rng`. Refrescar atÃ© quebrar (ver erro 500 do Nginx, vindo da API).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa a configuraÃ§Ã£o bÃ¡sica do Nginx para atuar como reverse proxy (Listagem 20.1). A diretiva chave Ã© `proxy_pass`, que informa ao Nginx para onde encaminhar a requisiÃ§Ã£o recebida. Ele usa o nome do serviÃ§o Docker (`whoami`) como hostname, confiando no DNS interno do Docker para resoluÃ§Ã£o. `proxy_set_header Host` tambÃ©m Ã© importante para que a aplicaÃ§Ã£o backend receba o nome de host que o usuÃ¡rio originalmente digitou.
*   Explica que essa configuraÃ§Ã£o bÃ¡sica apenas repassa a comunicaÃ§Ã£o. Se a aplicaÃ§Ã£o por trÃ¡s do proxy (`whoami` ou `numbers-api`) retornar um erro (como o erro 500 da API `numbers` apÃ³s algumas chamadas), o Nginx simplesmente enviarÃ¡ essa resposta de erro de volta ao cliente.
*   O exercÃ­cio aplica a mesma tÃ©cnica de proxy para a API `numbers-api`, tornando-a acessÃ­vel via `http://api.numbers.local/rng`. Ao testar, espera-se ver o Nginx retornando o erro 500 apÃ³s algumas atualizaÃ§Ãµes, demonstrando o comportamento pass-through.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 api.numbers.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Rodar API `numbers`:** (Estando em `ch20/exercises`)
    ```powershell
    # Assume que numbers/docker-compose.yml define serviÃ§o 'numbers-api' e rede 'ch20'
    docker-compose -f .\numbers\docker-compose.yml up -d
    ```
3.  **Copiar Config Nginx:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\api.numbers.local -Destination .\nginx\sites-enabled\
    ```
4.  **Reiniciar Nginx:** (Use o comando que iniciou o Nginx)
    ```powershell
    # Se Linux mode
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx
    # Se Windows Containers mode
    # docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-windows.yml restart nginx
    ```
5.  **Acessar API via Proxy:** Abra navegador em `http://api.numbers.local/rng`. Atualize (F5) 4 ou 5 vezes. VocÃª deve ver o nÃºmero aleatÃ³rio nas primeiras vezes e depois uma pÃ¡gina de erro (provavelmente Erro 500 ou 502 do Nginx, refletindo a falha da API `numbers-api`).

---

ğŸ“„ PÃ¡gina 411

ğŸ” Resumo dos principais pontos:

*   Figura 20.4: Mostra erro 500 servido pelo Nginx vindo da API `numbers`.
*   Reverse proxies oferecem mais que roteamento (ponto central de config).
*   **20.2 Handling routing and SSL in the reverse proxy** (Lidando com roteamento e SSL no proxy reverso)
*   Processo de adicionar apps no Nginx: Start app -> Copy config -> Restart Nginx.
    *   Ordem importa: Nginx checa se upstreams (apps backend) estÃ£o acessÃ­veis na inicializaÃ§Ã£o/reload. Se nÃ£o, Nginx falha ao iniciar.
*   Nginx cria lista interna de roteamento (host -> IP).
*   **Load Balancing:** Se DNS Docker retorna mÃºltiplos IPs para upstream (serviÃ§o escalado), Nginx automaticamente faz balanceamento de carga (round-robin por padrÃ£o) entre eles.
*   **ExercÃ­cio Try-it-now:** Proxy para app `image-gallery` web escalado.
    *   1. Adicionar `image-gallery.local` ao `hosts`.
    *   2. Rodar app Image Gallery (Compose file em `image-gallery/`) com escala de 3 para serviÃ§o `image-gallery` (`--scale image-gallery=3`).
    *   3. Copiar config Nginx `image-gallery.local.conf`.
    *   4. Reiniciar Nginx.
    *   5. Testar com `curl -i --head http://image-gallery.local` mÃºltiplas vezes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o resultado do exercÃ­cio anterior (Nginx repassando erro).
*   A seÃ§Ã£o 20.2 comeÃ§a a explorar funcionalidades mais avanÃ§adas do reverse proxy, comeÃ§ando pelo **balanceamento de carga (Load Balancing)**.
*   Menciona uma caracterÃ­stica do Nginx: ao iniciar ou recarregar configuraÃ§Ãµes, ele tenta resolver os nomes dos servidores `upstream` (definidos em `proxy_pass`). Se um nome nÃ£o puder ser resolvido (porque o contÃªiner ainda nÃ£o iniciou, por exemplo), o Nginx pode falhar ao iniciar. Por isso, a ordem "iniciar app, depois configurar/reiniciar proxy" Ã© geralmente mais segura.
*   A funcionalidade chave Ã© que se o nome do `upstream` (ex: `http://image-gallery` no `proxy_pass`) for resolvido pelo DNS Docker para *mÃºltiplos* endereÃ§os IP (porque o serviÃ§o `image-gallery` foi escalado com `docker-compose --scale`), o Nginx automaticamente distribuirÃ¡ as requisiÃ§Ãµes entre esses IPs/contÃªineres, realizando o balanceamento de carga (por padrÃ£o, usando um mÃ©todo round-robin simples).
*   O exercÃ­cio configura isso:
    1.  Adiciona o domÃ­nio `image-gallery.local` ao `hosts`.
    2.  Inicia a aplicaÃ§Ã£o Image Gallery, mas desta vez escalando o serviÃ§o web (`image-gallery`) para 3 rÃ©plicas usando `--scale`.
    3.  Copia a configuraÃ§Ã£o Nginx para este site.
    4.  Reinicia o Nginx.
    5.  Usa `curl -i --head ...` (que mostra os cabeÃ§alhos da resposta) repetidamente para observar o balanceamento.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 image-gallery.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Rodar Image Gallery (Escalado):** (Estando em `ch20/exercises`)
    ```powershell
    # Assume que image-gallery/docker-compose.yml define serviÃ§os e rede ch20
    docker-compose -f .\image-gallery\docker-compose.yml up -d --scale image-gallery=3
    # Verifique com 'docker-compose -f ... ps'. Deve ver 3 containers image-gallery.
    ```
3.  **Copiar Config Nginx:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery.local -Destination .\nginx\sites-enabled\
    ```
4.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
5.  **Testar Load Balancing:** Execute vÃ¡rias vezes:
    ```powershell
    curl -i --head http://image-gallery.local
    # Procure pelo header 'X-Host:' (do Nginx) e 'Hostname:' (do app whoami, se estiver lÃ¡)
    # Ou use Invoke-WebRequest -UseBasicParsing -HeadersOnly http://image-gallery.local
    # A prÃ³xima pÃ¡gina explicarÃ¡ como ver o IP do backend.
    ```

---

ğŸ“„ PÃ¡gina 412

ğŸ” Resumo dos principais pontos:

*   Config Nginx para `image-gallery` (em `image-gallery.local.conf`) Ã© similar, mas adiciona header `X-Upstream` na resposta mostrando IP do container backend que serviu.
*   Figura 20.5: Mostra saÃ­da do `curl` com header `X-Upstream` tendo IPs diferentes (ex: 172.20...) em chamadas repetidas, provando load balancing.
*   BenefÃ­cio: Rodar/testar app com load balancing em mÃ¡quina Ãºnica (dev/teste) sem Swarm/K8s. Sem mudanÃ§as no cÃ³digo/config do app.
*   **Roteamento por Path:** Proxy pode rotear para diferentes backends baseado no path da URL (alÃ©m do hostname).
*   Permite expor partes de um app (ex: API) sob o mesmo domÃ­nio da UI.
*   **ExercÃ­cio Try-it-now:** Expor API `iotd` como `/api/image` sob `image-gallery.local`.
    *   1. Remover config Nginx anterior (`image-gallery.local`).
    *   2. Copiar nova config (`image-gallery-2.local`) que inclui regra para `/api/image`.
    *   3. Reiniciar Nginx.
    *   4. Testar API via proxy: `curl -i http://image-gallery.local/api/image`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica que a configuraÃ§Ã£o Nginx usada no exercÃ­cio anterior (`image-gallery.local.conf`) foi ligeiramente modificada para adicionar um cabeÃ§alho de resposta `X-Upstream`. Este cabeÃ§alho Ã© preenchido pelo Nginx com o endereÃ§o IP do contÃªiner backend (`upstream`) que ele escolheu para servir aquela requisiÃ§Ã£o especÃ­fica. Ao fazer chamadas repetidas com `curl --head`, observar a variaÃ§Ã£o no valor de `X-Upstream` confirma que o Nginx estÃ¡ distribuindo a carga entre as diferentes rÃ©plicas do `image-gallery`.
*   Isso demonstra a capacidade do Nginx de fornecer balanceamento de carga localmente.
*   Em seguida, introduz o **roteamento baseado em Path**. AlÃ©m de escolher o backend com base no `Host:` (ex: `image-gallery.local`), o Nginx pode usar diretivas `location` mais especÃ­ficas para encaminhar requisiÃ§Ãµes baseadas no caminho da URL (ex: `/api/...` vai para o container da API, enquanto `/` vai para o container da UI).
*   O exercÃ­cio demonstra isso:
    1.  Remove-se a configuraÃ§Ã£o Nginx anterior.
    2.  Copia-se uma nova configuraÃ§Ã£o (`image-gallery-2.local`) que provavelmente contÃ©m:
        *   Um `location / { proxy_pass http://image-gallery; ... }` (para a UI).
        *   Um `location /api/image { proxy_pass http://iotd/image; ... }` (para a API).
    3.  Reinicia-se o Nginx.
    4.  Testa-se o acesso ao path da API (`/api/image`) atravÃ©s do domÃ­nio principal (`image-gallery.local`). O Nginx deve rotear essa requisiÃ§Ã£o especÃ­fica para o contÃªiner `iotd`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Remover Config Antiga:** (Estando em `ch20/exercises`)
    ```powershell
    Remove-Item -Path .\nginx\sites-enabled\image-gallery.local
    ```
2.  **Copiar Nova Config:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery-2.local -Destination .\nginx\sites-enabled\image-gallery.local
    # Renomeia para .local no destino para sobrescrever/ativar com nome esperado
    ```
3.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
4.  **Testar Rota API:**
    ```powershell
    curl -i http://image-gallery.local/api/image
    # VocÃª deve obter a resposta JSON da API 'iotd'.
    # Teste tambÃ©m http://image-gallery.local (deve servir a UI de 'image-gallery').
    ```

---

ğŸ“„ PÃ¡gina 413

ğŸ” Resumo dos principais pontos:

*   Figura 20.6: Mostra resposta JSON da API vindo de `image-gallery.local/api/image`.
*   PadrÃ£o Ãºtil para expor partes do app, montar UI de vÃ¡rios componentes sob mesmo domÃ­nio.
*   **SSL Termination:** Outra funÃ§Ã£o do proxy: lidar com HTTPS.
    *   Melhor centralizar no proxy do que em cada app.
    *   Nginx pode usar certs reais (Let's Encrypt) ou auto-assinados (nÃ£o-prod).
*   **ExercÃ­cio Try-it-now:** Configurar HTTPS para `image-gallery.local` usando certs auto-assinados.
    *   1. Gerar certs auto-assinados usando container `diamol/cert-generator`.
        *   Comando monta pasta local `nginx/certs` em `/certs` no container. `-e HOST_NAME=image-gallery.local` diz para qual domÃ­nio gerar.
        *   `docker run -v "$(pwd)/nginx/certs:/certs" ... diamol/cert-generator` (Linux).
        *   `docker run -v "$(pwd)/nginx/certs:C:\certs" ... diamol/cert-generator` (Windows).
    *   2. Remover config Nginx anterior.
    *   3. Copiar nova config (`image-gallery-3.local`) que usa SSL/certs.
    *   4. Reiniciar Nginx.
    *   5. Acessar `http://image-gallery.local` (deve redirecionar para HTTPS). Navegador darÃ¡ aviso de seguranÃ§a.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina confirma o sucesso do roteamento por path (Figura 20.6) e introduz a prÃ³xima funÃ§Ã£o importante do reverse proxy: **TerminaÃ§Ã£o SSL/TLS**.
*   Em vez de configurar cada um dos seus contÃªineres de aplicaÃ§Ã£o para lidar com HTTPS (o que exigiria gerenciar certificados e chaves privadas em cada um), vocÃª configura o **reverse proxy** para:
    1.  Ouvir na porta 443 (HTTPS).
    2.  Possuir o certificado SSL e a chave privada para o domÃ­nio (ex: `image-gallery.local`).
    3.  Descriptografar a requisiÃ§Ã£o HTTPS recebida do cliente.
    4.  Encaminhar a requisiÃ§Ã£o (agora como HTTP simples) para o contÃªiner da aplicaÃ§Ã£o backend apropriado via rede interna Docker.
    5.  Receber a resposta HTTP do backend.
    6.  Criptografar a resposta e enviÃ¡-la de volta ao cliente via HTTPS.
*   Isso centraliza todo o gerenciamento de SSL no proxy. Para ambientes de desenvolvimento/teste, podem-se usar certificados **auto-assinados** (gerados localmente, nÃ£o confiÃ¡veis por navegadores). Para produÃ§Ã£o, usam-se certificados reais de uma Autoridade Certificadora (CA) ou via Let's Encrypt.
*   O exercÃ­cio demonstra o setup com certificados auto-assinados:
    1.  Usa um contÃªiner utilitÃ¡rio (`diamol/cert-generator`) que contÃ©m ferramentas (como OpenSSL) para gerar um par certificado/chave para o domÃ­nio `image-gallery.local` e salvÃ¡-los na pasta local `nginx/certs` (que tambÃ©m estÃ¡ montada no contÃªiner Nginx).
    2.  Substitui a configuraÃ§Ã£o do Nginx por uma (`image-gallery-3.local`) que habilita SSL, aponta para os arquivos de certificado/chave gerados e provavelmente redireciona HTTP para HTTPS.
    3.  Reinicia o Nginx.
    4.  Ao acessar o site via HTTP, espera-se um redirecionamento para HTTPS, onde o navegador exibirÃ¡ um aviso de seguranÃ§a porque o certificado auto-assinado nÃ£o Ã© reconhecido por uma CA pÃºblica.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Gerar Certificados:** (Estando em `ch20/exercises`)
    ```powershell
    $CertPath = Join-Path -Path (Get-Location).Path -ChildPath "nginx\certs"
    mkdir $CertPath -ErrorAction SilentlyContinue
    # Se imagem base do generator for Linux (mais provÃ¡vel)
    docker container run --rm -v "$CertPath`:/certs" -e HOST_NAME=image-gallery.local diamol/cert-generator
    # Se imagem for Windows
    # docker container run --rm -v "$CertPath`:C:\certs" -e HOST_NAME=image-gallery.local diamol/cert-generator
    # Verifique se arquivos .pem ou .crt/.key apareceram em nginx\certs
    ```
2.  **Remover Config Antiga:**
    ```powershell
    Remove-Item -Path .\nginx\sites-enabled\image-gallery.local
    ```
3.  **Copiar Nova Config (SSL):**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery-3.local -Destination .\nginx\sites-enabled\image-gallery.local
    ```
4.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
5.  **Acessar no Navegador:**
    *   VÃ¡ para `http://image-gallery.local`. Deve redirecionar para `https://image-gallery.local`.
    *   VocÃª verÃ¡ um aviso de seguranÃ§a (ex: "Sua conexÃ£o nÃ£o Ã© privada", "Risco de seguranÃ§a em potencial"). Clique em "AvanÃ§ado" ou "Detalhes" e procure uma opÃ§Ã£o para "Continuar para image-gallery.local (nÃ£o seguro)" ou "Aceitar o risco e continuar".
    *   A aplicaÃ§Ã£o Image Gallery deve carregar via HTTPS.

---

ğŸ“„ PÃ¡gina 414

ğŸ” Resumo dos principais pontos:

*   Explica o passo de geraÃ§Ã£o de certificado (contÃªiner `cert-generator` usa OpenSSL, salva em pasta local montada no Nginx).
*   Figura 20.7: Mostra aviso de seguranÃ§a do navegador (Firefox) para certificado auto-assinado.
*   Nginx permite config detalhada de SSL (protocolos, ciphers, etc.; ver SSL Labs).
*   **Listagem 20.2:** Config Nginx para HTTPS + Redirect HTTP.
    ```nginx
    server { # Bloco HTTP
        server_name image-gallery.local;
        listen 80;
        return 301 https://$server_name$request_uri; # Redireciona para HTTPS
    }
    server { # Bloco HTTPS
        server_name image-gallery.local;
        listen 443 ssl; # Ouve na porta 443 com SSL
        # Caminhos para cert/key DENTRO do container Nginx
        ssl_certificate /etc/nginx/certs/server-cert.pem;
        ssl_certificate_key /etc/nginx/certs/server-key.pem;
        ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Exemplo de config SSL
        # ... (bloco location com proxy_pass para http://image-gallery) ...
    }
    ```
*   Explica config: Bloco porta 80 redireciona; bloco 443 habilita `ssl`, especifica arquivos de cert/key (no path *interno* do container Nginx, onde foram montados), define protocolos.
*   Certs sÃ£o por domÃ­nio; precisa de par cert/key para cada app (ou cert wildcard/multi-domÃ­nio).
*   Em produÃ§Ã£o: usar Secrets Swarm/K8s para armazenar certs/keys confidenciais.
*   BenefÃ­cio: Tira complexidade SSL dos apps; devs podem testar com HTTP simples.
*   PrÃ³xima feature Nginx: **Caching**.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina detalha a configuraÃ§Ã£o do Nginx para habilitar HTTPS (Listagem 20.2). A configuraÃ§Ã£o Ã© dividida em dois blocos `server`:
    1.  **Bloco HTTP (porta 80):** Configurado apenas para capturar requisiÃ§Ãµes HTTP e imediatamente retornar uma resposta de redirecionamento permanente (HTTP 301) para a versÃ£o HTTPS do mesmo URL (`https://$server_name$request_uri`).
    2.  **Bloco HTTPS (porta 443):**
        *   `listen 443 ssl;`: Habilita SSL/TLS nesta porta.
        *   `ssl_certificate ...;` e `ssl_certificate_key ...;`: Especificam os caminhos, *dentro do contÃªiner Nginx*, onde encontrar o arquivo do certificado pÃºblico e o arquivo da chave privada correspondente (esses arquivos foram colocados lÃ¡ pelo bind mount da pasta `certs`).
        *   `ssl_protocols ...;`: Exemplo de configuraÃ§Ã£o para definir quais versÃµes do protocolo TLS permitir.
        *   (Oculto na listagem, mas necessÃ¡rio): Este bloco tambÃ©m conteria a(s) diretiva(s) `location ... { proxy_pass ...; }` para encaminhar as requisiÃ§Ãµes (agora descriptografadas) para o serviÃ§o backend (`image-gallery`).
*   Reafirma que certificados sÃ£o por domÃ­nio e que em produÃ§Ã£o devem ser gerenciados como segredos. Centralizar o SSL no proxy simplifica muito as aplicaÃ§Ãµes backend.
*   Introduz a prÃ³xima capacidade do Nginx: atuar como um **cache** para as respostas das aplicaÃ§Ãµes backend.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Explorar Config:** Abra o arquivo `ch20\exercises\nginx\sites-available\image-gallery-3.local` no VS Code. Veja a estrutura completa com os dois blocos `server` e as diretivas SSL.
*   O prÃ³ximo exercÃ­cio modificarÃ¡ esta configuraÃ§Ã£o para adicionar caching.

---

ğŸ“„ PÃ¡gina 415

ğŸ” Resumo dos principais pontos:

*   Figura 20.7: Relembra aviso do navegador.
*   **20.3 Improving performance and reliability with the proxy** (Melhorando desempenho e confiabilidade com o proxy)
*   Nginx Ã© de alta performance; pode servir conteÃºdo estÃ¡tico muito rÃ¡pido.
*   Pode usar essa performance para **cachear respostas** dinÃ¢micas das apps backend (upstream).
*   **Proxy Cache:**
    *   Nginx armazena cÃ³pia da resposta do upstream (em disco/memÃ³ria local do Nginx).
    *   RequisiÃ§Ãµes futuras para *mesmo* conteÃºdo sÃ£o servidas *diretamente* pelo Nginx (do cache), sem contatar o upstream.
*   Diagrama (Figura 20.8): Ilustra fluxo com cache. Request -> Nginx -> Cache HIT? -> [Sim: Serve do cache] / [NÃ£o: Pega do Upstream -> Salva no Cache -> Serve para cliente].
*   **BenefÃ­cios Cache:**
    1.  **Menor LatÃªncia:** Servir do cache Nginx Ã© mais rÃ¡pido que app gerar resposta.
    2.  **Menor Carga no App:** Reduz trÃ¡fego para app containers -> podem lidar com mais usuÃ¡rios com mesma infra.
*   Cache sÃ³ para conteÃºdo nÃ£o-especÃ­fico do usuÃ¡rio (ou bypass cache se cookie de auth presente). App Image Gallery Ã© bom candidato.
*   **ExercÃ­cio Try-it-now:** Configurar Nginx cache para Image Gallery (web e API).
    *   1. Remover config anterior.
    *   2. Copiar nova config (`image-gallery-4.local`) com diretivas de cache.
    *   3. Reiniciar Nginx.
    *   4. Testar com `curl -i --head --insecure https://image-gallery.local` (duas vezes).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta seÃ§Ã£o introduz o uso do Nginx como um **cache de proxy reverso**. A ideia Ã© que o Nginx pode armazenar temporariamente as respostas que ele recebe das suas aplicaÃ§Ãµes backend (upstreams).
*   **Fluxo com Cache (Figura 20.8):**
    1.  Cliente faz requisiÃ§Ã£o ao Nginx.
    2.  Nginx verifica se tem uma resposta vÃ¡lida e nÃ£o expirada para essa requisiÃ§Ã£o em seu cache local.
    3.  **Cache Hit:** Se tiver, Nginx envia a resposta do cache diretamente para o cliente (muito rÃ¡pido!). A aplicaÃ§Ã£o backend nem Ã© contatada.
    4.  **Cache Miss:** Se nÃ£o tiver, Nginx encaminha a requisiÃ§Ã£o para a aplicaÃ§Ã£o backend.
    5.  Nginx recebe a resposta do backend.
    6.  Nginx **armazena** uma cÃ³pia dessa resposta em seu cache (para uso futuro).
    7.  Nginx envia a resposta para o cliente.
*   Isso melhora muito a performance percebida pelo usuÃ¡rio (respostas mais rÃ¡pidas em hits de cache) e reduz drasticamente a carga nos servidores de aplicaÃ§Ã£o. Ã‰ preciso ter cuidado para cachear apenas conteÃºdo que possa ser compartilhado entre usuÃ¡rios (pÃ¡ginas pÃºblicas, dados de API genÃ©ricos) e definir tempos de expiraÃ§Ã£o apropriados.
*   O exercÃ­cio configura o Nginx para cachear respostas tanto da UI web quanto da API do Image Gallery. O teste usa `curl` duas vezes para o mesmo URL; a primeira vez deve ser um "MISS" (Nginx busca no backend e popula o cache), a segunda deve ser um "HIT" (Nginx serve do cache).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Remover Config SSL:** (Estando em `ch20/exercises`)
    ```powershell
    Remove-Item -Path .\nginx\sites-enabled\image-gallery.local
    ```
2.  **Copiar Nova Config (Cache):**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery-4.local -Destination .\nginx\sites-enabled\image-gallery.local
    ```
3.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
4.  **Testar Cache:** Execute o comando duas vezes seguidas:
    ```powershell
    curl -i --head --insecure https://image-gallery.local
    # Ou Invoke-WebRequest -Uri https://image-gallery.local -HeadersOnly -SkipCertificateCheck
    ```
    Analise os cabeÃ§alhos da resposta (prÃ³xima pÃ¡gina explica o header `X-Cache`).

---

ğŸ“„ PÃ¡gina 416

ğŸ” Resumo dos principais pontos:

*   Nova config Nginx adiciona header `X-Cache` (MISS/HIT) na resposta.
*   Figura 20.9: Mostra saÃ­das do `curl`. Primeira tem `X-Cache: MISS` e `X-Upstream`. Segunda tem `X-Cache: HIT` e *sem* `X-Upstream` (nÃ£o contatou backend).
*   Nginx permite tunar cache (ex: tempo de expiraÃ§Ã£o).
*   Config (`image-gallery-4.local`) usa cache `SHORT` (1 min) para API, `LONG` (6h) para Web UI.
*   **Listagem 20.3:** Trechos da config Nginx com cache.
    ```nginx
    # Bloco location para API
    location = /api/image {
        proxy_pass http://iotd/image;
        proxy_cache SHORT; # Usa zona de cache nomeada 'SHORT'
        proxy_cache_valid 200 1m; # Cacheia respostas 200 por 1 minuto
        # ... proxy_set_header, add_header ...
    }
    # Bloco location para UI Web
    location / {
        proxy_pass http://image-gallery;
        proxy_cache LONG; # Usa zona de cache 'LONG'
        proxy_cache_valid 200 6h; # Cacheia 200 por 6 horas
        # Usa cache VENCIDO se backend der erro/timeout/etc:
        proxy_cache_use_stale error timeout invalid_header updating
                              http_500 http_502 http_503 http_504;
        # ... proxy_set_header, add_header ...
    }
    ```
*   Zonas de cache `SHORT`/`LONG` definidas na config Nginx core (`diamol/nginx`).
*   **`proxy_cache_use_stale`:** Feature de **resiliÃªncia**. Permite Nginx servir conteÃºdo do cache *mesmo que esteja expirado* (`stale`) se o servidor backend (`upstream`) estiver indisponÃ­vel ou retornando certos erros (configurados na diretiva). MantÃ©m app parcialmente online.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina analisa o resultado do teste de cache (Figura 20.9), confirmando o funcionamento (MISS -> HIT) atravÃ©s do cabeÃ§alho customizado `X-Cache`.
*   Explica que a configuraÃ§Ã£o Nginx usada (Listagem 20.3) define diferentes polÃ­ticas de cache para a API e para a UI:
    *   `proxy_cache SHORT/LONG;`: Referencia "zonas" de cache prÃ©-definidas (na imagem `diamol/nginx`) que especificam onde e como armazenar os dados cacheados (memÃ³ria/disco, tamanho).
    *   `proxy_cache_valid 200 <tempo>;`: Define por quanto tempo uma resposta HTTP 200 (sucesso) deve ser considerada "fresca" no cache (1 minuto para API, 6 horas para UI). ApÃ³s esse tempo, a resposta fica "stale" (vencida).
*   Introduz a diretiva `proxy_cache_use_stale`. Ela Ã© muito importante para **resiliÃªncia**. Ela diz ao Nginx: "Se vocÃª precisar buscar conteÃºdo do backend, mas ele falhar (com erro, timeout, etc.), verifique se vocÃª tem uma versÃ£o *antiga* (stale) desse conteÃºdo no cache. Se tiver, sirva essa versÃ£o antiga mesmo assim." Isso pode manter partes do seu site funcionando para o usuÃ¡rio final, mesmo que a aplicaÃ§Ã£o backend esteja temporariamente fora do ar.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Explorar Config:** Abra `ch20\exercises\nginx\sites-available\image-gallery-4.local` no VS Code. Veja as diretivas `proxy_cache`, `proxy_cache_valid`, `proxy_cache_use_stale`.
*   O prÃ³ximo exercÃ­cio testarÃ¡ a funcionalidade `proxy_cache_use_stale`.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   `proxy_cache_valid`: Data de validade no produto da prateleira do supermercado (cache).
*   `proxy_cache_use_stale`: PolÃ­tica do gerente: "Se o caminhÃ£o de entrega (backend app) atrasar ou nÃ£o vier, podemos vender o produto que venceu ontem (cache stale) para nÃ£o deixar o cliente de mÃ£os vazias, mas sÃ³ para certos tipos de problema (tipos de erro configurados)".

---

ğŸ“„ PÃ¡gina 417

ğŸ” Resumo dos principais pontos:

*   `proxy_cache_use_stale` mantÃ©m app online (com dados antigos) mesmo se containers app caÃ­rem. Ãštil para falhas transitÃ³rias ou rollbacks.
*   **ExercÃ­cio Try-it-now:** Testar `use_stale`.
    *   1. Acessar site e API para popular cache: `curl ... https://...`, `curl ... https://.../api/image`.
    *   2. Matar containers web: `docker rm -f $(docker container ls -f name=imagegallery_image-gallery_* -q)`.
    *   3. Tentar acessar web de novo: `curl -i --head --insecure https://image-gallery.local`. Deve funcionar (HIT, servido do cache stale).
    *   4. Matar container API: `docker rm -f image-gallery_iotd_1`.
    *   5. Tentar acessar API de novo: `curl -i --head --insecure https://image-gallery.local/api/image`. Deve falhar (Erro 502), pois API tinha cache curto e *nÃ£o* `use_stale` configurado (ou com menos condiÃ§Ãµes).
*   Figura 20.10: Mostra saÃ­da: Web retorna 200 (cache stale), API retorna 502 (Bad Gateway).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra o poder (e a configuraÃ§Ã£o) do `proxy_cache_use_stale`.
*   Primeiro, acessa-se a UI e a API para garantir que o Nginx tenha respostas cacheadas para ambas.
*   Depois, os contÃªineres da aplicaÃ§Ã£o web (`image-gallery`) sÃ£o **removidos forÃ§adamente**.
*   Ao tentar acessar a UI web novamente, como a configuraÃ§Ã£o dela incluÃ­a `proxy_cache_use_stale error timeout ...`, o Nginx detecta que o backend (`http://image-gallery`) estÃ¡ inacessÃ­vel (erro), mas encontra a resposta antiga (agora "stale") no cache `LONG` e a serve mesmo assim. O usuÃ¡rio vÃª o site, embora possa estar desatualizado.
*   Em seguida, o contÃªiner da API (`iotd`) Ã© removido.
*   Ao tentar acessar a API (`/api/image`), a configuraÃ§Ã£o do cache `SHORT` para ela provavelmente *nÃ£o* incluÃ­a `proxy_cache_use_stale` (ou tinha um tempo de validade muito curto que jÃ¡ expirou). O Nginx tenta contatar o backend `http://iotd`, falha (contÃªiner nÃ£o existe), nÃ£o encontra ou nÃ£o pode usar uma resposta stale, e retorna um erro **HTTP 502 Bad Gateway** para o cliente, indicando que o proxy nÃ£o conseguiu obter uma resposta do servidor upstream.
*   A Figura 20.10 mostra exatamente esses resultados diferentes, destacando como `use_stale` pode adicionar resiliÃªncia seletiva.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Popular Cache:**
    ```powershell
    curl -s --insecure https://image-gallery.local | Out-Null
    curl -s --insecure https://image-gallery.local/api/image | Out-Null
    ```
2.  **Matar Containers Web:**
    ```powershell
    # Encontrar IDs dos containers web e remover
    $webContainers = docker container ls -f name=image-gallery_image-gallery_* -q
    if ($webContainers) { docker container rm -f $webContainers }
    ```
3.  **Testar Web (Deve Funcionar - Cache Stale):**
    ```powershell
    Invoke-WebRequest -Uri https://image-gallery.local -SkipCertificateCheck -HeadersOnly -UseBasicParsing
    # Verifique o status 200 e o header X-Cache (deve ser HIT ou STALE)
    ```
4.  **Matar Container API:**
    ```powershell
    # Encontrar nome/ID do container iotd e remover
    $apiContainer = docker container ls -f name=image-gallery_iotd_* -q # Nome pode variar
    if ($apiContainer) { docker container rm -f $apiContainer }
    ```
5.  **Testar API (Deve Falhar - 502):**
    ```powershell
    Invoke-WebRequest -Uri https://image-gallery.local/api/image -SkipCertificateCheck -HeadersOnly -UseBasicParsing
    # Deve falhar com status 502 Bad Gateway
    ```

---

ğŸ“„ PÃ¡gina 418

ğŸ” Resumo dos principais pontos:

*   Figura 20.10: Confirma web OK (stale), API falha (502).
*   Fim dos exercÃ­cios Nginx. Ã‰ capaz (cache, GZip, headers, etc.).
*   Nginx Ã© "container-agnostic" (trabalha com DNS/IP, nÃ£o integra com API Docker). Requer config manual + reload.
*   Alternativa moderna: Proxy Cloud-Native (container-aware, integra com Docker).
*   **20.4 Using a cloud-native reverse proxy** (Usando um proxy reverso cloud-native)
    *   Revisita CI com Jenkins (Cap 11) rodando em container, que controlava Docker Engine do host via socket/pipe montado.
    *   Mesma tÃ©cnica permite apps em container consultarem API Docker (descobrir outros containers, etc.).
    *   **Traefik:** Proxy reverso cloud-native que usa essa tÃ©cnica.
*   **Traefik:**
    *   NÃ£o usa arquivos de config estÃ¡ticos por app.
    *   Usa **labels** nos containers de aplicaÃ§Ã£o para **configuraÃ§Ã£o dinÃ¢mica**.
    *   Traefik monitora API Docker; detecta containers com labels Traefik e auto-configura roteamento.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui a seÃ§Ã£o sobre Nginx, resumindo suas capacidades mas apontando sua principal caracterÃ­stica (e limitaÃ§Ã£o no mundo dos contÃªineres): ele opera puramente na camada de rede (resolvendo nomes DNS para IPs), sem ter consciÃªncia da plataforma Docker subjacente. Isso significa que a configuraÃ§Ã£o dos sites/proxies Ã© feita em arquivos estÃ¡ticos que precisam ser gerenciados manualmente, e o Nginx precisa ser recarregado ou reiniciado para aplicar mudanÃ§as.
*   A seÃ§Ã£o 20.4 introduz uma abordagem alternativa, mais moderna e integrada ao ecossistema de contÃªineres: os **proxies reversos "cloud-native"**, exemplificados pelo **Traefik**.
*   A chave para o Traefik Ã© que ele roda em um contÃªiner que tem acesso Ã  API do Docker Engine (usando a mesma tÃ©cnica de montar o socket/pipe que o Jenkins usou no Cap. 11).
*   Em vez de vocÃª criar arquivos de configuraÃ§Ã£o para cada aplicaÃ§Ã£o que quer expor, vocÃª adiciona **labels** especiais do Traefik diretamente na definiÃ§Ã£o do *contÃªiner da sua aplicaÃ§Ã£o* (no `docker run` ou, mais comumente, no `docker-compose.yml`).
*   O Traefik continuamente **monitora** a API do Docker em busca de contÃªineres que possuam essas labels. Quando ele encontra um, ele automaticamente lÃª as labels e **gera dinamicamente** as regras de roteamento necessÃ¡rias para expor aquele contÃªiner, sem necessidade de arquivos de configuraÃ§Ã£o manuais ou reinÃ­cios.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª rodarÃ¡ o Traefik em um contÃªiner, montando o `docker.sock` (Linux) ou `docker_engine` pipe (Windows).
*   VocÃª modificarÃ¡ os arquivos `docker-compose.yml` das aplicaÃ§Ãµes (whoami, image-gallery) para adicionar seÃ§Ãµes `labels:` com diretivas `traefik...`.
*   VocÃª observarÃ¡ o Traefik Dashboard (`http://localhost:8080`) se atualizando automaticamente Ã  medida que vocÃª inicia/para as aplicaÃ§Ãµes.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Nginx Config: Recepcionista segue um manual impresso e estÃ¡tico. Para adicionar uma nova empresa ou mudar um ramal, precisa atualizar o manual e avisar o recepcionista para reler.
*   Traefik Config (Labels): Recepcionista tem um sistema online conectado ao RH (API Docker). Quando uma nova empresa (contÃªiner) se registra no prÃ©dio e coloca uma placa na porta (label) dizendo "Quero receber visitantes no nome X", o sistema do recepcionista atualiza automaticamente suas regras de encaminhamento.

---

ğŸ“„ PÃ¡gina 419

ğŸ” Resumo dos principais pontos:

*   **Config DinÃ¢mica Traefik:** BenefÃ­cio maior. NÃ£o precisa iniciar upstreams antes, nÃ£o precisa reiniciar Traefik para mudanÃ§as.
*   Traefik tem API/UI web (dashboard) para ver regras/estado.
*   **ExercÃ­cio Try-it-now:** Rodar Traefik e ver dashboard inicial.
    *   1. Limpar containers.
    *   2. Rodar Traefik via Compose (usa override linux/windows para montar socket/pipe Docker).
        *   `docker-compose -f traefik/docker-compose.yml -f traefik/override-... up -d`.
    *   3. Acessar dashboard Traefik: `http://localhost:8080`.
*   Figura 20.11: Mostra dashboard Traefik (inicialmente vazio ou sÃ³ com serviÃ§os internos).
*   **Conceitos Traefik:**
    *   **Entrypoints:** Portas que Traefik ouve (80, 443, 8080 para dashboard).
    *   **Routers:** Regras (Host, Path) que ligam Entrypoint a Service.
    *   **Services:** Upstreams (containers app). Traefik descobre automaticamente via Docker API.
    *   **Middlewares:** Opcional, modifica request/response entre Router e Service (ex: strip prefix, auth, headers).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina destaca a **configuraÃ§Ã£o dinÃ¢mica** como o principal benefÃ­cio do Traefik. Ele se adapta automaticamente Ã s mudanÃ§as no ambiente Docker (contÃªineres iniciando, parando, sendo escalados) lendo as labels.
*   O exercÃ­cio instrui a iniciar o contÃªiner do Traefik usando `docker-compose`. O arquivo Compose (`traefik/docker-compose.yml` + override) configura o Traefik, publica suas portas (80, 443 para trÃ¡fego de app, 8080 para o dashboard) e, crucialmente, monta o socket/pipe do Docker para que o Traefik possa monitorar a API Docker.
*   ApÃ³s iniciar, acessar o dashboard (`http://localhost:8080`) permite visualizar a configuraÃ§Ã£o atual do Traefik. Inicialmente (Figura 20.11), ele nÃ£o terÃ¡ nenhuma regra de roteamento para aplicaÃ§Ãµes externas, apenas talvez para seus prÃ³prios serviÃ§os internos (como o prÃ³prio dashboard).
*   A pÃ¡gina entÃ£o define os **conceitos chave** usados pelo Traefik para configurar o roteamento:
    *   `Entrypoint`: Por onde o trÃ¡fego entra (Porta 80, Porta 443).
    *   `Router`: Regra que diz "Se trÃ¡fego chegar no Entrypoint X e corresponder Ã  condiÃ§Ã£o Y (ex: Host for 'a.com'), envie para o Service Z".
    *   `Service`: Representa a aplicaÃ§Ã£o backend (o(s) contÃªiner(es) que servem o conteÃºdo). Traefik descobre os IPs dos contÃªineres automaticamente.
    *   `Middleware`: FunÃ§Ãµes opcionais que podem ser inseridas entre o Router e o Service para modificar a requisiÃ§Ã£o ou resposta (ex: adicionar cabeÃ§alhos, fazer autenticaÃ§Ã£o, reescrever URL).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Limpar:** `docker container rm -f $(docker container ls -aq)`
2.  **Navegar:** `cd C:\DockerLabs\diamol\ch20\exercises`
3.  **Rodar Traefik:** (Use override apropriado)
    ```powershell
    # Linux mode
    docker-compose -f .\traefik\docker-compose.yml -f .\traefik\override-linux.yml up -d
    # Windows mode
    # docker-compose -f .\traefik\docker-compose.yml -f .\traefik\override-windows.yml up -d
    ```
4.  **Verificar:** `docker ps` (ver container traefik).
5.  **Acessar Dashboard:** Abra navegador em `http://localhost:8080`. Explore a UI (Entrypoints, Routers, Services - devem estar quase vazios).

---

ğŸ“„ PÃ¡gina 420

ğŸ” Resumo dos principais pontos:

*   Config mais simples: sÃ³ Router (com regra) ligado a Service.
*   **ExercÃ­cio Try-it-now:** Deployar `whoami` com labels Traefik.
    *   1. Deployar `whoami`: `docker-compose -f whoami/docker-compose.yml -f whoami/override-traefik.yml up -d`. (Override adiciona labels).
    *   2. Verificar dashboard Traefik (`:8080`). Procurar router `whoami@docker`.
    *   3. Testar rota: `curl -i http://whoami.local`.
*   Figura 20.12: Mostra dashboard Traefik agora com router `whoami@docker` configurado (Host `whoami.local` -> service `whoami`).
*   **Listagem 20.4:** Labels no `override-traefik.yml` para `whoami`.
    ```yaml
    services:
      whoami:
        labels:
          - "traefik.enable=true" # Habilita Traefik para este serviÃ§o
          # Define router 'whoami', regra Ã© match no Host header
          - "traefik.http.routers.whoami.rule=Host(`whoami.local`)"
    ```
*   ExplicaÃ§Ã£o Labels: `traefik.enable=true` ativa. `traefik.http.routers.<nome_router>.rule=...` define a regra de roteamento. Traefik infere o serviÃ§o backend a partir do prÃ³prio container/serviÃ§o onde a label estÃ¡.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O exercÃ­cio demonstra a configuraÃ§Ã£o dinÃ¢mica do Traefik na prÃ¡tica.
    1.  Inicia-se a aplicaÃ§Ã£o `whoami` usando um arquivo Compose (`override-traefik.yml`) que contÃ©m **labels** especÃ­ficas para o Traefik (Listagem 20.4).
    2.  O contÃªiner `traefik`, que estÃ¡ monitorando a API Docker, detecta o novo contÃªiner `whoami` e suas labels.
    3.  Ele lÃª as labels e **automaticamente** cria a configuraÃ§Ã£o interna necessÃ¡ria: um `Router` chamado `whoami` que corresponde ao `Host: whoami.local` e o direciona para o `Service` `whoami` (o contÃªiner que acabou de iniciar).
    4.  Ao verificar o dashboard do Traefik (Figura 20.12), essa nova rota aparece.
    5.  Ao acessar `http://whoami.local`, o Traefik (que estÃ¡ ouvindo na porta 80) aplica a regra e encaminha a requisiÃ§Ã£o para o contÃªiner `whoami`.
*   A Listagem 20.4 mostra as labels mÃ­nimas necessÃ¡rias:
    *   `traefik.enable=true`: Sinaliza ao Traefik para gerenciar este serviÃ§o.
    *   `traefik.http.routers.<nome_router>.rule=<regra>`: Define um roteador HTTP. `<nome_router>` Ã© um nome que vocÃª escolhe (aqui, `whoami`). `<regra>` especifica como identificar as requisiÃ§Ãµes para este router (aqui, `Host(\`whoami.local\`)` - note as backticks ` ou aspas simples para a string dentro da regra).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir Traefik Rodando:** (Passo da p.419).
2.  **Deployar `whoami` com Labels:** (Estando em `ch20/exercises`)
    ```powershell
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml up -d
    ```
3.  **Verificar Dashboard:** Atualize `http://localhost:8080`. VÃ¡ para HTTP > Routers. VocÃª deve ver o router `whoami@docker`. Clique nele para ver detalhes (regra Host). Verifique tambÃ©m HTTP > Services; deve haver um serviÃ§o `whoami`.
4.  **Testar Rota:**
    ```powershell
    curl -i http://whoami.local
    # Deve retornar a saÃ­da do whoami.
    ```

---

ğŸ“„ PÃ¡gina 421

ğŸ” Resumo dos principais pontos:

*   Figura 20.12: Confirma router `whoami` no dashboard.
*   Traefik suporta roteamento sofisticado (Host, Path, PathPrefix) + Middlewares (StripPrefix).
*   Exemplo: Expor API Image Gallery (`iotd`) via path `/api/` no domÃ­nio principal.
    *   Traefik recebe `/api/image`, middleware remove `/api`, envia `/image` para serviÃ§o `iotd`.
*   **ExercÃ­cio Try-it-now:** Deployar Image Gallery com labels Traefik para UI e API (com StripPrefix).
    *   1. Deployar app: `docker-compose -f image-gallery/... -f image-gallery/override-traefik.yml up -d`. (Override contÃ©m labels para `image-gallery` e `iotd`, incluindo middleware).
    *   2. Testar UI: `curl --head http://image-gallery.local`.
    *   3. Testar API via path: `curl -i http://image-gallery.local/api/image`.
*   Figura 20.13: Mostra resposta JSON da API via path `/api/image`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz o uso de **Middlewares** no Traefik para manipulaÃ§Ãµes mais complexas, como o **StripPrefix**. O cenÃ¡rio Ã© o mesmo da p.412 com Nginx: queremos que `http://image-gallery.local/api/image` seja roteado para o serviÃ§o `iotd`, mas o serviÃ§o `iotd` espera receber a requisiÃ§Ã£o apenas como `/image` (sem o `/api`).
*   A configuraÃ§Ã£o via labels no Traefik (dentro de `override-traefik.yml`) farÃ¡ o seguinte:
    1.  Define um Router para a API (`iotd-api`) que corresponde a `Host(\`image-gallery.local\`) && PathPrefix(\`/api\`)`.
    2.  Define um Middleware chamado `api-strip` do tipo `StripPrefix`, configurado para remover o prefixo `/api`.
    3.  Associa o middleware `api-strip` ao router `iotd-api`.
    4.  Associa o router `iotd-api` ao serviÃ§o `iotd`.
    5.  (TambÃ©m define um router `ig-web` para `Host(\`image-gallery.local\`)` ligado ao serviÃ§o `image-gallery` para a UI).
*   O exercÃ­cio implanta a aplicaÃ§Ã£o com essas labels. Traefik detecta e configura tudo dinamicamente. O teste com `curl` para `/api/image` deve funcionar, pois o Traefik farÃ¡ a remoÃ§Ã£o do prefixo antes de chamar o backend `iotd`.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Parar `whoami` (Opcional):**
    ```powershell
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml down
    ```
2.  **Deployar Image Gallery (Traefik):** (Estando em `ch20/exercises`)
    ```powershell
    docker-compose -f .\image-gallery\docker-compose.yml -f .\image-gallery\override-traefik.yml up -d
    # Isso iniciarÃ¡ os 3 serviÃ§os do app (image-gallery, iotd, accesslog)
    ```
3.  **Verificar Dashboard:** Atualize `http://localhost:8080`. VocÃª deve ver novos Routers (ex: `ig-web`, `iotd-api`), Services e o Middleware `api-strip`. Explore as configuraÃ§Ãµes deles.
4.  **Testar Rotas:**
    ```powershell
    curl --head http://image-gallery.local # Deve funcionar (UI)
    curl -i http://image-gallery.local/api/image # Deve funcionar (API via path)
    ```

---

ğŸ“„ PÃ¡gina 422

ğŸ” Resumo dos principais pontos:

*   Figura 20.13: Confirma resposta API via path.
*   Config StripPrefix Ã© "crÃ­ptica" (define router, define middleware, associa middleware ao router, associa router ao serviÃ§o via labels).
*   Transparente para consumidor.
*   **Comparativo Traefik vs Nginx (cont.):**
    *   Traefik (v2.1) nÃ£o tem cache; Nginx Ã© melhor para isso.
    *   Traefik tem melhor suporte SSL (auto Let's Encrypt, default self-signed).
*   **ExercÃ­cio Try-it-now:** Adicionar HTTPS (com cert default Traefik) para Image Gallery.
    *   1. Deployar app com `override-traefik-ssl.yml`. (Labels adicionam redirect HTTP->HTTPS e configuram router para usar entrypoint HTTPS).
    *   2. Testar HTTPS web: `curl --head --insecure https://image-gallery.local`.
    *   3. Testar HTTPS API: `curl --insecure https://image-gallery.local/api/image`.
*   Esperar aviso navegador (cert default nÃ£o confiÃ¡vel). NÃ£o precisa gerar certs.
*   Figura 20.14: Mostra `curl --insecure` funcionando via HTTPS.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reconhece que configurar middlewares como StripPrefix via labels pode ser um pouco verboso, mas funciona.
*   Faz a comparaÃ§Ã£o de features: Nginx ganha em **caching**, Traefik ganha em **gerenciamento de SSL**.
*   O Traefik pode ser configurado para:
    *   Gerar certificados **auto-assinados** automaticamente (bom para dev/teste, como no exercÃ­cio).
    *   Integrar-se com **Let's Encrypt** para obter e renovar certificados SSL vÃ¡lidos automaticamente para seus domÃ­nios pÃºblicos (muito Ãºtil para produÃ§Ã£o).
*   O exercÃ­cio demonstra a facilidade de habilitar HTTPS com o certificado padrÃ£o do Traefik:
    1.  Usa-se um novo override (`override-traefik-ssl.yml`) que adiciona mais labels. Essas labels provavelmente:
        *   Definem um middleware para redirecionar HTTP para HTTPS.
        *   Associam esse middleware ao router que ouve no entrypoint HTTP (porta 80).
        *   Modificam/criam routers para tambÃ©m ouvirem no entrypoint HTTPS (porta 443) e habilitam TLS (`routers.<name>.tls=true`). Como nenhum certificado especÃ­fico Ã© fornecido nas labels, o Traefik usa seu certificado default auto-assinado.
    2.  ApÃ³s o deploy, testa-se o acesso via `https://...`. O `curl` precisa da flag `--insecure` (ou `-k`) para ignorar o erro de certificado nÃ£o confiÃ¡vel. O navegador mostraria o aviso de seguranÃ§a.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Deployar com SSL Labels:** (Estando em `ch20/exercises`)
    ```powershell
    # Parar anterior (opcional)
    # docker-compose -f .\image-gallery\docker-compose.yml -f .\image-gallery\override-traefik.yml down
    # Deploy com SSL
    docker-compose -f .\image-gallery\docker-compose.yml -f .\image-gallery\override-traefik-ssl.yml up -d
    ```
2.  **Verificar Dashboard:** Atualize `http://localhost:8080`. Veja os routers; eles agora devem ter configuraÃ§Ã£o TLS e talvez referÃªncia ao middleware de redirect.
3.  **Testar HTTPS:**
    ```powershell
    curl --head --insecure https://image-gallery.local
    # Deve retornar status 200 OK

    curl --insecure https://image-gallery.local/api/image
    # Deve retornar JSON da API
    ```
4.  **Testar Redirect (Opcional):**
    ```powershell
    curl -i http://image-gallery.local
    # Deve retornar status 3xx Redirect com Location: https://...
    ```

---

ğŸ“„ PÃ¡gina 423

ğŸ” Resumo dos principais pontos:

*   Figura 20.14: Confirma acesso HTTPS via `curl --insecure`.
*   RecapitulaÃ§Ã£o features proxy: Roteamento, Load Balancing, SSL Termination. Traefik faz via labels dinÃ¢micas.
*   Ãšltima feature: **Sticky Sessions (Afinidade de SessÃ£o)**.
    *   NecessÃ¡rio para apps legados (stateful) que requerem que mesmo cliente sempre acesse mesma instÃ¢ncia backend.
    *   Habilitado via config de load balancer do serviÃ§o Traefik (via label).
*   **ExercÃ­cio Try-it-now:** Demonstrar sticky sessions com `whoami`.
    *   1. Escalar `whoami` para 3 rÃ©plicas (usando `override-traefik.yml` - sem sticky).
    *   2. Fazer chamadas `curl` repetidas usando cookies (`-c c.txt -b c.txt`). Observar que Hostname na resposta muda (load balancing normal).
    *   3. Deployar `whoami` de novo com `override-traefik-sticky.yml` (adiciona label para sticky), escala 3.
    *   4. Fazer chamadas `curl` repetidas (com cookies). Observar que Hostname na resposta permanece o *mesmo*.
*   Figura 20.15: Mostra saÃ­da confirmando sticky sessions (mesmo hostname/container servindo requests).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz a Ãºltima funcionalidade a ser explorada: **Sticky Sessions**, tambÃ©m conhecida como afinidade de sessÃ£o.
*   Enquanto aplicaÃ§Ãµes modernas devem ser stateless (permitindo que qualquer instÃ¢ncia atenda qualquer requisiÃ§Ã£o), aplicaÃ§Ãµes legadas frequentemente armazenam o estado da sessÃ£o do usuÃ¡rio na memÃ³ria do servidor especÃ­fico que atendeu a primeira requisiÃ§Ã£o. Nesses casos, Ã© crucial que todas as requisiÃ§Ãµes subsequentes *daquele mesmo usuÃ¡rio* sejam direcionadas para a *mesma instÃ¢ncia* do servidor backend.
*   Reverse proxies como o Traefik podem habilitar sticky sessions. Geralmente, eles fazem isso inspecionando a primeira resposta do backend, adicionando um cookie especial Ã  resposta que identifica aquele backend especÃ­fico, e entÃ£o, para requisiÃ§Ãµes futuras do mesmo cliente (identificado pelo cookie), o proxy usa a informaÃ§Ã£o do cookie para rotear diretamente para o backend original.
*   O exercÃ­cio demonstra isso com o `whoami`:
    1.  Primeiro, roda-se sem sticky sessions e confirma-se (via `curl` com cookies) que requisiÃ§Ãµes diferentes vÃ£o para rÃ©plicas diferentes.
    2.  Depois, re-implanta-se usando um override (`override-traefik-sticky.yml`) que adiciona uma label como `traefik.http.services.whoami.loadbalancer.sticky.cookie=true` (ou similar).
    3.  Ao repetir as chamadas com `curl`, observa-se que todas as requisiÃ§Ãµes agora sÃ£o atendidas pela mesma rÃ©plica (o mesmo hostname Ã© retornado), pois o Traefik estÃ¡ usando o cookie para "fixar" a sessÃ£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Escalar `whoami` (Sem Sticky):** (Estando em `ch20/exercises`)
    ```powershell
    # Parar/remover versÃ£o anterior do whoami se houver
    # docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml down
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml up -d --scale whoami=3
    ```
2.  **Testar Load Balancing (Sem Sticky):**
    ```powershell
    # Limpar cookie file se existir
    Remove-Item c.txt -ErrorAction SilentlyContinue
    # Fazer 2 chamadas. Use o header 'Hostname' da resposta do whoami
    curl -c c.txt -b c.txt http://whoami.local
    curl -c c.txt -b c.txt http://whoami.local
    # Hostnames devem (provavelmente) ser diferentes.
    ```
3.  **Deployar com Sticky:**
    ```powershell
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik-sticky.yml up -d --scale whoami=3
    ```
4.  **Testar Sticky:**
    ```powershell
    # Limpar cookie file
    Remove-Item c.txt -ErrorAction SilentlyContinue
    # Fazer 2 chamadas
    curl -c c.txt -b c.txt http://whoami.local
    curl -c c.txt -b c.txt http://whoami.local
    # Hostnames agora devem ser os MESMOS.
    ```

---

ğŸ“„ PÃ¡gina 424

ğŸ” Resumo dos principais pontos:

*   Figura 20.15: Confirma sticky sessions (mesmo hostname).
*   Explica funcionamento sticky Traefik: usa cookie (com IP backend?).
*   LimitaÃ§Ã£o: Se container "fixado" morrer, cliente perde sessÃ£o (Traefik roteia para outro).
*   Fim Traefik.
*   **20.5 Understanding the patterns a reverse proxy enables** (Entendendo os padrÃµes que um proxy reverso habilita)
*   Proxy Ã© essencial para produÃ§Ã£o com containers. 3 padrÃµes principais:
    1.  **Virtual Hosting:** MÃºltiplos apps/domÃ­nios (site1.com, site2.com) na mesma porta 80/443 do cluster. Proxy usa `Host` header para rotear. (Figura 20.16).
    2.  **API Gateway / ComposiÃ§Ã£o:** MÃºltiplos microsserviÃ§os backend (containers) compÃµem uma Ãºnica aplicaÃ§Ã£o/domÃ­nio externo. Proxy usa `Path` (ex: `/users`, `/orders`) para rotear para serviÃ§o correto. (Figura 20.17).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica brevemente o mecanismo de sticky session do Traefik (baseado em cookie) e sua limitaÃ§Ã£o inerente no ambiente dinÃ¢mico de contÃªineres (se o contÃªiner de destino some, a sessÃ£o Ã© perdida).
*   A seÃ§Ã£o 20.5 resume os **principais padrÃµes arquiteturais** que sÃ£o habilitados ou facilitados pelo uso de um reverse proxy na frente de aplicaÃ§Ãµes conteinerizadas:
    1.  **Hospedagem Virtual (Virtual Hosting):** Permite que vocÃª execute mÃºltiplos websites ou aplicaÃ§Ãµes, cada um com seu prÃ³prio nome de domÃ­nio (ex: `app1.minhaempresa.com`, `app2.minhaempresa.com`), mas todos compartilhando o mesmo endereÃ§o IP pÃºblico e as mesmas portas padrÃ£o (80/443). O reverse proxy recebe todas as requisiÃ§Ãµes e usa o cabeÃ§alho `Host` enviado pelo navegador para direcionar a requisiÃ§Ã£o ao contÃªiner da aplicaÃ§Ã£o correta. (Figura 20.16 ilustra isso).
    2.  **API Gateway / ComposiÃ§Ã£o de MicrosserviÃ§os:** Em uma arquitetura de microsserviÃ§os, vocÃª pode ter dezenas ou centenas de pequenas APIs internas rodando em contÃªineres. Um reverse proxy pode atuar como um **API Gateway**, expondo um Ãºnico ponto de entrada para o mundo externo (ex: `api.minhaempresa.com`). O proxy entÃ£o usa o **caminho (path)** da URL na requisiÃ§Ã£o (ex: `/api/users/...`, `/api/products/...`) para rotear a chamada para o contÃªiner do microsserviÃ§o apropriado (serviÃ§o de usuÃ¡rio, serviÃ§o de produto). Isso esconde a complexidade interna da arquitetura de microsserviÃ§os dos consumidores externos. (Figura 20.17 ilustra isso).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Os exercÃ­cios que vocÃª fez com Nginx e Traefik usando `whoami.local`, `api.numbers.local`, `image-gallery.local` demonstram o PadrÃ£o 1 (Virtual Hosting).
*   O exercÃ­cio que expÃ´s a API `iotd` como `image-gallery.local/api/image` demonstra o PadrÃ£o 2 (API Gateway/ComposiÃ§Ã£o via Path).

---

ğŸ“„ PÃ¡gina 425

ğŸ” Resumo dos principais pontos:

*   Figura 20.16: Diagrama Virtual Hosting.
*   Figura 20.17: Diagrama API Gateway / Microservice Composition.
*   **PadrÃ£o 3: Strangler Fig (Estrangulamento de Monolito).**
    *   Para migrar/modernizar apps monolÃ­ticos legados.
    *   Proxy fica na frente do monolito (rodando em container).
    *   Identifica-se uma funcionalidade no monolito. Cria-se um *novo* microsserviÃ§o (em container, talvez tecnologia moderna) para essa funcionalidade.
    *   Configura-se o proxy para rotear *apenas* as requisiÃ§Ãµes daquela funcionalidade especÃ­fica para o *novo* microsserviÃ§o. Todas as *outras* requisiÃ§Ãµes continuam indo para o monolito antigo.
    *   Gradualmente, "estrangula-se" o monolito, movendo mais funcionalidades para microsserviÃ§os, atÃ© que monolito possa ser desativado.
*   Diagrama (Figura 20.18): Ilustra padrÃ£o Strangler Fig. Proxy roteia alguns paths para microsserviÃ§os, resto para monolito.
*   PadrÃµes nÃ£o exclusivos; proxy pode suportar todos simultaneamente.
*   **20.6 Lab** (LaboratÃ³rio)
    *   App novo: Calculadora de Pi (`http://localhost:8031/?dp=50000`). Computacionalmente intensivo.
*   **Tarefa:** Rodar app Pi atrÃ¡s de reverse proxy (Nginx ou Traefik).
    1.  App acessÃ­vel via `pi.local` (porta 80).
    2.  Proxy deve **cachear** respostas. RequisiÃ§Ãµes repetidas (mesmo nÂº de casas decimais `dp`) devem ser rÃ¡pidas (servidas do cache).
    3.  Proxy deve adicionar **resiliÃªncia**: servir do cache **stale** se app container cair.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina apresenta o terceiro padrÃ£o importante habilitado pelo reverse proxy: o **PadrÃ£o Strangler Fig** (Figura de Estrangulamento, nome inspirado em figueiras que crescem em volta de outras Ã¡rvores). Este padrÃ£o Ã© uma tÃ©cnica poderosa para **modernizar aplicaÃ§Ãµes monolÃ­ticas legadas gradualmente**.
    *   Coloca-se o proxy na frente do monolito (que jÃ¡ pode estar rodando em um contÃªiner).
    *   Escolhe-se uma funcionalidade do monolito para refatorar ou reescrever como um microsserviÃ§o separado (em seu prÃ³prio contÃªiner).
    *   Configura-se o proxy para interceptar as URLs especÃ­ficas daquela funcionalidade e roteÃ¡-las para o novo microsserviÃ§o. Todo o resto do trÃ¡fego continua indo para o monolito.
    *   Iterativamente, mais funcionalidades sÃ£o migradas para microsserviÃ§os, e o trÃ¡fego Ã© redirecionado pelo proxy. Com o tempo, o monolito original vai sendo "estrangulado" (recebendo cada vez menos trÃ¡fego) atÃ© poder ser completamente desativado. O proxy esconde essa transiÃ§Ã£o dos usuÃ¡rios finais. (Figura 20.18 ilustra).
*   O **Lab do CapÃ­tulo 20** foca nos benefÃ­cios de **caching** e **resiliÃªncia** do proxy. VocÃª usarÃ¡ uma aplicaÃ§Ã£o que calcula Pi (lenta) e a colocarÃ¡ atrÃ¡s de um Nginx (ou Traefik, se preferir) configurado para:
    1.  Servir em `pi.local`.
    2.  Cachear as respostas (baseado na URL, que inclui o parÃ¢metro `dp`). CÃ¡lculos repetidos para o mesmo `dp` devem vir do cache rapidamente.
    3.  Servir respostas do cache mesmo que estejam vencidas (`stale`) se o contÃªiner da aplicaÃ§Ã£o Pi cair (`proxy_cache_use_stale` no Nginx).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 20 (usando Nginx):**
    1.  Navegue para `ch20\lab`. Examine o `docker-compose.yml` para rodar o app `pi-calculator`. Ele deve definir o serviÃ§o `pi-calc` e a rede `ch20`.
    2.  Adicione `pi.local` ao seu arquivo `hosts`.
    3.  Crie um arquivo de configuraÃ§Ã£o Nginx (`pi.local.conf`) na pasta `nginx\sites-available`.
        *   Defina `server_name pi.local;`.
        *   Defina `location / { ... }`.
        *   Dentro do `location`:
            *   `proxy_pass http://pi-calc:8080;` (Verifique a porta interna do app pi-calc).
            *   Adicione diretivas de cache: `proxy_cache MYCACHE;` (Use uma zona de cache definida na config Nginx base, ou defina uma), `proxy_cache_valid 200 1h;` (Cache por 1 hora, por exemplo), `proxy_cache_use_stale error timeout http_502 http_503;`.
            *   Adicione `proxy_set_header Host $host;`.
            *   Adicione `add_header X-Cache $upstream_cache_status;` (VariÃ¡vel Nginx para status do cache).
    4.  Copie `pi.local.conf` para `nginx\sites-enabled\`.
    5.  Certifique-se que Nginx estÃ¡ rodando (`docker-compose -f nginx/... up -d`).
    6.  Rode o app Pi: `docker-compose -f pi-calculator/docker-compose.yml up -d`.
    7.  Reinicie Nginx: `docker-compose -f nginx/... restart nginx`.
    8.  **Teste Cache:**
        *   Acesse `http://pi.local/?dp=10000` no navegador (ou `curl -i`). Anote o tempo. Verifique `X-Cache: MISS`.
        *   Acesse a *mesma URL* novamente. Deve ser muito mais rÃ¡pido. Verifique `X-Cache: HIT`.
    9.  **Teste Stale:**
        *   Pare o container `pi-calc` (`docker stop <container_id>`).
        *   Acesse `http://pi.local/?dp=10000` novamente. Deve funcionar (rÃ¡pido). Verifique `X-Cache: STALE` ou `HIT` (depende se ainda estava fresco).
        *   Acesse com `dp` diferente, ex: `http://pi.local/?dp=5000`. Deve falhar (502 Bad Gateway), pois nÃ£o estava no cache stale.

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch20/lab/README.md

---

ğŸ“„ PÃ¡gina 426

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 20 no GitHub.
*   **CapÃ­tulo 21: Asynchronous communication with a message queue** (ComunicaÃ§Ã£o assÃ­ncrona com uma fila de mensagens)
*   Ãšltimo capÃ­tulo completo. Introduz nova forma de comunicaÃ§Ã£o: **Filas de Mensagens (Message Queues)**.
*   **Desacoplamento:** Componentes nÃ£o se comunicam diretamente, mas via fila.
*   Flexibilidade: Fila entrega para 1 ou muitos destinatÃ¡rios.
*   Foco Cap. 21: CenÃ¡rios habilitados por filas.
    1.  Melhorar performance/escalabilidade.
    2.  Adicionar features com zero downtime.
*   Exemplos com **Redis** e **NATS** (modernas, rodam bem em Docker).
*   **21.1 What is asynchronous messaging?** (O que Ã© mensageria assÃ­ncrona?)
    *   ComunicaÃ§Ã£o **SÃ­ncrona** (padrÃ£o: REST, SOAP, gRPC): Cliente conecta -> Envia request -> **Espera** response -> Desconecta. (Como ligaÃ§Ã£o telefÃ´nica).
    *   **Desafios SÃ­ncronos:** Ambas as partes precisam estar disponÃ­veis; servidor pode estar offline/ocupado; processamento longo pode dar timeout no cliente; tratamento de falhas de rede Ã© complexo (re-tentativa segura?).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 21, o Ãºltimo com conteÃºdo tÃ©cnico principal, abordando **comunicaÃ§Ã£o assÃ­ncrona** atravÃ©s de **filas de mensagens (message queues)**.
*   Contrasta a comunicaÃ§Ã£o **sÃ­ncrona** (a mais comum, como uma chamada API REST onde o cliente espera pela resposta) com a **assÃ­ncrona**.
*   **ComunicaÃ§Ã£o SÃ­ncrona:**
    *   **PrÃ³s:** Simples de entender o fluxo (request -> response).
    *   **Contras:** Fortemente acoplada (cliente depende da disponibilidade *imediata* do servidor), sensÃ­vel a falhas (rede, servidor lento/ocupado), pode bloquear o cliente enquanto espera, difÃ­cil de escalar se servidor for gargalo.
*   **ComunicaÃ§Ã£o AssÃ­ncrona (via Fila):**
    *   Cliente envia mensagem para a fila (operaÃ§Ã£o rÃ¡pida) e *nÃ£o espera* resposta direta. Continua seu trabalho.
    *   Servidor(es) "ouvem" a fila, pegam mensagens quando podem, processam no seu prÃ³prio ritmo.
    *   Se resposta for necessÃ¡ria, servidor envia outra mensagem para outra fila (ou usa padrÃµes request/reply especÃ­ficos da fila).
    *   **PrÃ³s:** Desacoplamento (cliente e servidor nÃ£o precisam estar online ao mesmo tempo), resiliÃªncia (fila guarda mensagem se servidor offline), escalabilidade (mÃºltiplos servidores podem consumir da mesma fila), absorÃ§Ã£o de picos (fila funciona como buffer).
*   O capÃ­tulo usarÃ¡ **Redis** (um data store em memÃ³ria muito rÃ¡pido que tambÃ©m tem funcionalidades de pub/sub e filas) e **NATS** (um sistema de mensageria moderno e de alta performance, parte da CNCF) como exemplos de message brokers que rodam em contÃªineres.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   VocÃª rodarÃ¡ Redis e NATS em contÃªineres.
*   VocÃª usarÃ¡ CLIs ou apps de exemplo para publicar mensagens na fila e outros para subscrever/consumir mensagens da fila.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   SÃ­ncrono: Ligar para alguÃ©m. Precisa que a pessoa atenda na hora, vocÃª espera na linha pela resposta. Se ocupado ou nÃ£o atender, a comunicaÃ§Ã£o falha.
*   AssÃ­ncrono (Fila): Mandar um email (ou mensagem WhatsApp). VocÃª envia a mensagem para a "caixa postal" (fila), nÃ£o espera resposta imediata. O destinatÃ¡rio lÃª e responde quando puder. A comunicaÃ§Ã£o funciona mesmo que um dos dois esteja offline temporariamente.

---

ğŸ“„ PÃ¡gina 427

ğŸ” Resumo dos principais pontos:

*   Diagrama (Figura 21.1): Ilustra comunicaÃ§Ã£o assÃ­ncrona via fila (Client -> Queue <- Server).
*   Velocidade da Fila: Enviar msg Ã© rÃ¡pido (protocolo otimizado, sem processamento complexo na fila). Filas aguentam alta taxa de mensagens.
*   **Exemplo Redis:** Usar Redis como message broker simples (Pub/Sub).
*   **ExercÃ­cio Try-it-now:** Rodar servidor Redis.
    *   1. Criar rede `ch21`.
    *   2. Rodar container Redis: `docker run -d --name redis --network ch21 diamol/redis`.
    *   3. Verificar logs (Redis pronto para conexÃµes).
*   Figura 21.2: Log Redis indicando "Ready to accept connections".

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A Figura 21.1 visualiza o fluxo assÃ­ncrono: o cliente e o servidor interagem com a Fila, nÃ£o diretamente entre si.
*   A pÃ¡gina destaca a **performance** das filas: enviar uma mensagem Ã© tipicamente muito rÃ¡pido, permitindo que o cliente publique muitas mensagens em pouco tempo, sem ficar bloqueado esperando processamento.
*   Inicia o exemplo prÃ¡tico usando **Redis**. Embora Redis seja mais conhecido como cache ou banco de dados em memÃ³ria, ele possui funcionalidades de **Publish/Subscribe (Pub/Sub)** que permitem usÃ¡-lo como um message broker simples.
*   O exercÃ­cio instrui a iniciar um contÃªiner Redis (`diamol/redis`) conectado a uma nova rede Docker (`ch21`). Os logs indicarÃ£o quando o servidor Redis estiver pronto para aceitar conexÃµes de clientes.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Criar Rede:**
    ```powershell
    # Se Linux mode
    docker network create ch21
    # Se Windows mode
    # docker network create -d nat ch21
    ```
2.  **Rodar Redis:**
    ```powershell
    docker container run -d --name redis --network ch21 diamol/redis
    ```
3.  **Verificar Logs:** Espere alguns segundos.
    ```powershell
    docker container logs redis --tail 5 # Ver Ãºltimas linhas
    # Procure por "Ready to accept connections" ou similar.
    ```

---

ğŸ“„ PÃ¡gina 428

ğŸ” Resumo dos principais pontos:

*   ConexÃ£o cliente-fila Ã© rÃ¡pida (envia bytes, espera ACK).
*   Terminologia Mensageria:
    *   **Publisher:** Envia mensagem.
    *   **Subscriber:** Recebe mensagem.
    *   **Channel (Redis):** TÃ³pico/Fila onde mensagens sÃ£o publicadas/subscritas. Separa fluxos de msg.
*   **ExercÃ­cio Try-it-now:** Rodar cliente Redis como Publisher.
    *   Usar `diamol/redis-cli` (CLI para Redis).
    *   Comando publica msg "ping" no channel `channel21`, 50x, a cada 5s, em background.
    *   `docker run -d --name publisher --network ch21 diamol/redis-cli -r 50 -i 5 PUBLISH channel21 ping`.
    *   Verificar logs do publisher (`docker logs publisher`). Deve mostrar `0` (OK) a cada envio.
*   Figura 21.3: Mostra logs do publisher com respostas `0`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz a terminologia padrÃ£o de mensageria: **Publisher** (quem envia), **Subscriber** (quem recebe). No caso do Redis Pub/Sub, a comunicaÃ§Ã£o ocorre atravÃ©s de **Channels** (canais) nomeados. Publishers enviam mensagens para um channel especÃ­fico, e Subscribers se inscrevem para receber mensagens daquele channel.
*   O exercÃ­cio usa uma imagem (`diamol/redis-cli`) que contÃ©m a ferramenta de linha de comando do Redis para atuar como um **Publisher**.
*   O comando `docker run ... diamol/redis-cli -r 50 -i 5 PUBLISH channel21 ping` instrui o cliente Redis a:
    *   Conectar-se ao servidor `redis` (encontrado via DNS na rede `ch21`).
    *   Executar o comando `PUBLISH` no channel `channel21` com a mensagem `ping`.
    *   Repetir (`-r`) 50 vezes.
    *   Com intervalo (`-i`) de 5 segundos entre repetiÃ§Ãµes.
    *   O contÃªiner (`publisher`) roda em background (`-d`).
*   Verificar os logs do `publisher` mostra as respostas do servidor Redis para cada comando `PUBLISH` (o nÃºmero `0` geralmente indica sucesso no Redis CLI para essa operaÃ§Ã£o).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar Publisher:**
    ```powershell
    docker run -d --name publisher --network ch21 diamol/redis-cli -r 50 -i 5 PUBLISH channel21 ping
    ```
2.  **Verificar Logs Publisher:**
    ```powershell
    docker logs publisher
    # VocÃª verÃ¡ vÃ¡rios '0' sendo adicionados a cada 5 segundos.
    ```

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Channel (Redis): Como um quadro de avisos especÃ­fico ou um grupo de chat. VocÃª posta (`PUBLISH`) no quadro/grupo `channel21`, e quem estiver olhando (`SUBSCRIBE`) para `channel21` verÃ¡ a mensagem.

---

ğŸ“„ PÃ¡gina 429

ğŸ” Resumo dos principais pontos:

*   Publisher envia para `channel21`. Subscriber precisa ouvir no mesmo channel.
*   **ExercÃ­cio Try-it-now:** Rodar cliente Redis como Subscriber.
    *   Usar `diamol/redis-cli` novamente.
    *   Rodar interativamente (`-it`) para ver msgs chegando.
    *   Comando se inscreve no `channel21`: `docker run -it --network ch21 --name subscriber diamol/redis-cli SUBSCRIBE channel21`.
*   SaÃ­da do subscriber:
    *   Primeiro, confirmaÃ§Ã£o da subscriÃ§Ã£o.
    *   Depois, a cada 5s (quando publisher envia), mostra detalhes da mensagem recebida ("message", "channel21", "ping").
*   Figura 21.4: Mostra logs do subscriber recebendo as mensagens "ping".

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Agora que temos um Publisher enviando mensagens para o `channel21`, o exercÃ­cio instrui a iniciar um **Subscriber** para receber essas mensagens.
*   Usa-se a mesma imagem `diamol/redis-cli`, mas com o comando `SUBSCRIBE channel21`.
*   A flag `-it` Ã© usada para rodar o contÃªiner `subscriber` em **primeiro plano interativo**, para que possamos ver as mensagens sendo impressas no terminal assim que chegarem.
*   O Redis Pub/Sub funciona da seguinte forma: quando uma mensagem Ã© publicada em um channel, o servidor Redis envia uma cÃ³pia dessa mensagem para **todos** os clientes que estÃ£o atualmente subscritos naquele channel.
*   A saÃ­da no terminal do `subscriber` mostrarÃ¡ primeiro uma confirmaÃ§Ã£o de que ele se conectou e subscreveu com sucesso. Depois, a cada 5 segundos, quando o contÃªiner `publisher` enviar a mensagem "ping", o servidor Redis a encaminharÃ¡ para o `subscriber`, e a CLI do Redis no `subscriber` imprimirÃ¡ os detalhes da mensagem recebida.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Rodar Subscriber:** (Em um novo terminal PowerShell/VS Code, ou no mesmo se o publisher estiver em background)
    ```powershell
    docker run -it --network ch21 --name subscriber diamol/redis-cli SUBSCRIBE channel21
    ```
2.  **Observar:** O terminal ficarÃ¡ ativo. VocÃª verÃ¡ algo como:
    ```
    Reading messages... (press Ctrl-C to quit)
    1) "subscribe"
    2) "channel21"
    3) (integer) 1
    # (ApÃ³s ~5 segundos)
    1) "message"
    2) "channel21"
    3) "ping"
    # (ApÃ³s mais ~5 segundos)
    1) "message"
    2) "channel21"
    3) "ping"
    ... etc ...
    ```
3.  **Parar:** Pressione `Ctrl+C` quando quiser parar de receber mensagens e sair do contÃªiner subscriber. (O publisher continuarÃ¡ rodando em background atÃ© completar 50 mensagens ou ser parado/removido).

---

ğŸ“„ PÃ¡gina 430

ğŸ” Resumo dos principais pontos:

*   Sair do subscriber (`Ctrl+C` ou `docker rm -f subscriber`).
*   Exemplo confirma **comunicaÃ§Ã£o assÃ­ncrona**: Publisher enviou antes de subscriber existir; subscriber continuaria ouvindo se publisher parasse. Componentes desacoplados via fila.
*   PrÃ³ximo: Usar fila para melhorar performance/escala do app To-Do.
*   **21.2 Using a cloud-native message queue** (Usando uma fila de mensagens cloud-native)
    *   Revisita app To-Do (Web + SQL DB). ComunicaÃ§Ã£o sÃ­ncrona Web -> DB limita escala (pool de conexÃµes BD).
    *   **SoluÃ§Ã£o Async para `Save`:**
        *   Web app (usuÃ¡rio adiciona item) -> Publica msg "NewItem" na Fila (rÃ¡pido).
        *   Fila aguenta mais conexÃµes/carga que BD.
        *   Separar handler lÃª msg da fila -> Insere no BD (pode ser escalado/rate-limited).
    *   Tecnologia: **NATS**. Projeto CNCF, maduro, usado. Em memÃ³ria -> muito rÃ¡pido, bom para comunicaÃ§Ã£o inter-container.
*   **ExercÃ­cio Try-it-now:** Rodar NATS server e checar API admin.
    *   Navegar para `ch21/exercises/todo-list`.
    *   Iniciar serviÃ§o `message-queue` (NATS) via Compose: `docker-compose up -d message-queue`.
    *   Verificar logs: `docker logs todo-list_message-queue_1`.
    *   Verificar conexÃµes via API NATS: `curl http://localhost:8222/connz`.
*   Figura 21.5: Mostra log NATS "Server is ready" e saÃ­da JSON vazia de `/connz`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina resume a demonstraÃ§Ã£o do Redis Pub/Sub como exemplo de comunicaÃ§Ã£o assÃ­ncrona e desacoplada.
*   A seÃ§Ã£o 21.2 aplica o conceito a um problema real: o gargalo de escalabilidade no aplicativo To-Do causado pelas chamadas sÃ­ncronas diretas da aplicaÃ§Ã£o web para o banco de dados. Muitas requisiÃ§Ãµes web simultÃ¢neas podem esgotar o pool de conexÃµes do banco de dados.
*   A soluÃ§Ã£o proposta Ã© tornar a operaÃ§Ã£o de **salvar um novo item** assÃ­ncrona:
    1.  A aplicaÃ§Ã£o web, ao receber um novo item, em vez de chamar o banco de dados, rapidamente publica uma mensagem (ex: "Novo Item: Comprar Leite") em uma fila de mensagens.
    2.  Um ou mais componentes separados (message handlers) ouvirÃ£o essa fila, pegarÃ£o as mensagens e realizarÃ£o a inserÃ§Ã£o no banco de dados.
*   Isso desacopla a escrita no banco de dados da requisiÃ§Ã£o web, tornando a UI mais responsiva e permitindo que a inserÃ§Ã£o no banco seja feita de forma controlada (limitando o nÃºmero de handlers ou escalando-os conforme a necessidade), protegendo o banco de dados contra picos de carga.
*   Para este exemplo, serÃ¡ usado o **NATS** como message broker. NATS Ã© conhecido por sua alta performance (por ser primariamente em memÃ³ria) e simplicidade, sendo uma boa escolha para comunicaÃ§Ã£o rÃ¡pida entre contÃªineres.
*   O exercÃ­cio instrui a iniciar o servidor NATS (definido como serviÃ§o `message-queue` em um `docker-compose.yml`) e verificar se ele estÃ¡ rodando checando seus logs e acessando sua API de administraÃ§Ã£o (`/connz` na porta 8222) que mostra as conexÃµes ativas (inicialmente zero).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch21\exercises\todo-list
    ```
2.  **Iniciar NATS:** (Assume que `docker-compose.yml` nesta pasta define o serviÃ§o `message-queue` usando uma imagem NATS)
    ```powershell
    docker-compose up -d message-queue
    ```
3.  **Verificar Logs NATS:**
    ```powershell
    docker logs todo-list_message-queue_1 # Nome pode variar ligeiramente
    # Procure por "Server is ready" ou similar
    ```
4.  **Verificar ConexÃµes NATS:**
    ```powershell
    curl http://localhost:8222/connz
    # Ou Invoke-WebRequest http://localhost:8222/connz | ConvertFrom-Json
    # Deve mostrar algo como {"num_connections":0, ... "connections":[]}
    ```

---

ğŸ“„ PÃ¡gina 431

ğŸ” Resumo dos principais pontos:

*   Figura 21.5: Confirma NATS rodando e sem conexÃµes.
*   NecessÃ¡rio trabalho de dev para mudar para async (app web agora publica msg).
*   **Listagem 21.1:** Trecho cÃ³digo C# (.NET) mostrando publicaÃ§Ã£o da msg `NewItemEvent`.
    ```csharp
    public void AddToDo(ToDo todo)
    {
        // Em vez de chamar DB:
        // _db.Todos.Add(todo); _db.SaveChanges();
        // Agora publica evento na fila NATS:
        MessageQueue.Publish(new NewItemEvent(todo));
        _NewTasksCounter.Inc(); // Incrementa mÃ©trica Prometheus
    }
    ```
*   **NATS Subjects:** NATS usa "subjects" (similar a channels/topics) para rotear msgs. String que identifica tipo/contexto.
*   Exemplo subject: `events.todo.newitem`.
*   Subscribers ouvem subjects especÃ­ficos. Se ninguÃ©m ouvir, msg pode ser descartada (depende da config NATS/Redis).
*   **ExercÃ­cio Try-it-now:** Rodar web app async e DB (sem handler).
    *   1. Iniciar serviÃ§os: `docker-compose up -d todo-web todo-db`.
    *   2. Acessar UI (`http://localhost:8080`), adicionar itens.
*   ObservaÃ§Ã£o: App funciona (sem erros), mas itens *nÃ£o* aparecem na lista.
    *   Causa: List page lÃª do DB; Add page publica msg na fila; nada estÃ¡ lendo da fila e salvando no DB.
*   Figura 21.6: Mostra UI To-Do vazia mesmo apÃ³s adicionar itens.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina mostra um exemplo de como o cÃ³digo da aplicaÃ§Ã£o web (em C#) foi modificado. A funÃ§Ã£o `AddToDo`, em vez de interagir diretamente com o banco de dados, agora chama `MessageQueue.Publish(...)`, enviando um objeto `NewItemEvent` (contendo os dados da nova tarefa) para a fila NATS. (A classe `MessageQueue` seria uma abstraÃ§Ã£o interna do app que encapsula a conexÃ£o e publicaÃ§Ã£o no NATS).
*   Explica que NATS usa **Subjects** para categorizar mensagens. SÃ£o strings hierÃ¡rquicas (ex: `minhaapp.usuario.criado`, `pedidos.regiao-sul.novo`) que os subscribers usam para filtrar quais mensagens querem receber. O exemplo usa `events.todo.newitem`.
*   Relembra que, com NATS e Redis (no modo Pub/Sub simples), se uma mensagem Ã© publicada mas nÃ£o hÃ¡ nenhum subscriber ativo para aquele subject/channel naquele momento, a mensagem Ã© geralmente **descartada**.
*   O exercÃ­cio demonstra o estado intermediÃ¡rio da migraÃ§Ã£o para async:
    1.  Inicia-se a aplicaÃ§Ã£o web (`todo-web`, que agora publica mensagens) e o banco de dados (`todo-db`).
    2.  **NÃ£o** se inicia nenhum componente que leia as mensagens da fila.
    3.  Ao usar a UI para adicionar itens, a operaÃ§Ã£o parece funcionar (a web app publica a mensagem com sucesso), mas os itens nunca chegam ao banco de dados e, portanto, nÃ£o aparecem na lista.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Garantir NATS rodando:** (Comando da p.430).
2.  **Iniciar Web e DB:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    docker-compose up -d todo-web todo-db
    ```
3.  **Verificar:** `docker-compose ps` (deve mostrar `message-queue`, `todo-db`, `todo-web` rodando).
4.  **Testar UI:**
    *   Abra `http://localhost:8080`.
    *   Adicione um item (ex: "Testar NATS"). Clique Save.
    *   A pÃ¡gina da lista Ã© exibida, mas o item **nÃ£o** estÃ¡ lÃ¡. Tente adicionar mais, eles nÃ£o aparecerÃ£o.

---

ğŸ“„ PÃ¡gina 432

ğŸ” Resumo dos principais pontos:

*   Redis/NATS (pub/sub simples) descartam msgs sem subscribers. Outras filas podem armazenar. Novos subscribers sÃ³ recebem msgs futuras.
*   Verificar se msgs estÃ£o sendo publicadas: Usar ferramenta subscriber NATS.
*   **ExercÃ­cio Try-it-now:** Usar `nats-sub` para ver mensagens.
    *   1. Rodar `diamol/nats-sub` container (`todo-sub`), subscrito a `events.todo.newitem`.
        *   `docker run -d --name todo-sub --network todo-list_app-net diamol/nats-sub events.todo.newitem`. (Nota: Nome da rede pode ser sÃ³ `ch21` se nÃ£o usou Compose para NATS).
    *   2. Verificar logs `todo-sub` (inicialmente vazio ou sÃ³ msg de conexÃ£o).
    *   3. Adicionar itens via web UI (`:8080`).
    *   4. Verificar logs `todo-sub` novamente.
*   Figura 21.7: Mostra logs do `nats-sub` recebendo mensagens JSON dos novos itens.
*   Confirma que web app estÃ¡ publicando corretamente. Falta componente handler.
*   **21.3 Consuming and handling messages** (Consumindo e tratando mensagens)
    *   Introduz **Message Handler**: componente que subscreve e processa mensagens.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica o comportamento padrÃ£o de NATS/Redis Pub/Sub (descartar mensagens sem ouvintes) e introduz uma ferramenta simples (`nats-sub`, empacotada na imagem `diamol/nats-sub`) para **verificar** se as mensagens estÃ£o sendo publicadas corretamente pela aplicaÃ§Ã£o web.
*   O exercÃ­cio instrui a rodar um contÃªiner (`todo-sub`) usando essa imagem, dizendo a ele para subscrever ao subject `events.todo.newitem`. O nome da rede (`todo-list_app-net`) Ã© importante para que o `todo-sub` consiga encontrar o servidor NATS (`message-queue`) via DNS. (O nome da rede Ã© prefixado pelo nome do projeto Compose, que parece ser `todo-list` aqui).
*   Ao adicionar novos itens na interface web, o `todo-web` publica as mensagens no NATS. O NATS as entrega para o `todo-sub`. Ao verificar os logs do `todo-sub`, vocÃª verÃ¡ as mensagens chegando (provavelmente em formato JSON), confirmando que a parte de publicaÃ§Ã£o estÃ¡ funcionando.
*   Isso prepara para a seÃ§Ã£o 21.3, que introduzirÃ¡ o componente que realmente *farÃ¡* algo Ãºtil com essas mensagens: o **Message Handler**.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Descobrir Nome da Rede:** (Se iniciou com `docker-compose` da p.431)
    ```powershell
    docker network ls | Select-String "todo-list"
    # Anote o nome completo da rede (ex: todo-list_default ou todo-list_app-net)
    $NetName = "todo-list_default" # Ou o nome correto
    ```
2.  **Rodar NATS Subscriber:**
    ```powershell
    docker container run -d --name todo-sub --network $NetName diamol/nats-sub events.todo.newitem
    ```
3.  **Verificar Logs Iniciais:**
    ```powershell
    docker logs todo-sub # Deve mostrar "Listening on [events.todo.newitem]"
    ```
4.  **Adicionar Itens na Web:** VÃ¡ para `http://localhost:8080` e adicione 1 ou 2 itens.
5.  **Verificar Logs Finais:**
    ```powershell
    docker logs todo-sub
    # Agora deve mostrar as mensagens JSON recebidas, como na Figura 21.7.
    ```

---

ğŸ“„ PÃ¡gina 433

ğŸ” Resumo dos principais pontos:

*   **Message Handler:** Componente que subscreve Ã  fila e processa mensagens. Geralmente 1 handler por tipo de msg (subject/channel).
*   Handler para To-Do: Ouve `events.todo.newitem`, insere dados no BD Postgres.
*   Diagrama (Figura 21.8): Arquitetura completa: Web (Pub) -> NATS -> Handler (Sub) -> DB.
*   **BenefÃ­cio Escala/Performance:** Fila age como buffer.
    *   Pode ter muitos Web containers, mas poucos Handlers.
    *   Handlers formam **grupo de consumidores**: NATS distribui msgs entre handlers do grupo (cada msg processada por 1 handler sÃ³).
    *   Limita carga no BD (nÂº conexÃµes = nÂº handlers).
    *   Se carga alta, msgs acumulam na fila (app continua responsivo), handlers processam depois.
*   Handler (`save-handler`) jÃ¡ construÃ­do/publicado.
*   **ExercÃ­cio Try-it-now:** Rodar o `save-handler`.
    *   1. Iniciar handler: `docker-compose up -d save-handler`.
    *   2. Verificar logs handler (conexÃ£o).
    *   3. Adicionar itens via web UI (`:8080`).
    *   4. Verificar logs handler novamente (processamento msgs).
*   ObservaÃ§Ã£o: **Eventual Consistency**. Item pode nÃ£o aparecer na lista *imediatamente* (UI carrega antes do handler salvar no BD). Precisa refresh.
*   Figura 21.9: Mostra logs do `save-handler` processando as mensagens.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina introduz o componente que faltava: o **Message Handler** (`save-handler` neste caso). Sua funÃ§Ã£o Ã©:
    1.  Subscrever ao subject `events.todo.newitem` na fila NATS.
    2.  Receber as mensagens publicadas pelo `todo-web`.
    3.  Para cada mensagem, extrair os dados do novo item e inseri-los no banco de dados (`todo-db`).
*   A Figura 21.8 mostra o fluxo completo agora.
*   Um conceito importante sobre como NATS (e muitas outras filas) lida com mÃºltiplos subscribers para o *mesmo* trabalho Ã© o **Grupo de Consumidores (Consumer Group)** ou **Fila de Entrega (Queue Delivery)**. Se vocÃª iniciar vÃ¡rias instÃ¢ncias do `save-handler` e todas elas subscreverem ao *mesmo* subject *usando o mesmo nome de grupo* (implÃ­cito ou explÃ­cito), o NATS tratarÃ¡ essas instÃ¢ncias como um pool de workers. Ele **distribuirÃ¡** as mensagens recebidas entre as instÃ¢ncias disponÃ­veis no grupo, garantindo que cada mensagem seja processada por **apenas uma** delas. Isso permite escalar horizontalmente o processamento das mensagens.
*   O exercÃ­cio inicia o `save-handler` (provavelmente definido no mesmo `docker-compose.yml`). Agora, ao adicionar itens na UI, eles devem ser salvos no banco de dados pelo handler e aparecer na lista (apÃ³s um refresh, devido Ã  "consistÃªncia eventual").
*   **ConsistÃªncia Eventual:** Ã‰ um efeito colateral comum de sistemas assÃ­ncronos. Como a UI publica a mensagem e redireciona para a lista *antes* de o handler ter tido tempo de processar a mensagem e salvar no BD, a lista que a UI busca no BD pode ainda nÃ£o conter o item recÃ©m-adicionado. Ele aparecerÃ¡ *eventualmente*, apÃ³s um refresh ou alguma atualizaÃ§Ã£o automÃ¡tica da UI.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Iniciar Handler:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    # Assume que 'save-handler' estÃ¡ definido no docker-compose.yml e usa a rede correta
    docker-compose up -d save-handler
    ```
2.  **Verificar Logs Handler (ConexÃ£o):**
    ```powershell
    docker logs todo-list_save-handler_1 # Nome pode variar
    # Deve mostrar que conectou ao NATS e subscreveu.
    ```
3.  **Adicionar Itens na Web:** VÃ¡ para `http://localhost:8080` e adicione novos itens. Observe que eles podem nÃ£o aparecer na lista imediatamente. Atualize (F5) a pÃ¡gina da lista apÃ³s alguns segundos; eles devem aparecer.
4.  **Verificar Logs Handler (Processamento):**
    ```powershell
    docker logs todo-list_save-handler_1
    # Agora deve mostrar logs indicando que recebeu e processou as mensagens dos itens adicionados.
    ```

---

ğŸ“„ PÃ¡gina 434

ğŸ” Resumo dos principais pontos:

*   Figura 21.9: Confirma logs do handler processando msgs.
*   Explica como escalar handlers (`save-handler`) para aumentar throughput.
*   NATS distribui msgs entre instÃ¢ncias no mesmo grupo (load balancing).
*   **ExercÃ­cio Try-it-now:** Escalar `save-handler` e observar load balancing.
    *   1. Escalar: `docker-compose up -d --scale save-handler=3`.
    *   2. Verificar logs de nova instÃ¢ncia (`_2`).
    *   3. Adicionar itens na web.
    *   4. Verificar logs de *todos* os handlers: `docker-compose logs --tail=1 save-handler`.
*   Figura 21.10: Mostra logs indicando que instÃ¢ncias `_1` e `_2` processaram msgs, `_3` nÃ£o (distribuiÃ§Ã£o pode nÃ£o ser perfeita com poucas msgs).
*   BenefÃ­cio: Aumentou poder de processamento (3x) sem mudar cÃ³digo.
*   PrÃ³ximo: Adicionar *novas features* usando mesmo evento/mensagens.
*   **21.4 Adding new features with message handlers** (Adicionando novas funcionalidades com message handlers)
    *   App To-Do agora Ã© parcialmente **Event-Driven Architecture (EDA)**: publica eventos (`NewItemEvent`) sem saber/se importar quem os consome.
    *   Flexibilidade: Pode-se mudar o que acontece em resposta a um evento sem mudar quem o publica.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina explica como escalar horizontalmente os message handlers para aumentar a capacidade de processamento. O comando `docker-compose up -d --scale save-handler=3` instrui o Compose (e o Docker) a garantir que 3 rÃ©plicas do serviÃ§o `save-handler` estejam rodando.
*   Como todas essas rÃ©plicas se conectam ao NATS e subscrevem ao mesmo subject (`events.todo.newitem`) usando o mesmo grupo de entrega (implÃ­cito ou explÃ­cito), o NATS automaticamente **distribui** as mensagens recebidas entre as 3 instÃ¢ncias. Isso significa que a capacidade de processar novos itens foi efetivamente triplicada sem nenhuma alteraÃ§Ã£o no cÃ³digo, apenas ajustando a contagem de rÃ©plicas.
*   O exercÃ­cio demonstra isso escalando para 3 e verificando (com `docker-compose logs --tail=1 save-handler`) que diferentes instÃ¢ncias recebem e processam as mensagens.
*   A seÃ§Ã£o 21.4 introduz outro benefÃ­cio poderoso da arquitetura orientada a eventos (EDA) que comeÃ§ou a ser implementada: a capacidade de adicionar **novas funcionalidades** simplesmente criando **novos subscribers** para os eventos existentes, sem precisar modificar os componentes que publicam os eventos.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Escalar Handlers:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    docker-compose up -d --scale save-handler=3
    ```
2.  **Verificar (Opcional):**
    ```powershell
    docker-compose ps # Deve mostrar 3 instÃ¢ncias de save-handler
    docker logs todo-list_save-handler_2 # Ver logs da nova instÃ¢ncia
    ```
3.  **Adicionar Itens na Web:** Adicione mais alguns itens em `http://localhost:8080`.
4.  **Verificar Logs DistribuÃ­dos:**
    ```powershell
    docker-compose logs --tail=1 save-handler
    # Veja a saÃ­da. Deve mostrar logs de diferentes instÃ¢ncias (ex: _1, _2, _3) indicando qual processou a Ãºltima msg que recebeu.
    ```

---

ğŸ“„ PÃ¡gina 435

ğŸ” Resumo dos principais pontos:

*   Adicionar feature: Criar novo handler (em novo grupo) que ouve *mesmo* evento (`events.todo.newitem`) mas faz algo *diferente*.
*   Exemplo: `audit-handler`. SÃ³ loga info do novo item (trilha de auditoria).
*   NATS entrega cÃ³pia da msg para CADA grupo de subscriÃ§Ã£o.
    *   Grupo `save-handler` (com 3 instÃ¢ncias): 1 instÃ¢ncia recebe msg.
    *   Grupo `audit-handler` (com 1 instÃ¢ncia): A instÃ¢ncia recebe msg.
*   **ExercÃ­cio Try-it-now:** Deployar `audit-handler`.
    *   Usar override `docker-compose-audit.yml` (define serviÃ§o `audit-handler`).
    *   1. Deploy aditivo: `docker-compose -f ... -f ...audit.yml up -d --scale save-handler=3`. (MantÃ©m escala anterior).
    *   2. Verificar logs `audit-handler`.
    *   3. Adicionar itens na web.
    *   4. Verificar logs `audit-handler` novamente (mostra info do item).
*   Figura 21.11: Mostra logs do `audit-handler`.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina explica como adicionar uma funcionalidade completamente nova (uma trilha de auditoria) ao sistema **sem tocar** nos componentes existentes (`todo-web`, `save-handler`, `todo-db`).
*   A nova funcionalidade Ã© implementada como um novo serviÃ§o, `audit-handler`. Este handler tambÃ©m subscreve ao subject `events.todo.newitem` no NATS.
*   **Comportamento de Entrega NATS (Pub/Sub + Queue Groups):**
    *   Quando `todo-web` publica uma mensagem em `events.todo.newitem`.
    *   NATS vÃª que hÃ¡ dois "grupos" interessados: o grupo dos `save-handler`s e o grupo do `audit-handler`.
    *   NATS entrega a mensagem para **um** membro do grupo `save-handler` (balanceamento de carga dentro do grupo).
    *   NATS entrega uma **cÃ³pia** da mesma mensagem para **um** membro do grupo `audit-handler` (que sÃ³ tem 1 membro).
*   O resultado Ã© que o mesmo evento agora dispara duas aÃ§Ãµes diferentes e independentes (salvar no BD e registrar auditoria).
*   O exercÃ­cio usa um arquivo override (`docker-compose-audit.yml`) para adicionar a definiÃ§Ã£o do serviÃ§o `audit-handler` ao deploy existente. O comando `docker-compose up -d` Ã© "aditivo": ele verifica o estado atual e apenas cria/atualiza o que for novo ou diferente nos arquivos combinados. ApÃ³s o deploy, ao adicionar itens na web, os logs do `audit-handler` devem mostrar as entradas de auditoria.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Deployar Audit Handler:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-audit.yml up -d --scale save-handler=3
    ```
    *Nota: Assume que `docker-compose.yml` Ã© o arquivo base com web, db, nats, save-handler. `docker-compose-audit.yml` sÃ³ adiciona `audit-handler`.*
2.  **Verificar ServiÃ§os:** `docker-compose -f ... -f ... ps` (deve mostrar `audit-handler` rodando).
3.  **Verificar Logs Audit (ConexÃ£o):**
    ```powershell
    docker logs todo-list_audit-handler_1 # Nome pode variar
    ```
4.  **Adicionar Item na Web:** VÃ¡ para `http://localhost:8080` e adicione um novo item (ex: "Verificar Audit").
5.  **Verificar Logs Audit (Processamento):**
    ```powershell
    docker logs todo-list_audit-handler_1
    # Agora deve mostrar o log de auditoria para o item adicionado.
    # Verifique tambÃ©m os logs de um dos save-handlers; ele tambÃ©m deve ter processado.
    ```

---

ğŸ“„ PÃ¡gina 436

ğŸ” Resumo dos principais pontos:

*   Figura 21.11: Confirma logs do audit-handler.
*   Deploy de nova feature (audit) com **zero downtime** para app existente. Pode remover feature parando handler.
*   Flexibilidade EDA: MÃºltiplos subscribers, mÃºltiplos publishers.
*   **Adicionar Novo Publisher:** Criar REST API para adicionar itens (publica mesmo evento `NewItemEvent`).
*   **ExercÃ­cio Try-it-now:** Deployar `todo-api`.
    *   Usar override `docker-compose-api.yml`.
    *   1. Deploy aditivo da API: `docker-compose -f ... -f ...audit.yml -f ...api.yml up -d todo-api`. (SÃ³ inicia/atualiza `todo-api`).
    *   2. Adicionar item via API (`curl POST` para `http://localhost:8081/todo`).
    *   3. Verificar log `audit-handler`.
*   Item adicionado via API deve ser processado por `save-handler` (aparece na UI) e `audit-handler` (aparece nos logs).
*   Figura 21.12: Mostra log de auditoria apÃ³s POST na API.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina reforÃ§a o benefÃ­cio do padrÃ£o EDA habilitado pela fila: novas funcionalidades (como o `audit-handler`) podem ser adicionadas (e removidas) dinamicamente sem interromper ou modificar os componentes existentes.
*   Leva o conceito um passo adiante: assim como pode haver mÃºltiplos *subscribers* para um evento, pode haver mÃºltiplos *publishers* do mesmo evento.
*   O exercÃ­cio introduz um novo componente: `todo-api`. Ã‰ uma API REST simples que expÃµe um endpoint `POST /todo`. Quando recebe uma requisiÃ§Ã£o, ela simplesmente **publica o mesmo evento** `events.todo.newitem` na fila NATS, exatamente como a aplicaÃ§Ã£o web (`todo-web`) faz.
*   O exercÃ­cio usa outro override (`docker-compose-api.yml`) para adicionar este serviÃ§o `todo-api` ao ambiente.
*   ApÃ³s iniciar a API, usa-se `curl` para enviar um novo item atravÃ©s dela. A expectativa Ã© que:
    1.  `todo-api` publique a mensagem no NATS.
    2.  NATS entregue cÃ³pias para o grupo `save-handler` e para o `audit-handler`.
    3.  `save-handler` salve no banco de dados.
    4.  `audit-handler` registre no log.
*   Isso demonstra como diferentes interfaces (Web UI, REST API) podem acionar a mesma lÃ³gica de negÃ³cio (salvar + auditar) de forma desacoplada atravÃ©s da fila de mensagens.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

1.  **Deployar API Handler:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    # Combina base + audit + api overrides, mas sÃ³ afeta 'todo-api' por causa do argumento final
    docker-compose -f docker-compose.yml -f docker-compose-audit.yml -f docker-compose-api.yml up -d todo-api
    ```
    *Nota: Pode ser mais simples/claro rodar `up -d` sem o nome do serviÃ§o para garantir que todos os serviÃ§os definidos nos arquivos combinados estejam rodando.*
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-audit.yml -f docker-compose-api.yml up -d --scale save-handler=3 # Garante tudo rodando
    ```
2.  **Verificar ServiÃ§os:** `docker-compose -f ... ps` (deve mostrar `todo-api` rodando).
3.  **Adicionar Item via API:**
    ```powershell
    curl -X POST http://localhost:8081/todo -d '{\"item\":\"Item via API\"}' -H 'Content-Type: application/json'
    # Ou com PowerShell:
    # Invoke-RestMethod -Method Post -Uri http://localhost:8081/todo -Body '{"item":"Item via API"}' -ContentType 'application/json'
    ```
4.  **Verificar Logs Audit:**
    ```powershell
    docker logs todo-list_audit-handler_1 # Nome pode variar
    # Deve mostrar o log para "Item via API".
    ```
5.  **Verificar UI:** Acesse `http://localhost:8080`. O "Item via API" deve aparecer na lista (apÃ³s refresh talvez).

---

ğŸ“„ PÃ¡gina 437

ğŸ” Resumo dos principais pontos:

*   Figura 21.12: Confirma log de auditoria do item adicionado via API.
*   Confirma que ambos handlers (save, audit) processaram evento da API.
*   Mensageria assÃ­ncrona -> apps flexÃ­veis, escalÃ¡veis, evolutivos.
*   **21.5 Understanding async messaging patterns** (Entendendo padrÃµes de mensageria assÃ­ncrona)
    *   Mensageria Ã© tÃ³pico avanÃ§ado, mas Docker facilita experimentaÃ§Ã£o.
    *   Existem diferentes padrÃµes de envio/recebimento.
    *   PadrÃ£o usado no capÃ­tulo: **Publish-Subscribe (Pub/Sub)**.
        *   0 ou mais subscribers recebem cÃ³pias.
        *   Publisher nÃ£o sabe quem consome, o que faz, quando termina.
*   Diagrama (Figura 21.13): Ilustra Pub/Sub. Publisher -> Queue -> MÃºltiplos Subscribers independentes.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina conclui o exemplo prÃ¡tico, confirmando que a arquitetura agora suporta mÃºltiplas entradas (UI, API) e mÃºltiplos processamentos (Salvar BD, Auditoria) para o mesmo evento, graÃ§as ao desacoplamento da fila NATS.
*   A seÃ§Ã£o 21.5 introduz uma visÃ£o geral de **padrÃµes de mensageria assÃ­ncrona**. O objetivo Ã© mostrar que existem diferentes formas de usar filas, adequadas a diferentes necessidades.
*   O padrÃ£o implÃ­cito nos exemplos com Redis e NATS atÃ© agora Ã© o **Publish/Subscribe (Pub/Sub)**:
    *   Um Publisher envia uma mensagem (evento) para um tÃ³pico/subject/channel.
    *   A fila entrega uma cÃ³pia dessa mensagem para **todos** os Subscribers (ou grupos de subscribers) interessados naquele tÃ³pico.
    *   Os Subscribers processam a mensagem de forma independente.
    *   O Publisher nÃ£o tem conhecimento dos Subscribers nem do resultado do processamento. Ã‰ uma comunicaÃ§Ã£o "disparada" (broadcast-like).
*   A Figura 21.13 ilustra isso: um publisher, mÃºltiplos subscribers recebendo a mesma mensagem. (Nota: A Figura 21.8 mostrava um mix, onde save-handlers eram um grupo e audit-handler era outro, ambos recebendo cÃ³pias).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   O fluxo com `todo-web` publicando e *ambos* `save-handler` (grupo) e `audit-handler` recebendo Ã© um exemplo de Pub/Sub.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Pub/Sub: TransmissÃ£o de RÃ¡dio/TV. A estaÃ§Ã£o (Publisher) transmite o sinal (mensagem) em uma frequÃªncia (tÃ³pico). Qualquer pessoa com um rÃ¡dio/TV (Subscriber) sintonizado na frequÃªncia correta recebe o sinal. A estaÃ§Ã£o nÃ£o sabe quantos ou quem estÃ¡ ouvindo.

---

ğŸ“„ PÃ¡gina 438

ğŸ” Resumo dos principais pontos:

*   Pub-Sub nÃ£o serve para tudo (ex: publisher precisa de resposta/confirmaÃ§Ã£o).
*   **Alternativa: Request-Response Messaging**.
    *   Cliente envia msg de *request* para fila, **espera** (talvez nÃ£o bloqueante) por msg de *response*.
    *   Handler pega request da fila, processa, envia msg de *response* (para fila de resposta ou via correlaÃ§Ã£o).
    *   Fila roteia response de volta para cliente original.
    *   Substitui chamadas sÃ­ncronas (API, RPC), mas com benefÃ­cios async (handler nÃ£o sobrecarrega, cliente pode fazer outras coisas enquanto espera).
*   Diagrama (Figura 21.14): Ilustra Request-Response via Fila. Client -> Fila Request -> Handler -> Fila Response -> Client.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina introduz um segundo padrÃ£o comum de mensageria: **Request-Response** (ou Request/Reply).
*   Este padrÃ£o Ã© usado quando o componente que inicia a comunicaÃ§Ã£o (o "cliente" ou "requisitante") precisa de uma **resposta** especÃ­fica do componente que realiza o trabalho (o "servidor" ou "handler").
*   **Fluxo:**
    1.  O Cliente envia uma mensagem de **RequisiÃ§Ã£o** para uma fila especÃ­fica. A mensagem geralmente contÃ©m os dados da requisiÃ§Ã£o e informaÃ§Ãµes de como/onde enviar a resposta (ex: nome de uma fila de resposta temporÃ¡ria, um ID de correlaÃ§Ã£o).
    2.  O Cliente *pode* esperar pela resposta (de forma bloqueante ou nÃ£o-bloqueante).
    3.  Um Handler pega a mensagem de RequisiÃ§Ã£o da fila.
    4.  O Handler processa a requisiÃ§Ã£o.
    5.  O Handler cria uma mensagem de **Resposta** (com o resultado e/ou o ID de correlaÃ§Ã£o) e a envia para a fila de resposta especificada pelo Cliente (ou uma fila padrÃ£o).
    6.  O Cliente, que estava ouvindo na fila de resposta, recebe a mensagem de Resposta correspondente Ã  sua RequisiÃ§Ã£o original.
*   A Figura 21.14 ilustra esse fluxo de ida e volta atravÃ©s de filas.
*   **Vantagem:** MantÃ©m o desacoplamento e a resiliÃªncia da mensageria (cliente e handler nÃ£o precisam estar online ao mesmo tempo, fila age como buffer), mas permite um fluxo de comunicaÃ§Ã£o bidirecional com resultado. Ã‰ uma alternativa assÃ­ncrona a uma chamada API REST sÃ­ncrona.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Implementar Request/Response geralmente requer mais configuraÃ§Ã£o na fila (ex: filas dedicadas para requests e responses) e lÃ³gica mais complexa no cliente (para correlacionar respostas com requests) e no handler (para enviar a resposta ao local correto). Tecnologias como RabbitMQ tÃªm suporte mais explÃ­cito para este padrÃ£o do que Redis/NATS pub/sub simples.

ğŸ’¡ Analogias ou observaÃ§Ãµes Ãºteis (se houver):

*   Request-Response AssÃ­ncrono: Ã‰ como enviar uma pergunta para um departamento por malote interno (fila de request) com um envelope auto-endereÃ§ado (fila de response) dentro. O departamento responde quando pode, coloca a resposta no envelope e manda de volta pelo malote. VocÃª nÃ£o fica esperando na porta do departamento, mas eventualmente recebe a resposta na sua caixa de entrada.

---

ğŸ“„ PÃ¡gina 439

ğŸ” Resumo dos principais pontos:

*   Figura 21.14: Diagrama Request-Response.
*   Outros padrÃµes: Fire-and-Forget (comando via msg, sem resposta), Scatter-Gather (publica para muitos, coleta/agrega respostas).
*   Outra tecnologia de fila: **RabbitMQ**.
    *   Mais avanÃ§ada que Redis/NATS (usados no cap).
    *   Suporta roteamento complexo (exchanges, bindings).
    *   Suporta **mensagens persistentes** (salvas em disco, sobrevivem a restart da fila/container).
    *   DisponÃ­vel como imagem oficial Docker Hub.
*   ConclusÃ£o: Filas liberam design (EDA, escalÃ¡vel, flexÃ­vel).
*   **21.6 Lab** (LaboratÃ³rio)
    *   Ãšltimo lab. "Sneaky" (com pegadinha).
    *   Objetivo: Adicionar *outro* handler para To-Do: `mutating-handler` (muda texto do item *apÃ³s* ser salvo).
    *   Handler existe: imagem `diamol/ch21-mutating-handler`.
*   **Tarefa:**
    1.  Adicionar serviÃ§o `mutating-handler` ao Compose file.
    2.  Handler ouve evento `events.todo.itemsaved`. **Problema:** Nada publica esse evento ainda. Precisa achar config em componente *existente* (provavelmente `save-handler`) para fazÃª-lo publicar `itemsaved` *apÃ³s* salvar no BD.
    3.  Handler `mutating-handler` tem config padrÃ£o *errada* para endereÃ§o NATS. Precisa achar setting de config (provavelmente env var) para corrigir o endereÃ§o NATS.
*   Dica: Respostas estÃ£o nos Dockerfiles (procurar `ENV`, `CMD`, etc.) ou docs das imagens. Setar valores no Compose file.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina menciona brevemente outros padrÃµes de mensageria e introduz **RabbitMQ** como uma alternativa mais robusta e rica em funcionalidades comparada a Redis (simples Pub/Sub) e NATS (alta performance, mas primariamente em memÃ³ria por padrÃ£o). A principal vantagem do RabbitMQ mencionada Ã© o suporte a **mensagens persistentes** (garantia de que mensagens nÃ£o se perdem se o broker reiniciar) e roteamento avanÃ§ado.
*   O **Lab do CapÃ­tulo 21** apresenta um desafio de configuraÃ§Ã£o e descoberta. O objetivo Ã© integrar um terceiro handler, `mutating-handler`, que modifica os itens da lista *depois* de serem salvos. Isso requer uma cadeia de eventos: Web -> `newitem` -> SaveHandler -> (Salva BD) -> **SaveHandler publica `itemsaved`** -> MutatingHandler -> (Modifica BD).
*   As "pegadinhas" sÃ£o:
    1.  O evento `events.todo.itemsaved` nÃ£o estÃ¡ sendo publicado por padrÃ£o. VocÃª precisa investigar o `save-handler` (provavelmente seu Dockerfile ou alguma documentaÃ§Ã£o implÃ­cita) para descobrir como habilitar a publicaÃ§Ã£o desse evento secundÃ¡rio (talvez via uma variÃ¡vel de ambiente?).
    2.  O novo `mutating-handler` nÃ£o consegue conectar ao NATS por padrÃ£o. VocÃª precisa investigar *esse* handler para descobrir qual variÃ¡vel de ambiente (ou outro mecanismo) ele usa para configurar o endereÃ§o do servidor NATS e defini-la corretamente no `docker-compose.yml` para apontar para o serviÃ§o `message-queue`.
*   Este lab simula uma situaÃ§Ã£o realista de integrar componentes onde a configuraÃ§Ã£o nÃ£o Ã© imediatamente Ã³bvia e exige um pouco de investigaÃ§Ã£o (olhar Dockerfiles, experimentar variÃ¡veis de ambiente).

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **EstratÃ©gia para o Lab 21:**
    1.  **Investigar `save-handler`:**
        *   `docker run --rm diamol/ch21-save-handler --help` (Ver se tem ajuda)
        *   `docker image inspect diamol/ch21-save-handler` (Procurar `ENV` ou `CMD` no histÃ³rico/config)
        *   *HipÃ³tese:* Pode haver uma `ENV PUBLISH_SAVED_EVENT=false` por padrÃ£o.
    2.  **Investigar `mutating-handler`:**
        *   `docker run --rm diamol/ch21-mutating-handler --help`
        *   `docker image inspect diamol/ch21-mutating-handler`
        *   *HipÃ³tese:* Pode haver uma `ENV NATS_URL=nats://default-nats:4222` por padrÃ£o.
    3.  **Modificar `docker-compose.yml`:** (Baseado nos arquivos dos exercÃ­cios anteriores)
        *   No serviÃ§o `save-handler`, adicione `environment:` para habilitar o evento (ex: `PUBLISH_SAVED_EVENT=true`).
        *   Adicione um novo serviÃ§o `mutating-handler`:
            *   `image: diamol/ch21-mutating-handler`
            *   `networks: [app-net]`
            *   `restart: unless-stopped`
            *   Adicione `environment:` para corrigir o NATS URL (ex: `NATS_URL=nats://message-queue:4222`).
            *   Adicione `depends_on: [message-queue]` (boa prÃ¡tica).
    4.  **Rodar e Testar:** `docker-compose up -d`. Adicione um item na UI. Verifique os logs do `save-handler` (deve mencionar `itemsaved`?), logs do `mutating-handler` (deve receber `itemsaved` e logar modificaÃ§Ã£o), e a UI (o texto do item deve mudar apÃ³s alguns segundos/refresh).

*   Link da SoluÃ§Ã£o: https://github.com/sixeyed/diamol/blob/master/ch21/lab/README.md

---

ğŸ“„ PÃ¡gina 440

ğŸ” Resumo dos principais pontos:

*   Link para soluÃ§Ã£o do Lab 21 no GitHub.
*   **CapÃ­tulo 22: Never the end** (Nunca o fim)
    *   Docker Ã© excitante, muitos usos (Git server, migrar legado, cloud-native).
    *   Livro deu a base; capÃ­tulo final dÃ¡ dicas para continuar.
*   **22.1 Run your own proof-of-concept** (Rode sua prÃ³pria prova de conceito)
    *   Melhor forma de aprender mais: aplicar no seu prÃ³prio trabalho/projeto.
    *   PoC para migrar app prÃ³prio para Docker -> consolida habilidades.
    *   Escopo recomendado para PoC impactante (alÃ©m do bÃ¡sico `build`/`run`):
        *   Dockerizar >1 componente (usar Compose, Cap 10).
        *   Seguir boas prÃ¡ticas (multi-stage, otimizaÃ§Ã£o, golden images, Cap 17).
        *   Incluir observabilidade (logs centralizados Cap 19, mÃ©tricas Cap 9).
        *   Montar pipeline CI/CD simples (Jenkins em container, Cap 11).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Inicia o CapÃ­tulo 22, a conclusÃ£o do livro. O objetivo Ã© fornecer orientaÃ§Ã£o sobre os **prÃ³ximos passos** apÃ³s aprender os fundamentos do Docker.
*   A seÃ§Ã£o 22.1 sugere fortemente que a melhor maneira de solidificar e expandir o conhecimento Ã© atravÃ©s da **prÃ¡tica em um projeto real**: uma **Prova de Conceito (Proof of Concept - PoC)**.
*   A ideia Ã© escolher uma aplicaÃ§Ã£o existente (ou uma nova simples) do seu trabalho ou interesse pessoal e passar pelo processo de **conteinerizÃ¡-la**, aplicando as tÃ©cnicas aprendidas ao longo do livro.
*   Para que a PoC seja convincente e demonstre o valor real do Docker, o autor recomenda ir alÃ©m do bÃ¡sico `docker build` e `docker run`, e incluir no escopo da PoC:
    *   Gerenciamento de mÃºltiplos componentes com **Docker Compose**.
    *   AplicaÃ§Ã£o de **boas prÃ¡ticas** na construÃ§Ã£o das imagens (multi-stage, otimizaÃ§Ãµes).
    *   ImplementaÃ§Ã£o de **observabilidade** (coleta de logs e mÃ©tricas).
    *   CriaÃ§Ã£o de um **pipeline automatizado** de CI/CD.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Planejar sua PoC:**
    1.  **Escolher App:** Selecione uma aplicaÃ§Ã£o relativamente simples que vocÃª conheÃ§a bem (pode ser .NET, Java, Node, Python, etc.). Idealmente, uma com 2-3 componentes (ex: Web API + BD, ou Web UI + Web API).
    2.  **Dockerizar Componentes:** Crie `Dockerfile`s (multi-stage, otimizados) para cada componente. Use golden images se apropriado. Adicione `HEALTHCHECK`s.
    3.  **Definir com Compose:** Crie `docker-compose.yml` (e overrides `dev`/`test`?) para rodar a aplicaÃ§Ã£o completa. Configure redes, volumes/bind mounts, configs/secrets.
    4.  **Adicionar Observabilidade:** Instrumente os apps para expor mÃ©tricas Prometheus. Adicione serviÃ§os Prometheus/Grafana ao Compose. Crie um dashboard bÃ¡sico. Configure driver `fluentd` e rode EFK stack (ou outro backend) para logs.
    5.  **Automatizar CI/CD:** Crie um `Jenkinsfile` (ou use GitHub Actions, etc.). Configure pipeline para buildar, testar (mesmo que um teste simples), escanear (opcional), fazer push para um registro (Hub ou local), e talvez deployar para um ambiente "teste" (pode ser no mesmo host Docker via Compose com `-p`).

---

ğŸ“„ PÃ¡gina 441

ğŸ” Resumo dos principais pontos:

*   PoC nÃ£o precisa ser enorme (ex: 5 dias, app simples). Pode ser projeto paralelo inicial.
*   Se Docker bloqueado no trabalho -> Tentar em casa (ex: com Raspberry Pi).
*   **22.2 Make a case for Docker in your organization** (Apresente um caso para Docker na sua organizaÃ§Ã£o)
    *   Docker Ã© mudanÃ§a grande, nem todos aceitam fÃ¡cil.
    *   Argumentos chave para diferentes stakeholders:
        *   **Desenvolvedores:** Ambiente consistente (igual prod), menos setup, mesmas ferramentas Ops -> colaboraÃ§Ã£o.
        *   **OperaÃ§Ãµes/Admins:** Ferramentas/processos padrÃ£o para todos apps, API padrÃ£o (log, mÃ©trica, config), deploy/rollback automatizado, releases mais frequentes/seguros.
        *   **DBAs:** Self-service de BDs para Dev/Teste via containers. CI/CD para schemas BD.
        *   **SeguranÃ§a:** Secure supply chain (golden image, scan, sign). Ferramentas runtime (Aqua, Twistlock) para monitorar/bloquear ataques.
        *   **NegÃ³cio/Product Owners:** Entendem linguagem de releases. Docker -> melhor qualidade, entrega mais rÃ¡pida de features.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   A pÃ¡gina continua as sugestÃµes para a PoC, dimensionando o esforÃ§o e sugerindo alternativas se houver resistÃªncia inicial no ambiente de trabalho.
*   A seÃ§Ã£o 22.2 foca em como **"vender" a ideia do Docker** dentro da sua organizaÃ§Ã£o, reconhecendo que diferentes grupos (stakeholders) tÃªm diferentes prioridades e preocupaÃ§Ãµes. O autor fornece argumentos direcionados para cada grupo:
    *   **Desenvolvedores:** Focar na eliminaÃ§Ã£o de problemas de ambiente ("funciona na minha mÃ¡quina"), na facilidade de setup e na colaboraÃ§Ã£o com Ops.
    *   **OperaÃ§Ãµes:** Destacar a padronizaÃ§Ã£o de ferramentas e processos, a automaÃ§Ã£o de deploy/rollback e a melhoria na observabilidade e frequÃªncia de releases.
    *   **DBAs:** Mostrar como contÃªineres agilizam o provisionamento de bancos de dados para ambientes nÃ£o-produtivos e como podem modernizar o desenvolvimento de BDs.
    *   **SeguranÃ§a:** Enfatizar as prÃ¡ticas de "secure supply chain" (imagens base controladas, varredura, assinatura) e a possibilidade de usar ferramentas de seguranÃ§a especÃ­ficas para contÃªineres em runtime.
    *   **NegÃ³cio:** Traduzir os benefÃ­cios tÃ©cnicos em valor de negÃ³cio: software de maior qualidade entregue mais rapidamente ao mercado.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Use os resultados da sua PoC (aplicaÃ§Ã£o rodando, dashboard Grafana, pipeline CI/CD) para **demonstrar** esses benefÃ­cios concretamente para os diferentes stakeholders na sua empresa. Prepare uma apresentaÃ§Ã£o focada nos ganhos relevantes para cada grupo.

---

ğŸ“„ PÃ¡gina 442

ğŸ” Resumo dos principais pontos:

*   Argumentos para stakeholders (cont.):
    *   **GerÃªncia SÃªnior:** Foco no negÃ³cio + OrÃ§amento TI. Containers -> consolidaÃ§Ã£o hardware -> economia de custos (servidores, licenÃ§as OS).
    *   **GerÃªncia TI:** TendÃªncia de mercado (containers nÃ£o vÃ£o sumir), nuvens oferecem K8s gerenciado. Adotar Docker moderniza stack, mantÃ©m times felizes/atualizados.
*   **22.3 Plan the path to production** (Planeje o caminho para produÃ§Ã£o)
    *   Importante ter visÃ£o clara de onde se quer chegar com Docker.
    *   Revisita 5 cenÃ¡rios do Cap 1 (migrar legado, cloud-native, serverless, etc.). PoC deve se alinhar a um objetivo.
    *   DecisÃ£o maior: **Swarm vs. Kubernetes** para orquestraÃ§Ã£o.
        *   Nuvem: K8s gerenciado costuma ser mais fÃ¡cil.
        *   Datacenter: Swarm mais fÃ¡cil de operar.
        *   K8s: Curva aprendizado Ã­ngreme, YAML diferente, mas mais features.
*   **22.4 Meet the Docker community** (ConheÃ§a a comunidade Docker)
    *   NÃ£o estÃ¡ sozinho; comunidade Docker Ã© grande, ativa, amigÃ¡vel. Online e presencial.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Continua os argumentos para convencer diferentes nÃ­veis da organizaÃ§Ã£o sobre o Docker:
    *   **Alta GerÃªncia:** Focar nos benefÃ­cios financeiros (reduÃ§Ã£o de custos de infraestrutura e licenciamento) e de negÃ³cio (agilidade).
    *   **Gestores de TI:** Posicionar Docker como uma modernizaÃ§Ã£o necessÃ¡ria para acompanhar o mercado, atrair/reter talentos e se preparar para a nuvem.
*   A seÃ§Ã£o 22.3 enfatiza a necessidade de **planejamento estratÃ©gico** ao adotar Docker, especialmente pensando no caminho para produÃ§Ã£o. Ã‰ preciso definir o objetivo (ex: migrar X aplicaÃ§Ãµes legadas, construir novo produto Y como cloud-native) e tomar a decisÃ£o chave sobre qual **orquestrador** usar em produÃ§Ã£o: Swarm ou Kubernetes. A escolha depende dos fatores jÃ¡ discutidos (nuvem vs datacenter, complexidade vs features, curva de aprendizado).
*   A seÃ§Ã£o 22.4 introduz a **comunidade Docker** como um recurso valioso.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **Planejamento:** Defina qual(is) aplicaÃ§Ã£o(Ãµes) seriam os melhores candidatos para sua PoC e qual o objetivo de longo prazo (ex: rodar no AKS do Azure? Rodar no datacenter com Swarm?). Isso influenciarÃ¡ as tecnologias a serem focadas apÃ³s o livro.
*   **Comunidade:** Explore os links fornecidos na prÃ³xima pÃ¡gina para encontrar fÃ³runs, grupos e eventos.

---

ğŸ“„ PÃ¡gina 443

ğŸ” Resumo dos principais pontos:

*   **Recursos da Comunidade Docker:**
    *   Slack ComunitÃ¡rio Docker: `https://dockr.ly/slack`.
    *   Meetups (presenciais/virtuais): `https://events.docker.com`.
    *   Docker Captains (experts reconhecidos): `www.docker.com/community/captains`.
    *   DockerCon (conferÃªncia): `https://dockercon.com`.
*   **Contato do Autor:**
    *   Slack: `@eltonstoneman`.
    *   Twitter: `@EltonStoneman`.
*   Encerramento do CapÃ­tulo 22.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina fornece links concretos para se conectar com a **comunidade Docker**. Participar da comunidade Ã© uma Ã³tima maneira de tirar dÃºvidas, aprender com os outros, compartilhar suas prÃ³prias experiÃªncias e se manter atualizado sobre as novidades do ecossistema.
*   Os recursos listados incluem:
    *   **Slack:** Canal de chat em tempo real com milhares de usuÃ¡rios Docker.
    *   **Meetups:** Grupos locais (ou virtuais) que se reÃºnem para discutir Docker.
    *   **Docker Captains:** Programa que reconhece indivÃ­duos que sÃ£o especialistas e contribuem ativamente para a comunidade (muitos tÃªm blogs, canais no YouTube, etc.).
    *   **DockerCon:** A conferÃªncia oficial do Docker, com palestras tÃ©cnicas, workshops e anÃºncios.
*   O autor tambÃ©m fornece seus prÃ³prios contatos para interaÃ§Ã£o.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   **AÃ§Ã£o:** Explore os links! Entre no Slack, procure por meetups na sua regiÃ£o (ou virtuais), siga alguns Docker Captains no Twitter ou seus blogs.

---

ğŸ“„ PÃ¡gina 444

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco, possivelmente antes do Ã­ndice.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   PÃ¡gina em branco.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

ğŸ“„ PÃ¡ginas 455 a 463

ğŸ” Resumo dos principais pontos:

*   Estas pÃ¡ginas contÃªm o **Ã­ndice remissivo** do livro.
*   Listam termos chave, comandos, conceitos e os nÃºmeros das pÃ¡ginas onde sÃ£o mencionados.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   O Ã­ndice remissivo Ã© uma ferramenta padrÃ£o em livros tÃ©cnicos para ajudar o leitor a localizar rapidamente informaÃ§Ãµes sobre um tÃ³pico especÃ­fico. Ele lista palavras-chave em ordem alfabÃ©tica e indica as pÃ¡ginas onde essas palavras aparecem de forma significativa.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Se vocÃª quiser reler sobre "bind mounts", procure por "bind mounts" no Ã­ndice e vÃ¡ para as pÃ¡ginas indicadas (ex: 85-93, 113).
*   Se quiser rever o comando `docker stack deploy`, procure por "stack deploy command" e veja as pÃ¡ginas (ex: 231, 257).

---

ğŸ“„ PÃ¡gina 464

ğŸ” Resumo dos principais pontos:

*   **RELATED MANNING TITLES** (TÃ­tulos Relacionados da Manning)
*   Lista outros livros da editora Manning sobre temas relacionados:
    *   Kubernetes in Action, Second Edition (Marko LukÅ¡a)
    *   Docker in Action, Second Edition (Jeff Nickoloff, Stephen Kuenzli)
    *   Docker in Practice, Second Edition (Ian Miell, Aidan Hobson Sayers)
    *   Cloud Native Patterns (Cornelia Davis)
*   Inclui ISBN, nÃºmero de pÃ¡ginas, preÃ§o e data de publicaÃ§Ã£o (aproximada).
*   Link para informaÃ§Ãµes de compra: www.manning.com.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Esta pÃ¡gina funciona como publicidade para outros livros da mesma editora que complementam ou aprofundam os tÃ³picos abordados. Se vocÃª gostou deste livro e quer aprender mais sobre Kubernetes, ou ter uma visÃ£o diferente sobre Docker, ou entender padrÃµes de arquitetura cloud-native, essas sÃ£o sugestÃµes da editora.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando. Apenas referÃªncias para leitura adicional.

---

ğŸ“„ PÃ¡gina 465

ğŸ” Resumo dos principais pontos:

*   Texto curto, provavelmente da contracapa ou material promocional.
*   "Docker makes it easy to adopt architectures which are usually complex." (Docker facilita adotar arquiteturas geralmente complexas).
*   "A reverse proxy increases performance and reliability, and you can run it in a container alongside your web apps." (Um proxy reverso aumenta performance e confiabilidade, e vocÃª pode rodÃ¡-lo em um contÃªiner junto com seus apps web).

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Este Ã© um pequeno trecho de texto, provavelmente usado em marketing, que resume dois dos benefÃ­cios e padrÃµes discutidos no livro: a capacidade do Docker de simplificar arquiteturas complexas e o uso de um reverse proxy (rodando em contÃªiner) para melhorar performance e confiabilidade, como visto no CapÃ­tulo 20.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando. Apenas um reforÃ§o de conceitos.

---

ğŸ“„ PÃ¡gina 466

ğŸ” Resumo dos principais pontos:

*   PÃ¡gina em branco.

ğŸ§  ExplicaÃ§Ã£o detalhada e didÃ¡tica:

*   Ãšltima pÃ¡gina do PDF, provavelmente em branco.

ğŸ’» Exemplos prÃ¡ticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicÃ¡vel nesta pÃ¡gina.

---

âœ… Fim da leitura. Todas as pÃ¡ginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code.

