**You**

🧠 Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), página por página.

Para cada página, siga este modelo de resposta:

📄 Página X

🔍 Resumo dos principais pontos:

\[bullet points com os tópicos abordados na página\]

🧠 Explicação detalhada e didática:

\[explique os conceitos em português claro, com boa didática, como um professor\]

💻 Exemplos práticos (para meu ambiente Windows):

\[comandos ou trechos de código aplicáveis no Windows, usando Docker Desktop e Visual Studio Code\]

💡 Analogias ou observações úteis (se houver):

\[comparações simples ou observações para reforçar o entendimento\]

✅ Importante: percorra automaticamente todas as páginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "próxima".

Quando terminar todas as páginas, diga:

"✅ Fim da leitura. Todas as páginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Olá Alexandre! Combinado, farei a leitura completa do PDF "Learn Docker in a Month of Lunches", página por página, seguindo o modelo que você pediu. Vamos começar!

---

📄 Página 1

🔍 Resumo dos principais pontos:

*   Página em branco, provavelmente uma página de rosto ou separadora inicial do livro.

🧠 Explicação detalhada e didática:

*   Esta página está vazia. Em livros impressos ou digitais, páginas em branco como esta são frequentemente usadas no início, no fim ou entre seções importantes para separar conteúdos, melhorar a diagramação ou marcar o começo de uma nova parte.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

💡 Analogias ou observações úteis (se houver):

*   Pense nela como a capa de uma pasta antes de você ver os documentos dentro.

---

📄 Página 2

🔍 Resumo dos principais pontos:

*   Define contêineres como ambientes isolados e eficientes.
*   Menciona que múltiplos contêineres podem rodar em um único computador, compartilhando o mesmo SO, CPU e memória.

🧠 Explicação detalhada e didática:

*   A ideia central aqui é apresentar o conceito fundamental dos contêineres Docker. Eles são como "caixas" isoladas onde seus aplicativos podem rodar. O ponto chave é a eficiência: diferente de máquinas virtuais (que precisam de um sistema operacional completo para cada uma), os contêineres compartilham o sistema operacional da máquina hospedeira. Isso significa que eles usam muito menos recursos (CPU, memória), permitindo que você rode muitos mais contêineres em comparação com máquinas virtuais no mesmo hardware. O isolamento garante que o que acontece dentro de um contêiner não afeta os outros.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando específico nesta página, mas o conceito é a base do Docker Desktop que você usa. Ao rodar `docker ps` no PowerShell ou terminal do VS Code, você verá contêineres rodando, todos compartilhando seu Windows (ou o kernel Linux dentro do Docker Desktop, dependendo do modo) mas isolados entre si.

💡 Analogias ou observações úteis (se houver):

*   Analogia: Pense em apartamentos em um prédio. Cada apartamento é um contêiner isolado, mas todos compartilham a estrutura do prédio (sistema operacional), eletricidade (CPU) e água (memória). Máquinas virtuais seriam como casas separadas, cada uma com sua própria fundação, estrutura e utilidades.

---

📄 Página 3

🔍 Resumo dos principais pontos:

*   Título do livro: Learn Docker in a Month of Lunches.

🧠 Explicação detalhada e didática:

*   Esta página exibe apenas o título principal do livro. A série "Month of Lunches" da Manning é conhecida por dividir tópicos complexos em lições diárias que podem ser aprendidas durante o horário de almoço, focando na prática. Este livro aplica essa metodologia ao Docker.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

💡 Analogias ou observações úteis (se houver):

*   É a "capa" do conteúdo que estamos prestes a explorar.

---

📄 Página 4

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Assim como a página 1, esta é uma página em branco, servindo provavelmente como separador visual ou para ajuste de paginação antes do início do conteúdo mais formal do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 5

🔍 Resumo dos principais pontos:

*   Repete o título do livro: Learn Docker in a Month of Lunches.
*   Nome do autor: ELTON STONEMAN.
*   Nome da editora: MANNING.
*   Localização da editora: SHELTER ISLAND.

🧠 Explicação detalhada e didática:

*   Esta é a página de título formal do livro. Ela apresenta as informações essenciais: o que é o livro (título), quem o escreveu (autor) e quem o publicou (editora e local). Elton Stoneman é uma figura conhecida na comunidade Docker.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 6

🔍 Resumo dos principais pontos:

*   Informações de direitos autorais e publicação da Manning Publications Co. (©2020).
*   Detalhes sobre como obter o livro e descontos por quantidade.
*   Declaração sobre o uso de marcas registradas.
*   Política de uso de papel reciclado e sem cloro.
*   Créditos da equipe editorial (editores, revisores, designers, etc.).
*   ISBN: 9781617297052.
*   Informação de que foi impresso nos EUA.

🧠 Explicação detalhada e didática:

*   Esta página contém os detalhes legais e editoriais do livro. Inclui informações sobre direitos autorais (copyright), como comprar o livro, políticas ambientais da editora, e reconhece as várias pessoas envolvidas na produção do livro (edição, revisão técnica, design, etc.). O ISBN é um identificador único para esta edição do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página. O ISBN pode ser útil se você quiser procurar exatamente esta edição em livrarias ou bibliotecas.

---

📄 Página 7

🔍 Resumo dos principais pontos:

*   Dedicatória do autor (Elton Stoneman).
*   O autor dedica o livro à sua esposa, Nikki, e aos seus filhos, Jackson e Eris.
*   Menciona que escreveu o livro em um celeiro em Gloucestershire, Inglaterra.

🧠 Explicação detalhada e didática:

*   Esta é a página de dedicatória. É um espaço pessoal onde o autor agradece ou homenageia pessoas importantes para ele, frequentemente familiares que o apoiaram durante o processo de escrita. Também dá um toque humano, mencionando o local onde o livro foi escrito.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 8

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Mais uma página em branco, provavelmente usada para separar a dedicatória do início do sumário ou conteúdo principal.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 9

🔍 Resumo dos principais pontos:

*   Início do sumário breve ("brief contents").
*   **PARTE 1: UNDERSTANDING DOCKER CONTAINERS AND IMAGES** (Entendendo Contêineres e Imagens Docker).
    *   Capítulo 1: Before you begin (Antes de começar).
    *   Capítulo 2: Understanding Docker and running Hello World (Entendendo Docker e rodando Hello World).
    *   Capítulo 3: Building your own Docker images (Construindo suas próprias imagens Docker).
    *   Capítulo 4: Packaging applications from source code into Docker Images (Empacotando aplicações a partir do código-fonte em Imagens Docker).
    *   Capítulo 5: Sharing images with Docker Hub and other registries (Compartilhando imagens com Docker Hub e outros registros).
    *   Capítulo 6: Using Docker volumes for persistent storage (Usando volumes Docker para armazenamento persistente).
*   **PARTE 2: RUNNING DISTRIBUTED APPLICATIONS IN CONTAINERS** (Rodando Aplicações Distribuídas em Contêineres).
    *   Capítulo 7: Running multi-container apps with Docker Compose (Rodando apps multi-contêiner com Docker Compose).
    *   Capítulo 8: Supporting reliability with health checks and dependency checks (Suportando confiabilidade com verificações de saúde e dependência).
    *   Capítulo 9: Adding observability with containerized monitoring (Adicionando observabilidade com monitoramento conteinerizado).
    *   Capítulo 10: Running multiple environments with Docker Compose (Rodando múltiplos ambientes com Docker Compose).
    *   Capítulo 11: Building and testing applications with Docker and Docker Compose (Construindo e testando aplicações com Docker e Docker Compose).

🧠 Explicação detalhada e didática:

*   Esta página inicia o sumário resumido, dando uma visão geral da estrutura do livro. Ele é dividido em partes temáticas, e aqui vemos as duas primeiras.
    *   A Parte 1 foca nos fundamentos do Docker: o que são contêineres e imagens, como criar suas próprias imagens (inclusive a partir do código-fonte), como compartilhá-las e como lidar com dados que precisam persistir (volumes). É a base essencial.
    *   A Parte 2 avança para aplicações mais complexas, compostas por múltiplos contêineres que precisam trabalhar juntos. Introduz o Docker Compose como ferramenta para orquestrar esses apps, e aborda temas importantes para rodá-los de forma confiável, como verificações de saúde, monitoramento e gerenciamento de diferentes ambientes (desenvolvimento, teste, etc.).

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando direto aqui, mas este sumário te ajuda a localizar rapidamente onde encontrar informações sobre tópicos específicos, como `Docker Compose` (Capítulo 7) ou `Volumes` (Capítulo 6).

---

📄 Página 10

🔍 Resumo dos principais pontos:

*   Continuação do sumário breve ("brief contents").
*   **PARTE 3: RUNNING AT SCALE WITH A CONTAINER ORCHESTRATOR** (Rodando em Escala com um Orquestrador de Contêineres).
    *   Capítulo 12: Understanding orchestration: Docker Swarm and Kubernetes (Entendendo orquestração: Docker Swarm e Kubernetes).
    *   Capítulo 13: Deploying distributed applications as stacks in Docker Swarm (Implantando aplicações distribuídas como stacks no Docker Swarm).
    *   Capítulo 14: Automating releases with upgrades and rollbacks (Automatizando lançamentos com atualizações e reversões).
    *   Capítulo 15: Configuring Docker for secure remote access and CI/CD (Configurando Docker para acesso remoto seguro e CI/CD).
    *   Capítulo 16: Building Docker images that run anywhere: Linux, Windows, Intel, and Arm (Construindo imagens Docker que rodam em qualquer lugar: Linux, Windows, Intel e Arm).
*   **PARTE 4: GETTING YOUR CONTAINERS READY FOR PRODUCTION** (Preparando Seus Contêineres para Produção).
    *   Capítulo 17: Optimizing your Docker images for size, speed, and security (Otimizando suas imagens Docker para tamanho, velocidade e segurança).
    *   Capítulo 18: Application configuration management in containers (Gerenciamento de configuração de aplicação em contêineres).
    *   Capítulo 19: Writing and managing application logs with Docker (Escrevendo e gerenciando logs de aplicação com Docker).
    *   Capítulo 20: Controlling HTTP traffic to containers with a reverse proxy (Controlando tráfego HTTP para contêineres com um proxy reverso).
    *   Capítulo 21: Asynchronous communication with a message queue (Comunicação assíncrona com uma fila de mensagens).
    *   Capítulo 22: Never the end (Nunca o fim).

🧠 Explicação detalhada e didática:

*   Esta página conclui o sumário resumido, apresentando as Partes 3 e 4.
    *   A Parte 3 trata de rodar aplicações em múltiplos servidores (um cluster) usando orquestradores como Docker Swarm e Kubernetes. Cobre como implantar, atualizar, reverter e acessar remotamente essas aplicações em escala. O Capítulo 16 é particularmente interessante, pois aborda como criar imagens que funcionam em diferentes sistemas operacionais e arquiteturas de CPU.
    *   A Parte 4 foca em refinar as aplicações e imagens para o ambiente de produção. Tópicos incluem otimização de imagens, gerenciamento de configurações e logs de forma eficiente, controle de tráfego com proxies reversos e introdução a padrões de comunicação assíncrona (filas de mensagens). O último capítulo oferece conselhos para continuar a jornada com Docker.

💻 Exemplos práticos (para meu ambiente Windows):

*   Novamente, sem comandos diretos. Este sumário mostra onde aprender sobre `Docker Swarm` (Capítulo 12-15), `Kubernetes` (Capítulo 12), otimização de imagens (Capítulo 17), gerenciamento de logs (Capítulo 19) e proxies reversos (Capítulo 20).

---

📄 Página 11

🔍 Resumo dos principais pontos:

*   Início do sumário detalhado ("contents").
*   Links para prefácio (xvii), agradecimentos (xviii), sobre este livro (xix), sobre o autor (xxiii).
*   **PARTE 1: UNDERSTANDING DOCKER CONTAINERS AND IMAGES** (detalhado).
    *   **Capítulo 1: Before you begin**
        *   1.1 Por que contêineres dominarão o mundo (cenários: migração para nuvem, modernização de apps legados, construção de apps cloud-native, inovação técnica como Serverless, transformação digital com DevOps).
        *   1.2 Este livro é para você?
        *   1.3 Criando seu ambiente de laboratório (Instalando Docker, verificando a instalação, baixando o código-fonte, lembrando comandos de limpeza).
        *   1.4 Sendo imediatamente eficaz.

🧠 Explicação detalhada e didática:

*   Esta página inicia o sumário detalhado, começando com os links para as seções introdutórias (prefácio, agradecimentos, etc.) e detalhando o conteúdo da Parte 1, especificamente o Capítulo 1.
*   O Capítulo 1 serve como introdução, justificando a importância dos contêineres em diversos cenários de TI modernos, desde mover aplicações existentes para a nuvem até criar novas aplicações e adotar práticas DevOps. Ele também define o público-alvo do livro e guia o leitor na configuração do ambiente necessário (instalação do Docker, download dos exemplos) para acompanhar os exercícios práticos. A seção final reforça a abordagem prática do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   O Capítulo 1.3 guiará na instalação do **Docker Desktop para Windows**, na verificação com comandos como `docker version` e `docker-compose version` no PowerShell ou terminal do VS Code, e no download do código-fonte do livro (provavelmente via `git clone` ou download de ZIP). Comandos de limpeza como `docker container rm -f $(docker container ls -aq)` também serão apresentados.

---

📄 Página 12

🔍 Resumo dos principais pontos:

*   Continuação do sumário detalhado da Parte 1.
    *   **Capítulo 2: Understanding Docker and running Hello World**
        *   Rodando Hello World em um contêiner.
        *   O que é um contêiner?
        *   Conectando a um contêiner como um computador remoto.
        *   Hospedando um site em um contêiner.
        *   Entendendo como o Docker roda contêineres.
        *   Lab: Explorando o sistema de arquivos do contêiner.
    *   **Capítulo 3: Building your own Docker images**
        *   Usando uma imagem de contêiner do Docker Hub.
        *   Escrevendo seu primeiro Dockerfile.
        *   Construindo sua própria imagem de contêiner.
        *   Entendendo imagens Docker e camadas de imagem.
        *   Otimizando Dockerfiles para usar o cache de camadas.
        *   Lab.
    *   **Capítulo 4: Packaging applications from source code into Docker Images**
        *   Quem precisa de um servidor de build quando se tem um Dockerfile?
        *   Passo a passo da aplicação: código-fonte Java.
        *   Passo a passo da aplicação: código-fonte Node.js.
        *   Passo a passo da aplicação: código-fonte Go.
        *   Entendendo Dockerfiles multi-estágio.
        *   Lab.
    *   **Capítulo 5: Sharing images with Docker Hub and other registries**
        *   Trabalhando com registros, repositórios e tags de imagem.
        *   Enviando (push) suas próprias imagens para o Docker Hub.
        *   Rodando e usando seu próprio registro Docker.
        *   Usando tags de imagem eficazmente.
        *   Transformando imagens oficiais em "golden images".
        *   Lab.

🧠 Explicação detalhada e didática:

*   Esta página detalha os Capítulos 2 a 5, o núcleo da Parte 1.
    *   O Capítulo 2 introduz a prática com Docker, começando com o clássico "Hello World", explicando o que é um contêiner de forma mais concreta, como interagir com ele e como o Docker funciona por baixo dos panos.
    *   O Capítulo 3 ensina a criar suas próprias imagens usando o `Dockerfile`, abordando a estrutura das imagens em camadas e como otimizar o processo de build usando o cache.
    *   O Capítulo 4 mostra um uso poderoso do Docker: compilar e empacotar aplicações diretamente do código-fonte (Java, Node.js, Go) usando `Dockerfiles multi-estágio`, eliminando a necessidade de ter ferramentas de build instaladas localmente.
    *   O Capítulo 5 foca em como compartilhar as imagens criadas, explicando o papel do Docker Hub e outros registros, a importância das tags para versionamento e o conceito de "golden images" (imagens base customizadas).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Cap. 2:** Comandos como `docker container run`, `docker ps`, `docker exec -it <container> powershell` (ou `cmd`), `docker logs`, `docker inspect`.
*   **Cap. 3:** Edição de `Dockerfile` no VS Code, comandos `docker image build -t <nome> .`, `docker image ls`, `docker history`.
*   **Cap. 4:** Uso de `Dockerfiles multi-estágio` para compilar código Java, Node.js, Go dentro do contêiner de build.
*   **Cap. 5:** Comandos `docker login`, `docker image tag <origem> <destino>`, `docker image push <imagem>`, `docker run` para um registro local.

---

📄 Página 13

🔍 Resumo dos principais pontos:

*   Continuação do sumário detalhado.
    *   **Capítulo 6: Using Docker volumes for persistent storage**
        *   Por que dados em contêineres não são permanentes.
        *   Rodando contêineres com volumes Docker.
        *   Rodando contêineres com montagens de sistema de arquivos (filesystem mounts).
        *   Limitações das montagens de sistema de arquivos.
        *   Entendendo como o sistema de arquivos do contêiner é construído.
        *   Lab.
*   **PARTE 2: RUNNING DISTRIBUTED APPLICATIONS IN CONTAINERS** (detalhado).
    *   **Capítulo 7: Running multi-container apps with Docker Compose**
        *   A anatomia de um arquivo Docker Compose.
        *   Rodando uma aplicação multi-contêiner com Compose.
        *   Como o Docker conecta os contêineres.
        *   Configuração de aplicação no Docker Compose.
        *   Entendendo o problema que o Docker Compose resolve.
        *   Lab.
    *   **Capítulo 8: Supporting reliability with health checks and dependency checks**
        *   Construindo verificações de saúde (health checks) nas imagens Docker.
        *   Iniciando contêineres com verificações de dependência.
        *   Escrevendo utilitários customizados para lógica de verificação da aplicação.
        *   Definindo verificações de saúde e dependência no Docker Compose.
        *   Entendendo como as verificações potencializam apps auto-reparáveis (self-healing).
        *   Lab.
    *   **Capítulo 9: Adding observability with containerized monitoring**
        *   A pilha de monitoramento para aplicações conteinerizadas.
        *   Expondo métricas da sua aplicação.
        *   Rodando um contêiner Prometheus para coletar métricas.
        *   Rodando um contêiner Grafana para visualizar métricas.

🧠 Explicação detalhada e didática:

*   Esta página finaliza o detalhamento da Parte 1 com o Capítulo 6 e inicia a Parte 2 com os Capítulos 7 a 9.
    *   O Capítulo 6 aborda um ponto crucial: como lidar com dados que precisam sobreviver além da vida de um contêiner. Explica por que o armazenamento padrão do contêiner é temporário e introduz `volumes` e `bind mounts` como soluções para persistência de dados.
    *   A Parte 2 (Capítulos 7-11) foca em aplicações distribuídas. O Capítulo 7 introduz o `Docker Compose` para definir e gerenciar múltiplos contêineres como uma única aplicação.
    *   O Capítulo 8 discute como tornar essas aplicações mais robustas usando `health checks` (para monitorar a saúde interna do app no contêiner) e `dependency checks` (para garantir que serviços dos quais o app depende estejam disponíveis na inicialização).
    *   O Capítulo 9 trata de `observabilidade`, mostrando como coletar `métricas` das aplicações (usando `Prometheus`) e visualizá-las (usando `Grafana`) para entender o desempenho e identificar problemas.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Cap. 6:** Comandos `docker volume create`, `docker run -v <volume>:<caminho_container>`, `docker run -v C:\caminho\local:<caminho_container>`. Exploração do sistema de arquivos via `docker exec`.
*   **Cap. 7:** Edição de arquivos `docker-compose.yml` no VS Code, comandos `docker-compose up`, `docker-compose down`, `docker-compose ps`, `docker network create`.
*   **Cap. 8:** Adição da instrução `HEALTHCHECK` no Dockerfile, uso de `depends_on` e `healthcheck` no `docker-compose.yml`.
*   **Cap. 9:** Rodar contêineres do `Prometheus` e `Grafana`, configurar o Prometheus para coletar métricas, acessar dashboards do Grafana.

---

📄 Página 14

🔍 Resumo dos principais pontos:

*   Continuação do sumário detalhado da Parte 2.
    *   **Capítulo 9 (continuação): Adding observability with containerized monitoring**
        *   9.5 Entendendo os níveis de observabilidade.
        *   9.6 Lab.
    *   **Capítulo 10: Running multiple environments with Docker Compose**
        *   10.1 Implantando muitas aplicações com Docker Compose.
        *   10.2 Usando arquivos de override (sobreposição) do Docker Compose.
        *   10.3 Injetando configuração com variáveis de ambiente e secrets.
        *   10.4 Reduzindo duplicação com campos de extensão (extension fields).
        *   10.5 Entendendo o fluxo de trabalho de configuração com Docker.
        *   10.6 Lab.
    *   **Capítulo 11: Building and testing applications with Docker and Docker Compose**
        *   11.1 Como o processo de CI (Integração Contínua) funciona com Docker.
        *   11.2 Criando infraestrutura de build com Docker.
        *   11.3 Capturando configurações de build com Docker Compose.
        *   11.4 Escrevendo jobs de CI sem dependências exceto Docker.
        *   11.5 Entendendo contêineres no processo de CI.
        *   11.6 Lab.
*   **PARTE 3: RUNNING AT SCALE WITH A CONTAINER ORCHESTRATOR** (detalhado).
    *   **Capítulo 12: Understanding orchestration: Docker Swarm and Kubernetes**
        *   12.1 O que é um orquestrador de contêineres?
        *   12.2 Configurando um cluster Docker Swarm.
        *   12.3 Rodando aplicações como serviços Docker Swarm.
        *   12.4 Gerenciando tráfego de rede no cluster.
        *   12.5 Entendendo a escolha entre Docker Swarm e Kubernetes.
        *   12.6 Lab.

🧠 Explicação detalhada e didática:

*   Esta página conclui o detalhamento da Parte 2 (Capítulos 9-11) e inicia a Parte 3 (Capítulo 12).
    *   O Capítulo 10 aprofunda o uso do `Docker Compose`, mostrando como gerenciar diferentes ambientes (dev, teste, prod) usando `arquivos de override`, `variáveis de ambiente`, `secrets` e `campos de extensão` para evitar duplicação e customizar configurações.
    *   O Capítulo 11 integra o Docker ao processo de `Integração Contínua (CI)`, demonstrando como usar Docker e Compose para construir, testar e até mesmo hospedar a própria infraestrutura de CI (como Jenkins).
    *   A Parte 3 (Capítulos 12-16) introduz a `orquestração` para rodar aplicações em múltiplos servidores (clusters). O Capítulo 12 explica o conceito de orquestração e apresenta os dois principais players: `Docker Swarm` e `Kubernetes`, focando em como configurar e usar o Swarm.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Cap. 10:** Uso de múltiplos arquivos `-f` com `docker-compose`, definição de variáveis de ambiente (`$env:VAR="valor"` no PowerShell), uso de `secrets` e `x-` (extension fields) no `docker-compose.yml`.
*   **Cap. 11:** Configuração de um pipeline de CI (ex: Jenkins) para executar comandos `docker build` e `docker-compose build/up/push`.
*   **Cap. 12:** Comandos do Docker Swarm como `docker swarm init`, `docker node ls`, `docker service create`, `docker service ls`, `docker service ps`, `docker network create --driver overlay`.

---

📄 Página 15

🔍 Resumo dos principais pontos:

*   Continuação do sumário detalhado da Parte 3.
    *   **Capítulo 13: Deploying distributed applications as stacks in Docker Swarm**
        *   13.1 Usando Docker Compose para implantações de produção.
        *   13.2 Gerenciando configuração de app com objetos de configuração (config objects).
        *   13.3 Gerenciando configurações confidenciais com secrets.
        *   13.4 Armazenando dados com volumes no Swarm.
        *   13.5 Entendendo como o cluster gerencia stacks.
        *   13.6 Lab.
    *   **Capítulo 14: Automating releases with upgrades and rollbacks**
        *   14.1 O processo de atualização de aplicação com Docker.
        *   14.2 Configurando lançamentos (rollouts) de produção com Compose.
        *   14.3 Configurando reversões (rollbacks) de serviço.
        *   14.4 Gerenciando tempo de inatividade (downtime) para seu cluster.
        *   14.5 Entendendo alta disponibilidade (high availability) em clusters Swarm.
        *   14.6 Lab.
    *   **Capítulo 15: Configuring Docker for secure remote access and CI/CD**
        *   15.1 Opções de endpoint para a API Docker.
        *   15.2 Configurando Docker for acesso remoto seguro.
        *   15.3 Usando Contextos Docker para trabalhar com engines remotos.
        *   15.4 Adicionando implantação contínua (CD) ao seu pipeline de CI.
        *   15.5 Entendendo o modelo de acesso para Docker.
        *   15.6 Lab.
    *   **Capítulo 16: Building Docker images that run anywhere: Linux, Windows, Intel, and Arm**
        *   16.1 Por que imagens multi-arquitetura são importantes.
        *   16.2 Construindo imagens multi-arch a partir de um ou mais Dockerfiles.
        *   16.3 Enviando (push) imagens multi-arch para registros com manifestos.
        *   16.4 Construindo imagens multi-arch com Docker Buildx.

🧠 Explicação detalhada e didática:

*   Esta página continua detalhando a Parte 3, cobrindo os Capítulos 13 a 16.
    *   O Capítulo 13 mostra como usar arquivos `Docker Compose` para implantar aplicações como `stacks` no `Docker Swarm`, introduzindo `config objects` e `secrets` para gerenciamento de configuração em cluster, além de volumes específicos do Swarm.
    *   O Capítulo 14 foca no ciclo de vida das aplicações no Swarm, detalhando como configurar `atualizações (upgrades)` e `reversões (rollbacks)` automáticas de forma segura, e como gerenciar a disponibilidade do cluster.
    *   O Capítulo 15 aborda o `acesso remoto seguro` à API Docker (essencial para administração e `CI/CD`), usando TLS ou SSH, e como gerenciar múltiplas conexões com `Docker Contexts`. Mostra como integrar a Implantação Contínua (CD) ao pipeline.
    *   O Capítulo 16 retorna à construção de imagens, explicando como criar `imagens multi-arquitetura` que rodam em diferentes sistemas operacionais (Linux, Windows) e CPUs (Intel, Arm) usando `manifestos` e a ferramenta `Docker Buildx`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Cap. 13:** Comando `docker stack deploy -c <arquivo.yml> <nome_stack>`, `docker config create`, `docker secret create`, uso de `configs` e `secrets` no Compose.
*   **Cap. 14:** Configuração da seção `deploy` (com `update_config`, `rollback_config`) no Compose, comandos `docker service update --rollback`.
*   **Cap. 15:** Configuração do Docker Engine para acesso remoto (não recomendado diretamente no Docker Desktop, mas o conceito é explicado), uso de `docker context create/use/ls`, configuração de pipeline de CI/CD (ex: Jenkins) com credenciais para deploy remoto.
*   **Cap. 16:** Uso do `docker buildx build`, `docker manifest create/push/inspect`, construção de imagens para diferentes `--platform`.

---

📄 Página 16

🔍 Resumo dos principais pontos:

*   Conclusão do sumário detalhado da Parte 3 e início da Parte 4.
    *   **Capítulo 16 (continuação): Building Docker images that run anywhere: Linux, Windows, Intel, and Arm**
        *   16.5 Entendendo onde imagens multi-arch se encaixam no seu roadmap.
        *   16.6 Lab.
*   **PARTE 4: GETTING YOUR CONTAINERS READY FOR PRODUCTION** (detalhado).
    *   **Capítulo 17: Optimizing your Docker images for size, speed, and security**
        *   17.1 Como otimizar imagens Docker.
        *   17.2 Escolhendo as imagens base corretas.
        *   17.3 Minimizando a contagem e o tamanho das camadas da imagem.
        *   17.4 Levando seus builds multi-estágio para o próximo nível.
        *   17.5 Entendendo por que a otimização conta.
        *   17.6 Lab.
    *   **Capítulo 18: Application configuration management in containers**
        *   18.1 Uma abordagem multi-camadas para configuração de app.
        *   18.2 Empacotando configuração para cada ambiente.
        *   18.3 Carregando configuração do runtime.
        *   18.4 Configurando apps legados da mesma forma que apps novos.
        *   18.5 Entendendo por que um modelo de configuração flexível compensa.
        *   18.6 Lab.
    *   **Capítulo 19: Writing and managing application logs with Docker**
        *   19.1 Bem-vindo ao stderr e stdout!
        *   19.2 Retransmitindo logs de outros coletores (sinks) para stdout.
        *   19.3 Coletando e encaminhando logs de contêiner.
        *   19.4 Gerenciando sua saída e coleta de logs.
        *   19.5 Entendendo o modelo de logging de contêiner.
        *   19.6 Lab.

🧠 Explicação detalhada e didática:

*   Esta página finaliza a Parte 3 com o restante do Capítulo 16 e inicia a Parte 4 (Capítulos 17-19).
    *   O restante do Capítulo 16 discute considerações estratégicas sobre o uso de imagens multi-arquitetura.
    *   A Parte 4 foca em preparar os contêineres para produção. O Capítulo 17 aborda técnicas para `otimizar imagens Docker`, tornando-as menores, mais rápidas de construir e mais seguras, incluindo a escolha de imagens base adequadas e o uso eficiente de camadas e builds multi-estágio.
    *   O Capítulo 18 revisita o `gerenciamento de configuração`, detalhando modelos flexíveis para carregar configurações de diferentes fontes (arquivos, variáveis de ambiente) e como aplicar isso até mesmo a aplicações legadas.
    *   O Capítulo 19 explora o `logging em contêineres`, explicando o padrão de usar `stdout/stderr`, como lidar com apps que logam em arquivos, e como centralizar logs usando ferramentas como Fluentd e Elasticsearch (pilha EFK).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Cap. 17:** Análise do tamanho das imagens (`docker image ls`), uso de `.dockerignore`, técnicas para reduzir o tamanho das camadas (combinar comandos `RUN`), comparação de imagens base (ex: `-slim`, `-alpine` para Linux; `nanoserver` para Windows).
*   **Cap. 18:** Implementação de padrões de configuração em código (Node.js, .NET Core, Go), uso de variáveis de ambiente e montagem de volumes/secrets para injetar configuração.
*   **Cap. 19:** Configuração de aplicações para logar em `stdout`, uso de `docker logs`, configuração de `logging drivers` (como `fluentd`), execução da pilha EFK com Docker Compose.

---

📄 Página 17

🔍 Resumo dos principais pontos:

*   Conclusão do sumário detalhado da Parte 4.
    *   **Capítulo 20: Controlling HTTP traffic to containers with a reverse proxy**
        *   20.1 O que é um proxy reverso?
        *   20.2 Lidando com roteamento e SSL no proxy reverso.
        *   20.3 Melhorando desempenho e confiabilidade com o proxy.
        *   20.4 Usando um proxy reverso cloud-native.
        *   20.5 Entendendo os padrões que um proxy reverso habilita.
        *   20.6 Lab.
    *   **Capítulo 21: Asynchronous communication with a message queue**
        *   21.1 O que é mensageria assíncrona?
        *   21.2 Usando uma fila de mensagens cloud-native.
        *   21.3 Consumindo e tratando mensagens.
        *   21.4 Adicionando novas funcionalidades com manipuladores de mensagem (message handlers).
        *   21.5 Entendendo padrões de mensageria assíncrona.
        *   21.6 Lab.
    *   **Capítulo 22: Never the end**
        *   22.1 Rode sua própria prova de conceito (Proof-of-Concept - PoC).
        *   22.2 Apresente um caso para Docker na sua organização.
        *   22.3 Planeje o caminho para produção.
        *   22.4 Conheça a comunidade Docker.
*   Link para o índice (429).

🧠 Explicação detalhada e didática:

*   Esta página finaliza o sumário detalhado com os Capítulos 20, 21 e 22.
    *   O Capítulo 20 introduz o conceito de `proxy reverso` (como Nginx ou Traefik) para gerenciar o tráfego HTTP/S que chega aos contêineres, permitindo hospedar múltiplos sites na mesma porta, lidar com certificados SSL e melhorar o desempenho com caching.
    *   O Capítulo 21 explora a `comunicação assíncrona` usando `filas de mensagens` (como Redis ou NATS), um padrão que desacopla componentes, melhora a escalabilidade e permite adicionar funcionalidades sem alterar o código existente.
    *   O Capítulo 22 serve como conclusão, incentivando o leitor a aplicar o conhecimento em uma Prova de Conceito (PoC), a advogar pelo uso do Docker em sua organização, a planejar a adoção em produção e a se conectar com a comunidade Docker.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Cap. 20:** Rodar Nginx ou Traefik em contêineres, configurar roteamento baseado em host ou path, configurar terminação SSL, habilitar caching ou sticky sessions.
*   **Cap. 1:** Rodar Redis ou NATS em contêineres, usar clientes (CLI ou código de aplicação) para publicar e subscrever mensagens, implementar message handlers.
*   **Cap. 22:** Nenhum comando específico, mas oferece um guia prático para aplicar o conhecimento adquirido.

---

📄 Página 18

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Uma página em branco, provavelmente separando o sumário detalhado do prefácio.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 19

🔍 Resumo dos principais pontos:

*   **Prefácio**
*   O autor relata sua experiência com Docker desde 2014 e a falta de um livro abrangente que pudesse recomendar.
*   O objetivo do livro é preencher essa lacuna, sendo inclusivo para desenvolvedores e operadores, usuários de Linux e Windows.
*   Destaca o conceito central do Docker: empacotar uma aplicação com suas dependências para rodar da mesma forma em qualquer lugar.
*   Menciona que essa portabilidade quebra barreiras entre equipes de desenvolvimento e operações e habilita projetos de TI modernos.
*   Enfatiza a abordagem prática da série "Month of Lunches", com exercícios e laboratórios.

🧠 Explicação detalhada e didática:

*   No prefácio, Elton Stoneman contextualiza a criação do livro, motivado pela necessidade de um guia prático e acessível sobre Docker para um público amplo (devs, ops, Linux, Windows). Ele reitera o benefício principal do Docker - a portabilidade ("build, share, run anywhere") - e como isso facilita a colaboração e modernização de TI. Ele também justifica a escolha do formato "Month of Lunches" pela sua ênfase no aprendizado prático através de exercícios.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando específico, mas reforça que os exercícios do livro são projetados para funcionar no seu ambiente Windows com Docker Desktop.

💡 Analogias ou observações úteis (se houver):

*   O conceito de "empacotar tudo junto" é como preparar uma marmita completa para o almoço: você leva a comida (aplicação) e todos os utensílios necessários (dependências) em uma única caixa (contêiner), garantindo que você possa comer (rodar) em qualquer lugar (ambiente) sem problemas.

---

📄 Página 20

🔍 Resumo dos principais pontos:

*   **Agradecimentos (Acknowledgments)**
*   O autor agradece à equipe da Manning Publications pelo suporte na criação do livro.
*   Agradece aos revisores e aos participantes do programa de acesso antecipado (Early Access Program - MEAP) pelo feedback e contribuições.
*   Lista nominalmente vários revisores que ajudaram a melhorar o livro.

🧠 Explicação detalhada e didática:

*   Esta página é dedicada aos agradecimentos. O autor expressa sua gratidão à editora Manning, aos revisores técnicos e editoriais, e especialmente à comunidade que participou do processo de desenvolvimento do livro através do programa MEAP, fornecendo feedback valioso. É um reconhecimento padrão da colaboração envolvida na publicação de um livro técnico.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 21

🔍 Resumo dos principais pontos:

*   **Sobre este livro (About this book)**
*   Objetivo claro: capacitar o leitor a rodar suas próprias aplicações em Docker e saber os passos para levá-las à produção.
*   Foco em tarefas do mundo real e construção incremental de experiência com Docker, aplicações distribuídas, orquestração e ecossistema.
*   Público-alvo: usuários novos e em aprimoramento de Docker, sem exigir muito conhecimento prévio, abrangendo arquitetura, desenvolvimento e operações.
*   Ênfase na importância de realizar os exercícios e laboratórios práticos.
*   Compatibilidade: os exemplos funcionam em Windows, Mac, Linux (incluindo Raspberry Pi).
*   Código-fonte dos exemplos disponível no GitHub.

🧠 Explicação detalhada e didática:

*   Esta seção define claramente o propósito e o público do livro. O autor quer que, ao final da leitura (e prática), Alexandre (você!) se sinta confiante para usar Docker no dia a dia, desde a conteinerização de uma aplicação até o planejamento para produção. O livro é projetado para ser acessível, mesmo sem profundo conhecimento prévio em áreas específicas, e a chave para o aprendizado é a execução dos exemplos práticos fornecidos, que são multiplataforma e estão disponíveis online.

💻 Exemplos práticos (para meu ambiente Windows):

*   Reforça que você poderá usar o Docker Desktop no seu Windows para todos os exercícios.
*   Menciona a necessidade de baixar o código do GitHub (provavelmente via `git clone https://github.com/sixeyed/diamol.git` no PowerShell ou terminal do VS Code, ou baixando o ZIP).

💡 Analogias ou observações úteis (se houver):

*   O livro é como um curso prático de culinária: a teoria é importante, mas você só aprende de verdade a cozinhar (usar Docker) colocando a mão na massa (fazendo os exercícios).

---

📄 Página 22

🔍 Resumo dos principais pontos:

*   **Como usar este livro (How to use this book)**
    *   Segue os princípios "Month of Lunches": capítulos projetados para serem concluídos em cerca de uma hora (leitura + exercícios + lab).
    *   Enfatiza a importância de "trabalhar" nos capítulos, não apenas ler.
*   **Sua jornada de aprendizado (Your learning journey)**
    *   Estrutura do livro dividida em partes:
        *   **Cap. 1:** Introdução, instalação do Docker, download dos recursos.
        *   **Cap. 2-6:** Fundamentos (rodar contêineres, empacotar apps, compartilhar imagens, armazenamento persistente com volumes).
        *   **Cap. 7-11:** Aplicações distribuídas (Docker Compose, healthchecks, monitoramento, múltiplos ambientes, CI com Docker).
        *   **Cap. 12-16:** Rodando em escala com orquestrador (Docker Swarm, deploy em cluster, builds multi-arquitetura - Linux, Windows, Intel, Arm).
        *   **Cap. 17-21:** Tópicos avançados para produção (otimização de imagem, configuração, logging, proxy reverso, filas de mensagens).
        *   **Cap. 22:** Próximos passos (PoC, adoção organizacional, caminho para produção).
    *   Objetivo final: confiança para usar Docker no trabalho diário.

🧠 Explicação detalhada e didática:

*   Esta página detalha a metodologia do livro e a estrutura do aprendizado. A ideia é dedicar cerca de uma hora por dia durante um mês para cobrir todo o conteúdo, com foco na execução prática. A jornada de aprendizado é progressiva: começa com os conceitos básicos do Docker, avança para aplicações com múltiplos contêineres usando Docker Compose, depois aborda a execução em escala com orquestradores (Swarm), e finaliza com tópicos essenciais para preparar as aplicações para o ambiente de produção, além de orientações para os próximos passos.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando específico, mas estrutura o que esperar em termos de comandos e conceitos em cada parte do livro. Por exemplo, você sabe que `Docker Compose` será introduzido nos capítulos 7-11 e `Docker Swarm` nos capítulos 12-16.

---

📄 Página 23

🔍 Resumo dos principais pontos:

*   **Try-it-nows (Experimente agora)**
    *   Exercícios guiados em todos os capítulos.
    *   Código-fonte no GitHub: https://github.com/sixeyed/diamol.
    *   Os capítulos se baseiam nos anteriores, mas não é estritamente necessário segui-los em ordem, pois as imagens necessárias já estão publicadas no Docker Hub.
    *   Recomenda fazer os exercícios para melhor aprendizado.
*   **Hands-on labs (Laboratórios práticos)**
    *   No final de cada capítulo, com desafios menos guiados.
    *   Instruções e dicas são fornecidas.
    *   Soluções disponíveis no repositório GitHub.
*   **Recursos adicionais (Additional resources)**
    *   Documentação oficial do Docker: https://docs.docker.com.
    *   Mídias sociais: Docker no Twitter/Facebook, autor Elton Stoneman (@EltonStoneman no Twitter, blog https://blog.sixeyed.com, YouTube https://youtube.com/eltonstoneman).
*   **Sobre o código (About the code)**
    *   Exemplos de Dockerfiles e manifestos no livro.
    *   Código disponível para download no site da Manning e no GitHub (https://github.com/sixeyed/diamol).
*   **Fórum de discussão liveBook (liveBook discussion forum)**
    *   Acesso gratuito a um fórum privado para compradores do livro para tirar dúvidas e interagir com o autor e outros usuários.

🧠 Explicação detalhada e didática:

*   Esta página descreve os recursos de aprendizado prático do livro e onde encontrar ajuda adicional.
    *   Os `Try-it-nows` são exercícios passo a passo integrados aos capítulos.
    *   Os `Labs` são desafios mais abertos ao final de cada capítulo para consolidar o conhecimento.
    *   Além do livro, a `documentação oficial` do Docker e os `canais do autor` (blog, Twitter, YouTube) são fontes valiosas.
    *   Todo o `código` usado nos exemplos está disponível online.
    *   Há um `fórum` exclusivo para quem comprou o livro interagir e tirar dúvidas.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `git clone https://github.com/sixeyed/diamol.git` (ou download do ZIP) para obter os arquivos dos exercícios e labs.
*   Os arquivos `Dockerfile` e `docker-compose.yml` serão editados no VS Code.
*   Você rodará comandos `docker` e `docker-compose` no PowerShell ou terminal integrado do VS Code.
*   A documentação em `docs.docker.com` será uma referência constante.

---

📄 Página 24

🔍 Resumo dos principais pontos:

*   **Fórum de discussão liveBook (continuação)**
    *   Link para acesso ao fórum: https://livebook.manning.com/#!/book/learn-docker-in-a-month-of-lunches/discussion.
    *   Link para regras de conduta: https://livebook.manning.com/#!/discussion.
    *   O objetivo do fórum é proporcionar diálogo significativo.
    *   A participação do autor é voluntária.
    *   O fórum permanecerá acessível enquanto o livro estiver em catálogo.

🧠 Explicação detalhada e didática:

*   Esta página continua detalhando o fórum de discussão online associado ao livro (liveBook). Fornece os links diretos para acessar o fórum e suas regras. Esclarece que o fórum é um benefício para facilitar a interação e o aprendizado, mas a participação do autor não é garantida em tempo integral. O acesso ao fórum está vinculado à compra do livro e dura enquanto ele for comercializado pela editora.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando, mas um recurso útil (o fórum) para tirar dúvidas que possam surgir ao longo dos exercícios no seu ambiente Windows.

---

📄 Página 25

🔍 Resumo dos principais pontos:

*   **Sobre o autor (About the author)**
    *   Elton Stoneman é Docker Captain e Microsoft MVP.
    *   Autor de cursos online na Pluralsight.
    *   Experiência como consultor em sistemas .NET empresariais.
    *   Trabalhou na Docker Inc. por três anos.
    *   Atualmente é consultor e instrutor freelancer focado em contêineres.
    *   Escreve sobre Docker e Kubernetes em seu blog (https://blog.sixeyed.com) e Twitter (@EltonStoneman).

🧠 Explicação detalhada e didática:

*   Esta página apresenta o autor, Elton Stoneman. Destaca suas credenciais e experiência na área de Docker, Kubernetes e desenvolvimento de software (especialmente .NET). Ser um Docker Captain e Microsoft MVP indica reconhecimento pela comunidade e pelas empresas. Sua experiência prática, tanto como consultor quanto trabalhando diretamente na Docker, confere credibilidade ao conteúdo do livro. Seus canais online (blog, Twitter) são mencionados como locais para encontrar mais conteúdo dele.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando. Saber que o autor tem experiência com .NET e Windows (sendo MVP) pode ser relevante para você, indicando que ele provavelmente entende os desafios específicos desse ambiente ao trabalhar com Docker.

---

📄 Página 26

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, servindo de separador antes do início da Parte 1 do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 27

🔍 Resumo dos principais pontos:

*   **Início da Parte 1: Understanding Docker containers and images**
*   Boas-vindas ao livro.
*   Objetivo da Parte 1: familiarizar o leitor rapidamente com os conceitos centrais do Docker (contêineres, imagens, registros).
*   Conteúdo da Parte 1: aprender a rodar aplicações em contêineres, empacotar suas próprias aplicações e compartilhá-las.
*   Também abordará o armazenamento de dados com volumes Docker para aplicações stateful (que guardam estado).
*   Promessa: ao final da Parte 1, o leitor estará confortável com os fundamentos e terá aprendido boas práticas desde o início.

🧠 Explicação detalhada e didática:

*   Esta página serve como introdução à Parte 1 do livro. Ela estabelece o foco desta seção nos conceitos fundamentais do Docker: o que são contêineres, como são construídos a partir de imagens e como essas imagens são gerenciadas e compartilhadas através de registros. A parte prática envolverá rodar, criar e compartilhar contêineres/imagens. Um ponto importante mencionado é o tratamento de dados persistentes (usando volumes), essencial para aplicações que não podem perder informações quando o contêiner é recriado.

💻 Exemplos práticos (para meu ambiente Windows):

*   Antecipa os comandos que serão usados nesta parte: `docker run`, `docker build`, `docker push`, `docker pull`, `docker volume create`, etc., todos executáveis via PowerShell/terminal no seu Docker Desktop.

---

📄 Página 28

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, provavelmente separando a introdução da Parte 1 do início do Capítulo 1.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 29

🔍 Resumo dos principais pontos:

*   **Capítulo 1: Before you begin**
*   Introduz Docker como plataforma para rodar aplicações em contêineres.
*   Menciona a popularidade crescente do Docker (citando pesquisa Stack Overflow 2019).
*   Reforça a simplicidade de aprender Docker e a abordagem prática do livro (exemplos e labs multiplataforma).
*   Destaca a importância de entender os casos de uso e problemas que contêineres resolvem antes de começar a usá-los.
*   Lista os cinco principais cenários de sucesso com Docker que serão abordados no capítulo.

🧠 Explicação detalhada e didática:

*   Esta página inicia o Capítulo 1. Ela reafirma o que é Docker (uma plataforma para executar aplicações em unidades leves chamadas contêineres) e sua relevância no mercado atual. O capítulo tem como objetivo contextualizar o Docker, explicando *por que* ele é importante e *quais problemas* ele resolve no mundo real, antes de mergulhar nos aspectos técnicos nos capítulos seguintes. A ideia é motivar o aprendizado mostrando o valor prático da tecnologia.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ainda, mas prepara o terreno para a instalação e verificação do Docker no seu ambiente Windows nas próximas páginas.

---

📄 Página 30

🔍 Resumo dos principais pontos:

*   **1.1 Why containers will take over the world** (Por que contêineres dominarão o mundo)
*   Relato da jornada pessoal do autor com Docker desde 2014.
*   Exemplo de projeto onde Docker simplificou desenvolvimento, testes, produção e handover.
*   Docker centraliza ferramentas e facilita a colaboração entre desenvolvedores e administradores.
*   Visão de que Docker se tornaria ubíquo devido à facilidade e flexibilidade (apps antigos/novos, Windows/Linux).
*   **1.1.1 Migrating apps to the cloud** (Migrando apps para a nuvem)
    *   Desafios da migração para nuvem: opções IaaS (Infrastructure as a Service) vs PaaS (Platform as a Service).
    *   IaaS: VMs, portabilidade entre nuvens, mas custo alto e ineficiência.
    *   PaaS: Serviços gerenciados, custo menor, mas aprisionamento tecnológico (lock-in) e esforço de migração.
    *   Docker como terceira opção: conteinerizar a aplicação e rodar em serviços de contêineres (AKS, ECS) ou cluster próprio.

🧠 Explicação detalhada e didática:

*   O autor começa compartilhando sua própria experiência positiva com Docker, enfatizando como a tecnologia simplificou drasticamente um projeto complexo, desde o desenvolvimento até a entrega e manutenção, unificando as ferramentas usadas por desenvolvedores e operadores.
*   Em seguida, aborda o primeiro grande cenário de uso: a migração de aplicações para a nuvem. Explica as limitações das abordagens tradicionais: IaaS (basicamente alugar VMs na nuvem) que é flexível mas cara, e PaaS (usar serviços específicos da nuvem) que é mais barata mas exige reescrita e prende você a um provedor. Docker surge como uma solução intermediária ideal: você empacota sua aplicação em contêineres e pode rodá-la de forma eficiente e portátil em qualquer nuvem (ou até mesmo no seu próprio datacenter) usando serviços de orquestração de contêineres.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando direto, mas contextualiza por que usar Docker (instalado via Docker Desktop) é vantajoso ao pensar em rodar suas aplicações na Azure, AWS ou Google Cloud.

💡 Analogias ou observações úteis (se houver):

*   Migrar para nuvem: IaaS é como alugar uma casa vazia (VM), você traz tudo e cuida de tudo. PaaS é como alugar um quarto mobiliado num hotel específico, conveniente mas limitado. Docker é como colocar seus móveis (aplicação) num contêiner de mudança, você pode levar para qualquer casa ou hotel que aceite o contêiner.

---

📄 Página 31

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 1.1) ilustrando as opções IaaS vs PaaS para migração de um app distribuído para a nuvem.
    *   Mostra a ineficiência (muitas VMs) do IaaS e o aprisionamento (lock-in) do PaaS.
*   Introdução ao Docker como alternativa que combina benefícios de custo (PaaS) e portabilidade (IaaS).
*   Diagrama (Figura 1.2) ilustrando a migração via Docker para a nuvem.
    *   Mostra a mesma aplicação rodando em contêineres em poucos servidores, destacando portabilidade e eficiência.
*   Menciona que os Capítulos 7, 13 e 14 detalharão como empacotar e rodar aplicações distribuídas em contêineres e em escala.

🧠 Explicação detalhada e didática:

*   Esta página continua a discussão sobre migração para a nuvem, usando diagramas para visualizar as opções.
    *   A Figura 1.1 mostra um aplicativo com várias partes (web, API, banco de dados). Em IaaS, cada parte roda em sua própria VM, o que é caro. Em PaaS, cada parte é substituída por um serviço específico da nuvem, o que dificulta a mudança para outra nuvem.
    *   A Figura 1.2 mostra a mesma aplicação onde cada parte foi colocada em um contêiner Docker. Esses contêineres rodam de forma eficiente em menos máquinas virtuais na nuvem (ou onde você quiser), combinando baixo custo com a liberdade de escolher ou trocar de provedor de nuvem. O livro promete ensinar como fazer isso nos capítulos subsequentes.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando. Os diagramas ajudam a visualizar os benefícios conceituais do Docker que você explorará com comandos práticos mais tarde.

💡 Analogias ou observações úteis (se houver):

*   Relembrando a analogia da mudança: A Figura 1.1 mostra ou alugar várias casas (caro - IaaS) ou se adaptar totalmente a um hotel específico (restritivo - PaaS). A Figura 1.2 mostra usar seus contêineres de mudança (Docker) para morar eficientemente em qualquer lugar que os aceite.

---

📄 Página 32

🔍 Resumo dos principais pontos:

*   Reconhece que migrar para contêineres exige investimento: criar `Dockerfiles` (scripts de build) e `manifestos de aplicação` (Docker Compose ou Kubernetes).
*   Vantagem: não é necessário mudar o código da aplicação.
*   Resultado: a aplicação roda da mesma forma em qualquer ambiente (laptop, nuvem), usando a mesma pilha tecnológica.
*   **1.1.2 Modernizing legacy apps** (Modernizando apps legados)
    *   Contêineres rodam bem aplicações monolíticas (grandes e antigas), mas elas limitam a agilidade (ex: longos ciclos de teste/release).
    *   Docker facilita a modernização gradual sem reescrever tudo.
    *   Abordagem:
        1.  Colocar o monolito em um único contêiner.
        2.  Começar a extrair funcionalidades para seus próprios contêineres.
        3.  Usar a rede virtual do Docker para comunicação entre os novos contêineres e o monolito.
*   Diagrama (Figura 1.3) ilustrando a decomposição gradual de um monolito.

🧠 Explicação detalhada e didática:

*   Após discutir a migração para nuvem, a página aborda outro cenário: modernizar aplicações antigas (monolitos). Rodar um monolito em um contêiner já traz benefícios de portabilidade, mas não resolve problemas de agilidade (um pequeno bug exige testar e reimplantar toda a aplicação gigante).
*   O Docker oferece um caminho para modernizar essas aplicações gradualmente:
    1.  **Empacotar:** Primeiro, coloque o monolito como está dentro de um contêiner (usando um Dockerfile).
    2.  **Decompor:** Identifique uma funcionalidade e crie um novo serviço (em seu próprio contêiner) para ela. A comunicação entre o monolito e o novo serviço acontece pela rede interna do Docker.
    3.  **Iterar:** Repita o processo, extraindo mais funcionalidades para contêineres separados.
*   A Figura 1.3 visualiza isso: um "roteador" (geralmente um proxy reverso, como veremos no Cap. 20) direciona algumas requisições para o monolito original e outras para os novos microsserviços em contêineres. Isso permite modernizar sem parar tudo para uma reescrita completa.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker build` para criar a imagem do monolito a partir de um `Dockerfile`.
*   Você usará `Docker Compose` (Cap. 7) para definir como o monolito e os novos microsserviços rodam juntos e se comunicam pela rede Docker.
*   Os Capítulos 20 e 21 mostrarão exemplos práticos dessa decomposição.

💡 Analogias ou observações úteis (se houver):

*   Modernizar um monolito com Docker é como reformar uma casa antiga quarto por quarto, em vez de demolir tudo e reconstruir. Você continua morando na casa (rodando a aplicação) enquanto moderniza partes dela (extrai funcionalidades para contêineres).

---

📄 Página 33

🔍 Resumo dos principais pontos:

*   Benefícios da modernização gradual com Docker:
    *   Agilidade: Testes rápidos focados no componente alterado.
    *   Escalabilidade: Escalar funcionalidades específicas independentemente.
    *   Flexibilidade: Usar diferentes tecnologias para novos componentes.
*   **1.1.3 Building new cloud-native apps** (Construindo novos apps cloud-native)
    *   Docker também acelera projetos novos baseados em princípios cloud-native.
    *   Definição da CNCF (Cloud Native Computing Foundation): usar software open source, implantar como microsserviços, empacotar em contêineres e orquestrar dinamicamente.
*   Diagrama (Figura 1.4) mostrando uma arquitetura típica de microsserviços.
    *   Exemplo: app de demonstração `microservices-demo` no GitHub (https://github.com/microservices-demo).
    *   Cada componente tem seus próprios dados e API.
    *   Usa várias linguagens e bancos de dados.
    *   Cada componente tem um Dockerfile e o app é definido em um Docker Compose.

🧠 Explicação detalhada e didática:

*   Após discutir apps legados, a página foca em como o Docker beneficia a construção de aplicações *novas*, especialmente as "cloud-native". Essas aplicações são projetadas desde o início para rodar bem na nuvem, geralmente seguindo o padrão de `microsserviços`.
*   A CNCF define as características chave: uso de tecnologias open source, divisão da aplicação em pequenos serviços independentes (microsserviços), cada um rodando em seu próprio contêiner, e gerenciamento desses contêineres por um orquestrador.
*   A Figura 1.4 exemplifica isso com uma aplicação de e-commerce de demonstração. Cada caixinha (carrinho, catálogo, pedidos, frontend) é um microsserviço rodando em um contêiner. Eles se comunicam entre si (provavelmente via APIs), e cada um pode usar a tecnologia mais adequada (linguagem, banco de dados). O Docker é fundamental aqui, pois cada microsserviço é empacotado com seu `Dockerfile`, e a aplicação como um todo é definida com `Docker Compose`.

💻 Exemplos práticos (para meu ambiente Windows):

*   O Capítulo 4 ensinará como usar `Dockerfiles multi-estágio` para compilar e empacotar esses microsserviços (escritos em Java, Go, Node.js, etc.) sem precisar das ferramentas de build instaladas no seu Windows.
*   Você usará `Docker Compose` para rodar toda essa aplicação de microsserviços localmente com um único comando.

---

📄 Página 34

🔍 Resumo dos principais pontos:

*   Docker permite compilar código dentro do Dockerfile (Capítulo 4), eliminando a necessidade de ferramentas de desenvolvimento instaladas localmente. Desenvolvedores só precisam do Docker.
*   Facilidade de integrar software de terceiros (bancos de dados, filas, ferramentas de monitoramento) disponíveis como imagens no Docker Hub.
*   Mapa de projetos open source da CNCF disponíveis como imagens Docker.
*   **1.1.4 Technical innovation: Serverless and more** (Inovação técnica: Serverless e mais)
    *   Consistência como motor da TI moderna: usar as mesmas ferramentas/processos para todos os projetos.
    *   Docker permite isso para apps antigos (.NET Framework/Windows) e novos (Go/Linux).
    *   Um único cluster Docker pode rodar toda a variedade de aplicações da empresa.

🧠 Explicação detalhada e didática:

*   Esta página continua destacando os benefícios do Docker para aplicações novas e introduz o cenário de inovação técnica.
*   Um ponto importante é que o Docker simplifica o ambiente de desenvolvimento. Como o build da aplicação pode ocorrer *dentro* do processo de criação da imagem Docker (usando builds multi-estágio), o desenvolvedor não precisa mais instalar compiladores, SDKs, etc., em sua máquina local. Apenas o Docker é necessário.
*   Além disso, o ecossistema Docker (especialmente o Docker Hub) facilita enormemente a incorporação de componentes de terceiros (como bancos de dados, ferramentas de cache, etc.), pois eles já vêm prontos para rodar como contêineres.
*   O Docker também se posiciona no centro de inovações como `Serverless`. A ideia do Serverless é rodar código em resposta a eventos, sem gerenciar servidores. Por baixo dos panos, as plataformas Serverless (como AWS Lambda, Azure Functions) usam contêineres.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker build` com Dockerfiles multi-estágio (Cap. 4).
*   Você usará `docker pull` para baixar imagens de softwares como bancos de dados (Postgres, Redis) ou ferramentas (Prometheus, Grafana) do Docker Hub.
*   Você poderá rodar frameworks Serverless open source (como OpenFaaS) localmente usando Docker.

💡 Analogias ou observações úteis (se houver):

*   Usar imagens do Docker Hub é como usar blocos de Lego pré-fabricados: você não precisa construir cada peça (software) do zero; você pega as peças prontas (imagens) e as encaixa para montar sua aplicação.

---

📄 Página 35

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 1.5) mostrando um único cluster Docker rodando monolitos legados, apps cloud-native e funções serverless.
*   Explica o conceito de Serverless: desenvolvedor escreve a função, a plataforma cuida do build, empacotamento e execução sob demanda.
*   Plataformas Serverless na nuvem usam Docker internamente, mas criam lock-in.
*   Alternativa: rodar plataformas Serverless open source (Nuclio, OpenFaaS, Fn Project) no seu próprio cluster Docker.

🧠 Explicação detalhada e didática:

*   A Figura 1.5 ilustra a versatilidade de um cluster Docker: ele pode hospedar diferentes tipos de aplicações simultaneamente – desde as antigas e monolíticas até as modernas baseadas em microsserviços e as funções Serverless. Isso promove a consistência no gerenciamento da infraestrutura.
*   O conceito de `Serverless` é explicado: o desenvolvedor foca apenas no código da função, e a plataforma se encarrega de todo o resto (construir, empacotar, escalar, executar quando necessário). Embora as grandes nuvens (AWS, Azure, Google) ofereçam serviços Serverless populares, eles prendem o usuário àquela plataforma específica (lock-in). Uma alternativa é usar frameworks Serverless `open source` que rodam sobre Docker (ou Kubernetes), permitindo ter Serverless no seu próprio datacenter ou em qualquer nuvem, sem lock-in.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você poderá rodar contêineres do OpenFaaS ou Fn Project usando `docker run` ou `Docker Compose` no seu Docker Desktop para experimentar Serverless localmente.

💡 Analogias ou observações úteis (se houver):

*   Serverless é como ter um cozinheiro sob demanda: você só entrega a receita (código da função), e ele aparece com todos os ingredientes e equipamentos (infraestrutura gerenciada pela plataforma), cozinha o prato (executa a função) e desaparece, cobrando apenas pelo tempo que cozinhou.

---

📄 Página 36

🔍 Resumo dos principais pontos:

*   Outras inovações como Machine Learning (TensorFlow), Blockchain (Hyperledger) e IoT também se beneficiam do empacotamento e deploy consistentes do Docker.
*   Docker + Arm: parceria para tornar contêineres o padrão para dispositivos Edge e IoT.
*   **1.1.5 Digital transformation with DevOps** (Transformação digital com DevOps)
    *   Problema operacional comum: silos entre desenvolvedores ("dev") e operadores ("ops"), levando a ciclos de culpa e processos de release lentos e arriscados.
    *   DevOps: visa agilidade através de uma única equipe responsável por todo o ciclo de vida da aplicação.
    *   DevOps é principalmente cultural, mas tecnologia ajuda. Docker como ferramenta comum que une devs e ops (Dockerfiles, Compose files).
    *   Framework CALMS (Culture, Automation, Lean, Metrics, Sharing) e como Docker se encaixa em cada iniciativa.
*   **1.2 Is this book for you?** (Este livro é para você?)
    *   Confirmação: Se você quer usar Docker para resolver problemas reais como os descritos, sim.
    *   Leva do zero ao deploy em cluster de produção.

🧠 Explicação detalhada e didática:

*   A página menciona brevemente outras áreas de inovação (ML, Blockchain, IoT) onde Docker é relevante, destacando a parceria com a Arm para dispositivos de borda (Edge) e Internet das Coisas (IoT).
*   O foco principal muda para o `DevOps`. O problema clássico são as equipes separadas de desenvolvimento e operações, com atritos e processos lentos. DevOps busca quebrar esses silos, promovendo colaboração e automação. O Docker atua como um catalisador tecnológico para o DevOps, fornecendo uma linguagem e ferramentas comuns (`Dockerfile`, `Docker Compose`) que tanto desenvolvedores quanto operadores usam, facilitando a comunicação e a automação (um dos pilares do CALMS).
*   Finalmente, a seção 1.2 confirma que o livro é destinado a quem quer aplicar Docker nesses cenários práticos, prometendo guiar o leitor desde o básico até a execução em um cluster.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `Dockerfiles` e `docker-compose.yml` extensivamente, ferramentas centrais na colaboração DevOps habilitada por Docker.
*   O Capítulo 11 mostrará a automação (CI/CD) na prática.
*   O Capítulo 9 (métricas) e o Capítulo 19 (logs) se relacionam com a parte de "Metrics" do CALMS.
*   O Capítulo 5 (compartilhamento de imagens) se relaciona com "Sharing".

---

📄 Página 37

🔍 Resumo dos principais pontos:

*   **O que o livro NÃO cobre em detalhes:**
    *   Internals profundos do Docker (containerd, cgroups, namespaces, Host Compute Service). Recomenda "Docker in Action, 2ª ed." para isso.
    *   Amostras não são focadas apenas em Windows (.NET Framework), usam linguagens multiplataforma (.NET Core, etc.). Recomenda o blog do autor para Windows Containers.
    *   Foco em Docker Swarm para orquestração de produção, não Kubernetes em detalhes (embora discuta a escolha no Cap. 12). Tudo aprendido se aplica a Kubernetes.
*   **1.3 Creating your lab environment** (Criando seu ambiente de laboratório)
    *   Necessário: Docker e o código-fonte dos exemplos.
    *   **1.3.1 Installing Docker** (Instalando Docker)
        *   Usar Docker Community Edition (gratuito).
        *   Opções: Docker Desktop (Windows 10 Pro/Enterprise recente, macOS recente), Docker Toolbox (versões mais antigas), pacotes Linux.
        *   Necessário criar conta no Docker Hub (gratuita).
    *   **Instalando Docker Desktop no Windows 10:**
        *   Requisitos: Windows 10 Pro/Enterprise (mínimo release 1809), verificar com `winver`.
        *   Baixar instalador estável de www.docker.com/products/docker-desktop.
        *   Rodar instalador com padrões. Ícone da baleia na barra de tarefas indica que está rodando.
    *   **Instalando Docker Desktop no macOS:**
        *   Requisitos: macOS Sierra 10.12+.
        *   Baixar instalador estável do mesmo site.
        *   Rodar instalador com padrões. Ícone da baleia na barra de menus.

🧠 Explicação detalhada e didática:

*   Esta página define o escopo do livro, esclarecendo o que *não* será aprofundado (detalhes internos do Docker, foco exclusivo em Windows Containers, Kubernetes detalhado) e recomendando outros recursos para esses tópicos.
*   Em seguida, inicia a configuração do ambiente prático (lab). O essencial é ter o Docker instalado e acesso ao código dos exemplos.
*   Começa a detalhar a instalação do Docker, recomendando a versão Community Edition (CE), que é gratuita. As opções variam conforme o sistema operacional:
    *   **Docker Desktop:** A melhor opção para Windows 10 (Pro/Enterprise) e macOS recentes. Ele integra o Docker de forma mais nativa.
    *   **Docker Toolbox:** Uma opção mais antiga para versões legadas de Windows/macOS, que usa uma máquina virtual (como VirtualBox) por baixo dos panos.
    *   **Pacotes Linux:** Instruções específicas para distribuições Linux.
*   A página detalha os passos específicos para instalar o **Docker Desktop no Windows 10** (seu caso) e no macOS, incluindo requisitos de sistema e como verificar se a instalação foi bem-sucedida (procurando pelo ícone da baleia). A criação de uma conta no Docker Hub também é mencionada como necessária para downloads.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** Seguir as instruções para **instalar o Docker Desktop para Windows**.
    *   Verificar sua versão do Windows com o comando `winver` (digitar no menu Iniciar ou Executar).
    *   Ir ao site `docker.com`, baixar e executar o instalador do Docker Desktop (versão Stable).
    *   Criar uma conta no `hub.docker.com` se ainda não tiver.
    *   Após a instalação, verificar se o ícone da baleia aparece na área de notificação.

---

📄 Página 38

🔍 Resumo dos principais pontos:

*   **Instalando Docker Toolbox:**
    *   Para versões antigas de Windows/OS X.
    *   Requer software de VM como VirtualBox.
    *   Link para docs: https://docs.docker.com/toolbox.
    *   Docker Desktop é preferível se possível.
*   **Instalando Docker Community Edition e Docker Compose (Linux):**
    *   Não usar a versão que vem com a distribuição (geralmente antiga).
    *   Usar script de instalação do Docker em https://get.docker.com (para ambientes não-produção).
    *   Instalar Docker Compose separadamente via https://docs.docker.com/compose/install.
*   **Instalando Docker em Windows Server ou Distros Linux de Servidor:**
    *   Pode usar Community Edition (CE) ou Docker Enterprise (EE - versão comercial suportada).
    *   EE é construído sobre CE, então o livro se aplica a ambos.
    *   Versões EE disponíveis para Linux e Windows Server 2016/2019.
    *   Instruções de instalação do EE no Docker Hub: http://mng.bz/K29E.
*   **1.3.2 Verifying your Docker setup** (Verificando sua instalação Docker)
    *   Verificar Docker com `docker version`.
        *   Deve mostrar informações do Cliente (Client) e do Servidor (Server).
    *   Exemplo de saída do `docker version`.

🧠 Explicação detalhada e didática:

*   Esta página continua as instruções de instalação do Docker para diferentes cenários:
    *   **Docker Toolbox:** Mencionado como alternativa para sistemas mais antigos, mas desencorajado se o Docker Desktop for compatível.
    *   **Linux (CE):** Instruções específicas para usuários Linux, recomendando o script oficial em vez das versões dos repositórios da distro, e a instalação separada do Docker Compose.
    *   **Servidores (Windows/Linux - CE/EE):** Esclarece que tanto a versão gratuita (CE) quanto a paga e suportada (EE) podem ser usadas, e que o conteúdo do livro vale para ambas. O EE é mais comum em produção.
*   Em seguida, passa para a **verificação da instalação**. O comando essencial é `docker version`. É crucial que este comando retorne informações tanto para o "Client" (a ferramenta de linha de comando que você usa) quanto para o "Server" (o Docker Engine que executa os contêineres). Se ambos aparecerem, a instalação básica está funcionando.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** Abrir o PowerShell (como administrador, se possível) ou o terminal do VS Code e executar:
    ```powershell
    docker version
    ```
*   Analisar a saída: Você deve ver seções "Client" e "Server". A seção "Server" no Docker Desktop para Windows provavelmente mostrará "OS/Arch: linux/amd64" se você estiver no modo Linux (padrão) ou "windows/amd64" se estiver no modo Windows. O importante é não haver erros e ambas as seções estarem presentes.

---

📄 Página 39

🔍 Resumo dos principais pontos:

*   Exemplo de saída do `docker version` (continuação).
    *   Detalha as informações retornadas (versão, API, Go, Git commit, OS/Arch, Experimental).
    *   Não se preocupar com os detalhes de cliente/servidor ainda (serão explicados no Cap. 2).
*   Verificar Docker Compose com `docker-compose version`.
    *   Deve retornar versões sem erros.
*   Exemplo de saída do `docker-compose version`.
*   **1.3.3 Downloading the source code for the book** (Baixando o código-fonte do livro)
    *   Repositório Git público no GitHub: https://github.com/sixeyed/diamol.
    *   Comando para clonar (se tiver Git): `git clone https://github.com/sixeyed/diamol.git`.
    *   Alternativa: Baixar como ZIP do site do GitHub e extrair.
*   **1.3.4 Remembering the cleanup commands** (Lembrando os comandos de limpeza)
    *   Docker não limpa automaticamente contêineres/imagens.
    *   Contêineres parados não usam CPU/memória, mas ocupam disco.
    *   Comando para remover todos os contêineres: `docker container rm -f $(docker container ls -aq)`.
    *   Comando para remover imagens do livro (prefixo 'diamol/*'): `docker image rm -f $(docker image ls -f reference='diamol/*' -q)`.
    *   Docker baixa o que precisa se não encontrar localmente.

🧠 Explicação detalhada e didática:

*   A página continua a verificação da instalação, agora focando no `Docker Compose`. O comando `docker-compose version` confirma que essa ferramenta essencial (que você usará muito a partir do Capítulo 7) também está funcionando.
*   Em seguida, instrui como obter o **código-fonte dos exemplos** do livro. A forma preferida é usar o Git com o comando `git clone`, que baixa todo o histórico do repositório. Se você não tiver o Git instalado, pode baixar um arquivo ZIP diretamente do GitHub.
*   Por fim, uma dica importante sobre **limpeza**. O Docker acumula contêineres parados e imagens que não estão mais em uso, consumindo espaço em disco. São fornecidos dois comandos úteis para remover todos os contêineres e as imagens específicas baixadas/criadas para os exercícios do livro. É bom saber desses comandos para manter seu ambiente organizado.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  No PowerShell/terminal, execute:
        ```powershell
        docker-compose version
        ```
        Verifique se não há erros.
    2.  Escolha um diretório para os exemplos (ex: `C:\DockerLabs`). Navegue até ele no terminal.
    3.  Se você tem Git instalado (recomendado, pode baixar de `git-scm.com`), execute:
        ```powershell
        git clone https://github.com/sixeyed/diamol.git
        ```
        Se não tiver Git, baixe o ZIP do link, extraia-o para `C:\DockerLabs`, e talvez renomeie a pasta resultante de `diamol-main` para `diamol`.
    4.  **Anote** os comandos de limpeza para usar periodicamente (especialmente após cada capítulo ou se ficar sem espaço):
        ```powershell
        # Remover todos os contêineres (parados ou rodando)
        docker container rm -f $(docker container ls -aq)

        # Remover imagens dos exemplos do livro (com prefixo diamol/)
        docker image rm -f $(docker image ls -f reference='diamol/*' -q)
        ```
        *Observação:* A sintaxe `$(comando)` funciona bem no PowerShell moderno e no Bash (Linux/Mac), executando o comando interno e passando sua saída como argumento para o comando externo.

---

📄 Página 40

🔍 Resumo dos principais pontos:

*   **1.4 Being immediately effective** (Sendo imediatamente eficaz)
*   Reitera o princípio "Month of Lunches": foco em aprender habilidades e colocá-las em prática.
*   Estrutura dos próximos capítulos: introdução -> exercícios "try-it-now" -> recapitulação/detalhes -> lab prático.
*   Tópicos focados em tarefas úteis do mundo real.
*   Convite para começar a rodar contêineres.

🧠 Explicação detalhada e didática:

*   Esta página conclui o Capítulo 1, reforçando a filosofia prática do livro. A estrutura típica dos capítulos seguintes é apresentada: uma breve teoria seguida de exercícios guiados (`try-it-now`), uma explicação um pouco mais detalhada e, por fim, um desafio prático (`lab`) para aplicar o que foi aprendido. O objetivo é que cada capítulo ensine uma habilidade útil e aplicável imediatamente. O convite final é para iniciar a parte prática, começando a rodar contêineres no próximo capítulo.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando. Prepara você para a estrutura dos próximos capítulos e a necessidade de executar os comandos propostos.

---

📄 Página 41

🔍 Resumo dos principais pontos:

*   **Capítulo 2: Understanding Docker and running Hello World**
*   Início da parte prática com Docker.
*   Foco do capítulo: rodar aplicações em contêineres, entender o que é um contêiner e por que são leves.
*   Uso de exercícios `try-it-now` com comandos simples.
*   **2.1 Running Hello World in a container** (Rodando Hello World em um contêiner)
    *   Primeiro comando prático: `docker container run diamol/ch02-hello-diamol`.
    *   Instrução para abrir um terminal (PowerShell recomendado para Windows).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 2, marcando o começo da interação prática com o Docker. O objetivo é rodar o primeiro contêiner, entender o que acontece nesse processo e começar a se familiarizar com a linha de comando do Docker. O capítulo usa o exemplo clássico "Hello World" adaptado para contêineres. A seção 2.1 introduz o comando fundamental `docker container run` para executar um contêiner a partir de uma imagem específica (`diamol/ch02-hello-diamol`).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** Abrir o PowerShell ou terminal do VS Code.
*   Executar o comando:
    ```powershell
    docker container run diamol/ch02-hello-diamol
    ```
*   Observar a saída (que será analisada nas próximas páginas). Você provavelmente verá o Docker baixar a imagem ("pulling") antes de executá-la, pois é a primeira vez que você a usa.

---

📄 Página 42

🔍 Resumo dos principais pontos:

*   Apresenta e analisa a saída completa do comando `docker container run diamol/ch02-hello-diamol` (Figura 2.1).
*   **Explicação do processo:**
    1.  O comando `docker container run` instrui o Docker a rodar um app em um contêiner.
    2.  A imagem (`diamol/ch02-hello-diamol`) contém o app empacotado e publicado.
    3.  Docker verifica se a imagem existe localmente. Se não ("unable to find image locally"), ele baixa ("pulling") a imagem do registro (Docker Hub, por padrão).
    4.  Docker inicia um contêiner usando a imagem. A imagem contém instruções de como iniciar o app.
    5.  O app (um script simples) executa e sua saída é exibida.
*   **Análise da Saída do App:**
    *   Mensagem "Hello from Chapter 2!".
    *   Informações sobre o "computador" onde está rodando: nome da máquina (ex: `e5943557213b`), sistema operacional (ex: `Linux 4.9.125-linuxkit x86_64`), endereço de rede (ex: `172.17.0.2`).

🧠 Explicação detalhada e didática:

*   Esta página disseca o que aconteceu quando você rodou o primeiro comando. O Docker agiu como um gerenciador inteligente:
    1.  Recebeu a ordem: "rode um contêiner com a imagem X".
    2.  Procurou a "receita" (imagem X) localmente.
    3.  Não achou? Foi buscar na "biblioteca pública" (Docker Hub).
    4.  Baixou a receita.
    5.  Usou a receita para "montar" e "ligar" o ambiente isolado (contêiner).
    6.  Executou o programa que estava dentro do contêiner.
    7.  Mostrou o que o programa imprimiu na tela.
*   A saída do programa mostra que ele está rodando em um ambiente que *parece* ser uma máquina separada, com seu próprio nome, OS e endereço IP. Isso demonstra o isolamento do contêiner.

💻 Exemplos práticos (para meu ambiente Windows):

*   Revisar a saída que você obteve ao rodar `docker container run diamol/ch02-hello-diamol` na página anterior. Compare com a Figura 2.1. Você deve ter visto as etapas de "pulling" seguidas pela saída do "Hello".

💡 Analogias ou observações úteis (se houver):

*   `docker container run` é como pedir uma pizza (`diamol/ch02-hello-diamol`). Se a pizzaria (`Docker Engine`) não tem a receita (`imagem`) na hora, ela busca no livro de receitas central (`Docker Hub`). Depois, ela prepara (`cria o contêiner`) e entrega (`executa o app`) a pizza para você (`mostra a saída`).

---

📄 Página 43

🔍 Resumo dos principais pontos:

*   A saída pode variar dependendo do sistema operacional e arquitetura do host.
*   Exemplo de saída em Windows Containers: `Microsoft Windows [Version 10.0.17763.557]`.
*   Exemplo de saída em Raspberry Pi (Arm): `Linux 4.19.42-v7+ armv7l`.
*   Introduz o fluxo Docker: **Build, Share, Run** (Construir, Compartilhar, Rodar).
    *   Alguém empacota (Build) -> Publica (Share) -> Qualquer pessoa pode usar (Run).
*   Reforça que o fluxo é o mesmo para apps simples ou complexos (ex: Java com múltiplos componentes).
*   Portabilidade é um benefício chave do Docker.
*   **Exercício Try-it-now:** Repetir o comando `docker container run diamol/ch02-hello-diamol`.

🧠 Explicação detalhada e didática:

*   A página explica que a informação do sistema operacional dentro do contêiner reflete o ambiente real onde o Docker está rodando (ou o kernel que ele está usando, como no caso do Docker Desktop modo Linux no Windows). Isso demonstra que, embora isolado, o contêiner compartilha o kernel do SO hospedeiro.
*   O fluxo **Build, Share, Run** é apresentado como o ciclo de vida fundamental do Docker:
    1.  **Build:** Um desenvolvedor cria um `Dockerfile` para empacotar a aplicação e suas dependências em uma `imagem`.
    2.  **Share:** Essa imagem é enviada (`push`) para um `registro` (como o Docker Hub).
    3.  **Run:** Qualquer outra pessoa (ou sistema) pode baixar (`pull`) essa imagem e executar (`run`) um `contêiner` a partir dela, em qualquer máquina que tenha Docker, obtendo o mesmo resultado.
*   A `portabilidade` (rodar em qualquer lugar) é um dos maiores trunfos do Docker.
*   O exercício pede para rodar o mesmo comando novamente para observar o que muda.

💻 Exemplos práticos (para meu ambiente Windows):

*   No seu Docker Desktop modo Linux, a saída do OS dentro do contêiner será Linux (provavelmente `linuxkit`). Se você mudar para o modo Windows Containers (clicando com o botão direito no ícone da baleia > Switch to Windows containers...) e rodar uma imagem Windows compatível, a saída refletiria o Windows Server Core.
*   **Ação principal:** Execute novamente no PowerShell/terminal:
    ```powershell
    docker container run diamol/ch02-hello-diamol
    ```
*   Observe a saída (analisada na próxima página). Note que desta vez não deve haver a etapa de "pulling", pois a imagem já está local.

---

📄 Página 44

🔍 Resumo dos principais pontos:

*   Análise da saída da segunda execução do comando:
    *   Sem download ("pulling") da imagem, pois já existe localmente.
    *   Execução direta do contêiner.
    *   Saída do app mostra o mesmo SO, mas **nome de máquina** e **endereço IP diferentes** (ex: `858a26ee2741`, `172.17.0.5`).
*   Pergunta levantada: De onde vêm esses nomes e IPs diferentes se estão rodando no mesmo computador?
*   Introdução à próxima seção (2.2) que explicará o que é um contêiner.

🧠 Explicação detalhada e didática:

*   Ao rodar o comando pela segunda vez, o Docker foi mais rápido porque já tinha a "receita" (imagem) guardada. Ele apenas criou e rodou um novo contêiner.
*   A observação crucial é que, embora o sistema operacional *dentro* do contêiner seja o mesmo (pois compartilham o kernel do host), cada novo contêiner obtém uma identidade *única* dentro do ambiente Docker. Isso se manifesta como um nome de máquina aleatório diferente e um endereço IP diferente (dentro da rede virtual gerenciada pelo Docker).
*   Isso reforça a ideia de isolamento: cada contêiner, mesmo sendo baseado na mesma imagem, é uma instância separada e independente com sua própria identidade de rede e nome. A página prepara o terreno para explicar *como* o Docker consegue criar esses ambientes isolados e virtuais.

💻 Exemplos práticos (para meu ambiente Windows):

*   Compare a saída da segunda execução com a primeira. Verifique se o nome da máquina (ex: `858a26ee2741`) e o endereço IP (ex: `172.17.0.5`) são diferentes dos da primeira execução e diferentes entre si a cada nova execução.

💡 Analogias ou observações úteis (se houver):

*   É como usar o mesmo molde (imagem) para fazer vários bolos (contêineres). Cada bolo é feito com os mesmos ingredientes e formato, mas cada um é um bolo individual. Se você colocar uma etiqueta em cada um (nome da máquina, IP), cada etiqueta será única.

---

📄 Página 45

🔍 Resumo dos principais pontos:

*   **2.2 So what is a container?** (Então, o que é um contêiner?)
*   Analogia: Contêiner físico = caixa com uma aplicação dentro.
*   Dentro da "caixa", a aplicação pensa ter um computador só para ela:
    *   Nome de máquina próprio.
    *   Endereço IP próprio.
    *   Disco (sistema de arquivos) próprio.
    *   (Para Windows) Registro do Windows próprio.
*   Diagrama (Figura 2.2) mostrando o app "encaixotado" pelo contêiner.
*   Recursos são virtuais, criados e gerenciados pelo Docker.
*   O aplicativo dentro da caixa não vê nada fora dela.
*   Muitas "caixas" (contêineres) rodam no mesmo computador host.
*   Contêineres compartilham CPU, memória e Sistema Operacional do host.
*   Diagrama (Figura 2.3) mostrando múltiplos contêineres isolados no mesmo host.

🧠 Explicação detalhada e didática:

*   Esta página define formalmente o que é um contêiner Docker, usando a analogia de uma caixa.
*   **Isolamento:** O ponto principal é que, para a aplicação *dentro* do contêiner, parece que ela está rodando sozinha em sua própria máquina. Ela tem seus próprios recursos virtualizados (nome, IP, disco). Isso é o isolamento. A Figura 2.2 ilustra essa "bolha" em torno do app.
*   **Compartilhamento:** No entanto, essa "máquina" é virtual. Na realidade (Figura 2.3), vários desses contêineres rodam no mesmo computador físico (o host) e compartilham os recursos reais: o processador (CPU), a memória RAM e, crucialmente, o *kernel* do Sistema Operacional do host.
*   É essa combinação de isolamento (a aplicação não interfere nas outras) e compartilhamento (usam eficientemente os recursos do host) que torna os contêineres tão poderosos.

💻 Exemplos práticos (para meu ambiente Windows):

*   Os nomes de máquina (ex: `858a26ee2741`) e IPs (ex: `172.17.0.5`) que você viu nas execuções anteriores são exemplos desses recursos virtuais criados pelo Docker para cada contêiner. O sistema de arquivos que o contêiner vê é uma construção virtual sobre o sistema de arquivos do seu Windows (ou da VM Linux no Docker Desktop).

💡 Analogias ou observações úteis (se houver):

*   Retomando a analogia dos apartamentos (contêineres) em um prédio (host): cada apartamento tem seu próprio número (nome/IP) e layout interno (disco virtual), mas todos usam a fundação e estrutura do prédio (kernel do SO), além da eletricidade (CPU) e água (memória) fornecidas centralmente.

---

📄 Página 46

🔍 Resumo dos principais pontos:

*   Contêineres resolvem o conflito entre **isolamento** e **densidade** na computação.
    *   **Densidade:** Rodar o máximo de apps no mesmo hardware para usar eficientemente CPU/memória.
    *   **Isolamento:** Necessário porque apps podem ter conflitos (versões diferentes de Java/.NET, bibliotecas incompatíveis, consumo excessivo de recursos). Isolamento tradicional impede alta densidade.
*   Solução anterior: **Máquinas Virtuais (VMs)**.
    *   Conceito similar (caixa para app), mas a caixa da VM contém seu **próprio Sistema Operacional**.
    *   Não compartilham o SO do host.
*   Diagrama (Figura 2.4) mostrando múltiplas VMs, cada uma com seu SO, no mesmo host.
*   **Implicações das VMs:**
    *   Cada SO da VM consome GBs de memória e CPU.
    *   Custos de licenciamento de SO.
    *   Manutenção de atualizações de SO para cada VM.
    *   VMs fornecem isolamento, mas sacrificam a densidade.
*   **Contêineres oferecem ambos:**
    *   Compartilham o SO do host -> extremamente leves ("lightweight").
    *   Iniciam rápido, usam poucos recursos.
    *   Permitem rodar muito mais contêineres do que VMs no mesmo hardware (5 a 10x mais).
    *   Oferecem densidade E isolamento.
*   **Eficiência** é outro benefício chave do Docker.

🧠 Explicação detalhada e didática:

*   Esta página aprofunda a comparação entre contêineres e a tecnologia anterior para isolamento, as Máquinas Virtuais (VMs), explicando por que os contêineres são mais eficientes.
*   O problema fundamental que ambas tentam resolver é o dilema **Densidade vs. Isolamento**: queremos rodar muitos aplicativos (densidade) no mesmo servidor para economizar custos, mas precisamos mantê-los separados (isolamento) para evitar conflitos.
*   As VMs resolvem o isolamento dando a cada aplicativo sua própria máquina virtual completa, *incluindo um sistema operacional inteiro*. A Figura 2.4 ilustra isso. O preço dessa abordagem é a **baixa densidade**: cada SO da VM consome muitos recursos (disco, memória, CPU) que poderiam ser usados pelos aplicativos, além de custos de licença e manutenção.
*   Os contêineres, como visto na página anterior, também fornecem isolamento, mas resolvem o problema da densidade de forma diferente: eles **compartilham o sistema operacional** da máquina hospedeira. Isso os torna muito mais leves e eficientes. Você pode iniciar contêineres quase instantaneamente e rodar um número significativamente maior deles no mesmo hardware em comparação com VMs.
*   A conclusão é que os contêineres oferecem o melhor dos dois mundos: **isolamento E alta densidade (eficiência)**.

💻 Exemplos práticos (para meu ambiente Windows):

*   O Docker Desktop no Windows usa uma VM Linux leve (no modo Linux) ou a tecnologia de contêineres nativa do Windows (no modo Windows). Mesmo no modo Linux, a sobrecarga da VM do Docker Desktop é muito menor do que rodar múltiplas VMs completas para cada aplicativo. Você pode observar o baixo consumo de recursos dos contêineres usando `docker stats` no PowerShell.

💡 Analogias ou observações úteis (se houver):

*   VMs são como casas completas, cada uma precisando de seu próprio terreno, fundação, estrutura, telhado (SO completo). Contêineres são como apartamentos pré-fabricados; eles oferecem isolamento interno, mas compartilham a estrutura maior do prédio (SO do host), sendo muito mais rápidos de instalar e ocupando menos espaço geral.

---

📄 Página 47

🔍 Resumo dos principais pontos:

*   **2.3 Connecting to a container like a remote computer** (Conectando a um contêiner como um computador remoto)
*   Contêineres podem executar tarefas únicas e sair (ex: scripts) ou podem rodar continuamente.
*   Caso de uso para tarefas únicas: empacotar scripts e suas ferramentas em uma imagem Docker, facilitando o compartilhamento e execução sem instalação manual.
*   Outra forma de interagir: rodar um contêiner e conectar-se a um terminal *dentro* dele.
*   Comando: `docker container run --interactive --tty diamol/base`.
    *   `--interactive` (ou `-i`): Mantém a entrada padrão (STDIN) aberta, permitindo interatividade.
    *   `--tty` (ou `-t`): Aloca um pseudo-TTY (terminal), simulando uma sessão de terminal.
*   Abreviação comum: `-it`.
*   Resultado: O Docker baixa a imagem (se necessário) e apresenta um prompt de comando *de dentro do contêiner*.
*   Diagrama (Figura 2.5) mostrando a execução interativa e o prompt do contêiner.

🧠 Explicação detalhada e didática:

*   Até agora, os contêineres que rodamos executaram uma tarefa e terminaram. Esta seção introduz outra forma de usar contêineres: executar um contêiner e "entrar" nele para interagir com seu ambiente interno através de um terminal, como se você estivesse acessando uma máquina remota via SSH.
*   Isso é útil para explorar o ambiente do contêiner, depurar problemas ou executar comandos manualmente dentro dele.
*   O comando chave é `docker container run` com as flags `--interactive` (ou `-i`) e `--tty` (ou `-t`), frequentemente combinadas como `-it`.
    *   `-i`: Permite que você digite comandos no contêiner.
    *   `-t`: Dá a você uma interface de terminal (prompt de comando) para interagir.
*   A imagem usada (`diamol/base`) é uma imagem base simples que provavelmente contém um shell (como `sh` no Linux ou `cmd`/`powershell` no Windows) que será iniciado quando você se conectar.
*   A Figura 2.5 ilustra o resultado: seu terminal local agora está mostrando um prompt que pertence ao ambiente *dentro* do contêiner.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** No PowerShell ou terminal do VS Code, execute:
    ```powershell
    docker container run --interactive --tty diamol/base
    # Ou a forma abreviada:
    # docker container run -it diamol/base
    ```
*   Observar o resultado: Você verá o Docker baixar a imagem `diamol/base` (se for a primeira vez) e, em seguida, seu prompt mudará. Se estiver no modo Linux do Docker Desktop, será algo como `/ #`. Se estiver no modo Windows, será `C:\>`. Você está agora "dentro" do contêiner.

💡 Analogias ou observações úteis (se houver):

*   Rodar `docker run -it` é como abrir uma conexão de Acesso Remoto (RDP) ou SSH para uma máquina, mas a "máquina" é o contêiner rodando localmente.

---

📄 Página 48

🔍 Resumo dos principais pontos:

*   Exemplo de prompt dentro de um contêiner Windows: `C:\>`.
*   Dentro do contêiner, pode-se executar comandos normais do SO correspondente.
*   **Exercício Try-it-now:** Dentro do contêiner, rodar `hostname` e `date`.
    *   Exemplo de saída: `f1695de1f2ec` e `Thu Jun 20 12:18:26 UTC 2019`.
*   A experiência é similar a conectar via SSH (Linux) ou RDP (Windows Server Core).
*   O contêiner compartilha o kernel do SO do host, por isso o shell corresponde (Linux no Linux/Mac/DockerDesktop-Linux; Windows no Windows).
*   Alguns comandos são iguais (ex: `ping google.com`), outros diferentes (`ls` vs `dir`).
*   O comportamento do *Docker* (gerenciamento) é o mesmo, independentemente do SO ou CPU. É o *app dentro* do contêiner que percebe a diferença.
*   **Exercício Try-it-now:** Abrir um *novo* terminal no *host* e rodar `docker container ls`.
    *   Mostra informações dos contêineres *em execução*.
    *   Exemplo de saída (abreviada): ID, Imagem, Comando, Criado, Status.
    *   Observação: O ID do contêiner é o mesmo que o hostname interno.
*   Docker atribui IDs aleatórios; partes do ID são usadas para hostname.
*   Pode-se usar os primeiros caracteres do ID para referenciar um contêiner específico nos comandos.
*   **Exercício Try-it-now:** Listar processos dentro do contêiner com `docker container top <id_curto>`.
    *   Exemplo: `docker container top f1`.
    *   Mostra o processo principal (`/bin/sh`) rodando.
    *   Contêineres Windows podem mostrar mais processos de fundo.

🧠 Explicação detalhada e didática:

*   Uma vez "dentro" do contêiner (via `docker run -it`), você pode usar os comandos do sistema operacional daquele contêiner. Os exemplos `hostname` e `date` confirmam que você está em um ambiente distinto com seu próprio nome e relógio.
*   É crucial entender que, embora o *ambiente interno* do contêiner seja específico do SO (Linux ou Windows), os comandos do *Docker* para gerenciar esses contêineres (`docker ps`, `docker stop`, `docker rm`, etc.) são os mesmos em qualquer plataforma onde o Docker roda.
*   A página introduz comandos para observar contêineres *do lado de fora*, a partir do seu terminal no host:
    *   `docker container ls` (ou `docker ps`): Lista os contêineres que estão *atualmente em execução*. Mostra informações úteis como ID, imagem usada e status.
    *   `docker container top <id>`: Mostra os processos que estão rodando *dentro* de um contêiner específico.
*   Uma dica útil é que você pode usar apenas os primeiros caracteres únicos do ID de um contêiner para identificá-lo nos comandos, em vez de digitar o ID completo.

💻 Exemplos práticos (para meu ambiente Windows):

*   **No terminal conectado ao contêiner (da página anterior):**
    ```powershell
    # Se for contêiner Linux (prompt / #)
    hostname
    date
    ping google.com
    ls /
    exit # Para sair do contêiner
    # Se for contêiner Windows (prompt C:\>)
    hostname
    date
    ping google.com
    dir C:\
    exit # Para sair do contêiner
    ```
*   **Abrir um NOVO PowerShell/terminal no seu Windows:**
    ```powershell
    # Listar contêineres em execução (o que você iniciou com -it deve aparecer)
    docker container ls
    # Pegue os primeiros caracteres do CONTAINER ID (ex: f169) e use-os:
    docker container top <id_curto>
    ```

---

📄 Página 49

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Exibir logs coletados do contêiner com `docker container logs <id_curto>`.
    *   Exemplo: `docker container logs f1`.
    *   Mostra a saída que o app do contêiner produziu (neste caso, os comandos executados no terminal interativo).
    *   Para apps reais (ex: web), mostraria logs de requisição HTTP, etc.
*   **Exercício Try-it-now:** Mostrar todos os detalhes de um contêiner com `docker container inspect <id_curto>`.
    *   Exemplo: `docker container inspect f1`.
    *   Saída é um JSON longo com informações de baixo nível (filesystem, comando, rede, etc.). Útil para troubleshooting.
*   Comandos (`run`, `logs`, `top`, `inspect`) são consistentes para gerenciar qualquer tipo de aplicação em contêiner (Java antigo/Linux, .NET antigo/Windows, Go novo/Raspberry Pi).
*   Docker adiciona camada de gerenciamento consistente sobre qualquer app.
*   Instrução para fechar o segundo terminal e sair do primeiro (que estava conectado ao contêiner) com `exit`.
*   **2.4 Hosting a website in a container** (Hospedando um site em um contêiner)
    *   Revisão: Contêineres executam tarefas e saem, ou rodam interativamente até serem fechados.
    *   `docker container ls` só mostra contêineres *em execução*.

🧠 Explicação detalhada e didática:

*   Continuando a exploração de comandos de gerenciamento do Docker a partir do host:
    *   `docker container logs <id>`: Recupera toda a saída (stdout/stderr) que foi produzida pelo processo principal dentro do contêiner desde que ele iniciou. É essencial para ver o que sua aplicação está fazendo ou por que falhou.
    *   `docker container inspect <id>`: Fornece uma visão extremamente detalhada da configuração e do estado atual de um contêiner, em formato JSON. É uma ferramenta poderosa para diagnóstico avançado.
*   A página reforça a **consistência** do gerenciamento Docker: não importa a tecnologia dentro do contêiner, os comandos para gerenciá-lo são os mesmos.
*   Prepara para a próxima seção, que abordará contêineres que rodam serviços continuamente em background, como servidores web.
*   Relembra que `docker ps` (ou `docker container ls`) por padrão só lista contêineres ativos.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ainda no segundo terminal (host):**
    ```powershell
    # Use o ID curto do contêiner da página anterior
    docker container logs <id_curto>
    docker container inspect <id_curto> # Observe a quantidade de detalhes em JSON
    ```
*   **No primeiro terminal (que estava conectado ao contêiner):** Se ainda não saiu, digite `exit`.
*   **No segundo terminal (host):** Execute `docker container ls`. Agora ele não deve listar mais nada, pois o contêiner interativo foi encerrado quando você digitou `exit`.

---

📄 Página 50

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Listar *todos* os contêineres (incluindo os parados) com `docker container ls --all`.
    *   Exemplo de saída mostrando contêineres com status `Exited`.
*   **Pontos chave sobre contêineres parados (Exited):**
    1.  Contêineres rodam *apenas* enquanto o processo principal dentro deles está rodando. Quando o processo termina, o contêiner entra no estado `Exited`.
    2.  Contêineres `Exited` não usam CPU ou memória, mas *ainda existem* e ocupam espaço em disco (seu sistema de arquivos é mantido).
    3.  Pode-se reiniciar (`docker start`), verificar logs (`docker logs`), copiar arquivos (`docker cp`) de contêineres parados.
    4.  Docker *não* remove contêineres parados automaticamente; é preciso usar `docker rm`.
*   Caso de uso principal do Docker: rodar aplicações de servidor (sites, APIs, bancos de dados) que ficam rodando em background.
*   **Exercício Try-it-now:** Rodar um contêiner de site em background.
    *   Comando: `docker container run --detach --publish 8088:80 diamol/ch02-hellodiamol-web`.
    *   `--detach` (ou `-d`): Roda o contêiner em background e imprime apenas o ID longo do contêiner.
    *   `--publish` (ou `-p`): Mapeia uma porta do host para uma porta do contêiner (`<porta_host>:<porta_container>`).
*   **Exercício Try-it-now:** Listar contêineres em execução com `docker container ls` para ver o novo contêiner com status `Up`.
    *   Exemplo de saída mostrando ID, Imagem, Comando, Criado, Status, Portas, Nome (aleatório se não especificado).

🧠 Explicação detalhada e didática:

*   Esta página esclarece o ciclo de vida dos contêineres e introduz como rodar serviços em background.
*   O comando `docker container ls --all` (ou `docker ps -a`) é importante para ver o "histórico" de contêineres que já rodaram e pararam. Eles não somem sozinhos e continuam ocupando espaço em disco até serem explicitamente removidos (`docker rm`).
*   O foco muda para o cenário mais comum: rodar serviços como servidores web. Para isso, usamos duas flags importantes no `docker run`:
    *   `--detach` (ou `-d`): Libera seu terminal imediatamente após iniciar o contêiner, deixando-o rodar em segundo plano.
    *   `--publish` (ou `-p`) `<porta_host>:<porta_container>`: Expõe a aplicação dentro do contêiner para o mundo exterior (ou pelo menos para a máquina host). O Docker direciona o tráfego que chega na `<porta_host>` da sua máquina para a `<porta_container>` dentro do contêiner. No exemplo, o tráfego na porta 8088 do seu Windows será enviado para a porta 80 do contêiner.
*   A imagem `diamol/ch02-hellodiamol-web` contém um servidor web (Apache) pronto para rodar.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # Listar todos os contêineres (você verá os 'hello-diamol' e 'base' como Exited)
    docker container ls --all

    # Rodar o servidor web em background e mapear a porta
    docker container run --detach --publish 8088:80 diamol/ch02-hellodiamol-web
    # Anote o ID longo que é impresso

    # Listar contêineres em execução (agora você verá o 'hello-diamol-web')
    docker container ls
    # Observe a coluna PORTS mostrando '0.0.0.0:8088->80/tcp'
    # Observe a coluna NAMES com um nome aleatório (ex: reverent_dubinsky)
    ```

---

📄 Página 51

🔍 Resumo dos principais pontos:

*   A imagem `diamol/ch02-hello-diamol-web` contém o servidor web Apache e uma página HTML simples.
*   Explicação da flag `--publish`:
    *   Contêineres têm IP próprio em uma rede virtual Docker, não expostos por padrão.
    *   `--publish <host>:<container>` faz o Docker ouvir na porta do host e redirecionar o tráfego para a porta do contêiner.
    *   Exemplo: Tráfego para `host:8088` vai para `container:80`.
*   Diagrama (Figura 2.6) ilustrando redes físicas e virtuais:
    *   Computador Host (ex: 192.168.2.150) na rede física.
    *   Contêiner (ex: 172.0.5.1) na rede virtual Docker.
    *   Mapeamento de porta (8088 -> 80) permite acesso externo ao contêiner via IP/porta do host.
*   Contêineres não são diretamente acessíveis por outros na rede física pelo seu IP Docker.

🧠 Explicação detalhada e didática:

*   A página explica com mais detalhes o funcionamento do mapeamento de portas (`--publish`). É essencial entender que os contêineres operam em sua própria rede virtual isolada, criada e gerenciada pelo Docker. Eles recebem endereços IP nessa rede (como `172.x.x.x`), que não são diretamente acessíveis de fora da máquina host.
*   A flag `--publish` cria uma "ponte" entre a rede física da sua máquina (onde ela tem um IP como `192.168.x.x` ou `localhost`) e a rede virtual do contêiner. O Docker intercepta o tráfego que chega na porta especificada do host (8088 no exemplo) e o encaminha para a porta correspondente dentro do contêiner (80 no exemplo).
*   A Figura 2.6 ilustra bem essa separação entre a rede física (onde seu PC está) e a rede virtual Docker (onde o contêiner "vive"), e como o mapeamento de porta conecta as duas para permitir o acesso ao serviço no contêiner.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo, mas a explicação é fundamental para entender por que você acessará o site (no próximo passo) usando `localhost:8088` no seu navegador Windows, mesmo que o servidor web esteja rodando na porta 80 *dentro* do contêiner.

💡 Analogias ou observações úteis (se houver):

*   Pense na rede virtual Docker como um condomínio fechado (com seus próprios endereços internos). O comando `--publish` é como dizer ao porteiro (Docker) para receber entregas (tráfego de rede) em um endereço específico do lado de fora (porta 8088 do host) e levá-las para um apartamento específico lá dentro (porta 80 do contêiner).

---

📄 Página 52

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Acessar o site no navegador via `http://localhost:8088`.
    *   Mostra a página HTML servida pelo contêiner (Figura 2.7).
*   Benefícios mesmo para um site simples: portabilidade e eficiência.
    *   Imagem contém tudo (conteúdo web + servidor).
    *   Desenvolvedor roda localmente da mesma forma que operador rodaria em produção (mesma imagem).
*   Contêiner de servidor roda indefinidamente (enquanto o processo do servidor estiver ativo).
*   Pode-se usar comandos `docker container` (ls, logs, inspect, etc.) para gerenciá-lo.
*   **Exercício Try-it-now:** Usar `docker container stats <id_ou_nome>` para ver uso de CPU, memória, rede e disco em tempo real.
    *   Exemplo: `docker container stats e53` (usando ID curto).
    *   Saída ligeiramente diferente entre Linux e Windows.
*   Remover contêineres (incluindo os em execução) com `docker container rm --force <id_ou_nome>`.

🧠 Explicação detalhada e didática:

*   Agora você testa o mapeamento de porta acessando `http://localhost:8088` no seu navegador. `localhost` refere-se à sua própria máquina Windows, e a porta `8088` é a que você mandou o Docker escutar. O Docker então encaminha essa requisição para a porta 80 do contêiner, que serve a página web de volta.
*   A página reforça que mesmo um exemplo simples como este demonstra os benefícios do Docker: a imagem é autocontida e garante que a aplicação rode da mesma forma em qualquer ambiente.
*   Introduz outro comando útil: `docker container stats`. Ele fornece uma visão ao vivo (atualizada continuamente no terminal) dos recursos (CPU, memória, I/O de rede/disco) que um ou mais contêineres estão consumindo. É ótimo para monitoramento rápido.
*   Menciona como remover um contêiner, usando a flag `--force` (ou `-f`) se ele ainda estiver rodando.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Abra seu navegador web (Chrome, Edge, Firefox) e acesse `http://localhost:8088`. Você deve ver a página "Hello World" da Figura 2.7.
    2.  No PowerShell/terminal, execute `docker container ls` para pegar o ID ou o nome aleatório do contêiner web (ex: `e53085ff0cc4` ou `reverent_dubinsky`).
    3.  Execute `docker container stats <id_ou_nome>` (substitua pelo ID/nome real). Observe as estatísticas sendo atualizadas. Pressione `Ctrl+C` para sair do `stats`.
    4.  Para remover o contêiner:
        ```powershell
        docker container rm --force <id_ou_nome>
        # Ou use o ID curto:
        # docker container rm -f <id_curto>
        ```
        *Nota:* Execute este comando de remoção apenas se quiser parar o site agora. O próximo exercício também fará a limpeza.

---

📄 Página 53

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Comando para remover *todos* os contêineres (parados ou rodando) de uma vez.
    *   Comando: `docker container rm --force $(docker container ls --all --quiet)`.
    *   Explicação da sintaxe `$()`: envia a saída de um comando (lista de IDs) para outro (rm). Funciona em PowerShell, Linux, Mac.
    *   Combina `ls --all --quiet` (lista IDs de todos os contêineres) com `rm --force` (remove forçadamente).
    *   Aviso: Usar com cautela, pois não pede confirmação.
*   **2.5 Understanding how Docker runs containers** (Entendendo como Docker roda contêineres)
    *   Revisão do fluxo "build, share, run". Facilita distribuição de software (exemplos: Elasticsearch, SQL Server, Ghost).
    *   Introdução à arquitetura do Docker para entender o que acontece nos comandos.
    *   Componentes envolvidos (Figura 2.8):
        *   **Docker Engine (ou Daemon):** Componente de gerenciamento. Cuida do cache de imagens, download/reuso, interação com SO para criar contêineres, redes, etc. Roda em background (serviço/daemon).

🧠 Explicação detalhada e didática:

*   A página apresenta um comando poderoso e conveniente para limpeza geral: `docker container rm -f $(docker container ls -aq)`.
    *   `docker container ls -aq` (ou `--all --quiet`): Lista *apenas* os IDs (`-q`) de *todos* os contêineres (`-a`).
    *   `$(...)`: Captura essa lista de IDs.
    *   `docker container rm -f ...`: Remove forçadamente (`-f`) todos os contêineres cujos IDs foram capturados.
    *   É um atalho útil para limpar seu ambiente, mas perigoso se você tiver contêineres importantes rodando que não queria remover.
*   Inicia a seção 2.5, que explica a arquitetura por trás dos comandos Docker. O objetivo é entender melhor como o Docker funciona internamente.
*   O primeiro componente chave é o **Docker Engine** (também chamado de Docker Daemon). Ele é o "cérebro" e "músculo" do Docker, rodando continuamente em segundo plano. É ele quem gerencia as imagens, cria os contêineres, configura as redes e volumes, interagindo diretamente com o sistema operacional (ou a VM no caso do Docker Desktop).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal (limpeza):** Execute o comando de remoção total:
    ```powershell
    docker container rm --force $(docker container ls -aq)
    ```
    *Confirme executando `docker container ls -a` depois; a lista deve estar vazia.*
*   O Docker Engine no seu caso é o serviço gerenciado pelo Docker Desktop. Você não interage diretamente com ele, mas sim através da API (próximo ponto).

💡 Analogias ou observações úteis (se houver):

*   O comando de remoção total é como apertar um botão de "reset" para os contêineres, limpando tudo que estava rodando ou parado.
*   O Docker Engine é o "gerente da fábrica" Docker. Ele fica no escritório (background) recebendo ordens e coordenando toda a produção (criação/gerenciamento de contêineres).

---

📄 Página 54

🔍 Resumo dos principais pontos:

*   Continuação da arquitetura Docker (Figura 2.8):
    *   **Docker API:** Interface REST baseada em HTTP usada pelo Engine para expor suas funcionalidades. Pode ser configurada para acesso local (padrão) ou remoto.
    *   **Docker CLI (Command-Line Interface):** Cliente da API Docker. Comandos como `docker run` são enviados pela CLI para a API, e o Engine executa o trabalho.
*   Entender a arquitetura é bom: a única forma de interagir com o Engine é via API.
*   CLI pode apontar para uma API remota (outra máquina com Docker) para gerenciar contêineres remotamente (ex: build servers, test, prod).
*   API Docker é a mesma em qualquer SO, permitindo gerenciamento cross-platform (ex: CLI no Windows gerenciando Docker no Raspberry Pi ou Linux na nuvem).
*   API tem especificação publicada; CLI não é o único cliente.
*   Existem GUIs (Interfaces Gráficas) que conectam à API Docker.
    *   Exemplo: **Universal Control Plane (UCP)** (produto comercial da Docker Inc., Figura 2.9).
    *   Exemplo: **Portainer** (projeto open source).
    *   UCP e Portainer rodam como contêineres.
*   Detalhes de baixo nível (containerd, OCI) não serão aprofundados.
    *   `containerd`: Componente usado pelo Engine para gerenciar contêineres (agora um projeto CNCF).
    *   `OCI (Open Container Initiative)`: Especificação aberta para formato de imagem e runtime de contêiner.
*   Docker é a plataforma mais popular, mas não a única. O uso de padrões abertos evita lock-in.

🧠 Explicação detalhada e didática:

*   Esta página completa a visão geral da arquitetura Docker.
*   O **Docker Engine** não é acessado diretamente. Ele expõe uma **API** (Application Programming Interface), que é um conjunto de regras e endpoints (endereços) baseados no padrão HTTP/REST, permitindo que outros programas conversem com ele.
*   A **Docker CLI** (o comando `docker` que você usa no terminal) é apenas um desses programas. Quando você digita `docker container ls`, a CLI envia uma requisição formatada para a API do Engine, o Engine processa, e a CLI exibe a resposta que o Engine devolveu.
*   Essa separação Cliente-API-Engine é poderosa:
    *   Permite **gerenciamento remoto**: Você pode configurar sua CLI local para se conectar à API de um Docker Engine rodando em outro servidor.
    *   Permite **outras ferramentas**: Qualquer programa pode usar a API, como interfaces gráficas (UCP, Portainer) que oferecem uma forma visual de gerenciar contêineres.
*   A página também menciona brevemente `containerd` (o componente de runtime de baixo nível) e a `OCI` (padrões abertos), reforçando que o ecossistema Docker é baseado em tecnologias padronizadas, o que é bom para evitar dependência de um único fornecedor.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você já está usando a CLI (`docker`, `docker-compose`) que interage com a API do Engine gerenciado pelo Docker Desktop.
*   Você pode instalar o **Portainer** facilmente no Docker Desktop para ter uma interface gráfica:
    ```powershell
    docker volume create portainer_data
    docker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v \\.\pipe\docker_engine:\\.\pipe\docker_engine -v portainer_data:C:\data portainer/portainer-ce:latest
    ```
    Depois acesse `https://localhost:9443` (pode precisar aceitar certificado auto-assinado) e configure o acesso ao ambiente Docker local.
*   O Capítulo 15 cobrirá o acesso remoto à API em detalhes.

---

📄 Página 55

🔍 Resumo dos principais pontos:

*   **Figura 2.9:** Screenshot da interface gráfica do Docker Universal Control Plane (UCP).
*   **2.6 Lab: Exploring the container filesystem** (Lab: Explorando o sistema de arquivos do contêiner)
    *   Primeiro lab do livro: tarefa para fazer sozinho, cimentar o aprendizado.
    *   Instruções e dicas fornecidas, mas a solução é por conta do leitor.
    *   Solução de exemplo disponível no GitHub (link fornecido).
    *   **Tarefa:** Rodar o contêiner do site (`diamol/ch02-hellodiamol-web`), mas substituir o arquivo `index.html` interno para mostrar um conteúdo diferente ao acessar o site no navegador.
    *   Lembrete: O contêiner tem seu próprio sistema de arquivos.
    *   **Dicas:**
        *   Usar `docker container --help` para ver ações possíveis.

🧠 Explicação detalhada e didática:

*   A página mostra um exemplo de interface gráfica (UCP) e introduz o primeiro **Lab** do livro.
*   Os Labs são projetados para serem mais desafiadores que os exercícios "Try-it-now", exigindo que o leitor combine os conceitos aprendidos para resolver um problema menos guiado. Existe uma solução de referência no GitHub, mas o ideal é tentar resolver sozinho primeiro.
*   **O desafio do Lab 2:** Você rodou o contêiner `diamol/ch02-hellodiamol-web` anteriormente e acessou o site "Hello World". Agora, o objetivo é *modificar* o conteúdo desse site *sem reconstruir a imagem*. Você precisa descobrir como copiar um novo arquivo `index.html` (que você pode criar localmente com qualquer conteúdo) para *dentro* do sistema de arquivos do contêiner em execução (ou parado) antes de acessá-lo novamente pelo navegador.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab:**
    1.  Crie um arquivo `index.html` simples na sua máquina Windows (ex: em `C:\DockerLabs\diamol\ch02\lab`) usando o VS Code. Coloque um texto diferente, como `<h1>Olá, Alexandre!</h1>`.
    2.  Rode o contêiner (se ainda não estiver rodando ou se o removeu):
        ```powershell
        docker run -d --name meu-site-web -p 8088:80 diamol/ch02-hellodiamol-web
        ```
    3.  Use o comando `docker container cp` (introduzido na p. 76, mas aplicável aqui) para copiar seu `index.html` local para o diretório correto *dentro* do contêiner. A próxima página dará a dica do diretório.
        ```powershell
        # Comando exemplo (caminho do container será dado na p. 56):
        # docker container cp C:\DockerLabs\diamol\ch02\lab\index.html meu-site-web:<caminho_no_container>/index.html
        ```
    4.  Acesse `http://localhost:8088` no navegador e veja se seu novo conteúdo aparece.
*   Use `docker container --help` e `docker container cp --help` se precisar de ajuda com os comandos.

---

📄 Página 56

🔍 Resumo dos principais pontos:

*   **Continuação das dicas do Lab 2:**
    *   Adicionar `--help` a qualquer comando `docker` mostra ajuda detalhada.
    *   Na imagem `diamol/ch02-hello-diamol-web`, o conteúdo do site é servido do diretório `/usr/local/apache2/htdocs` (Linux) ou `C:\usr\local\apache2\htdocs` (Windows).
*   Encerramento do Capítulo 2.

🧠 Explicação detalhada e didática:

*   Esta página fornece as dicas finais cruciais para completar o Lab do Capítulo 2.
    *   A primeira dica reforça o uso da ajuda integrada do Docker (`--help`).
    *   A segunda dica revela o caminho exato dentro do contêiner onde o arquivo `index.html` precisa ser colocado para que o servidor web Apache o sirva: `/usr/local/apache2/htdocs/` (para a versão Linux da imagem) ou `C:\usr\local\apache2\htdocs\` (para uma versão Windows, embora a imagem `diamol/ch02-hellodiamol-web` seja provavelmente baseada em Linux). Você usará esse caminho como destino no comando `docker cp`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Completando o Lab (usando o caminho Linux, pois a imagem base é provavelmente Linux):**
    1.  Certifique-se de ter o arquivo `index.html` local (ex: em `C:\DockerLabs\diamol\ch02\lab`).
    2.  Certifique-se de que o contêiner está rodando (ex: com nome `meu-site-web`).
    3.  Execute o comando `docker cp`:
        ```powershell
        docker container cp C:\DockerLabs\diamol\ch02\lab\index.html meu-site-web:/usr/local/apache2/htdocs/index.html
        ```
    4.  Acesse `http://localhost:8088` no navegador. Você deve ver o conteúdo do seu `index.html` personalizado.

*   **Comandos úteis para explorar (opcional):**
    ```powershell
    # Ver a ajuda para o comando cp
    docker container cp --help

    # Entrar no contêiner para verificar o arquivo (se a imagem tiver um shell)
    # docker exec -it meu-site-web sh
    # (dentro do contêiner)
    # ls /usr/local/apache2/htdocs/
    # cat /usr/local/apache2/htdocs/index.html
    # exit
    ```

---

📄 Página 57

🔍 Resumo dos principais pontos:

*   **Capítulo 3: Building your own Docker images** (Construindo suas próprias imagens Docker)
*   Revisão: Capítulos anteriores rodaram contêineres e os gerenciaram.
*   Foco do Capítulo 3: Mudar de *consumidor* de imagens para *construtor* de imagens.
*   Aprendizado: Sintaxe do `Dockerfile` e padrões chave para conteinerizar seus próprios apps.
*   **3.1 Using a container image from Docker Hub** (Usando uma imagem de contêiner do Docker Hub)
    *   Começar com a versão finalizada da imagem que será construída no capítulo.
    *   Aplicativo de exemplo: `web-ping` (verifica se um site está ativo).
    *   O app roda em um contêiner, faz requisições HTTP para um URL (blog do autor) a cada 3 segundos.
    *   `docker container run` baixa a imagem se não estiver local.
    *   Distribuição de software é integrada à plataforma Docker.
    *   Pode-se baixar imagens explicitamente com `docker image pull`.
    *   **Exercício Try-it-now:** Baixar a imagem `diamol/ch03-web-ping`.
        *   Comando: `docker image pull diamol/ch03-web-ping`.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 3, que marca um passo importante: aprender a criar suas próprias imagens Docker, em vez de apenas usar as que outros fizeram. A ferramenta central para isso é o `Dockerfile`.
*   A seção 3.1 começa apresentando o objetivo final do capítulo: construir uma imagem para um aplicativo chamado `web-ping`. Antes de construir, o leitor é instruído a baixar e usar a imagem pronta do Docker Hub.
*   Relembra que o Docker baixa automaticamente imagens (`pull`) se elas não existirem localmente ao executar `docker run`. O comando `docker image pull` permite baixar uma imagem explicitamente, sem rodar um contêiner imediatamente.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** No PowerShell/terminal, execute o comando para baixar a imagem de exemplo:
    ```powershell
    docker image pull diamol/ch03-web-ping
    ```
*   Observe a saída (analisada na próxima página), que mostrará o download das camadas da imagem.

---

📄 Página 58

🔍 Resumo dos principais pontos:

*   Análise da saída do `docker image pull` (Figura 3.1).
*   Nome da imagem: `diamol/ch03-web-ping`.
*   Local padrão: Docker Hub (registro público gratuito).
*   Link para detalhes da imagem no Hub: https://hub.docker.com/r/diamol/ch03-web-ping.
*   Explicação das **camadas da imagem (image layers)**:
    *   Imagem Docker é logicamente uma coisa só (contém todo o app stack - ex: Node.js + código).
    *   Fisicamente, é armazenada como múltiplos arquivos pequenos (camadas).
    *   Docker monta essas camadas para criar o sistema de arquivos do contêiner.
*   **Exercício Try-it-now:** Rodar um contêiner da imagem `web-ping`.
    *   Comando: `docker container run -d --name web-ping diamol/ch03-web-ping`.
    *   `-d`: Roda em background (detached).
    *   `--name web-ping`: Atribui um nome específico (`web-ping`) ao contêiner, em vez de um aleatório. Útil para referenciá-lo depois.
    *   App roda como batch job, sem UI, sem necessidade de publicar portas.

🧠 Explicação detalhada e didática:

*   A página analisa a saída do `docker image pull`. O ponto mais importante aqui é a introdução do conceito de **camadas (layers)**. Embora pensemos em uma imagem como um pacote único, o Docker a armazena de forma inteligente como uma pilha de camadas. Cada camada representa uma mudança (como adicionar um arquivo, instalar um pacote). Esse sistema de camadas é a chave para a eficiência do Docker em armazenamento e transferência.
*   Em seguida, instrui a rodar o aplicativo `web-ping` em um contêiner. Desta vez, além da flag `-d` (detach) já vista, introduz a flag `--name`. Dar um nome fixo a um contêiner (`--name web-ping`) facilita muito o trabalho com ele posteriormente (em comandos como `docker logs`, `docker stop`, `docker rm`), pois você não precisa ficar procurando ou usando IDs aleatórios.

💻 Exemplos práticos (para meu ambiente Windows):

*   Revise a saída do `docker image pull` da página anterior. Você verá várias linhas com IDs e status (Downloading, Pull complete), representando as camadas.
*   **Ação principal:** Execute o comando para rodar o contêiner `web-ping` em background com um nome específico:
    ```powershell
    docker container run -d --name web-ping diamol/ch03-web-ping
    ```
*   Verifique se ele está rodando:
    ```powershell
    docker container ls
    # Ou docker ps
    ```
    Você deverá ver um contêiner com o nome `web-ping`.

💡 Analogias ou observações úteis (se houver):

*   Camadas de imagem são como transparências em um retroprojetor antigo (ou camadas no Photoshop). Cada uma adiciona algo. Juntas, formam a imagem final. Se duas imagens usam a mesma transparência base (ex: o sistema operacional), elas compartilham essa camada, economizando espaço.
*   Usar `--name` é como dar um apelido fácil de lembrar a um amigo (contêiner) em vez de chamá-lo pelo número de identidade (ID aleatório).

---

📄 Página 59

🔍 Resumo dos principais pontos:

*   O app `web-ping` está rodando em background, pingando o blog do autor.
*   Pode-se usar comandos `docker container` (logs, etc.) com o nome (`web-ping`).
*   **Exercício Try-it-now:** Ver os logs da aplicação.
    *   Comando: `docker container logs web-ping`.
    *   Saída (Figura 3.2) mostra o app fazendo requisições HTTP para `blog.sixeyed.com`.
*   Problema: App parece fixo ("hardcoded") para usar o blog do autor, tornando-o inútil para outros.
*   Solução: O app pode ser configurado! Lê configurações de **variáveis de ambiente (environment variables)** do sistema.
    *   Variáveis de ambiente: pares chave/valor simples fornecidos pelo SO (Windows/Linux).
    *   Contêineres Docker também têm variáveis de ambiente, definidas pelo Docker (não pelo SO do host).

🧠 Explicação detalhada e didática:

*   Após iniciar o contêiner `web-ping` em background, a página mostra como verificar o que ele está fazendo usando `docker container logs web-ping`. A saída confirma que ele está acessando o site `blog.sixeyed.com`.
*   Isso levanta um ponto importante sobre flexibilidade: um aplicativo que só faz uma coisa fixa não é muito útil. A página revela que o `web-ping` pode, na verdade, ser configurado.
*   A forma como ele é configurado é através de **variáveis de ambiente**. Estas são configurações simples (como `NOME=Alexandre` ou `SITE_URL=google.com`) que um processo pode ler do seu ambiente de execução. O Docker permite definir variáveis de ambiente específicas para cada contêiner no momento em que ele é iniciado. O aplicativo dentro do contêiner pode então ler essas variáveis e ajustar seu comportamento.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** Execute para ver os logs do contêiner `web-ping`:
    ```powershell
    docker container logs web-ping
    ```
    Observe as linhas indicando requisições para `blog.sixeyed.com`. Você pode rodar o comando várias vezes para ver novos logs.
*   Para ver as variáveis de ambiente *do seu Windows* (não do contêiner), você pode usar `Get-ChildItem Env:` no PowerShell. As variáveis de ambiente *dentro* do contêiner são um conceito separado, gerenciado pelo Docker.

💡 Analogias ou observações úteis (se houver):

*   Variáveis de ambiente são como post-its colados na "caixa" (contêiner) antes de ligar o aplicativo. O aplicativo pode ler esses post-its para saber como deve se comportar (ex: qual site pingar, com que frequência).

---

📄 Página 60

🔍 Resumo dos principais pontos:

*   A imagem `web-ping` tem valores *padrão* para as variáveis de ambiente.
*   Pode-se fornecer valores *diferentes* ao criar o contêiner para mudar o comportamento do app.
*   **Exercício Try-it-now:** Remover o contêiner existente e rodar um novo, especificando um valor para a variável de ambiente `TARGET`.
    *   Comando para remover: `docker rm -f web-ping`.
    *   Comando para rodar novo: `docker container run --env TARGET=google.com diamol/ch03-web-ping`.
    *   `--env` (ou `-e`): Flag para definir uma variável de ambiente (`CHAVE=VALOR`).
*   Análise da saída da nova execução (Figura 3.3):
    *   Contêiner roda interativamente (sem `-d`), mostrando logs no console.
    *   App agora pinga `google.com` em vez de `blog.sixeyed.com`.
    *   Contêiner continua rodando até `Ctrl+C`.
*   **Takeaway principal:** Imagens Docker podem ter configurações padrão, mas devem permitir overrides via variáveis de ambiente na execução.
*   Diagrama (Figura 3.4) ilustrando variáveis de ambiente:
    *   Imagem tem valor padrão (TARGET=blog.sixeyed.com).
    *   Contêiner 1 usa o padrão.
    *   Contêiner 2 usa override (TARGET=google.com).
    *   Host tem suas próprias variáveis, separadas dos contêineres.
*   App é o mesmo (mesma imagem), comportamento muda devido à configuração.

🧠 Explicação detalhada e didática:

*   Esta página demonstra na prática como usar variáveis de ambiente para configurar um contêiner.
*   Primeiro, remove-se o contêiner anterior (`web-ping`).
*   Depois, um novo contêiner é iniciado com `docker run`, mas desta vez usando a flag `--env` (ou `-e`) para definir a variável `TARGET` com o valor `google.com`. Isso *sobrescreve* o valor padrão que estava definido na imagem.
*   Como o comando não usou `-d`, o contêiner roda em primeiro plano, e vemos imediatamente nos logs que ele está pingando `google.com`. Isso prova que a configuração via variável de ambiente funcionou.
*   A Figura 3.4 ilustra visualmente: a imagem é o blueprint com um valor padrão, mas cada contêiner criado a partir dela pode receber instruções diferentes (variáveis de ambiente) que alteram seu comportamento final, sem modificar a imagem original. O ambiente do host (seu Windows) também tem suas variáveis, mas elas são independentes das dos contêineres.
*   A lição é: ao criar imagens, pense em torná-las configuráveis via variáveis de ambiente para que possam ser usadas em diferentes cenários sem precisar de uma imagem diferente para cada caso.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # Remover o contêiner anterior
    docker rm -f web-ping

    # Rodar novo contêiner configurado via variável de ambiente
    docker container run --env TARGET=google.com diamol/ch03-web-ping
    # Ou forma abreviada:
    # docker container run -e TARGET=google.com diamol/ch03-web-ping
    ```
*   Observe a saída no terminal mostrando "Pinging google.com...".
*   Pressione `Ctrl+C` para parar o contêiner quando quiser.

💡 Analogias ou observações úteis (se houver):

*   Usar `--env` é como dar um post-it específico (`TARGET=google.com`) para *este* contêiner em particular, que terá prioridade sobre o post-it padrão que veio com a receita (imagem).

---

📄 Página 61

🔍 Resumo dos principais pontos:

*   Cabe ao autor da imagem Docker fornecer essa flexibilidade de configuração.
*   Introdução à construção da sua primeira imagem Docker a partir de um `Dockerfile`.
*   **3.2 Writing your first Dockerfile** (Escrevendo seu primeiro Dockerfile)
    *   `Dockerfile`: script simples com instruções para empacotar uma aplicação.
    *   Saída do Dockerfile: uma imagem Docker.
    *   Sintaxe simples; flexível (pode usar comandos shell padrão - Bash/Linux, PowerShell/Windows).
*   **Listagem 3.1:** Dockerfile completo para empacotar o app `web-ping`.
    ```dockerfile
    FROM diamol/node
    ENV TARGET="blog.sixeyed.com"
    ENV METHOD="HEAD"
    ENV INTERVAL="3000"
    WORKDIR /web-ping
    COPY app.js .
    CMD ["node", "/web-ping/app.js"]
    ```
*   Explicação das instruções (convenção: maiúsculas, mas não obrigatório):
    *   `FROM`: Toda imagem começa de outra imagem base. Aqui, usa `diamol/node` (que tem Node.js).
    *   `ENV`: Define variáveis de ambiente *padrão* na imagem (`CHAVE="VALOR"`).
    *   `WORKDIR`: Cria um diretório na imagem e define como diretório de trabalho atual para instruções seguintes (`COPY`, `CMD`, `RUN`). Sintaxe `/` funciona em Linux/Windows.
    *   `COPY`: Copia arquivos/diretórios do contexto de build (máquina local) para a imagem (`<origem_local> <destino_imagem>`). `.` como destino refere-se ao `WORKDIR`.
    *   `CMD`: Especifica o comando *padrão* a ser executado quando um contêiner é iniciado a partir da imagem. Aqui, roda `node` com o script `app.js`.

🧠 Explicação detalhada e didática:

*   Esta página introduz o `Dockerfile`, o arquivo de texto que contém as instruções passo a passo para o Docker construir uma imagem.
*   A Listagem 3.1 mostra um Dockerfile real para o aplicativo `web-ping`. Vamos analisar cada linha:
    *   `FROM diamol/node`: Diz ao Docker para começar construindo sobre a imagem `diamol/node`. Essa imagem base já contém o Node.js, que é necessário para rodar o `web-ping`. Isso evita que tenhamos que instalar o Node.js manualmente no nosso Dockerfile.
    *   `ENV ...`: Define três variáveis de ambiente (`TARGET`, `METHOD`, `INTERVAL`) com valores padrão. Estes são os valores que o `web-ping` usará se nenhum override for fornecido via `docker run -e`.
    *   `WORKDIR /web-ping`: Cria um diretório chamado `/web-ping` dentro da imagem (ou `C:\web-ping` se fosse uma imagem Windows) e define que os próximos comandos (como `COPY` e `CMD`) acontecerão dentro desse diretório.
    *   `COPY app.js .`: Copia o arquivo `app.js` (que deve estar na mesma pasta do Dockerfile no seu computador) para o diretório `/web-ping` dentro da imagem.
    *   `CMD ["node", "/web-ping/app.js"]`: Define o comando que será executado quando um contêiner for iniciado a partir desta imagem. Ele executará o Node.js para rodar o script `app.js` que foi copiado. A forma `["executável", "param1", "param2"]` (formato JSON) é geralmente preferida sobre a forma de shell (`CMD node /web-ping/app.js`).
*   Essas poucas instruções são suficientes para empacotar a aplicação, definindo seu ambiente base, configurações padrão, arquivos necessários e como ela deve ser iniciada.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você precisará ter o arquivo `Dockerfile` (com o conteúdo da Listagem 3.1) e o arquivo `app.js` (do código fonte do livro) em um mesmo diretório no seu Windows para poder construir a imagem (próximos passos). Use o VS Code para criar/editar esses arquivos. O diretório seria algo como `C:\DockerLabs\diamol\ch03\exercises\web-ping`.

💡 Analogias ou observações úteis (se houver):

*   Dockerfile é a receita de bolo detalhada.
    *   `FROM`: Qual forma/base de bolo usar.
    *   `ENV`: Ingredientes padrão (açúcar, etc.) que já vêm na receita, mas podem ser ajustados.
    *   `WORKDIR`: Em qual bancada da cozinha trabalhar.
    *   `COPY`: Trazer ingredientes externos (o `app.js` do seu PC) para a bancada.
    *   `CMD`: A instrução final de "assar o bolo por X minutos" (como rodar o app).

---

📄 Página 62

🔍 Resumo dos principais pontos:

*   Essas 5 instruções (`FROM`, `ENV`, `WORKDIR`, `COPY`, `CMD`) cobrem o essencial para empacotar apps.
*   O Dockerfile já segue boas práticas.
*   **Exercício Try-it-now:** Navegar até o diretório do código-fonte (`ch03/exercises/web-ping`) e listar os arquivos.
    *   Comandos: `cd ch03/exercises/web-ping`, `ls` (ou `dir` no Windows).
    *   Arquivos esperados: `Dockerfile`, `app.js`, `README.md`.
*   Figura 3.5 ilustra esses arquivos.
*   Não é necessário entender Node.js/JavaScript para empacotar/rodar o app.
*   O código em `app.js` é básico e usa bibliotecas padrão do Node.js para fazer chamadas HTTP e ler variáveis de ambiente.
*   Com esses arquivos, tem-se tudo para construir a imagem `web-ping`.

🧠 Explicação detalhada e didática:

*   A página reforça que as instruções vistas (`FROM`, `ENV`, `WORKDIR`, `COPY`, `CMD`) são os blocos de construção fundamentais para criar a maioria das imagens Docker.
*   Instrui o leitor a navegar, no seu sistema de arquivos local, até a pasta que contém os arquivos necessários para construir a imagem `web-ping`. Esses arquivos foram obtidos no Capítulo 1 ao clonar ou baixar o repositório do GitHub.
*   Os arquivos essenciais são:
    *   `Dockerfile`: A receita (como visto na página anterior).
    *   `app.js`: O código da aplicação em si.
    *   `README.md`: Arquivo de documentação (não usado no build).
*   Um ponto importante é destacado: você, como pessoa construindo a imagem, não precisa ser um especialista na linguagem de programação do aplicativo (Node.js neste caso). O `Dockerfile` abstrai os detalhes de como o aplicativo é executado. O código `app.js` simplesmente usa funcionalidades padrão do Node.js para realizar sua tarefa e ler as configurações das variáveis de ambiente.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:**
    1.  Abra o PowerShell ou terminal do VS Code.
    2.  Navegue até o diretório onde você clonou/extraiu o código do livro. Se você colocou em `C:\DockerLabs`, o comando seria:
        ```powershell
        cd C:\DockerLabs\diamol\ch03\exercises\web-ping
        ```
    3.  Liste os arquivos para confirmar:
        ```powershell
        dir
        # ou Get-ChildItem
        ```
        Você deve ver `Dockerfile`, `app.js`, e `README.md`.
    4.  (Opcional) Abra os arquivos `Dockerfile` e `app.js` no VS Code para inspecioná-los.

---

📄 Página 63

🔍 Resumo dos principais pontos:

*   **3.3 Building your own container image** (Construindo sua própria imagem de contêiner)
*   Para construir uma imagem, Docker precisa:
    *   Nome para a imagem (`--tag`).
    *   Localização dos arquivos a serem empacotados (`contexto`).
*   **Exercício Try-it-now:** Construir a imagem a partir do Dockerfile.
    *   Comando: `docker image build --tag web-ping .`
    *   `--tag web-ping`: Define o nome (e tag padrão `latest`) da imagem como `web-ping`.
    *   `.` (ponto final): Indica que o **contexto de build** é o diretório atual (onde estão o `Dockerfile` e `app.js`).
*   Saída do build mostra a execução das instruções do Dockerfile (Figura 3.6).
*   **Troubleshooting de erros no build:**
    *   Verificar se o Docker Engine está rodando (ícone da baleia).
    *   Verificar se está no diretório correto (`ch03/exercises/web-ping`).
    *   Verificar se digitou o comando corretamente (incluindo o `.` no final).
*   **Aviso sobre permissões de arquivo (Windows buildando Linux):**
    *   Pode aparecer um aviso sobre permissões.
    *   Windows e Linux tratam permissões de forma diferente.
    *   Arquivos copiados do Windows para imagem Linux terão permissões de leitura/escrita completas. Isso geralmente não é um problema para os exemplos do livro.

🧠 Explicação detalhada e didática:

*   Esta página ensina como executar o processo de construção da imagem usando o comando `docker image build` (ou `docker build`).
*   Os dois parâmetros essenciais são:
    *   `--tag <nome>:<tag>` (ou `-t`): Dá um nome e, opcionalmente, uma tag (versão) à imagem que será criada. Se a tag for omitida, `:latest` é usado por padrão. Aqui, a imagem será chamada `web-ping:latest`.
    *   `<caminho_para_contexto>`: Especifica o diretório no seu computador que contém o `Dockerfile` e todos os outros arquivos que podem ser necessários durante o build (como o `app.js` que será copiado pela instrução `COPY`). Usar `.` significa "use o diretório atual como contexto".
*   O Docker lê o `Dockerfile` no contexto, executa cada instrução em ordem, criando uma camada para cada uma (usando o cache se possível), e ao final, monta a imagem final com o nome e tag especificados.
*   A seção de troubleshooting é útil caso o comando falhe.
*   O aviso sobre permissões é comum ao construir imagens Linux a partir do Windows usando Docker Desktop. Para a maioria dos casos, pode ser ignorado, mas em cenários de produção mais rigorosos, o gerenciamento de permissões pode exigir atenção.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** Certifique-se de que você está no diretório `ch03\exercises\web-ping` no seu terminal PowerShell/VS Code.
*   Execute o comando de build:
    ```powershell
    docker image build --tag web-ping .
    ```
*   Observe a saída. Você verá o Docker passar por cada "Step" do Dockerfile:
    *   Step 1/7 : FROM diamol/node
    *   Step 2/7 : ENV TARGET="blog.sixeyed.com"
    *   ...
    *   Step 6/7 : COPY app.js .
    *   Step 7/7 : CMD ["node", "/web-ping/app.js"]
    *   Successfully built <id_da_imagem>
    *   Successfully tagged web-ping:latest
*   Ignore o aviso de permissões se ele aparecer.

---

📄 Página 64

🔍 Resumo dos principais pontos:

*   Mensagens "successfully built" e "successfully tagged" indicam sucesso.
*   Imagem construída é armazenada localmente no cache de imagens.
*   **Exercício Try-it-now:** Listar imagens começando com "w".
    *   Comando: `docker image ls 'w*'` (Aspas podem ser necessárias dependendo do shell).
    *   Saída esperada: mostra a imagem `web-ping` recém-criada, com `TAG=latest`, ID, data de criação e tamanho.
    *   Exemplo de saída: REPOSITORY=web-ping, TAG=latest, IMAGE ID=f2a5c430ab2a, CREATED=14 minutes ago, SIZE=75.3MB.

🧠 Explicação detalhada e didática:

*   Após a execução bem-sucedida do `docker image build`, a nova imagem (`web-ping:latest`) está pronta para ser usada e reside no armazenamento local do Docker Engine (o cache de imagens).
*   O comando `docker image ls` (ou `docker images`) é usado para listar as imagens disponíveis localmente. Ele pode ser combinado com filtros (`-f` ou `--filter`) para encontrar imagens específicas. O exemplo usa um filtro de nome (`reference=w*`) para listar imagens cujo nome começa com "w".
*   A saída do `docker image ls` mostra informações importantes sobre cada imagem:
    *   `REPOSITORY`: O nome base da imagem.
    *   `TAG`: A versão ou identificador específico (padrão `latest`).
    *   `IMAGE ID`: Um identificador único para o conteúdo da imagem.
    *   `CREATED`: Quando a imagem foi construída.
    *   `SIZE`: O tamanho total (virtual) da imagem.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:** No PowerShell/terminal, execute:
    ```powershell
    docker image ls 'w*'
    # ou simplesmente para ver todas:
    # docker image ls
    ```
*   Confirme que a imagem `web-ping` com a tag `latest` está na lista. Anote o tamanho (SIZE) reportado.

💡 Analogias ou observações úteis (se houver):

*   `docker image build` é o processo de seguir a receita (Dockerfile) para assar o bolo (criar a imagem).
*   `docker image ls` é como olhar na sua despensa para ver quais bolos (imagens) você já tem prontos.

---

📄 Página 65

🔍 Resumo dos principais pontos:

*   A imagem `web-ping` que você construiu pode ser usada da mesma forma que a baixada do Docker Hub (`diamol/ch03-web-ping`).
*   O conteúdo do app é o mesmo, e a configuração via variáveis de ambiente funciona igual.
*   **Exercício Try-it-now:** Rodar um contêiner a partir da *sua* imagem `web-ping`, configurando-o para pingar `docker.com` a cada 5 segundos.
    *   Comando: `docker container run -e TARGET=docker.com -e INTERVAL=5000 web-ping`.
*   Saída esperada (Figura 3.7): Logs mostrando que o alvo é `docker.com` e o intervalo é `5000` ms.
*   Contêiner roda em primeiro plano; parar com `Ctrl+C`.
*   O contêiner entrará no estado `Exited`.
*   Recapitulação: Empacotou um app simples; processo é similar para apps complexos.
    *   Escrever Dockerfile com os passos.
    *   Coletar recursos necessários.
    *   Decidir como permitir configuração pelo usuário (ex: via ENV).
*   **3.4 Understanding Docker images and image layers** (Entendendo imagens Docker e camadas de imagem)
    *   Introdução à construção de mais imagens e à importância de entender camadas e sua relação com contêineres.
    *   Imagem Docker contém arquivos (sistema de arquivos do contêiner) e metadados (incluindo histórico de build).
    *   Pode-se ver o histórico de camadas de uma imagem.
    *   **Exercício Try-it-now:** Verificar o histórico da imagem `web-ping`.
        *   Comando: `docker image history web-ping`.

🧠 Explicação detalhada e didática:

*   Agora que você construiu sua própria versão da imagem `web-ping`, esta página mostra que ela funciona exatamente como a versão pré-construída do Docker Hub, incluindo a capacidade de ser configurada via variáveis de ambiente (`-e`). O exercício demonstra isso rodando um contêiner da sua imagem local, mas configurado para pingar `docker.com` em vez do padrão.
*   A seção 3.4 reintroduz o conceito de **camadas de imagem (image layers)**, mencionando que o histórico de como uma imagem foi construída (quais comandos do Dockerfile geraram quais camadas) pode ser inspecionado. Isso é fundamental para entender como otimizar os builds.
*   O comando `docker image history <imagem>` permite visualizar essa sequência de camadas e os comandos que as criaram.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Execute o contêiner a partir da sua imagem `web-ping` local, com configuração customizada:
        ```powershell
        docker container run -e TARGET=docker.com -e INTERVAL=5000 web-ping
        ```
        Observe os logs confirmando as configurações. Pressione `Ctrl+C` para parar.
    2.  Inspecione o histórico da sua imagem:
        ```powershell
        docker image history web-ping
        ```
        Observe a saída (analisada na próxima página). Cada linha corresponde (aproximadamente) a uma instrução no seu Dockerfile.

---

📄 Página 66

🔍 Resumo dos principais pontos:

*   Análise da saída do `docker image history` (exemplo abreviado).
    *   Mostra IMAGE ID (da camada), CREATED (quando), CREATED BY (comando Dockerfile), SIZE, COMMENT.
    *   Observação: `<missing>` pode aparecer para camadas da imagem base que não estão totalmente presentes localmente.
    *   Relação um-para-um: cada linha no Dockerfile cria uma camada de imagem.
*   **Teoria sobre camadas de imagem:**
    *   Imagem Docker é uma coleção *lógica* de camadas.
    *   Camadas são os arquivos *físicos* armazenados no cache do Docker Engine.
    *   **Importância:** Camadas podem ser **compartilhadas** entre diferentes imagens e contêineres.
    *   Exemplo: Múltiplos apps Node.js compartilharão as mesmas camadas base do Node.js runtime.
*   Diagrama (Figura 3.8): Ilustra como camadas são logicamente empilhadas.
    *   Imagem `diamol/node`: Camada OS + Camada Node.js runtime.
    *   Imagem `web-ping`: Começa com as camadas de `diamol/node` (via `FROM`) + adiciona camadas próprias (ENV, WORKDIR, COPY, CMD).
*   Tamanho da imagem `diamol/node` (Linux: ~75MB, Windows: ~300MB).
*   Arquivo `app.js` é pequeno (KB).
*   Pergunta: Qual o tamanho total da imagem `web-ping`?

🧠 Explicação detalhada e didática:

*   A página analisa a saída do `docker image history`, mostrando que cada instrução do `Dockerfile` (`FROM`, `ENV`, `COPY`, `CMD`, etc.) resulta na criação de uma **camada** na imagem final.
*   Aprofunda a teoria das camadas:
    *   **Compartilhamento:** Este é o conceito mais crucial. Se você tem várias imagens que usam a mesma base (ex: `FROM ubuntu:20.04`), o Docker armazena as camadas do Ubuntu apenas *uma vez* no disco. Todas as imagens que derivam dela simplesmente apontam para essas camadas compartilhadas. O mesmo vale para camadas intermediárias idênticas. Isso economiza muito espaço em disco e tempo de download/upload.
    *   **Construção:** Uma imagem final é formada pelo empilhamento lógico dessas camadas. A Figura 3.8 mostra isso claramente: a imagem `web-ping` "herda" as camadas da imagem `diamol/node` e adiciona suas próprias camadas por cima.
*   Levanta a questão sobre o tamanho da imagem `web-ping`, dado que ela reutiliza a maior parte da imagem `diamol/node`.

💻 Exemplos práticos (para meu ambiente Windows):

*   Revise a saída do `docker image history web-ping` que você executou. Tente mapear cada linha (especialmente a coluna `CREATED BY`) para as instruções no seu `Dockerfile` da Listagem 3.1. Note que algumas instruções como `ENV` podem não adicionar tamanho significativo (SIZE 0B).

💡 Analogias ou observações úteis (se houver):

*   Reutilização de camadas é como compartilhar ingredientes entre receitas. Se 10 receitas diferentes começam com "use 1kg de farinha base", você só precisa ter 1kg dessa farinha na despensa, não 10kg. A imagem `web-ping` usa a "farinha base" do `diamol/node` e só adiciona seus ingredientes extras (`app.js`, `ENV`, `CMD`).

---

📄 Página 67

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Listar imagens com `docker image ls` (sem filtro) para ver os tamanhos.
    *   Saída esperada (Figura 3.9): Mostra várias imagens Node.js (`diamol/node`, `diamol/ch03-web-ping`, `web-ping`), todas com tamanho similar (~75MB em Linux).
*   **Tamanho Lógico vs. Físico:**
    *   A coluna `SIZE` em `docker image ls` mostra o tamanho *lógico* (ou virtual) da imagem - o tamanho total se todas as camadas fossem descompactadas e não houvesse compartilhamento.
    *   Se as camadas são compartilhadas, o espaço *físico* real usado no disco é menor.
    *   Não é possível ver o compartilhamento diretamente em `docker image ls`.
*   Comando `docker system df` mostra o uso real de disco pelo Docker.
*   **Exercício Try-it-now:** Verificar o uso de disco do Docker.
    *   Comando: `docker system df`.
*   Análise da saída do `docker system df` (Figura 3.10):
    *   Mostra tamanho total das imagens (lógico), contêineres, volumes locais e cache de build.
    *   Mostra o espaço *real* usado pelas imagens (coluna SIZE) e quanto espaço é recuperável (RECLAIMABLE).
    *   Exemplo do autor: Tamanho lógico 363MB, tamanho real 202MB -> 163MB economizados (45%) devido ao compartilhamento.
*   Economia de disco é maior com muitas imagens compartilhando as mesmas bases.

🧠 Explicação detalhada e didática:

*   Esta página aborda a diferença entre o tamanho que o `docker image ls` reporta e o espaço real que as imagens ocupam no disco.
*   O comando `docker image ls` mostra o tamanho **lógico**, que é a soma dos tamanhos de todas as camadas da imagem, como se não houvesse compartilhamento. Isso pode ser enganoso, pois faz parecer que imagens similares ocupam muito mais espaço do que realmente ocupam.
*   O comando `docker system df` fornece uma visão mais precisa do uso de disco pelo Docker. Ele mostra separadamente o espaço ocupado por imagens, contêineres, volumes e o cache de build. Crucialmente, para as imagens, ele mostra o tamanho total *real* (considerando o compartilhamento de camadas) e quanto desse espaço poderia ser liberado removendo imagens não utilizadas (reclaimable).
*   O exemplo do autor demonstra uma economia significativa (45%) graças ao compartilhamento de camadas. Em ambientes com muitas aplicações baseadas nas mesmas tecnologias (ex: muitas aplicações Java, muitas Node.js), essa economia pode ser ainda maior.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # Listar todas as imagens e observar os tamanhos lógicos
    docker image ls

    # Verificar o uso real de disco e o compartilhamento
    docker system df
    ```
*   Compare o `TOTAL SIZE` das imagens em `docker image ls` com o `SIZE` da linha `Images` em `docker system df`. O segundo valor (real) provavelmente será menor que o primeiro (lógico), especialmente se você tiver outras imagens além das usadas neste capítulo.

---

📄 Página 68

🔍 Resumo dos principais pontos:

*   Economia via reuso é maior com muitas imagens compartilhando bases comuns (Java, .NET Core, PHP, etc.).
*   **Teoria final sobre camadas:**
    *   Como camadas são compartilhadas, elas *não podem* ser editadas (são **read-only / somente leitura**).
    *   Se uma camada pudesse ser alterada, afetaria todas as imagens que a compartilham.
*   Docker garante isso tornando as camadas de imagem imutáveis após criadas.
*   Pode-se usar esse conhecimento para otimizar Dockerfiles, tornando builds mais rápidos e imagens menores.
*   **3.5 Optimizing Dockerfiles to use the image layer cache** (Otimizando Dockerfiles para usar o cache de camadas da imagem)
    *   A camada da imagem `web-ping` que contém `app.js` é específica dela.
    *   Se `app.js` for alterado e a imagem reconstruída, uma *nova* camada será gerada para a instrução `COPY`.
    *   Docker assume sequência definida de camadas. Se uma camada muda no meio, Docker não assume que pode reusar camadas *posteriores*.
    *   **Exercício Try-it-now:** Modificar `app.js` (ex: adicionar linha vazia) e reconstruir a imagem com uma nova tag (`web-ping:v2`).
        *   Comando: `docker image build -t web-ping:v2 .`

🧠 Explicação detalhada e didática:

*   A página introduz uma propriedade fundamental das camadas: elas são **imutáveis (read-only)**. Uma vez que uma camada é criada durante o `docker build`, ela nunca mais é alterada. Isso é essencial para permitir o compartilhamento seguro entre múltiplas imagens.
*   Essa imutabilidade é a base para a **otimização do cache de build**. O Docker tenta reutilizar camadas de builds anteriores sempre que possível para acelerar o processo.
*   A seção 3.5 começa a explicar como funciona o cache:
    *   Quando você reconstrói uma imagem, o Docker compara cada instrução do `Dockerfile` (e os arquivos que ela usa, como no `COPY`) com as camadas que ele já tem no cache de builds anteriores.
    *   Se a instrução e seus arquivos de entrada não mudaram, o Docker reutiliza a camada do cache (você verá "Using cache" na saída do build).
    *   **Importante:** Assim que uma instrução *não* pode usar o cache (porque ela mudou, ou um arquivo que ela copia mudou), o Docker executa essa instrução *e todas as instruções subsequentes* no Dockerfile, mesmo que elas não tenham mudado. Isso é chamado de "quebra do cache" (cache busting).
*   O exercício prepara para demonstrar isso: modificando o `app.js` e reconstruindo a imagem, a instrução `COPY app.js .` não poderá usar o cache, quebrando-o naquele ponto.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Abra o arquivo `app.js` (em `C:\DockerLabs\diamol\ch03\exercises\web-ping`) no VS Code.
    2.  Faça uma pequena alteração. Adicionar um comentário (`// comentario`) ou uma linha em branco no final é suficiente. Salve o arquivo.
    3.  No terminal (no mesmo diretório), reconstrua a imagem com uma nova tag:
        ```powershell
        docker image build -t web-ping:v2 .
        ```
    4.  Observe atentamente a saída do build (analisada na próxima página). Veja quais steps usam o cache e a partir de qual step o cache é quebrado.

---

📄 Página 69

🔍 Resumo dos principais pontos:

*   Análise da saída do build `web-ping:v2` (Figura 3.11).
    *   Steps 1-5 (FROM, ENV, WORKDIR) usam o cache (`---> Using cache`).
    *   Step 6 (COPY app.js .) executa novamente, pois `app.js` mudou. Cache quebrado aqui.
    *   Step 7 (CMD) executa novamente, mesmo não tendo mudado, porque o cache foi quebrado no step anterior.
*   **Como funciona o cache:**
    *   Docker calcula um hash (impressão digital) da instrução e dos arquivos copiados.
    *   Se o hash coincide com uma camada existente, usa o cache.
    *   Se não há correspondência, executa a instrução -> cache quebrado.
    *   Após quebra do cache, todas as instruções seguintes são executadas.
*   **Impacto:** Mesmo pequenas mudanças podem invalidar o cache para várias etapas.
*   **Otimização:** Ordenar as instruções no Dockerfile pela frequência de mudança.
    *   Instruções que mudam raramente (ex: `FROM`, instalar dependências base) devem vir *primeiro*.
    *   Instruções que mudam frequentemente (ex: `COPY` do código fonte da aplicação) devem vir por *último*.
    *   Objetivo: Na maioria dos builds (onde só o código da app muda), apenas as últimas instruções são executadas, maximizando o uso do cache.
*   Exemplo de otimização para `web-ping`:
    *   Instrução `CMD` não precisa ser a última; pode ir para perto do topo, pois raramente muda.
    *   Instruções `ENV` podem ser combinadas em uma só.
*   **Listagem 3.2:** Dockerfile otimizado para `web-ping`.
    ```dockerfile
    FROM diamol/node
    CMD ["node", "/web-ping/app.js"] # CMD movido para cima
    ENV TARGET="blog.sixeyed.com" \ # ENV combinado
        METHOD="HEAD" \
        INTERVAL="3000"
    WORKDIR /web-ping
    COPY app.js . # COPY fica por último
    ```

🧠 Explicação detalhada e didática:

*   A página explica detalhadamente por que o build `v2` se comportou daquela forma: a mudança no `app.js` invalidou o cache para a instrução `COPY` (Step 6), e como consequência, a instrução `CMD` (Step 7) também teve que ser reexecutada.
*   A regra de ouro da otimização do cache é apresentada: **Ordene suas instruções da menos volátil para a mais volátil.**
    *   Coisas que quase nunca mudam (como a imagem base `FROM`, a instalação de ferramentas básicas do SO) devem vir no início do Dockerfile.
    *   Coisas que mudam com frequência (como copiar o código-fonte da sua aplicação, que você altera a todo momento durante o desenvolvimento) devem vir o mais tarde possível no Dockerfile.
*   Aplicando isso ao `web-ping`: O comando de inicialização (`CMD`) provavelmente não mudará, então pode ser movido para cima. As variáveis de ambiente (`ENV`) também são relativamente estáveis e podem ser combinadas. A cópia do `app.js` (`COPY`) é o que muda mais, então deve ficar por último. A Listagem 3.2 mostra essa versão otimizada.

💻 Exemplos práticos (para meu ambiente Windows):

*   Compare a Listagem 3.2 (otimizada) com a Listagem 3.1 (original). Note a mudança na ordem de `CMD` e `COPY`, e a combinação dos `ENV`.
*   Você testará essa versão otimizada na próxima página.

💡 Analogias ou observações úteis (se houver):

*   O cache do Docker é como seguir uma receita de bolo onde você já fez partes dela antes. Se você só mudar o último passo (a cobertura), você pode reutilizar todas as etapas anteriores (massa, assar). Mas se você mudar um ingrediente no meio da receita (ex: tipo de farinha), você terá que refazer todos os passos daquele ponto em diante. A otimização ordena a receita para que as mudanças mais comuns (a cobertura) fiquem no final.

---

📄 Página 70

🔍 Resumo dos principais pontos:

*   Dockerfile otimizado está no código fonte (`web-ping-optimized`).
*   **Exercício Try-it-now:** Navegar para a pasta `web-ping-optimized` e construir a imagem com a nova versão do Dockerfile, tag `web-ping:v3`.
    *   Comandos: `cd ../web-ping-optimized`, `docker image build -t web-ping:v3 .`
*   Resultado do build `v3`: similar ao anterior (5 steps em vez de 7), imagem final tem o mesmo comportamento.
*   **Benefício da otimização:** Se agora você alterar `app.js` e reconstruir `v3`, *apenas* o último step (`COPY`) será executado, pois os steps anteriores (`FROM`, `CMD`, `ENV`, `WORKDIR`) usarão o cache.
*   **Conclusão do capítulo sobre build:**
    *   Viu sintaxe do Dockerfile e instruções chave.
    *   Aprendeu a construir e trabalhar com imagens via CLI.
*   **Dois pontos importantes:**
    1.  Otimizar Dockerfiles (ordem das instruções).
    2.  Tornar a imagem portável/configurável (ex: via ENV) para usar a mesma imagem em diferentes ambientes.
    *   Benefícios: builds rápidos, deploys em produção usando a *mesma* imagem testada.
*   **3.6 Lab** (Laboratório)
    *   Objetivo: Produzir uma imagem Docker *sem* usar um Dockerfile.
    *   Cenário: Simula a necessidade de passos manuais pós-instalação que não podem ser scriptados.
    *   Tarefa:
        1.  Usar a imagem `diamol/ch03-lab` do Docker Hub.
        2.  Essa imagem tem um arquivo `/diamol/ch03.txt`.
        3.  Atualizar esse arquivo *dentro de um contêiner* (adicionar seu nome no final).
        4.  Produzir uma *nova imagem* com o arquivo modificado.
        5.  *Restrição:* Não pode usar `docker build` ou `Dockerfile`.
    *   Solução de exemplo no GitHub.

🧠 Explicação detalhada e didática:

*   A página instrui a construir a imagem `v3` usando o Dockerfile otimizado. O build em si não parecerá muito diferente, mas a estrutura agora está melhor preparada para mudanças futuras. O benefício real é percebido ao fazer uma alteração *após* esse build: somente a última instrução (`COPY`) precisará ser executada, tornando o build subsequente muito mais rápido.
*   Resume os aprendizados do capítulo (sintaxe Dockerfile, build de imagens) e reforça as boas práticas de otimização e configuração para portabilidade.
*   Introduz o **Lab do Capítulo 3**. O desafio aqui é interessante e ensina uma técnica diferente: como "salvar" o estado de um contêiner modificado como uma nova imagem, *sem* passar pelo processo de `docker build` a partir de um Dockerfile. Isso é útil em cenários específicos, como capturar um estado após uma configuração manual ou para depuração. A tarefa envolve iniciar um contêiner da imagem base, modificar um arquivo dentro dele e, em seguida, criar uma nova imagem a partir desse estado modificado.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais (Build v3):**
    ```powershell
    # Navegar para a pasta otimizada
    cd ..\web-ping-optimized
    # Ou C:\DockerLabs\diamol\ch03\exercises\web-ping-optimized

    # Construir a imagem v3
    docker image build -t web-ping:v3 .

    # (Opcional) Modifique app.js novamente e reconstrua v3 para ver o cache em ação:
    # (faça uma pequena edição em app.js e salve)
    # docker image build -t web-ping:v3 . # Observe que só o Step 5/5 deve rodar
    ```
*   **Estratégia para o Lab 3:**
    1.  Inicie um contêiner interativo a partir de `diamol/ch03-lab`: `docker run -it --name lab3-mod diamol/ch03-lab sh` (assumindo shell `sh`).
    2.  Dentro do contêiner, edite o arquivo `/diamol/ch03.txt` (use um editor como `vi` ou `nano` se disponível, ou comandos como `echo "Meu Nome" >> /diamol/ch03.txt`).
    3.  Saia do contêiner (`exit`). O contêiner estará parado (`Exited`).
    4.  Use o comando `docker container commit <nome_ou_id_container> <nome_nova_imagem>:<tag>` para criar a nova imagem a partir do contêiner parado. Ex: `docker container commit lab3-mod minha-imagem-lab3:latest`.
    5.  Verifique com `docker image ls` e `docker run minha-imagem-lab3:latest cat /diamol/ch03.txt`.

---

📄 Página 71

🔍 Resumo dos principais pontos:

*   **Continuação das dicas do Lab 3:**
    *   Relembra flags `-it` para rodar contêiner interativamente.
    *   Sistema de arquivos de um contêiner `Exited` ainda existe.
    *   Usar `docker container --help` para encontrar comandos úteis (dois deles ajudarão a resolver o lab - provavelmente `docker exec` ou `docker run -it` para modificar, e `docker commit` para salvar).
*   **Capítulo 4: Packaging applications from source code into Docker Images** (Empacotando aplicações a partir do código-fonte em Imagens Docker)
*   Revisão: Cap. 3 ensinou a construir imagens com `Dockerfile` e algumas instruções.
*   Novidade do Cap. 4: Instrução `RUN` para executar comandos *durante* o build.
    *   Comandos `RUN` executam dentro de um contêiner temporário.
    *   Quaisquer mudanças no sistema de arquivos (arquivos criados/modificados) são salvas na camada da imagem resultante da instrução `RUN`.
*   Flexibilidade: `RUN` permite fazer quase tudo (expandir zips, rodar instaladores, compilar código).
*   Foco do Cap. 4: Usar `RUN` para empacotar aplicações diretamente do código-fonte.
*   **4.1 Who needs a build server when you have a Dockerfile?** (Quem precisa de um servidor de build quando se tem um Dockerfile?)
    *   Processo tradicional de build: código no controle de versão (ex: GitHub), servidor de build separado que compila/testa o código.
    *   Desafios do servidor de build:
        *   Manutenção: Instalar e manter atualizadas todas as ferramentas necessárias (SDKs, compiladores, etc.) para todos os projetos.
        *   Inconsistência: Diferenças entre ferramentas locais dos devs e as do servidor podem causar falhas.
        *   Novos devs levam tempo para configurar o ambiente local.

🧠 Explicação detalhada e didática:

*   A página finaliza as dicas para o Lab 3, sugerindo os comandos que provavelmente serão necessários (`run -it` ou `exec` para entrar/modificar, e `commit` para salvar a imagem).
*   Inicia o Capítulo 4, que introduz uma capacidade crucial do `Dockerfile`: a instrução `RUN`. Diferente de `CMD` ou `ENTRYPOINT` (que definem o que roda quando o *contêiner* inicia), `RUN` executa um comando *durante o processo de construção da imagem*. O resultado desse comando (ex: arquivos criados por um compilador, pacotes instalados) se torna parte da imagem.
*   A seção 4.1 apresenta um caso de uso poderoso para a instrução `RUN`: realizar o processo de **compilação e build** da aplicação *dentro* do Dockerfile. Isso elimina a necessidade de um servidor de build tradicional separado, com todas as suas ferramentas instaladas. Os problemas comuns de servidores de build (manutenção complexa, inconsistências com ambientes locais) são resolvidos, pois o ambiente de build é definido no próprio Dockerfile e encapsulado em contêineres.

💻 Exemplos práticos (para meu ambiente Windows):

*   A instrução `RUN` no Dockerfile pode executar comandos do sistema operacional da imagem base.
    *   Exemplo (Linux): `RUN apt-get update && apt-get install -y curl`
    *   Exemplo (Windows): `RUN powershell -Command Install-WindowsFeature Web-Server`
*   No Capítulo 4, você verá `RUN` sendo usado para executar comandos de build como `mvn package` (Java), `npm install` (Node.js), `go build` (Go).

💡 Analogias ou observações úteis (se houver):

*   Instrução `RUN`: Pense nisso como passos intermediários na receita (Dockerfile) que modificam os ingredientes ou a massa *antes* do bolo final (imagem) estar pronto. Ex: "Misture a farinha e os ovos" (`RUN compile code`), "Adicione o fermento" (`RUN install dependency`).
*   Build com Dockerfile vs. Servidor de Build: É como ter uma cozinha portátil super equipada (contêiner de build definido no Dockerfile) que você pode levar para qualquer lugar e garante que o bolo sairá igual, versus depender de uma cozinha industrial fixa (servidor de build) que pode ter equipamentos diferentes ou desatualizados.

---

📄 Página 72

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 4.1): Ilustra a necessidade de ferramentas de build consistentes entre desenvolvedores e servidor de build.
*   Problemas com ferramentas de build tradicionais: sobrecarga de manutenção, inconsistência de versões, dificuldade para novos membros da equipe.
*   Solução Docker: Empacotar o conjunto de ferramentas de build em uma imagem Docker.
    *   Dockerfile define a instalação das ferramentas -> Imagem de ferramentas de build.
    *   Usar essa imagem nos Dockerfiles das aplicações para compilar o código-fonte.
    *   Resultado final: imagem da aplicação empacotada.
*   Introdução a **Dockerfiles multi-estágio (multi-stage Dockerfiles)**.
*   **Listagem 4.1:** Exemplo básico de Dockerfile multi-estágio.
    ```dockerfile
    FROM diamol/base AS build-stage # Estágio 1 nomeado 'build-stage'
    RUN echo 'Building...' > /build.txt

    FROM diamol/base AS test-stage  # Estágio 2 nomeado 'test-stage'
    COPY --from=build-stage /build.txt /build.txt # Copia do estágio anterior
    RUN echo 'Testing...' >> /build.txt

    FROM diamol/base # Estágio final (sem nome)
    COPY --from=test-stage /build.txt /build.txt # Copia do estágio anterior
    CMD cat /build.txt
    ```
*   **Características multi-estágio:**
    *   Múltiplas instruções `FROM` no mesmo Dockerfile.
    *   Cada `FROM` inicia um novo estágio de build.
    *   Estágios podem ser nomeados usando `AS <nome>`.
    *   A imagem final contém *apenas* o resultado do *último* estágio.
    *   Pode-se copiar artefatos (arquivos/diretórios) de estágios anteriores para estágios posteriores usando `COPY --from=<nome_estagio> <origem> <destino>`.

🧠 Explicação detalhada e didática:

*   A página continua argumentando contra a complexidade dos servidores de build tradicionais e propõe a solução Docker: criar uma "imagem de ferramentas de build" que contenha tudo que é necessário para compilar seus projetos.
*   A técnica chave para isso é o **Dockerfile multi-estágio**. Em vez de ter um único fluxo `FROM ... RUN ... COPY ... CMD`, um Dockerfile multi-estágio tem várias seções `FROM`, cada uma definindo um estágio de build independente.
*   **Listagem 4.1** demonstra isso:
    *   O primeiro `FROM` inicia o estágio `build-stage`. Ele usa a imagem `diamol/base` e cria um arquivo `/build.txt`.
    *   O segundo `FROM` inicia o estágio `test-stage`. Ele também usa `diamol/base`. Crucialmente, ele usa `COPY --from=build-stage` para copiar o `/build.txt` criado no estágio anterior. Depois, adiciona texto a esse arquivo.
    *   O terceiro `FROM` inicia o estágio final. Ele copia o `/build.txt` modificado do estágio `test-stage`. O `CMD` final exibe o conteúdo desse arquivo.
*   **Importante:** A imagem Docker resultante conterá *apenas* o sistema de arquivos do *último* estágio. Os estágios anteriores (`build-stage`, `test-stage`) são descartados, a menos que algo deles seja explicitamente copiado para o estágio final usando `COPY --from`. Isso é excelente para otimização, como veremos.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você criará um arquivo `Dockerfile` com o conteúdo da Listagem 4.1 em um diretório (ex: `C:\DockerLabs\diamol\ch04\exercises\multi-stage`).
*   Você usará `docker build -t multi-stage .` para construir a imagem a partir deste Dockerfile multi-estágio (próximo passo).

💡 Analogias ou observações úteis (se houver):

*   Dockerfile multi-estágio é como ter várias bancadas de cozinha para preparar um prato complexo:
    *   Bancada 1 (`build-stage`): Picar os vegetais.
    *   Bancada 2 (`test-stage`): Pegar os vegetais picados da Bancada 1 (`COPY --from`), misturar com o molho.
    *   Bancada Final: Pegar a mistura da Bancada 2 (`COPY --from`), colocar no prato final.
    *   Só o prato final (último estágio) é servido (imagem final). As bancadas intermediárias são limpas e descartadas.

---

📄 Página 73

🔍 Resumo dos principais pontos:

*   Introdução da instrução `RUN`: executa um comando *dentro* de um contêiner durante o build.
    *   Qualquer saída/mudança no filesystem é salva na camada da imagem.
    *   Pode executar qualquer comando que *exista* na imagem base usada no `FROM` daquele estágio.
    *   Exemplo usou `echo` que está em `diamol/base`.
*   Diagrama (Figura 4.2): Ilustra a execução sequencial dos estágios do Dockerfile multi-estágio.
*   **Isolamento dos estágios:**
    *   Cada estágio roda independentemente.
    *   Podem usar imagens base diferentes com ferramentas diferentes.
    *   Estágio final só contém o que for explicitamente copiado de estágios anteriores.
    *   Se um comando falhar em *qualquer* estágio, o build *inteiro* falha.
*   **Exercício Try-it-now:** Navegar para a pasta `ch04/exercises/multi-stage` e construir a imagem multi-estágio.
    *   Comandos: `cd ch04/exercises/multi-stage`, `docker image build -t multi-stage .`
*   Saída do build (Figura 4.3) mostra a execução sequencial dos estágios.

🧠 Explicação detalhada e didática:

*   A página formaliza a instrução `RUN`, explicando que ela executa comandos *durante o build*, e o resultado (novos arquivos, pacotes instalados, etc.) é capturado na camada da imagem criada por essa instrução. É fundamental que o comando usado no `RUN` esteja disponível na imagem base definida no `FROM` do estágio atual.
*   A Figura 4.2 reforça que os estágios são executados em sequência.
*   O **isolamento** entre os estágios é reiterado: cada um é um ambiente temporário separado. Apenas o que é explicitamente transferido entre eles usando `COPY --from` persiste no estágio seguinte. Isso é crucial para a otimização: pode-se usar uma imagem base grande com muitas ferramentas de build em um estágio inicial, compilar o aplicativo, e então copiar *apenas* o executável/binário compilado para um estágio final que usa uma imagem base mínima (sem as ferramentas de build), resultando em uma imagem final muito menor e mais segura.
*   O exercício instrui a construir a imagem do Dockerfile multi-estágio da Listagem 4.1.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:**
    1.  Certifique-se de ter o `Dockerfile` da Listagem 4.1 salvo em `C:\DockerLabs\diamol\ch04\exercises\multi-stage`.
    2.  No PowerShell/terminal, navegue até essa pasta:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\multi-stage
        ```
    3.  Construa a imagem:
        ```powershell
        docker image build -t multi-stage .
        ```
    4.  Observe a saída (Figura 4.3). Você verá o Docker processando os 3 estágios (provavelmente 9 steps no total). Note as mensagens "Running in ..." para as instruções `RUN` e "Copying ..." para as instruções `COPY --from`.

---

📄 Página 74

🔍 Resumo dos principais pontos:

*   Figura 4.3: Exemplo de saída do build multi-estágio.
*   Padrão multi-estágio é aplicável a apps de qualquer complexidade.
*   Diagrama (Figura 4.4): Ilustra um fluxo de trabalho multi-estágio para uma aplicação Java.
    *   **Estágio `build`:**
        *   Usa imagem base com ferramentas de build Java (ex: JDK, Maven).
        *   Copia código-fonte do host.
        *   Executa comando de build (`RUN mvn package`).
    *   **Estágio `test` (opcional):**
        *   Usa imagem base com framework de teste.
        *   Copia binários compilados do estágio `build`.
        *   Executa testes (`RUN ./run-tests.sh`).
    *   **Estágio final (aplicação):**
        *   Usa imagem base *apenas* com o runtime Java (JRE).
        *   Copia binários testados do estágio `build` (ou `test`).
        *   Define `CMD` ou `ENTRYPOINT` para rodar o app.

🧠 Explicação detalhada e didática:

*   A Figura 4.3 mostra a saída real do build do exemplo anterior.
*   A página então generaliza o conceito multi-estágio para um cenário mais realista: construir uma aplicação Java (Figura 4.4).
*   O fluxo típico é:
    1.  **Build Stage:** Usa uma imagem que contém o Java Development Kit (JDK) e a ferramenta de build (como Maven ou Gradle). O código-fonte é copiado para este estágio, e um comando `RUN` executa o build (ex: `mvn package`), gerando os arquivos `.jar` ou `.war`.
    2.  **Test Stage (Opcional):** Pode-se ter um estágio intermediário para rodar testes unitários ou de integração. Ele usaria uma imagem com as ferramentas de teste necessárias, copiaria os artefatos compilados do estágio de build e executaria os testes com um comando `RUN`. Se os testes falharem, o build inteiro falha.
    3.  **Final Stage (Runtime):** Este estágio usa uma imagem base *mínima*, contendo apenas o Java Runtime Environment (JRE), que é necessário para *executar* a aplicação, mas não para compilá-la. Ele copia *apenas* os artefatos compilados e testados (o `.jar`/`.war`) do estágio de build (ou teste) e define como iniciar a aplicação (`CMD` ou `ENTRYPOINT`).
*   **Benefício:** A imagem final contém apenas o necessário para rodar a aplicação (JRE + app compilado), sem incluir o pesado JDK e as ferramentas de build, resultando em uma imagem muito menor e mais segura.

💻 Exemplos práticos (para meu ambiente Windows):

*   Este é o padrão que será usado nos próximos exemplos com Java, Node.js e Go. Você verá Dockerfiles que seguem essa estrutura de múltiplos `FROM`, `COPY --from`, e `RUN` para compilar/testar, e um estágio final enxuto.

💡 Analogias ou observações úteis (se houver):

*   Continuando a analogia da cozinha:
    *   Estágio de Build: Uma cozinha industrial completa (imagem com JDK/Maven) onde você prepara a massa e assa o bolo (compila o JAR).
    *   Estágio de Teste: Uma área de controle de qualidade onde o bolo é provado (testes unitários).
    *   Estágio Final: Uma embalagem bonita e leve (imagem com JRE mínimo) onde você coloca *apenas* o bolo pronto e testado (JAR), sem levar a cozinha industrial junto.

---

📄 Página 75

🔍 Resumo dos principais pontos:

*   Abordagem multi-estágio torna a aplicação verdadeiramente portátil (construir e rodar em qualquer lugar só com Docker).
*   Servidor de build só precisa do Docker instalado.
*   Configuração de novos membros da equipe simplificada.
*   Ferramentas de build centralizadas em imagens Docker, sem risco de dessincronização.
*   Muitos frameworks já têm imagens públicas no Docker Hub com ferramentas de build e runtimes separados (mantidas pelas equipes dos projetos).
*   Pode-se usar essas imagens diretamente ou como base para imagens customizadas.
*   **4.2 App walkthrough: Java source code** (Passo a passo da aplicação: código-fonte Java)
    *   Exemplo real: aplicação Java Spring Boot simples.
    *   Não é preciso ser dev Java ou ter ferramentas Java instaladas.
    *   Ferramentas usadas: Maven (build, dependências via `pom.xml`, comando `mvn`) e OpenJDK (runtime/JDK gratuito).
*   **Listagem 4.2:** Dockerfile para construir o app Java com Maven (multi-estágio).
    ```dockerfile
    FROM diamol/maven AS builder # Estágio builder com Maven/JDK
    WORKDIR /usr/src/iotd
    COPY pom.xml .
    RUN mvn -B dependency:go-offline # Otimização: baixa dependências primeiro
    COPY . . # Copia resto do código fonte
    RUN mvn package # Compila e empacota (gera JAR)

    # app # Estágio final
    FROM diamol/openjdk # Base apenas com JRE
    WORKDIR /app
    COPY --from=builder /usr/src/iotd/target/iotd-service-0.1.0.jar . # Copia SÓ o JAR
    EXPOSE 80 # Informa que o app ouve na porta 80
    ENTRYPOINT ["java", "-jar", "/app/iotd-service-0.1.0.jar"] # Comando para rodar
    ```

🧠 Explicação detalhada e didática:

*   A página reforça os benefícios do build multi-estágio: portabilidade total (build/run), simplificação da infraestrutura de build e do onboarding de desenvolvedores, e consistência das ferramentas. Menciona que imagens oficiais para build/runtime já existem no Docker Hub.
*   Inicia o primeiro exemplo prático detalhado (Seção 4.2): construir uma aplicação Java.
*   A **Listagem 4.2** mostra o Dockerfile multi-estágio para essa aplicação:
    *   **Estágio `builder`:**
        *   Usa `diamol/maven` como base (contém JDK e Maven).
        *   Copia primeiro o `pom.xml` (arquivo de definição do build Maven) e roda `mvn dependency:go-offline`. Isso é uma **otimização de cache**: baixar as dependências é demorado e só precisa ser refeito se o `pom.xml` mudar. Se apenas o código Java mudar depois, esta camada será reutilizada do cache.
        *   Copia o restante do código (`COPY . .`).
        *   Roda `mvn package` para compilar o código e gerar o arquivo `.jar` final.
    *   **Estágio final (`app`):**
        *   Usa `diamol/openjdk` como base (contém apenas o JRE, mais leve).
        *   Copia *apenas* o arquivo `.jar` gerado no estágio `builder`. As ferramentas de build e o código-fonte são descartados.
        *   `EXPOSE 80`: Documenta (não executa) que a aplicação dentro do contêiner ouvirá na porta 80.
        *   `ENTRYPOINT`: Define o comando principal para executar a aplicação Java a partir do JAR.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você precisará dos arquivos `Dockerfile` (Listagem 4.2), `pom.xml` e o código-fonte Java (pasta `src`) no diretório `ch04\exercises\image-of-the-day`.
*   O comando para construir será `docker image build -t image-of-the-day .` (próximo passo).

---

📄 Página 76

🔍 Resumo dos principais pontos:

*   Análise detalhada do Dockerfile Java (Listagem 4.2):
    *   **Estágio `builder`:**
        *   Base `diamol/maven` (JDK + Maven).
        *   Copia `pom.xml`.
        *   `RUN mvn ... dependency:go-offline`: Otimização de cache para dependências.
        *   `COPY . .`: Copia o resto do código fonte.
        *   `RUN mvn package`: Compila e gera o JAR.
        *   Se build falhar (rede, erro de código), `RUN` falha -> build inteiro falha.
    *   **Estágio final (`app`):**
        *   Base `diamol/openjdk` (só JRE).
        *   Copia *apenas* o JAR do estágio `builder`.
        *   `EXPOSE 80`: Documenta a porta interna.
        *   `ENTRYPOINT`: Define como rodar o JAR com `java -jar`.
*   **Exercício Try-it-now:** Navegar para a pasta do código Java e construir a imagem.
    *   Comandos: `cd ch04/exercises/image-of-the-day`, `docker image build -t image-of-the-day .`

🧠 Explicação detalhada e didática:

*   Esta página fornece uma explicação linha a linha mais detalhada do Dockerfile Java apresentado na página anterior, reforçando o propósito de cada estágio e cada comando, especialmente a otimização de cache ao baixar dependências separadamente e a cópia seletiva do artefato final (`.jar`) para a imagem de runtime mínima. Destaca que uma falha em qualquer comando `RUN` interrompe todo o processo de build.
*   O exercício instrui a executar o build da imagem a partir deste Dockerfile.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:**
    1.  No PowerShell/terminal, navegue até o diretório:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\image-of-the-day
        ```
    2.  Execute o comando de build:
        ```powershell
        docker image build -t image-of-the-day .
        ```
    3.  Observe a saída (analisada na próxima página). A primeira vez pode demorar um pouco, especialmente no passo `mvn dependency:go-offline`, pois ele baixará as dependências Java da internet. Builds subsequentes (se nada mudar no `pom.xml`) serão muito mais rápidos nessa etapa devido ao cache.

---

📄 Página 77

🔍 Resumo dos principais pontos:

*   Saída do build Java mostra logs do Maven (baixando dependências, compilando). Figura 4.5 mostra trecho da saída.
*   **Descrição da aplicação construída (`image-of-the-day`):**
    *   API REST simples que busca a "Astronomy Picture of the Day" da NASA (APOD).
    *   Faz cache da imagem/detalhes para evitar acessos repetidos à NASA.
    *   API é apenas *uma parte* de uma aplicação maior que usará múltiplos contêineres comunicando-se entre si.
*   **Comunicação entre contêineres:**
    *   Contêineres se acessam através de uma **rede virtual Docker**.
    *   Docker atribui um IP virtual a cada contêiner na rede.
    *   Pode-se criar e gerenciar redes Docker via linha de comando.
*   **Exercício Try-it-now:** Criar uma rede Docker chamada `nat`.
    *   Comando: `docker network create nat`.
    *   Ignorar erro se a rede `nat` já existir (pode ter sido criada no Cap. 4).

🧠 Explicação detalhada e didática:

*   A página descreve a saída esperada do build Java (logs do Maven) e explica o que a aplicação `image-of-the-day` (IOTD) faz: é uma API que busca a foto astronômica do dia da NASA e a armazena localmente (cache).
*   Introduz um conceito fundamental para aplicações distribuídas: a **rede Docker**. Para que múltiplos contêineres (como a API IOTD e outros componentes que a consumirão) possam se comunicar, eles precisam estar conectados à mesma rede virtual gerenciada pelo Docker. O Docker fornece funcionalidades de rede, incluindo a atribuição de IPs e um sistema de DNS interno (que veremos mais tarde) que permite que contêineres encontrem uns aos outros pelo nome.
*   O exercício instrui a criar manualmente uma rede chamada `nat` usando o comando `docker network create`. Essa rede será usada para conectar os contêineres da aplicação. (O nome `nat` é frequentemente usado, especialmente no Windows, pois é o nome da rede padrão criada pelo Docker, mas aqui estamos criando uma rede personalizada com esse nome para compatibilidade entre exemplos).

💻 Exemplos práticos (para meu ambiente Windows):

*   Revise a saída do `docker build` da etapa anterior.
*   **Ação principal:** Crie a rede Docker:
    ```powershell
    docker network create nat
    ```
    Se você receber uma mensagem como `Error response from daemon: network with name nat already exists`, pode ignorá-la; significa que a rede já foi criada em um exercício anterior.
*   Você pode verificar as redes existentes com:
    ```powershell
    docker network ls
    ```
    A rede `nat` deve aparecer na lista.

---

📄 Página 78

🔍 Resumo dos principais pontos:

*   Ao rodar contêineres, pode-se conectá-los explicitamente à rede criada usando a flag `--network`.
*   Contêineres na mesma rede podem se alcançar usando seus nomes como hostname (DNS interno do Docker).
*   **Exercício Try-it-now:** Rodar um contêiner da imagem `image-of-the-day`.
    *   Comando: `docker container run --name iotd -d -p 800:80 --network nat image-of-the-day`.
    *   `--name iotd`: Nomeia o contêiner como `iotd`.
    *   `-d`: Roda em background.
    *   `-p 800:80`: Publica a porta 80 interna do contêiner na porta 800 do host.
    *   `--network nat`: Conecta o contêiner à rede `nat`.
*   **Teste:** Acessar `http://localhost:800/image` no navegador.
    *   Deve mostrar detalhes em JSON da imagem do dia da NASA (Figura 4.6).

🧠 Explicação detalhada e didática:

*   A página explica como usar a rede Docker criada: ao iniciar um contêiner com `docker run`, a flag `--network nat` o conecta a essa rede específica.
*   Um benefício crucial das redes Docker é o **DNS interno baseado em nome de contêiner/serviço**. Se dois contêineres, `app-a` e `app-b`, estão na mesma rede Docker, `app-a` pode simplesmente fazer uma requisição para `http://app-b/...`, e o Docker resolverá `app-b` para o endereço IP correto daquele contêiner dentro da rede virtual. Isso simplifica enormemente a configuração da comunicação entre serviços.
*   O exercício instrui a rodar o contêiner da API IOTD, nomeando-o como `iotd`, publicando a porta 800 e conectando-o à rede `nat`. O nome `iotd` será importante para que outros contêineres possam encontrá-lo nessa rede.
*   O teste final é acessar a API pelo navegador usando a porta publicada no host (`localhost:800`).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:**
    ```powershell
    docker container run --name iotd -d -p 800:80 --network nat image-of-the-day
    ```
*   Verifique se o contêiner está rodando:
    ```powershell
    docker ps
    ```
*   Abra o navegador e acesse `http://localhost:800/image`. Você deve ver uma resposta JSON com informações sobre a imagem astronômica do dia.

💡 Analogias ou observações úteis (se houver):

*   Conectar contêineres a uma rede Docker (`--network nat`) é como dar a eles um ramal telefônico dentro do mesmo prédio (`nat`). Eles podem ligar uns para os outros usando apenas o nome do apartamento (`--name iotd`), sem precisar saber o número exato do ramal (IP interno).

---

📄 Página 79

🔍 Resumo dos principais pontos:

*   Importância do exemplo Java:
    *   Build pode ser feito em qualquer máquina só com Docker (sem Java SDK local).
    *   Simples clonar o repo e rodar `docker build`, `docker run`.
*   Ferramentas de build (Maven, JDK) *não* fazem parte da imagem final da aplicação (`image-of-the-day`).
    *   Só o estágio final vai para a imagem.
    *   Pode-se verificar rodando um contêiner interativo da imagem final e tentando usar `mvn` (não estará lá).
*   **4.3 App walkthrough: Node.js source code** (Passo a passo da aplicação: código-fonte Node.js)
    *   Novo exemplo multi-estágio: aplicação Node.js.
    *   Importante para entender diferentes tipos de build (linguagens compiladas vs. interpretadas).
    *   Padrão similar a Python, PHP, Ruby.
    *   Código fonte em `ch04/exercises/access-log`.
*   **Diferença Node.js vs. Java/Compilados:**
    *   Java/C#: Código fonte -> Compilador (no estágio build) -> Binários (.jar, .dll). Imagem final só precisa dos binários + runtime.
    *   Node.js (JavaScript): Linguagem interpretada, sem compilação para binário. Imagem final precisa do **runtime Node.js + código fonte (.js)**.
*   **Necessidade de multi-estágio em Node.js:** Otimizar carregamento de dependências (`npm install`).
*   **Listagem 4.3:** Dockerfile para construir app Node.js com `npm`.
    ```dockerfile
    FROM diamol/node AS builder # Estágio builder com Node.js + npm
    WORKDIR /src
    COPY src/package.json . # Copia SÓ package.json
    RUN npm install # Otimização: instala dependências

    # app # Estágio final
    FROM diamol/node # Base com Node.js runtime
    EXPOSE 80
    CMD ["node", "server.js"] # Comando para rodar
    WORKDIR /app
    COPY --from=builder /src/node_modules/ /app/node_modules/ # Copia dependências
    COPY src/ . # Copia código fonte
    ```

🧠 Explicação detalhada e didática:

*   A página reforça a portabilidade do processo de build Dockerizado e um ponto crucial sobre builds multi-estágio: a imagem final é enxuta porque as ferramentas de build (JDK, Maven) ficam apenas no estágio intermediário (`builder`) e são descartadas.
*   Inicia o segundo exemplo prático (Seção 4.3): construir uma aplicação Node.js.
*   Explica a principal diferença em relação ao Java: Node.js usa JavaScript, uma linguagem *interpretada*. Não há um passo de compilação que gera um binário autocontido. Portanto, a imagem Docker final para uma aplicação Node.js precisa conter tanto o **interpretador/runtime (Node.js)** quanto o **próprio código-fonte (.js)**.
*   Apesar disso, o build multi-estágio ainda é útil para Node.js, principalmente para otimizar o download e instalação de dependências (gerenciadas pelo `npm`).
*   **Listagem 4.3** mostra o Dockerfile Node.js:
    *   **Estágio `builder`:**
        *   Usa `diamol/node` (que tem Node.js e `npm`).
        *   Copia *apenas* o `package.json` (que lista as dependências).
        *   Roda `npm install`. Similar à otimização do Maven, isso baixa as dependências (que ficam na pasta `node_modules`). Se só o código `.js` mudar depois, esta camada será cacheada.
    *   **Estágio final (`app`):**
        *   Usa a mesma base `diamol/node` (precisa do runtime Node.js).
        *   Define como expor a porta e rodar o app (`CMD`).
        *   Copia a pasta `node_modules` (com as dependências baixadas) do estágio `builder`.
        *   Copia o código-fonte (`.js`) da pasta `src` local.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você precisará dos arquivos `Dockerfile` (Listagem 4.3), `package.json` e do código-fonte Node.js (pasta `src`) no diretório `ch04\exercises\access-log`.
*   O comando para construir será `docker image build -t access-log .` (próximo passo).

---

📄 Página 80

🔍 Resumo dos principais pontos:

*   Análise do Dockerfile Node.js (Listagem 4.3):
    *   Objetivo: Empacotar e rodar só com Docker, sem ferramentas Node.js locais.
    *   Base `diamol/node` (Node.js + npm) usada em ambos os estágios.
    *   Estágio `builder`: Copia `package.json`, roda `npm install` para baixar dependências (otimização de cache). Sem compilação.
    *   Estágio final (`app`): Expõe porta, define `CMD`, copia `node_modules` do builder, copia código fonte (`src/`) do host. `server.js` é o ponto de entrada.
*   Mesmo padrão (multi-estágio para dependências), mas processo diferente (sem compilação, copia código fonte).
*   Processo de build/run é *exatamente o mesmo* para o usuário (só comandos Docker).
*   **Exercício Try-it-now:** Navegar para a pasta do código Node.js e construir a imagem.
    *   Comandos: `cd ch04/exercises/access-log`, `docker image build -t access-log .`
*   Saída do build mostra logs do `npm install` (pode ter warnings/errors ignoráveis). Figura 4.7 mostra trecho da saída.
*   Dependências baixadas (`node_modules`) ficam cacheadas na camada -> builds futuros rápidos se só o código mudar.

🧠 Explicação detalhada e didática:

*   A página detalha o Dockerfile Node.js, explicando cada passo e comparando com o fluxo Java. Reforça a otimização de cache vinda da separação da instalação de dependências (`npm install`) da cópia do código-fonte. Mesmo sem compilação, o multi-estágio é benéfico.
*   Enfatiza que, para quem usa a imagem ou a constrói, a experiência é a mesma do exemplo Java: usar comandos `docker build` e `docker run`, mesmo que o conteúdo da imagem e os passos internos do build sejam diferentes.
*   O exercício instrui a construir a imagem `access-log`. A etapa `npm install` pode levar algum tempo na primeira vez.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:**
    1.  No PowerShell/terminal, navegue até o diretório:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\access-log
        ```
    2.  Execute o comando de build:
        ```powershell
        docker image build -t access-log .
        ```
    3.  Observe a saída. Você verá logs extensos durante o passo `RUN npm install`. Builds subsequentes, se você alterar apenas arquivos `.js` na pasta `src`, devem ser bem mais rápidos nessa etapa.

---

📄 Página 81

🔍 Resumo dos principais pontos:

*   **Descrição da aplicação construída (`access-log`):**
    *   API REST simples para outros serviços escreverem logs.
    *   Endpoint `POST` para gravar novo log.
    *   Endpoint `GET` (/stats) para mostrar quantos logs foram gravados.
*   **Exercício Try-it-now:** Rodar um contêiner da imagem `access-log`.
    *   Comando: `docker container run --name accesslog -d -p 801:80 --network nat access-log`.
    *   `--name accesslog`: Nomeia o contêiner.
    *   `-d`: Roda em background.
    *   `-p 801:80`: Publica porta 80 interna na 801 do host.
    *   `--network nat`: Conecta à rede `nat`.
*   **Teste:** Acessar `http://localhost:801/stats` no navegador.
    *   Deve mostrar contagem de logs (inicialmente zero). Figura 4.8 mostra o resultado (Firefox formatando JSON).
*   App roda em Node.js 10.16, mas não é preciso ter Node.js instalado localmente.
*   Fluxo do Dockerfile (download dependências -> copia scripts) é aplicável a outras linguagens interpretadas (Python/Pip, Ruby/Gems).

🧠 Explicação detalhada e didática:

*   A página explica o que faz a aplicação `access-log`: é uma API simples onde outros serviços podem enviar mensagens de log para serem armazenadas (embora o mecanismo de armazenamento não seja detalhado aqui). O endpoint `/stats` permite verificar quantas mensagens foram recebidas.
*   O exercício instrui a rodar esta API em um contêiner, nomeando-o `accesslog`, publicando a porta 801 e conectando-o à mesma rede `nat` onde o contêiner `iotd` (da API Java) está rodando. Isso é importante porque eles precisarão se comunicar mais tarde.
*   O teste acessa o endpoint `/stats` via navegador para confirmar que a API está rodando.
*   A página conclui reforçando a vantagem de não precisar ter o Node.js instalado localmente e que o padrão de build multi-estágio focado em dependências é útil para outras linguagens interpretadas.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:**
    ```powershell
    docker container run --name accesslog -d -p 801:80 --network nat access-log
    ```
*   Verifique se está rodando:
    ```powershell
    docker ps
    ```
*   Abra o navegador e acesse `http://localhost:801/stats`. Você deve ver algo como `{"count":0}`.

💡 Analogias ou observações úteis (se houver):

*   A aplicação `access-log` é como um livro de visitas centralizado para os outros serviços (contêineres). Eles podem "assinar" o livro (enviar um log via POST), e você pode perguntar quantos assinaram (GET /stats).

---

📄 Página 82

🔍 Resumo dos principais pontos:

*   **4.4 App walkthrough: Go source code** (Passo a passo da aplicação: código-fonte Go)
    *   Último exemplo multi-estágio: aplicação web escrita em Go.
    *   Go: linguagem moderna, multiplataforma, compila para **binários nativos**.
    *   Binário nativo: executável completo, *não* precisa de runtime separado instalado (diferente de Java, .NET Core, Node.js, Python).
    *   Resulta em imagens Docker extremamente pequenas.
    *   Outras linguagens com compilação nativa: Rust, Swift. Go tem suporte amplo e é popular para apps cloud-native (Docker é escrito em Go).
    *   Build multi-estágio para Go é similar ao Java, mas com diferenças importantes.
*   **Listagem 4.4:** Dockerfile para construir app Go do código fonte.
    ```dockerfile
    FROM diamol/golang AS builder # Estágio builder com ferramentas Go
    COPY main.go .
    RUN go build -o /server # Compila para binário nativo '/server'

    # app # Estágio final
    FROM diamol/base # Base MÍNIMA (só OS base, sem Go runtime)
    ENV IMAGE_API_URL="http://iotd/image" \ # Configuração via ENV
        ACCESS_API_URL="http://accesslog/access-log"
    CMD ["/web/server"] # Roda o binário compilado
    WORKDIR web
    COPY index.html . # Copia HTML do host
    COPY --from=builder /server . # Copia SÓ o binário do builder
    RUN chmod +x server # Torna o binário executável (Linux)
    ```

🧠 Explicação detalhada e didática:

*   Introduz o terceiro exemplo: uma aplicação escrita em Go.
*   A característica chave do Go (e Rust, Swift, etc.) é que ele compila para um **binário nativo único**. Isso significa que o arquivo executável resultante já contém tudo que precisa para rodar naquele sistema operacional e arquitetura específicos, sem a necessidade de instalar um "runtime" separado (como o JRE para Java ou o Node.js para JavaScript).
*   Isso tem uma implicação direta e muito positiva para imagens Docker: a imagem final pode ser baseada em uma imagem de sistema operacional *extremamente mínima* (como `scratch` ou uma imagem base muito pequena como `alpine` ou a `diamol/base` usada aqui), contendo apenas o binário compilado. Isso resulta em imagens finais muito, muito pequenas e seguras (menos software = menos superfície de ataque).
*   **Listagem 4.4** mostra o Dockerfile Go:
    *   **Estágio `builder`:**
        *   Usa `diamol/golang` (que tem as ferramentas de compilação Go).
        *   Copia o código fonte (`main.go`).
        *   Roda `go build -o /server` para compilar o código e gerar o binário executável `/server`.
    *   **Estágio final (`app`):**
        *   Usa `diamol/base` (uma imagem mínima, *sem* Go instalado).
        *   Define variáveis de ambiente para configurar os URLs das APIs Java (`iotd`) e Node.js (`accesslog`) que este app Go consumirá.
        *   Define `CMD` para executar o binário `/web/server`.
        *   Copia um `index.html` (interface web) do host.
        *   Copia *apenas* o binário `/server` do estágio `builder`.
        *   `RUN chmod +x server`: No Linux, arquivos precisam de permissão de execução para serem rodados como programas. Este comando concede essa permissão ao binário copiado. (No Windows, isso não é necessário da mesma forma).

💻 Exemplos práticos (para meu ambiente Windows):

*   Você precisará dos arquivos `Dockerfile` (Listagem 4.4), `main.go` e `index.html` no diretório `ch04\exercises\image-gallery`.
*   O comando para construir será `docker image build -t image-gallery .` (próximo passo).
*   A instrução `RUN chmod +x server` funcionará sem erro mesmo ao construir no Windows (para a imagem Linux final), mas não teria efeito se a imagem final fosse Windows.

---

📄 Página 83

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Navegar para a pasta do código Go e construir a imagem.
    *   Comandos: `cd ch04/exercises/image-gallery`, `docker image build -t image-gallery .`
*   Saída do build Go é curta (só loga falhas). Figura 4.9 mostra trecho.
*   App Go é útil; antes de rodar, verificar tamanho das imagens.
*   **Exercício Try-it-now:** Comparar tamanho da imagem Go final (`image-gallery`) com a imagem de ferramentas Go (`diamol/golang`).
    *   Comando: `docker image ls -f reference=diamol/golang -f reference=image-gallery`.
    *   `-f reference=...`: Filtra a lista de imagens pelo nome/tag.
*   Exemplo de resultado: `image-gallery` (~25MB Linux), `diamol/golang` (~770MB Linux).
*   Tamanho reportado é virtual; reuso de camadas economiza espaço real.

🧠 Explicação detalhada e didática:

*   O exercício instrui a construir a imagem `image-gallery` a partir do Dockerfile Go. O processo de build tende a ser rápido e com pouca saída no console.
*   Antes de executar a aplicação, a página sugere uma verificação importante: comparar o tamanho da imagem final da aplicação (`image-gallery`) com o tamanho da imagem que continha as ferramentas de build (`diamol/golang`).
*   O comando `docker image ls` com múltiplos filtros `-f reference=...` é usado para mostrar apenas essas duas imagens.
*   A diferença de tamanho esperada é drástica (ex: 25MB vs 770MB). Isso demonstra vividamente o benefício do build multi-estágio para linguagens compiladas nativamente: a imagem final é minúscula porque não carrega o peso das ferramentas de desenvolvimento.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  No PowerShell/terminal, navegue até o diretório:
        ```powershell
        cd C:\DockerLabs\diamol\ch04\exercises\image-gallery
        ```
    2.  Execute o comando de build:
        ```powershell
        docker image build -t image-gallery .
        ```
    3.  Compare os tamanhos das imagens:
        ```powershell
        docker image ls -f reference=diamol/golang -f reference=image-gallery
        ```
        Observe a grande diferença nos valores da coluna `SIZE`. (Você precisará ter a imagem `diamol/golang` baixada para vê-la; se não tiver, o Docker pode baixá-la ou o comando mostrará apenas `image-gallery`).

---

📄 Página 84

🔍 Resumo dos principais pontos:

*   Benefício não é só espaço em disco, mas **segurança**: imagem final tem muito menos software -> menor superfície de ataque.
*   App Go (`image-gallery`) usa as APIs dos outros apps (Java `iotd`, Node.js `accesslog`).
*   Verificar se os contêineres `iotd` e `accesslog` (dos exercícios anteriores) estão rodando com esses nomes.
    *   Comando: `docker container ls`.
*   App Go usará os nomes dos contêineres (`iotd`, `accesslog`) para encontrá-los via DNS Docker na rede `nat`.
*   **Exercício Try-it-now:** Rodar o app Go (`image-gallery`).
    *   Comando: `docker container run -d -p 802:80 --network nat image-gallery`.
    *   `-d`: Background.
    *   `-p 802:80`: Publica porta 80 interna na 802 do host.
    *   `--network nat`: Conecta à mesma rede dos outros contêineres.
*   **Teste:** Acessar `http://localhost:802` no navegador.
    *   Deve mostrar a "Astronomy Picture of the Day" da NASA (Figura 4.10).

🧠 Explicação detalhada e didática:

*   A página reforça que a principal vantagem de imagens finais pequenas não é apenas economizar disco, mas sim a **segurança**. Quanto menos software (especialmente ferramentas de desenvolvimento ou utilitários não essenciais) houver na imagem que vai para produção, menor a chance de um invasor encontrar uma vulnerabilidade ou ferramenta para explorar caso consiga acesso ao contêiner.
*   Explica que a aplicação Go (`image-gallery`) que acabamos de construir funciona como um frontend web que, por sua vez, consome as APIs que rodamos anteriormente: a API Java (`iotd`) para buscar a imagem da NASA e a API Node.js (`accesslog`) para registrar o acesso.
*   É crucial que os contêineres `iotd` e `accesslog` estejam rodando e conectados à mesma rede Docker (`nat`) para que o contêiner `image-gallery` possa encontrá-los usando seus nomes (`http://iotd/...` e `http://accesslog/...`), conforme configurado nas variáveis de ambiente no Dockerfile do Go.
*   O exercício instrui a iniciar o contêiner `image-gallery`, conectando-o à rede `nat` e publicando a porta 802.
*   O teste final é acessar `http://localhost:802`, o que deve exibir a página web com a imagem da NASA.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Verifique se os contêineres `iotd` e `accesslog` estão rodando:
        ```powershell
        docker ps
        ```
        Se não estiverem, inicie-os novamente (comandos nas páginas 78 e 81).
    2.  Inicie o contêiner `image-gallery`:
        ```powershell
        docker container run -d -p 802:80 --network nat image-gallery
        ```
    3.  Abra o navegador e acesse `http://localhost:802`. Você deverá ver a imagem astronômica do dia.

---

📄 Página 85

🔍 Resumo dos principais pontos:

*   Figura 4.10: Mostra a aplicação web Go exibindo a imagem buscada da API Java.
*   Recapitulação: Aplicação distribuída rodando em 3 contêineres (Go web, Java API, Node.js log API).
*   Benefício: Conseguiu construir e rodar tudo *sem* instalar ferramentas Java, Node.js ou Go localmente, apenas com Docker e código fonte.
*   Dockerfiles multi-estágio tornam o projeto *inteiramente portátil*.
*   Pipeline de build (ex: Jenkins, AppVeyor, Azure DevOps) só precisa do Docker; a complexidade do build está no Dockerfile (`docker image build`).
*   **4.5 Understanding multi-stage Dockerfiles** (Entendendo Dockerfiles multi-estágio)
    *   Revisão dos pontos chave e benefícios:
        1.  **Padronização:** Builds consistentes independentemente do ambiente/máquina, usando as mesmas ferramentas definidas nas imagens base dos estágios de build. Simplifica onboarding e manutenção de servidores de build.
        2.  **Desempenho:** Cada *estágio* tem seu próprio cache. Se um estágio intermediário não muda, ele é reutilizado do cache, acelerando o build. Estruturar Dockerfiles otimiza o uso do cache.
        3.  **Lean Images (Imagens Enxutas):** Permite isolar ferramentas (compiladores, downloaders como `curl`) em estágios iniciais, garantindo que *não* estejam presentes na imagem final. Reduz tamanho e superfície de ataque. Exemplo: usar `curl` para baixar algo em um estágio, mas não incluir `curl` na imagem final.
*   **4.6 Lab** (Laboratório)

🧠 Explicação detalhada e didática:

*   A página celebra o sucesso: você acabou de rodar uma aplicação distribuída com três tecnologias diferentes (Go, Java, Node.js) usando apenas Docker! Isso demonstra o poder do Docker para gerenciar ambientes complexos.
*   A seção 4.5 resume os **benefícios cruciais dos builds multi-estágio**:
    1.  **Consistência/Padronização:** O ambiente de build é definido no Dockerfile, garantindo que o resultado seja o mesmo em qualquer máquina (dev local, servidor de CI), eliminando problemas de "funciona na minha máquina".
    2.  **Velocidade (Desempenho):** O cache funciona por estágio. Se você alterar apenas o código fonte (copiado no final do estágio de build), os estágios anteriores (ex: baixar dependências) podem vir do cache, tornando o build muito mais rápido.
    3.  **Tamanho/Segurança (Imagens Enxutas):** Permite usar ferramentas pesadas ou temporárias em estágios iniciais e descartá-las, copiando apenas o essencial (o app compilado/processado) para a imagem final mínima.
*   Prepara para o Lab do Capítulo 4.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo, mas consolida o entendimento do padrão multi-estágio que você acabou de usar para Java, Node.js e Go.

💡 Analogias ou observações úteis (se houver):

*   Revisitando a cozinha multi-estágio:
    *   **Padronização:** A receita (Dockerfile) especifica exatamente quais facas, panelas, forno usar (imagens base dos estágios), garantindo o mesmo resultado sempre.
    *   **Desempenho (Cache):** Se você só mudou a decoração final do bolo, não precisa refazer a massa ou assar de novo; pega essas partes prontas da "geladeira" (cache).
    *   **Imagens Enxutas:** Você usa a batedeira industrial (ferramentas de build) para fazer a massa, mas na hora de embalar o bolo para vender (imagem final), você leva só o bolo, não a batedeira.

---

📄 Página 86

🔍 Resumo dos principais pontos:

*   **Continuação do Lab 4:**
    *   Objetivo: Praticar builds multi-estágio e otimização de Dockerfile.
    *   Ponto de partida: Pasta `ch04/lab` com app web Go simples e um Dockerfile *não otimizado*.
    *   **Sua tarefa:** Otimizar o Dockerfile existente para atingir metas específicas.
*   **Metas do Lab:**
    1.  Construir imagem com Dockerfile atual, depois otimizar e construir nova imagem.
    2.  Imagem atual ~800MB (Linux) / 5.2GB (Windows). Imagem otimizada deve ser ~15MB (Linux) / 260MB (Windows).
    3.  Build atual executa 7 steps se HTML mudar.
    4.  Build otimizado deve executar *1 único step* se HTML mudar.
*   Solução de exemplo no GitHub. Incentiva a tentar fazer, pois otimizar Dockerfiles é habilidade valiosa.
*   Dica: App do lab é similar a um já construído no capítulo.

🧠 Explicação detalhada e didática:

*   Esta página detalha o **Lab do Capítulo 4**. O foco é aplicar os conceitos de build multi-estágio e otimização de cache/tamanho aprendidos.
*   Você recebe um Dockerfile "ruim" para uma aplicação Go e precisa refatorá-lo.
*   As metas são claras e quantificáveis:
    *   **Redução drástica de tamanho:** Isso será alcançado usando um build multi-estágio onde a imagem final é baseada em uma imagem mínima (como `scratch` ou `alpine` ou `diamol/base`) e contém apenas o binário Go compilado e o arquivo HTML, sem as ferramentas de build Go.
    *   **Otimização máxima do cache:** Isso será alcançado ordenando as instruções corretamente no Dockerfile otimizado. A cópia do arquivo HTML (que pode mudar) deve ser um dos últimos passos, idealmente o último passo relevante antes do `CMD`/`ENTRYPOINT`, para que mudanças apenas no HTML invalidem o mínimo possível do cache.
*   A dica sugere que o padrão de build multi-estágio para Go visto na Seção 4.4 será diretamente aplicável aqui.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 4:**
    1.  Navegue para `C:\DockerLabs\diamol\ch04\lab`.
    2.  Construa a imagem com o Dockerfile existente: `docker build -t lab4-v1 .` e anote o tamanho com `docker image ls lab4-v1`.
    3.  Analise o `Dockerfile` atual. Provavelmente ele está usando a imagem `golang` como base final e não é multi-estágio.
    4.  Crie um novo arquivo (ou modifique o existente, talvez salvando como `Dockerfile.optimized`) seguindo o padrão multi-estágio do exemplo Go da Seção 4.4 (Listagem 4.4):
        *   Estágio `builder` (usando `diamol/golang` ou `golang:alpine`): Copia o código Go (`main.go`), roda `go build`.
        *   Estágio final (usando `diamol/base` ou `alpine` ou `scratch`): Copia o binário compilado do estágio `builder`, copia o `index.html`, define `WORKDIR`, `EXPOSE`, `CMD`. **Crucial:** Coloque o `COPY index.html .` *depois* do `COPY --from=builder ...` para otimizar o cache para mudanças no HTML.
    5.  Construa a imagem otimizada: `docker build -t lab4-v2 -f Dockerfile.optimized .` e verifique o tamanho.
    6.  Teste o cache: Modifique `index.html`, reconstrua `lab4-v2` e veja quantos steps usam o cache (deve ser quase todos, exceto o `COPY index.html`).

---

📄 Página 87

🔍 Resumo dos principais pontos:

*   **Capítulo 5: Sharing images with Docker Hub and other registries** (Compartilhando imagens com Docker Hub e outros registros)
*   Revisão: Capítulos anteriores focaram nas partes "build" e "run" do fluxo Docker.
*   Foco do Cap. 5: Parte "share" (compartilhar).
*   Importância do compartilhamento: Empacotar software + dependências facilita o uso por qualquer pessoa, em qualquer máquina, eliminando problemas de ambiente e configuração.
*   **5.1 Working with registries, repositories, and image tags** (Trabalhando com registros, repositórios e tags de imagem)
    *   Distribuição de software é integrada ao Docker (download automático de imagens).
    *   Servidor central que armazena imagens: **Registro Docker (Docker registry)**.
    *   **Docker Hub:** Registro mais popular, hospeda milhões de imagens, padrão do Docker Engine.
    *   Nome completo da imagem (**referência da imagem**) contém informações para localização exata.
    *   Exemplos usados até agora: `image-gallery`, `diamol/golang`.
    *   **4 partes da referência completa da imagem** (Figura 5.1, usando `docker.io/diamol/golang:latest` como exemplo):
        1.  **Registry Domain (Domínio do Registro):** Ex: `docker.io` (Docker Hub), `gcr.io` (Google), `my-registry.local:5000`. Padrão: `docker.io`.
        2.  **User/Organization (Usuário/Organização):** Ex: `diamol`. No Docker Hub, é sua conta ou uma organização da qual faz parte.
        3.  **Repository Name (Nome do Repositório):** Ex: `golang`. O nome específico da imagem dentro da conta/organização.
        4.  **Tag:** Ex: `latest`. Identifica uma versão específica da imagem dentro do repositório. Padrão: `latest`.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 5, focado na etapa de **compartilhamento** de imagens Docker. O compartilhamento é vital porque permite que o trabalho de empacotar uma aplicação seja feito uma vez e, depois, qualquer pessoa possa rodá-la facilmente.
*   A seção 5.1 define a terminologia essencial para o compartilhamento:
    *   **Registro (Registry):** O servidor onde as imagens são armazenadas (ex: Docker Hub, Google Container Registry, ou um registro privado).
    *   **Referência da Imagem (Image Reference):** O nome completo e único que identifica uma imagem específica em um registro.
*   A estrutura completa de uma referência de imagem é explicada usando `docker.io/diamol/golang:latest` como exemplo:
    *   `docker.io`: O endereço do registro (Docker Hub). Se omitido, o Docker assume `docker.io`.
    *   `diamol`: O nome do usuário ou organização proprietário da imagem no registro.
    *   `golang`: O nome do repositório (basicamente, o nome da aplicação/imagem).
    *   `latest`: A tag, que geralmente indica uma versão ou variante. Se omitida, o Docker assume `latest`.
*   Compreender essa estrutura é fundamental para poder baixar imagens de registros específicos e, principalmente, para nomear e enviar suas próprias imagens para um registro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Relembre os comandos anteriores:
    *   `docker pull diamol/ch02-hello-diamol`: Aqui, `docker.io` é implícito, `diamol` é a organização, `ch02-hello-diamol` é o repositório, e `latest` é a tag implícita.
    *   `docker build -t web-ping .`: Aqui, `web-ping` é apenas um nome local (repositório), sem registro ou usuário/organização. Para compartilhar, precisaremos adicionar essa informação (próximos passos).

---

📄 Página 88

🔍 Resumo dos principais pontos:

*   Necessário usar todas as partes da referência para gerenciar suas próprias imagens em registros.
*   Nomes locais são flexíveis, mas para compartilhar, precisam de mais detalhes (identificador único no registro).
*   **Valores Padrão:**
    *   Registro padrão: Docker Hub (`docker.io`).
    *   Tag padrão: `latest`.
*   Exemplo: `diamol/golang` é versão curta de `docker.io/diamol/golang:latest`.
*   `diamol`: organização no Docker Hub. `golang`: repositório público. Só membros podem enviar (push).
*   **Registros Privados/Próprios:**
    *   Grandes empresas geralmente têm seus próprios registros (nuvem ou datacenter).
    *   Para usar um registro próprio, inclua o domínio no início da referência (ex: `r.sixeyed.com/diamol/golang`).
*   **Importância da Tag:**
    *   Identifica diferentes *versões* da mesma aplicação.
    *   Deve-se *sempre* usar tags explícitas ao construir/compartilhar suas imagens.
    *   Exemplo: Imagem oficial `openjdk` tem centenas de tags para diferentes versões (openjdk:13, openjdk:8u212-jdk, etc.) e variantes (Linux/Windows).
    *   Tag `latest` é enganosa; pode *não* ser a versão mais recente. Evite depender dela para suas imagens ou bases.

🧠 Explicação detalhada e didática:

*   A página reforça a estrutura da referência da imagem e explica como ela é usada na prática.
*   Destaca a existência de **registros privados**, comuns em ambientes corporativos para controle e segurança, e como acessá-los prefixando o nome da imagem com o domínio do registro privado.
*   Enfatiza a importância crucial das **tags**. Elas são usadas para versionamento. Em vez de ter imagens chamadas `meuapp-v1`, `meuapp-v2`, usa-se o mesmo nome de repositório (`meuapp`) com tags diferentes (`:v1`, `:v2`).
*   **Alerta:** A tag `:latest` é apenas um ponteiro padrão, frequentemente apontando para a última imagem enviada *sem* uma tag específica. Não há garantia de que ela seja realmente a versão mais recente ou estável. É uma **má prática** depender de `:latest` para imagens de produção ou como base (`FROM`) em seus Dockerfiles. Sempre use tags explícitas e significativas (como números de versão semântica: `:1.0.0`, `:2.1.5`).

💻 Exemplos práticos (para meu ambiente Windows):

*   Explore tags no Docker Hub: Vá para `https://hub.docker.com/_/openjdk` e clique na aba "Tags". Veja a enorme variedade de tags disponíveis para diferentes versões, JRE/JDK, e sistemas operacionais.
*   Ao construir suas imagens (Capítulos anteriores), você usou `-t web-ping`, que implicitamente criou a tag `:latest`. No futuro, você usará tags explícitas como `-t meuapp:1.0`.

💡 Analogias ou observações úteis (se houver):

*   Referência da Imagem = Endereço completo de um livro numa biblioteca gigante.
    *   Registro = Nome da Biblioteca (Docker Hub, Biblioteca Municipal).
    *   Usuário/Org = Seção da Biblioteca (Ficção, Não-Ficção, Seção Infantil 'diamol').
    *   Repositório = Título do Livro ('golang').
    *   Tag = Edição/Versão do Livro (1ª Edição, Edição Revisada ':v2', Edição de Bolso ':slim').
*   `:latest` é como pedir "o último livro que chegou nessa seção" - pode não ser o melhor ou o que você esperava.

---

📄 Página 89

🔍 Resumo dos principais pontos:

*   **5.2 Pushing your own images to Docker Hub** (Enviando suas próprias imagens para o Docker Hub)
*   Necessário: Conta no Docker Hub (gratuita, criar em https://hub.docker.com).
*   **Passos para enviar (push) uma imagem:**
    1.  **Login:** Autenticar-se no registro via linha de comando Docker (`docker login`).
    2.  **Tag:** Dar à imagem local uma referência que inclua seu nome de usuário/organização do registro.
*   **Exercício Try-it-now:** Capturar seu Docker ID (username) em uma variável de ambiente.
    *   Comando Windows (PowerShell): `$dockerId="<seu-docker-id>"`.
    *   Comando Linux/Mac (Bash): `export dockerId="<seu-docker-id>"`.
    *   Usar username, não email. Exemplo do autor: `$dockerId="sixeyed"`.
*   Verificar variável: `echo $dockerId` (funciona em ambos).
*   **Exercício Try-it-now:** Fazer login no Docker Hub.
    *   Comando: `docker login --username $dockerId`.
    *   Pedirá a senha (do Docker Hub).

🧠 Explicação detalhada e didática:

*   Esta seção inicia o processo prático de compartilhar uma imagem que você construiu localmente, enviando-a para o Docker Hub.
*   Os dois pré-requisitos são: ter uma conta no Docker Hub e estar logado nela através da ferramenta de linha de comando do Docker.
*   O primeiro exercício facilita os comandos subsequentes, pedindo para armazenar seu nome de usuário do Docker Hub (Docker ID) em uma variável de ambiente (`$dockerId` no PowerShell, `dockerId` no Bash). Isso permite copiar e colar os próximos comandos do livro, que usarão essa variável.
*   O segundo exercício executa o `docker login`. Este comando interage com o Docker Hub (ou outro registro, se especificado) para autenticar suas credenciais (usuário e senha). Uma vez logado com sucesso, o Docker Engine estará autorizado a enviar imagens para os repositórios associados à sua conta.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Se ainda não tem, crie uma conta em `https://hub.docker.com`. Lembre-se do seu **username** (Docker ID).
    2.  Abra o PowerShell. Defina a variável de ambiente (substitua `<seu-docker-id>`):
        ```powershell
        $dockerId="<seu-docker-id>"
        ```
    3.  Verifique se a variável foi definida:
        ```powershell
        echo $dockerId
        # Deverá imprimir seu Docker ID
        ```
    4.  Faça o login:
        ```powershell
        docker login --username $dockerId
        # Ou docker login -u $dockerId
        ```
        Digite sua senha do Docker Hub quando solicitado (ela não aparecerá na tela). Você deve receber uma mensagem "Login Succeeded".

💡 Analogias ou observações úteis (se houver):

*   `docker login`: É como fazer login no site de uma transportadora antes de poder enviar um pacote (imagem).
*   Variável `$dockerId`: É como escrever seu nome de remetente em um adesivo para usar em todos os pacotes que você vai enviar.

---

📄 Página 90

🔍 Resumo dos principais pontos:

*   Figura 5.2: Mostra a saída do `docker login` (senha não é exibida).
*   Após login, pode-se fazer push para sua conta ou organizações onde tem permissão.
*   Sem ser membro de organização, só pode fazer push para repositórios na sua conta (ex: `<seu-docker-id>/<nome-repo>`).
*   Imagem `image-gallery` construída no Cap. 4 tem nome local, não pode ser enviada diretamente.
*   **Retagging:** Não é preciso reconstruir a imagem para dar nova referência. Imagens podem ter múltiplas referências (tags) apontando para o mesmo conteúdo (mesmo ID).
*   **Exercício Try-it-now:** Criar uma nova tag para a imagem `image-gallery` existente, incluindo seu Docker ID e uma versão `v1`.
    *   Comando: `docker image tag image-gallery $dockerId/image-gallery:v1`.
*   Agora a imagem tem duas referências (local e para o Hub) apontando para o mesmo ID.
*   **Exercício Try-it-now:** Listar as referências da imagem `image-gallery`.
    *   Comando: `docker image ls --filter reference=image-gallery --filter reference='*/image-gallery'`.
    *   Usa dois filtros: um para o nome local, outro para qualquer nome que termine com `/image-gallery`.
*   Saída esperada (Figura 5.3): Duas linhas com o *mesmo* IMAGE ID, mas REPOSITORY/TAG diferentes (uma local `image-gallery:latest`, outra com seu Docker ID e tag `v1`).

🧠 Explicação detalhada e didática:

*   Uma vez logado, você está pronto para preparar a imagem para o envio. O problema é que a imagem `image-gallery` que você construiu tem apenas um nome local. Para enviá-la ao Docker Hub, ela precisa ser nomeada de acordo com o padrão `<seu-docker-id>/<nome-repositorio>:<tag>`.
*   A boa notícia é que você não precisa reconstruir a imagem. O comando `docker image tag` cria um **novo nome (referência)** que aponta para uma imagem *existente*. É como criar um atalho ou um apelido para a mesma imagem.
*   O primeiro exercício cria a tag `alexandre/image-gallery:v1` (substituindo `alexandre` pelo seu `$dockerId`), que agora aponta para o mesmo conteúdo da imagem que antes era só `image-gallery:latest`.
*   O segundo exercício usa `docker image ls` com filtros para mostrar que agora existem duas referências (`image-gallery:latest` e `<seu-docker-id>/image-gallery:v1`) mas ambas compartilham o mesmo `IMAGE ID`, confirmando que são apenas nomes diferentes para a mesma imagem.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Certifique-se de ter a imagem `image-gallery:latest` construída (do Cap. 4, p. 83).
    2.  Crie a nova tag (o PowerShell substituirá `$dockerId`):
        ```powershell
        docker image tag image-gallery $dockerId/image-gallery:v1
        ```
    3.  Liste as imagens para ver ambas as referências:
        ```powershell
        docker image ls --filter reference=image-gallery --filter reference="*/image-gallery"
        # Ou liste todas e procure por 'image-gallery':
        # docker image ls | Select-String "image-gallery"
        ```
        Confirme que as duas linhas têm o mesmo IMAGE ID.

💡 Analogias ou observações úteis (se houver):

*   `docker image tag`: É como colocar uma nova etiqueta de endereço de destinatário (`$dockerId/image-gallery:v1`) em um pacote (`image-gallery:latest`) que você já tinha pronto, sem precisar refazer o pacote. O conteúdo é o mesmo, só o rótulo mudou/foi adicionado.

---

📄 Página 91

🔍 Resumo dos principais pontos:

*   Agora com referência correta (`<user>/<repo>:<tag>`) e logado, a imagem está pronta para ser compartilhada.
*   Comando `docker image push`: Envia camadas da imagem local para o registro.
*   **Exercício Try-it-now:** Enviar a imagem tagueada para o Docker Hub.
    *   Comando: `docker image push $dockerId/image-gallery:v1`.
*   Análise da saída do `push`:
    *   Registros trabalham com camadas, assim como o Docker Engine local.
    *   `push` faz upload das camadas. Saída mostra progresso por ID de camada.
    *   Exemplo: `c8c60e5dbe37: Pushed`.
    *   Se uma camada já existe no registro (enviada antes por você ou outra pessoa), ela *não* é enviada novamente (mostra "Layer already exists").
*   Otimização de Dockerfiles (Cap. 3) é importante aqui: se 90% das camadas vêm do cache no build, 90% provavelmente já existirão no registro no push. Reduz tempo de build, espaço em disco *e* banda de rede.
*   **Verificação no Docker Hub:**
    *   Pode-se navegar até a página da imagem no Docker Hub para verificar.
    *   URL é baseado no nome de usuário e repositório.
*   **Exercício Try-it-now:** Script simples para gerar o URL da sua imagem no Docker Hub.
    *   Comando: `echo "https://hub.docker.com/r/$dockerId/image-gallery/tags"`.
*   Resultado esperado no Hub (Figura 5.4): Página mostrando a tag `v1`, quando foi atualizada, etc.
*   Repositório é criado automaticamente no Hub se não existir; por padrão, é público (qualquer um pode baixar).
*   Pode-se adicionar documentação no Docker Hub.

🧠 Explicação detalhada e didática:

*   Com a imagem corretamente nomeada (`<seu-docker-id>/image-gallery:v1`) e você logado, o comando `docker image push` envia sua imagem para o Docker Hub.
*   Assim como no `pull`, o `push` opera no nível das **camadas**. O Docker verifica quais camadas da sua imagem já existem no registro de destino. Apenas as camadas que o registro *não* possui são efetivamente transferidas pela rede. Isso torna o `push` (especialmente de atualizações) muito eficiente se as imagens compartilharem muitas camadas base ou se apenas pequenas partes foram alteradas.
*   Isso reforça a importância de otimizar os Dockerfiles (como visto no Cap. 3 e que será aprofundado no Cap. 17): imagens bem otimizadas com bom reuso de cache não só constroem mais rápido, mas também são enviadas (`push`) e baixadas (`pull`) mais rapidamente.
*   Após o `push`, a imagem fica disponível publicamente no Docker Hub. O exercício mostra como construir o URL para visualizar sua imagem recém-enviada no site do Hub.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Execute o comando push (PowerShell substituirá `$dockerId`):
        ```powershell
        docker image push $dockerId/image-gallery:v1
        ```
        Observe a saída mostrando o envio das camadas. Se você já tiver enviado imagens baseadas nas mesmas camadas antes, verá muitas mensagens "Layer already exists".
    2.  Execute o comando para obter o URL:
        ```powershell
        echo "https://hub.docker.com/r/$dockerId/image-gallery/tags"
        ```
    3.  Copie o URL gerado e cole no seu navegador. Você deverá ver sua imagem `image-gallery` com a tag `v1` listada na sua conta do Docker Hub.

---

📄 Página 92

🔍 Resumo dos principais pontos:

*   Figura 5.4: Mostra a listagem da imagem no Docker Hub.
*   Docker Hub é o registro mais fácil, gratuito, com funcionalidades extras pagas (repositórios privados).
*   Existem **registros alternativos**:
    *   API do registro é especificação aberta.
    *   Servidor de registro core é open source (`docker/distribution`).
    *   Provedores de nuvem têm seus próprios serviços de registro (ex: ACR no Azure, ECR na AWS, GCR no Google).
    *   Pode-se rodar registro próprio no datacenter (comercial: Docker Trusted Registry; simples: rodando o registro open source em um contêiner).
*   **5.3 Running and using your own Docker registry** (Rodando e usando seu próprio registro Docker)
    *   Utilidade de rodar um registro na rede local:
        *   Reduz uso de banda e tempo de transferência.
        *   Permite manter controle dos dados/imagens internamente.
        *   Opção de backup se registro principal estiver offline.
    *   Registro open source da Docker (`docker/distribution`):
        *   Funcionalidade básica de push/pull.
        *   Usa mesmo sistema de cache de camadas.
        *   Não tem interface web como o Hub.
        *   Leve, pode rodar em um contêiner.
    *   Imagem pré-empacotada pelo autor: `diamol/registry`.

🧠 Explicação detalhada e didática:

*   A página conclui a parte do Docker Hub e introduz a ideia de usar **outros registros Docker**. Além do Hub, existem muitas opções: serviços gerenciados nas nuvens públicas (Azure Container Registry, AWS Elastic Container Registry, Google Container Registry) e a possibilidade de você hospedar seu próprio registro, seja usando soluções comerciais robustas (como Docker Trusted Registry, agora parte do Mirantis Kubernetes Engine) ou simplesmente rodando o servidor de registro open source básico em um contêiner.
*   A seção 5.3 foca nesta última opção: rodar seu próprio registro localmente, usando uma imagem (`diamol/registry`) que empacota o servidor de registro open source. Isso é útil para desenvolvimento, testes locais rápidos, ou para ter um cache/backup local de imagens.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando para executar ainda, mas prepara para rodar o contêiner do registro local na próxima página.
*   Se você usa Azure, AWS ou GCP, pode explorar os serviços de registro de contêiner deles como alternativas ao Docker Hub para seus projetos.

💡 Analogias ou observações úteis (se houver):

*   Docker Hub é a biblioteca pública gigante. Registros da nuvem são bibliotecas grandes gerenciadas por empresas (Amazon, Google, Microsoft). Rodar seu próprio registro é como ter sua própria estante de livros em casa ou no escritório.

---

📄 Página 93

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar o registro Docker em um contêiner.
    *   Comando: `docker container run -d -p 5000:5000 --restart always --name registry diamol/registry`.
    *   `-p 5000:5000`: Publica a porta padrão do registro (5000).
    *   `--restart always`: Garante que o contêiner do registro reinicie automaticamente se o Docker for reiniciado.
    *   `--name registry`: Nomeia o contêiner.
*   Registro rodando em `localhost:5000`. Útil apenas na máquina local.
*   Para uso na rede: melhor dar um nome de domínio alias à máquina.
*   **Criando alias `registry.local`:**
    *   Edita o arquivo `hosts` do computador para mapear `registry.local` para `127.0.0.1` (localhost).
    *   Arquivo `hosts` existe em Windows, Linux e Mac, mas em locais diferentes.
*   **Exercício Try-it-now:** Adicionar a entrada no arquivo `hosts`.
    *   Comando Windows (PowerShell como Admin): `Add-Content -Value "127.0.0.1 registry.local" -Path C:\Windows\System32\drivers\etc\hosts`.
    *   Comando Linux/Mac (com `sudo`): `echo $'\n127.0.0.1 registry.local' | sudo tee -a /etc/hosts`.
    *   Pode dar erro de permissão; requer privilégios de administrador.
*   **Teste:** Rodar `ping registry.local`. Deve responder de `127.0.0.1` (Figura 5.5).

🧠 Explicação detalhada e didática:

*   O primeiro exercício inicia o contêiner do servidor de registro Docker básico (`diamol/registry`). Ele fica ouvindo na porta 5000. A opção `--restart always` é útil para serviços de infraestrutura como este, garantindo que ele volte a funcionar após reinicializações do Docker ou da máquina.
*   O problema: Acessar o registro como `localhost:5000` só funciona na própria máquina. Para que outras máquinas na rede (ou até mesmo outros contêineres na mesma máquina de forma mais elegante) possam usá-lo, e para que os nomes das imagens fiquem mais limpos (ex: `registry.local:5000/meuapp` em vez de `<ip_da_maquina>:5000/meuapp`), é uma boa prática criar um nome de domínio local.
*   O arquivo `hosts` do sistema operacional permite mapear nomes de domínio para endereços IP localmente, antes de consultar um servidor DNS externo. O segundo exercício adiciona uma linha a esse arquivo para que o nome `registry.local` seja resolvido para o endereço IP de loopback (`127.0.0.1`), ou seja, para a própria máquina. **Importante:** Editar o arquivo `hosts` requer permissões de administrador.
*   O teste com `ping registry.local` confirma que o sistema operacional agora reconhece esse nome e o direciona para `127.0.0.1`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  Inicie o contêiner do registro:
        ```powershell
        docker container run -d -p 5000:5000 --restart always --name registry diamol/registry
        ```
    2.  **Abra o PowerShell COMO ADMINISTRADOR.** (Clique com botão direito no ícone do PowerShell -> Executar como administrador).
    3.  Execute o comando para editar o arquivo hosts:
        ```powershell
        Add-Content -Value "127.0.0.1 registry.local" -Path C:\Windows\System32\drivers\etc\hosts
        ```
        *(Se der erro de acesso mesmo como admin, pode ser necessário ajustar permissões do arquivo ou usar um editor de texto como admin para adicionar a linha manualmente)*.
    4.  Teste o alias (em qualquer PowerShell, não precisa ser admin):
        ```powershell
        ping registry.local
        ```
        Você deve ver respostas de `127.0.0.1`.

---

📄 Página 94

🔍 Resumo dos principais pontos:

*   Agora pode-se usar `registry.local:5000` nas referências de imagem para usar o registro local.
*   Processo de taguear é o mesmo (`docker image tag`).
*   **Exercício Try-it-now:** Taguear a imagem `image-gallery` com o domínio do registro local.
    *   Comando: `docker image tag image-gallery registry.local:5000/gallery/ui:v

**You**

🧠 Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), página por página, a partir da página 93.

Para cada página, siga este modelo de resposta:

📄 Página X

🔍 Resumo dos principais pontos:

\[bullet points com os tópicos abordados na página\]

🧠 Explicação detalhada e didática:

\[explique os conceitos em português claro, com boa didática, como um professor\]

💻 Exemplos práticos (para meu ambiente Windows):

\[comandos ou trechos de código aplicáveis no Windows, usando Docker Desktop e Visual Studio Code\]

💡 Analogias ou observações úteis (se houver):

\[comparações simples ou observações para reforçar o entendimento\]

✅ Importante: percorra automaticamente todas as páginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "próxima".

Quando terminar todas as páginas, diga:

"✅ Fim da leitura. Todas as páginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Okay, Alexandre, continuando a partir da página 93.

---

📄 Página 93

🔍 Resumo dos principais pontos:

*   Agora pode-se usar `registry.local:5000` nas referências de imagem para usar o registro local.
*   Processo de taguear é o mesmo (`docker image tag`).
*   **Exercício Try-it-now:** Taguear a imagem `image-gallery` com o domínio do registro local e um nome/tag estruturado (`gallery/ui:v1`).
    *   Comando: `docker image tag image-gallery registry.local:5000/gallery/ui:v1`.
*   Explica que o registro local não tem autenticação/autorização (não pronto para produção, mas OK para times pequenos).
*   Sugere um esquema de nomenclatura para agrupar imagens relacionadas (ex: `gallery/ui`, `gallery/api`, `gallery/logs`).
*   Necessidade de configurar o Docker Engine para confiar em registros "inseguros" (HTTP vs HTTPS padrão).
    *   O registro rodando no contêiner usa HTTP simples. Docker não confia por padrão.
*   Configuração via arquivo `daemon.json` (C:\ProgramData\docker\config no Windows Server, /etc/docker no Linux) ou via UI do Docker Desktop (Daemon tab - *Nota: em versões mais recentes pode ser na seção "Docker Engine"*).

🧠 Explicação detalhada e didática:

*   Com o alias `registry.local` funcionando, agora podemos referenciar nosso registro Docker local usando o nome `registry.local:5000`. Para enviar uma imagem para lá, precisamos primeiro "etiquetá-la" (tag) com esse endereço, seguido pelo nome que queremos dar à imagem dentro do registro.
*   O comando `docker image tag <imagem_existente> <nova_referencia>` faz isso. A `<nova_referencia>` deve seguir o formato `<dominio_registro>/<usuario_ou_projeto>/<nome_imagem>:<tag>`. No exemplo, usamos `registry.local:5000/gallery/ui:v1`.
*   É importante notar que este registro básico que subimos não tem segurança (autenticação). Qualquer um na rede poderia acessá-lo. Isso é aceitável para testes locais, mas não para produção.
*   Outro ponto crucial: por padrão, o Docker se comunica com registros usando HTTPS para segurança. Como nosso registro local está rodando sobre HTTP simples (porta 5000), o Docker Engine irá recusar a conexão por considerá-la insegura. Precisamos configurar explicitamente o Docker para *permitir* a comunicação com este registro específico via HTTP. Isso é feito adicionando `registry.local:5000` à lista de "insecure-registries" na configuração do Docker Engine.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Taguear a imagem para o registro local:** (Certifique-se de ter a imagem `image-gallery:latest` do Cap. 4)
    ```powershell
    docker image tag image-gallery registry.local:5000/gallery/ui:v1
    ```
2.  **Configurar Registro Inseguro no Docker Desktop:**
    *   Clique com o botão direito no ícone da baleia Docker na área de notificação e selecione "Settings".
    *   Vá para a seção "Docker Engine".
    *   No editor JSON que aparece, adicione a chave `insecure-registries`. Se já houver outras chaves, adicione uma vírgula no final da linha anterior. O JSON deve ficar parecido com isto (mantenha outras configurações que já existirem):
      ```json
      {
        "builder": {
          "gc": {
            "defaultKeepStorage": "20GB",
            "enabled": true
          }
        },
        "experimental": false,
        "insecure-registries": [
          "registry.local:5000"
        ]
      }
      ```
    *   Clique em "Apply & Restart". O Docker Desktop reiniciará o Engine.

💡 Analogias ou observações úteis (se houver):

*   Configurar "insecure-registries" é como ajustar as configurações de segurança do seu navegador para permitir o acesso a um site específico que usa HTTP antigo, mesmo sabendo que HTTPS é mais seguro. Você só faz isso para sites que confia (neste caso, seu próprio registro local).

---

📄 Página 94

🔍 Resumo dos principais pontos:

*   Figura 5.6: Mostra a interface de configuração do Docker Desktop para adicionar "insecure registries".
*   Reinício do Docker Engine é necessário após alterar `daemon.json` (manualmente em servidores, automático no Docker Desktop).
    *   Comandos de reinício: `Restart-Service docker` (Windows Server), `service docker restart` (Linux).
*   Verificação da configuração: Comando `docker info`.
*   **Exercício Try-it-now:** Rodar `docker info` e verificar a lista de "Insecure Registries".
*   Aviso sobre riscos de segurança de registros inseguros (conexão pode ser comprometida, dados lidos ou injetados).
*   Registros comerciais e o registro open source podem ser configurados para usar HTTPS.

🧠 Explicação detalhada e didática:

*   Esta página foca na conclusão da configuração para permitir registros inseguros (HTTP). Mostra a interface gráfica do Docker Desktop (Figura 5.6) como uma forma fácil de fazer isso.
*   Explica que, se a configuração fosse feita editando o arquivo `daemon.json` diretamente (mais comum em servidores), seria necessário reiniciar o serviço Docker manualmente. O Docker Desktop simplifica isso reiniciando automaticamente após a aplicação das mudanças na interface.
*   O comando `docker info` é apresentado como forma de verificar se a configuração foi aplicada corretamente. Ele exibe uma vasta quantidade de informações sobre o Docker Engine, incluindo a lista de registros inseguros permitidos.
*   Reitera fortemente o **risco de segurança** ao usar HTTP para registros. A comunicação não é criptografada, tornando-a vulnerável a espionagem (alguém ver suas imagens) ou adulteração (alguém injetar código malicioso em imagens que você baixa). Para ambientes reais, especialmente produção, deve-se SEMPRE usar HTTPS.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Configuração:** Após o Docker Desktop reiniciar (depois de aplicar a configuração na página anterior), execute no PowerShell:
    ```powershell
    docker info
    ```
2.  Procure pela seção "Insecure Registries" na saída. Você deve ver `registry.local:5000` listado lá (como na Figura 5.7 da próxima página).

💡 Analogias ou observações úteis (se houver):

*   `docker info`: É como pedir um relatório completo do estado atual da "fábrica" Docker, incluindo suas configurações de segurança.

---

📄 Página 95

🔍 Resumo dos principais pontos:

*   Figura 5.7: Mostra a saída do `docker info` destacando a seção "Insecure Registries" com `registry.local:5000`.
*   Confirma que o risco de HTTP é aceitável para demonstração local.
*   Agora está tudo pronto para enviar a imagem para o registro local.
*   **Exercício Try-it-now:** Enviar (push) a imagem tagueada para o registro local.
    *   Comando: `docker image push registry.local:5000/gallery/ui:v1`.
*   Análise da saída do `push`:
    *   Se for o primeiro push, todas as camadas serão enviadas ("Pushed").
    *   Se repetir o comando, mostrará "Layer already exists" para todas as camadas.
*   Conclusão sobre rodar registro próprio: Pode-se compartilhar na rede usando IP ou nome real da máquina.
*   **5.4 Using image tags effectively** (Usando tags de imagem eficazmente)
    *   Tags podem ser qualquer string; múltiplas tags podem apontar para a mesma imagem.
    *   Uso principal: Versionamento de software.
    *   Esquema comum: Versionamento Semântico (`[major].[minor].[patch]`).
        *   Patch: Correções de bugs, sem quebra de compatibilidade.
        *   Minor: Novas funcionalidades, sem quebra de compatibilidade.
        *   Major: Mudanças que podem quebrar compatibilidade.
    *   Permite aos usuários escolherem nível de atualização/risco.

🧠 Explicação detalhada e didática:

*   Com a configuração de registro inseguro confirmada pelo `docker info`, finalmente podemos enviar nossa imagem para o registro local.
*   O comando `docker image push registry.local:5000/gallery/ui:v1` faz exatamente isso. Como o nome da imagem começa com `registry.local:5000`, o Docker sabe que deve enviá-la para o registro nesse endereço, usando o protocolo HTTP (porque configuramos como inseguro). A saída mostrará o progresso do envio das camadas. Se você rodar o comando novamente, ele será instantâneo, pois o registro já terá todas as camadas.
*   A seção 5.4 inicia uma discussão importante sobre como usar **tags** de forma eficaz, principalmente para **versionamento**. A ideia é usar tags para comunicar o significado das diferentes versões de uma imagem. O **Versionamento Semântico (SemVer)** é um padrão amplamente adotado:
    *   `MAJOR.MINOR.PATCH` (ex: `2.1.106`)
    *   Incrementar `PATCH`: Correção de bug, totalmente compatível.
    *   Incrementar `MINOR`: Adição de funcionalidade, ainda compatível com versões anteriores.
    *   Incrementar `MAJOR`: Mudança incompatível com versões anteriores.
*   Usar esse padrão nas tags ajuda os consumidores da sua imagem a decidirem o quão atualizados (ou estáveis) eles querem estar.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Enviar para Registro Local:**
    ```powershell
    docker image push registry.local:5000/gallery/ui:v1
    ```
    Observe a saída.
2.  **(Opcional) Enviar Novamente:**
    ```powershell
    docker image push registry.local:5000/gallery/ui:v1
    ```
    Observe as mensagens "Layer already exists".
3.  **Exemplos de Tags SemVer:**
    *   `my-app:1.0.0`
    *   `my-app:1.0.1` (apenas patch)
    *   `my-app:1.1.0` (nova funcionalidade menor)
    *   `my-app:2.0.0` (grande mudança, pode quebrar compatibilidade)

---

📄 Página 96

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Criar múltiplas tags (aliases) para a mesma imagem `image-gallery`, apontando para o registro local, simulando tags de versão major, minor, patch e latest.
    *   Comandos:
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:latest`
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:2`
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:2.1`
        *   `docker image tag image-gallery registry.local:5000/gallery/ui:2.1.106`
*   Diagrama (Figura 5.8): Ilustra a evolução das tags ao longo de vários releases (Julho a Outubro).
    *   Mostra como tags mais genéricas (`:2`, `:2.1`, `:latest`) mudam para apontar para diferentes versões específicas (`:2.1.106`, `:2.1.114`, `:2.2.11`, `:3.0.42`) a cada novo release.
    *   Tags específicas (`:2.1.106`) permanecem apontando para a mesma imagem imutável.
*   Explica a estratégia de escolha do consumidor:
    *   Usar tag de patch específica (`:2.1.106`): Garante que a imagem nunca mudará (estabilidade máxima), mas não recebe correções futuras.
    *   Usar tag minor (`:2.1`): Recebe correções de patch (`2.1.x`), mas não novas funcionalidades `2.y.z`. Risco baixo de quebra.
    *   Usar tag major (`:2`): Recebe patches e novas funcionalidades minor (`2.x.y`), mas não mudanças major `3.y.z`. Risco moderado.
    *   Usar `latest`: Recebe tudo, incluindo mudanças major. Maior risco de quebra.

🧠 Explicação detalhada e didática:

*   Esta página demonstra como aplicar múltiplas tags à mesma imagem e ilustra um esquema de versionamento comum usando SemVer.
*   O exercício cria vários "apelidos" (tags) para a imagem `image-gallery` original, todos apontando para o mesmo conteúdo (mesmo IMAGE ID), mas com nomes diferentes que representam níveis distintos de versionamento (`:latest`, `:2`, `:2.1`, `:2.1.106`).
*   A Figura 5.8 é fundamental: ela mostra que, a cada novo lançamento de software, as tags mais genéricas (`latest`, `:2`, `:2.1`) são *movidas* para apontar para a nova versão específica, enquanto as tags específicas (`:2.1.106`) continuam apontando para a versão antiga e imutável.
*   Isso permite que quem usa a imagem escolha seu nível de risco/atualização:
    *   Quer estabilidade total? Use a tag mais específica (`:2.1.106`).
    *   Quer correções de bugs mas sem novas funcionalidades arriscadas? Use a tag minor (`:2.1`).
    *   Quer novas funcionalidades compatíveis e correções? Use a tag major (`:2`).
    *   Quer sempre a última versão, custe o que custar? Use `latest` (geralmente não recomendado para produção).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Múltiplas Tags:** (Assumindo que `image-gallery:latest` ainda existe localmente)
    ```powershell
    docker image tag image-gallery registry.local:5000/gallery/ui:latest
    docker image tag image-gallery registry.local:5000/gallery/ui:2
    docker image tag image-gallery registry.local:5000/gallery/ui:2.1
    docker image tag image-gallery registry.local:5000/gallery/ui:2.1.106
    ```
2.  **Verificar as Tags:**
    ```powershell
    docker image ls registry.local:5000/gallery/ui*
    ```
    Você verá várias linhas com o *mesmo* IMAGE ID, mas tags diferentes.
3.  **(Opcional) Enviar Todas as Tags:**
    ```powershell
    docker image push --all-tags registry.local:5000/gallery/ui
    # Ou enviar uma por uma:
    # docker image push registry.local:5000/gallery/ui:latest
    # docker image push registry.local:5000/gallery/ui:2
    # ... etc
    ```

---

📄 Página 97

🔍 Resumo dos principais pontos:

*   Qualquer escolha de tag (patch, minor, major) é válida, depende do equilíbrio de risco.
*   **Importante:** Usar tags *específicas* para imagens base em seus Dockerfiles (`FROM <imagem>:<tag_especifica>`).
    *   Evita que builds futuros quebrem inesperadamente devido a atualizações automáticas da imagem base se você usar `latest` ou tags muito genéricas.
*   **5.5 Turning official images into golden images** (Transformando imagens oficiais em "golden images")
    *   Questão de confiança: Pode-se confiar nas imagens do Docker Hub?
    *   Qualquer um pode enviar imagens; risco de malware.
    *   Solução do Docker Hub: **Verified Publishers** e **Official Images**.
        *   **Verified Publishers:** Empresas (Microsoft, Oracle, IBM) cujas imagens passam por aprovação e varredura de segurança. Podem ser "Certified" (endossadas pelo Docker e publisher). Boa opção para software COTS.
        *   **Official Images:** Geralmente projetos open source, mantidos conjuntamente pelo projeto e Docker. Passam por varredura, atualizados, seguem boas práticas. Dockerfiles geralmente no GitHub.

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre tags enfatizando a prática crucial de usar tags **específicas** na instrução `FROM` de seus Dockerfiles. Se você usar `FROM ubuntu:latest`, seu build pode funcionar hoje, mas quebrar amanhã se uma nova versão incompatível do Ubuntu for lançada e marcada como `latest`. Usar `FROM ubuntu:22.04` garante que você sempre construirá sobre a mesma base testada.
*   A seção 5.5 aborda a questão da **confiança e segurança** das imagens que você usa, especialmente as do Docker Hub.
*   O Docker Hub combate o risco de imagens maliciosas através de dois mecanismos principais:
    *   **Verified Publisher:** Imagens publicadas por empresas conhecidas que passaram por um processo de verificação e varredura de segurança. Algumas são até "Certified", indicando um nível extra de confiança e suporte. Ideal para softwares comerciais ou de grandes fornecedores.
    *   **Official Image:** Imagens de projetos (geralmente open source populares como `nginx`, `python`, `node`, `postgres`) que são curadas e mantidas em colaboração entre a equipe do projeto e a Docker Inc. Elas também passam por varreduras de segurança, são atualizadas regularmente e seguem boas práticas. São a escolha mais comum e recomendada para imagens base.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Boas práticas `FROM`:**
    *   Ruim: `FROM python` (usa `latest` implícito)
    *   Melhor: `FROM python:3.10` (usa a última patch da 3.10)
    *   Ainda melhor: `FROM python:3.10.4` (fixa na patch específica)
    *   Considerar variantes: `FROM python:3.10-slim` ou `FROM python:3.10-alpine` (imagens menores)
*   Navegue no Docker Hub (hub.docker.com) e procure por imagens como `nginx`, `microsoft/dotnet/sdk`, `oracle/database`. Observe os selos "Official Image" ou "Verified Publisher".

💡 Analogias ou observações úteis (se houver):

*   Verified Publisher: É como comprar um produto de uma marca famosa e estabelecida na loja oficial.
*   Official Image: É como pegar um ingrediente "selo de qualidade" recomendado pelo próprio chef famoso (Docker).

---

📄 Página 98

🔍 Resumo dos principais pontos:

*   A maioria usa "Official Images" como base.
*   Eventualmente, surge a necessidade de mais controle -> Criação de **Golden Images**.
*   **Golden Image:** Imagem base *preferida/customizada* pela sua organização.
*   Diagrama (Figura 5.9): Fluxo Golden Image.
    *   Official Image (ex: `openjdk:11-jre-slim`) -> Sua Golden Image (ex: `my-org/openjdk:11.0.1`) -> Imagem da sua Aplicação (ex: `my-org/my-app:1.0`).
*   **Como funciona:**
    *   Golden Image usa uma Official Image como `FROM`.
    *   Adiciona customizações necessárias (ex: certificados de segurança internos, configurações padrão, ferramentas comuns da empresa).
    *   Golden Image é armazenada no registro da empresa (privado ou no Hub).
    *   *Todas* as imagens de aplicação da empresa usam a Golden Image como `FROM`.
*   **Benefícios das Golden Images:**
    *   Combina boas práticas da imagem oficial com customizações da empresa.
    *   Centraliza configurações/ferramentas comuns.
    *   Controla a versão base usada pelas aplicações (atualiza a golden image, depois as apps).
*   **Exercício Try-it-now:** Construir duas golden images para .NET Core (SDK e runtime).
    *   Navegar para `ch05/exercises/dotnet-sdk`, construir `golden/dotnetcore-sdk:3.0`.
    *   Navegar para `ch05/exercises/aspnet-runtime`, construir `golden/aspnet-core:3.0`.

🧠 Explicação detalhada e didática:

*   Mesmo usando imagens oficiais confiáveis, muitas organizações precisam adicionar suas próprias configurações, ferramentas ou padrões de segurança. Em vez de repetir essas adições em cada Dockerfile de aplicação, cria-se uma **Golden Image**.
*   A Golden Image é uma imagem intermediária: ela começa (`FROM`) com uma imagem oficial cuidadosamente escolhida (ex: uma versão específica e mínima do OpenJDK ou .NET Runtime) e adiciona tudo que é padrão ou necessário para *todas* as aplicações daquela tecnologia na sua empresa (ex: certificados raiz da empresa, agentes de monitoramento, configurações de proxy).
*   Essa Golden Image é então publicada no registro interno da empresa. A partir daí, **todos os Dockerfiles de aplicação** devem usar `FROM sua-golden-image` em vez de `FROM imagem-oficial`.
*   **Vantagens:** Centralização de padrões, controle sobre as dependências base (você decide quando atualizar a golden image), simplificação dos Dockerfiles de aplicação (não precisam repetir instalações comuns).
*   O exercício instrui a construir duas golden images de exemplo para .NET Core, uma com o SDK (para builds) e outra com o runtime (para execução).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    1.  No PowerShell/terminal, navegue e construa a golden image do SDK:
        ```powershell
        cd C:\DockerLabs\diamol\ch05\exercises\dotnet-sdk
        docker image build -t golden/dotnetcore-sdk:3.0 .
        ```
    2.  Navegue e construa a golden image do runtime:
        ```powershell
        cd ..\aspnet-runtime
        docker image build -t golden/aspnet-core:3.0 .
        ```
    3.  Verifique as novas imagens locais:
        ```powershell
        docker image ls golden/*
        ```

💡 Analogias ou observações úteis (se houver):

*   Golden Image é como criar o "Kit de Ferramentas Padrão da Empresa" ou o "Modelo de Documento Oficial". Em vez de cada funcionário montar seu kit do zero ou começar um documento em branco, eles partem do kit/modelo padrão, que já contém o essencial e segue as normas da empresa.

---

📄 Página 99

🔍 Resumo dos principais pontos:

*   Figura 5.9: Diagrama ilustrando o conceito de Golden Image.
*   Golden images não são especiais tecnicamente (construídas com Dockerfile), apenas conceitualmente (padronização).
*   Dockerfile da golden image construída adiciona metadados (`LABEL`) e configura defaults.
*   **Uso em Dockerfile de Aplicação:**
    *   Pode-se usar essas golden images em um Dockerfile multi-estágio para uma aplicação .NET Core.
*   **Listagem 5.1:** Exemplo de Dockerfile multi-estágio usando as golden images .NET Core.
    ```dockerfile
    FROM golden/dotnetcore-sdk:3.0 AS builder # Usa SDK golden image
    COPY . .
    RUN dotnet publish -o /out/app app.csproj # Build .NET

    FROM golden/aspnet-core:3.0 # Usa runtime golden image
    COPY --from=builder /out /app # Copia app compilado
    CMD ["dotnet", "/app/app.dll"] # Roda app
    ```
*   Dockerfile da aplicação tem formato multi-estágio padrão, mas agora baseado nas imagens da empresa.
*   **Governança:**
    *   Pode-se *forçar* o uso de golden images em pipelines de CI/CD.
    *   Ferramentas podem escanear Dockerfiles e falhar o build se não usarem `FROM golden/...`.
    *   Boa forma de controlar as imagens base permitidas.

🧠 Explicação detalhada e didática:

*   A página mostra (Figura 5.9) o fluxo: a imagem oficial é a base da golden image, que por sua vez é a base da imagem da aplicação.
*   Explica que as golden images que acabaram de ser construídas são simples, apenas adicionando alguns metadados (`LABEL`) e configurações.
*   A **Listagem 5.1** demonstra como um Dockerfile de aplicação .NET Core usaria essas golden images em um build multi-estágio:
    *   O estágio `builder` usa `FROM golden/dotnetcore-sdk:3.0` para obter o ambiente de build .NET Core já customizado pela empresa.
    *   O estágio final usa `FROM golden/aspnet-core:3.0` para obter o ambiente de runtime .NET Core customizado.
*   O benefício final é a **governança**. Ao padronizar o uso de golden images, a empresa pode controlar melhor as dependências e a segurança. É possível integrar verificações no pipeline de CI/CD para garantir que todos os projetos estejam usando as golden images aprovadas, bloqueando builds que tentem usar imagens base não autorizadas diretamente do Docker Hub, por exemplo.

💻 Exemplos práticos (para meu ambiente Windows):

*   Abra os arquivos `Dockerfile` nas pastas `dotnet-sdk` e `aspnet-runtime` (em `ch05\exercises`) no VS Code para ver o que as golden images realmente fazem (provavelmente adicionam `LABEL`s e talvez algumas configurações padrão).
*   Analise a Listagem 5.1. Se você tivesse um projeto .NET Core simples, poderia usar um Dockerfile como este, substituindo `app.csproj` e `app.dll` pelos nomes corretos, para construí-lo usando as golden images.

💡 Analogias ou observações úteis (se houver):

*   Forçar o uso de Golden Images é como a empresa dizer: "Para construir qualquer produto nosso (aplicação), você *deve* usar o nosso kit de ferramentas oficial (golden image SDK) e ele *deve* ser montado no nosso chassi padrão (golden image runtime)".

---

📄 Página 100

🔍 Resumo dos principais pontos:

*   **5.6 Lab** (Laboratório)
    *   Tarefa de "detetive", requer pesquisa na especificação da API v2 do Docker Registry (link fornecido).
    *   Motivo: CLI Docker padrão não permite buscar ou deletar imagens de um registro (ainda). Interação via API REST é necessária para o registro local básico.
*   **Objetivo do Lab:**
    1.  Enviar (push) *todas* as tags da imagem `gallery/ui` (criadas na p. 97: `:latest`, `:2`, `:2.1`, `:2.1.106`, `:v1`) para o registro local (`registry.local:5000`).
    2.  Verificar (via API REST) que todas as tags estão lá.
    3.  Deletar (via API REST) *todas* as tags/manifestos dessa imagem do registro local.
    4.  Verificar (via API REST) que foram removidas.
*   **Dicas:**
    *   Pode usar um único comando `push` com flag `--all-tags`.
    *   URL base da API do registro local: `http://registry.local:5000/v2`.
    *   Começar listando tags do repositório (`.../tags/list`).
    *   Precisará obter o "manifest" da imagem/tag para deletar.
    *   Deleção requer obter o *digest* do manifesto (um hash sha256).
    *   Deleção via API usa método HTTP `DELETE` em um endpoint específico com o digest.
    *   Documentação da API menciona um header HTTP específico necessário para obter o digest (requisição `HEAD` com `Accept: application/vnd.docker.distribution.manifest.v2+json`).
*   Solução no GitHub. Menciona que pode ser complicado e permite "colar".

🧠 Explicação detalhada e didática:

*   Este lab é mais avançado e foca na interação direta com a **API do Docker Registry**, especificamente o registro básico rodando localmente no contêiner `registry`. Como a linha de comando `docker` tem limitações para gerenciar conteúdos de um registro (ela é ótima para push/pull, mas não para listar tudo ou deletar remotamente de forma granular), precisamos usar ferramentas como `curl` para fazer requisições HTTP diretamente à API do registro.
*   A tarefa envolve:
    1.  Enviar todas as tags criadas para `gallery/ui` para o registro local. A dica `--all-tags` no `docker push` facilita isso.
    2.  Usar `curl` para acessar o endpoint `_catalog` (listar repositórios) e `/<repo>/tags/list` (listar tags de um repo) da API do registro local para confirmar o envio.
    3.  **Parte difícil:** Para deletar, a API exige o `digest` (um hash SHA256 que identifica unicamente o conteúdo do manifesto da imagem/tag). Para obter o digest, é preciso fazer uma requisição HTTP `HEAD` para o manifesto da tag, incluindo um header `Accept` específico (conforme a dica).
    4.  Com o digest em mãos, fazer uma requisição HTTP `DELETE` para o endpoint de manifestos do registro, incluindo o digest.
    5.  Verificar novamente com `curl .../tags/list` se a tag/manifesto foi removido. (Nota: O registro básico pode precisar de configuração extra ou reinício para realmente liberar o espaço em disco após a deleção via API - "garbage collection").
*   Este lab ensina sobre o funcionamento interno dos registros e como interagir programaticamente com eles.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Passos gerais (requer `curl` instalado no Windows ou usar `Invoke-WebRequest` do PowerShell):**
    1.  **Push All Tags:**
        ```powershell
        docker image push --all-tags registry.local:5000/gallery/ui
        ```
    2.  **Listar Repos e Tags (Verificação):**
        ```powershell
        curl http://registry.local:5000/v2/_catalog
        curl http://registry.local:5000/v2/gallery/ui/tags/list
        # Ou com PowerShell:
        # Invoke-WebRequest http://registry.local:5000/v2/_catalog | ConvertFrom-Json
        # Invoke-WebRequest http://registry.local:5000/v2/gallery/ui/tags/list | ConvertFrom-Json
        ```
    3.  **Obter Digest (Exemplo para tag 'v1'):**
        ```powershell
        # Com curl (pode precisar instalar):
        # curl --head -H "Accept: application/vnd.docker.distribution.manifest.v2+json" http://registry.local:5000/v2/gallery/ui/manifests/v1
        # Procure pelo header 'Docker-Content-Digest:' na saída.

        # Com PowerShell:
        $response = Invoke-WebRequest -Method Head -Uri http://registry.local:5000/v2/gallery/ui/manifests/v1 -Headers @{"Accept"="application/vnd.docker.distribution.manifest.v2+json"}
        $digest = $response.Headers.'Docker-Content-Digest'
        echo $digest # Anote este valor (ex: sha256:...)
        ```
    4.  **Deletar Manifesto (usando o digest obtido):**
        ```powershell
        # Com curl:
        # curl -X DELETE http://registry.local:5000/v2/gallery/ui/manifests/<digest_completo>

        # Com PowerShell:
        # Invoke-WebRequest -Method Delete -Uri http://registry.local:5000/v2/gallery/ui/manifests/$digest
        # (Nota: O registro básico pode retornar um erro 405 Method Not Allowed se a deleção não estiver habilitada na configuração dele)
        ```
    5.  **Verificar Deleção:** Repita o passo 2 para listar as tags. A tag `v1` (ou a que você deletou) não deve mais aparecer.

---

📄 Página 101

🔍 Resumo dos principais pontos:

*   **Capítulo 6: Using Docker volumes for persistent storage** (Usando volumes Docker para armazenamento persistente)
*   Contêineres são perfeitos para aplicações *stateless* (sem estado).
    *   Fácil escalar rodando múltiplos contêineres idênticos.
    *   Atualizações via rolling upgrade mantêm o app online.
*   Mas nem todo componente é stateless (ex: bancos de dados, caches em disco).
*   Pode-se rodar componentes *stateful* (com estado) em contêineres também.
*   Armazenamento adiciona complexidade; precisa entender como Dockerizar apps stateful.
*   Foco do Cap. 6: **Docker volumes** e **mounts**, como funciona o filesystem do contêiner.
*   **6.1 Why data in containers is not permanent** (Por que dados em contêineres não são permanentes)
    *   Contêiner tem filesystem (disco virtual) populado com arquivos da imagem (`COPY`).
    *   Filesystem é construído pela união das camadas da imagem (read-only).
    *   Cada contêiner tem seu próprio filesystem, independente de outros (mesmo da mesma imagem).
    *   Modificações em um contêiner não afetam outros ou a imagem.
    *   Demonstração com app que escreve número aleatório em arquivo.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 6, que aborda um tópico essencial: como lidar com dados que precisam **persistir** (durar mais que a vida de um contêiner).
*   Contêineres são ideais para aplicações *stateless* (que não armazenam dados internamente entre requisições), pois podem ser criados, destruídos e substituídos facilmente.
*   No entanto, muitas aplicações *precisam* armazenar dados (são *stateful*), como bancos de dados ou aplicações que mantêm sessões de usuário em arquivos. Rodar esses apps em contêineres exige uma solução para que os dados não sejam perdidos quando o contêiner for atualizado ou recriado.
*   A seção 6.1 começa explicando *por que* o armazenamento padrão dentro de um contêiner *não* é persistente. O filesystem que o contêiner "vê" é uma construção virtual feita a partir das camadas da imagem (que são somente leitura) mais uma camada gravável específica para aquele contêiner.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ainda, mas prepara para um exercício que demonstrará a natureza não persistente e isolada do filesystem padrão do contêiner.

💡 Analogias ou observações úteis (se houver):

*   Stateless vs. Stateful: Um caixa eletrônico (ATM) é stateless - não importa qual ATM você usa, a transação é a mesma. Sua conta bancária é stateful - o saldo precisa ser armazenado e persistir.
*   Filesystem do Contêiner (inicial): Pense em um bloco de anotações (camada gravável) colocado sobre um livro impresso (camadas da imagem). Você pode escrever no bloco, e até mesmo riscar ou cobrir partes do livro (escondê-las), mas você não altera o livro original. Se você jogar o bloco fora (remover o contêiner), suas anotações se perdem.

---

📄 Página 102

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar dois contêineres da imagem `diamol/ch06-random-number`.
    *   Comandos: `docker container run --name rn1 ...`, `docker container run --name rn2 ...`.
    *   App escreve número aleatório em `/random/number.txt` e sai (`Exited`).
*   Contêineres parados mantêm seus filesystems.
*   Comando `docker container cp`: Copia arquivos entre contêiner (parado ou rodando) e máquina host.
    *   Sintaxe: `docker cp <container>:<caminho_no_container> <caminho_local>` ou vice-versa.
*   **Exercício Try-it-now:** Copiar os arquivos `number.txt` de cada contêiner para o host e verificar o conteúdo.
    *   Comandos: `docker cp rn1:/random/number.txt number1.txt`, `docker cp rn2:/random/number.txt number2.txt`, `cat number1.txt`, `cat number2.txt` (ou `type` no Windows).
*   Resultado esperado (Figura 6.1): Conteúdos dos arquivos `number1.txt` e `number2.txt` são diferentes, mostrando independência dos filesystems.
*   **Explicação do Filesystem do Contêiner:**
    *   Parece um disco único (`/dev/sda1` Linux, `C:\` Windows).
    *   É virtual, construído pelo Docker a partir de várias fontes.
    *   Fontes base: **Camadas da Imagem (Image Layers)** (compartilhadas, read-only) + **Camada Gravável do Contêiner (Container's Writable Layer)** (única por contêiner).
*   Diagrama (Figura 6.2): Ilustra a construção do filesystem a partir das camadas da imagem (compartilhadas) e da camada gravável (única por contêiner).

🧠 Explicação detalhada e didática:

*   O primeiro exercício demonstra o isolamento do filesystem: dois contêineres (`rn1`, `rn2`), criados da *mesma* imagem, executam o mesmo script que cria um arquivo no mesmo caminho (`/random/number.txt`), mas o conteúdo do arquivo é diferente em cada um (um número aleatório).
*   O comando `docker container cp` é introduzido como forma de extrair arquivos de dentro de um contêiner para a sua máquina local (host), permitindo inspecionar o resultado do exercício anterior.
*   A explicação técnica do filesystem do contêiner é aprofundada:
    *   Ele é uma **união** de camadas. A base vem das camadas da imagem (que são somente leitura e podem ser compartilhadas).
    *   Por cima delas, cada contêiner recebe sua própria **camada gravável**, onde todas as *modificações* (arquivos novos ou alterados) são escritas.
    *   A Figura 6.2 mostra isso: `rn1` e `rn2` compartilham as camadas da imagem `random-number`, mas cada um tem sua própria camada gravável onde o `number.txt` (com conteúdo diferente) é armazenado.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # Rodar os dois contêineres
    docker container run --name rn1 diamol/ch06-random-number
    docker container run --name rn2 diamol/ch06-random-number

    # Verificar que estão parados (Exited)
    docker ps -a

    # Copiar os arquivos para o diretório atual no host
    docker container cp rn1:/random/number.txt number1.txt
    docker container cp rn2:/random/number.txt number2.txt

    # Exibir o conteúdo dos arquivos (no Windows, 'type' é similar a 'cat')
    type number1.txt
    type number2.txt
    # Ou use Get-Content no PowerShell
    # Get-Content number1.txt
    # Get-Content number2.txt
    ```
    Confirme que os números exibidos são diferentes.

---

📄 Página 103

🔍 Resumo dos principais pontos:

*   Dois pontos importantes da Figura 6.2:
    1.  Camadas da imagem são compartilhadas -> devem ser read-only.
    2.  Camada gravável é única por contêiner e tem o *mesmo ciclo de vida* do contêiner.
*   Ciclo de vida:
    *   Camadas de imagem: Persistem no cache até serem removidas (`docker image rm`).
    *   Camada gravável: Criada com `docker run`, deletada com `docker rm`. (Contêiner parado ainda tem sua camada gravável).
*   **Edição de arquivos das camadas da imagem:**
    *   Como editar se são read-only? Docker usa **Copy-on-Write (CoW)**.
    *   Quando contêiner tenta editar arquivo de uma camada da imagem:
        1.  Docker *copia* o arquivo da camada read-only para a camada *gravável*.
        2.  As edições ocorrem na *cópia* dentro da camada gravável.
    *   Processo é transparente para o app no contêiner.
    *   CoW é a base da eficiência de armazenamento do Docker.
*   **Exercício Try-it-now:** Demonstrar CoW e ciclo de vida da camada gravável.
    *   Imagem `diamol/ch06-file-display` exibe conteúdo de `/input.txt`.
    *   1. Rodar contêiner `f1` para ver conteúdo original (`docker run --name f1 ...`).
    *   2. Criar `url.txt` local com novo conteúdo (`echo ... > url.txt`).
    *   3. Copiar `url.txt` para *dentro* de `f1` como `/input.txt` (`docker cp url.txt f1:/input.txt`). Isso usa CoW.
    *   4. Reiniciar `f1` para ver conteúdo modificado (`docker start --attach f1`).

🧠 Explicação detalhada e didática:

*   A página explica a consequência mais importante do compartilhamento de camadas de imagem: elas **têm que ser somente leitura (read-only)**. Se pudessem ser modificadas, uma mudança em uma imagem afetaria todas as outras que a compartilham, o que seria caótico.
*   Então, como um contêiner pode modificar um arquivo que veio da imagem base (ex: um arquivo de configuração padrão)? A resposta é a técnica **Copy-on-Write (CoW)**:
    *   Quando o contêiner tenta *escrever* (modificar) um arquivo que está em uma camada read-only, o Docker primeiro *copia* esse arquivo para a camada *gravável* do contêiner.
    *   A partir daí, todas as modificações acontecem nessa cópia na camada gravável. O arquivo original na camada read-only permanece intocado.
    *   Para o aplicativo dentro do contêiner, parece que ele simplesmente editou o arquivo; o CoW é invisível para ele.
*   A página também reforça o **ciclo de vida da camada gravável**: ela está intrinsecamente ligada ao contêiner. Quando o contêiner é removido (`docker rm`), a camada gravável e todas as modificações feitas nela são **perdidas permanentemente**.
*   O exercício prepara a demonstração: rodar um contêiner, modificar um arquivo dentro dele (o `docker cp` para dentro do contêiner acionará o CoW se `/input.txt` vier da imagem) e depois reiniciar o *mesmo* contêiner para ver a modificação.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # 1. Rodar container f1 (ele imprime o conteúdo e sai)
    docker container run --name f1 diamol/ch06-file-display
    # (Saída deve ser o conteúdo padrão de input.txt)

    # 2. Criar arquivo local com novo conteúdo
    echo "http://eltonstoneman.com" > url.txt
    # Verifique o conteúdo: type url.txt

    # 3. Copiar o arquivo local para dentro do container f1 (que está parado)
    docker container cp url.txt f1:/input.txt

    # 4. Reiniciar f1 e ver a saída (--attach anexa ao output do container)
    docker container start --attach f1
    # (Saída AGORA deve ser http://eltonstoneman.com)
    ```

---

📄 Página 104

🔍 Resumo dos principais pontos:

*   Figura 6.3: Mostra a saída do exercício anterior, confirmando que o arquivo dentro do contêiner `f1` foi modificado.
*   Modificar arquivo em um contêiner *não* afeta a imagem original nem outros contêineres.
*   Arquivo modificado só existe na camada gravável daquele contêiner específico (`f1`).
*   **Exercício Try-it-now:** Verificar que a imagem não mudou e que os dados se perdem com `rm`.
    *   1. Rodar *novo* contêiner `f2` da *mesma imagem* para ver conteúdo original (`docker run --name f2 ...`).
    *   2. Remover contêiner `f1` (`docker rm -f f1`).
    *   3. Tentar copiar o arquivo de `f1` (que não existe mais) para o host (`docker cp f1:/input.txt .`). Deve falhar.

🧠 Explicação detalhada e didática:

*   A página confirma o resultado do exercício anterior (Figura 6.3): o reinício do contêiner `f1` mostrou o conteúdo modificado, provando que a alteração feita via `docker cp` foi persistida *naquele contêiner específico*, mesmo ele estando parado entre a cópia e o reinício.
*   O ponto crucial é reiterado: essa modificação vive *apenas* na camada gravável de `f1`. A imagem `diamol/ch06-file-display` original não foi alterada. Qualquer *novo* contêiner criado a partir dessa imagem começará com o conteúdo original do arquivo `/input.txt`.
*   O segundo exercício demonstra isso e a natureza efêmera da camada gravável:
    *   Um novo contêiner `f2` é criado da mesma imagem e mostra o conteúdo original, provando que a imagem está intacta.
    *   O contêiner `f1` é removido com `docker rm -f f1`. Isso **destrói permanentemente** sua camada gravável.
    *   A tentativa de copiar o arquivo `/input.txt` de `f1` *após* a remoção falha, pois o contêiner (e seus dados modificados) não existe mais.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # 1. Rodar novo container f2 (deve mostrar conteúdo original)
    docker container run --name f2 diamol/ch06-file-display

    # 2. Remover f1 forçadamente
    docker container rm -f f1

    # 3. Tentar copiar de f1 (deve dar erro "No such container")
    docker container cp f1:/input.txt .
    ```

💡 Analogias ou observações úteis (se houver):

*   Remover o contêiner (`docker rm`) é como jogar fora o bloco de anotações onde você escreveu sobre o livro. Suas anotações (camada gravável) se foram, mas o livro original (imagem) continua na estante, inalterado, pronto para outra pessoa ler (criar novo contêiner).

---

📄 Página 105

🔍 Resumo dos principais pontos:

*   Figura 6.4: Mostra a saída do exercício anterior, confirmando que `f2` tem o conteúdo original e `f1` não existe mais.
*   **Conclusão sobre filesystem padrão:**
    *   Tem o mesmo ciclo de vida do contêiner.
    *   Camada gravável é removida com `docker rm`.
    *   Qualquer dado modificado no contêiner é perdido.
*   **Implicação:** Remover contêineres é comum (ex: atualizações de app onde imagem nova substitui a antiga). Se dados importantes foram escritos na camada gravável, eles são perdidos na atualização.
*   **Cenários:**
    *   OK para dados transitórios (ex: cache local). O novo contêiner começa com cache vazio.
    *   Desastre para dados persistentes (ex: banco de dados). Não se espera perder tudo ao atualizar o SGBD.
*   **Solução Docker:** Fontes adicionais para o filesystem do contêiner além das camadas: **Volumes Docker** e **Mounts**.
    *   Têm ciclo de vida *separado* dos contêineres.
    *   Usados para armazenar dados que precisam persistir entre recriações/atualizações de contêineres.
*   Introdução à Seção 6.2 sobre Volumes.

🧠 Explicação detalhada e didática:

*   A página consolida a lição da seção 6.1: a camada gravável padrão de um contêiner é **efêmera**. Ela desaparece junto com o contêiner quando ele é removido (`docker rm`).
*   Isso é um problema sério para aplicações **stateful** (que precisam guardar dados), porque o processo normal de atualização de uma aplicação em contêineres envolve justamente remover os contêineres antigos e substituí-los por novos baseados na imagem atualizada. Se os dados estivessem apenas na camada gravável, seriam perdidos a cada atualização.
*   A solução do Docker para persistência de dados é usar mecanismos de armazenamento que existem *fora* do ciclo de vida normal do contêiner:
    1.  **Volumes Docker:** Uma área de armazenamento gerenciada *pelo Docker*, mas que existe independentemente dos contêineres.
    2.  **Bind Mounts:** Um mapeamento direto de um diretório ou arquivo da máquina *host* para dentro do contêiner.
*   Ambos permitem que um novo contêiner acesse os mesmos dados que um contêiner anterior (removido) estava usando. O capítulo explorará essas duas opções.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. Prepara para os conceitos e comandos de `docker volume` e `docker run -v` (ou `--mount`) nas próximas seções.

💡 Analogias ou observações úteis (se houver):

*   Volumes e Bind Mounts são como conectar um **HD externo** (Volume Docker) ou compartilhar uma **pasta de rede** (Bind Mount) com o seu contêiner (que antes só tinha o "disco interno" efêmero). Mesmo que você troque de contêiner (como trocar de computador), você pode reconectar o mesmo HD externo ou pasta de rede e seus arquivos estarão lá.

---

📄 Página 106

🔍 Resumo dos principais pontos:

*   **6.2 Running containers with Docker volumes** (Rodando contêineres com volumes Docker)
*   **Volume Docker:** Unidade de armazenamento gerenciada pelo Docker, como um "pendrive para contêineres".
    *   Existe independentemente dos contêineres.
    *   Tem seu próprio ciclo de vida.
    *   Pode ser anexado a contêineres.
*   Solução para dados persistentes em apps stateful.
*   **Fluxo de uso:**
    1.  Criar volume.
    2.  Anexar ao contêiner da aplicação (aparece como um diretório).
    3.  Contêiner escreve dados no diretório (que vão para o volume).
    4.  Ao atualizar o app, anexa-se o *mesmo* volume ao novo contêiner.
    5.  Novo contêiner tem acesso aos dados originais.
*   **Duas formas de usar volumes:**
    1.  **Manualmente:** Criar volumes (`docker volume create`) e anexá-los com `docker run -v <nome_volume>:<caminho_container>`.
    2.  **Via Dockerfile:** Usar instrução `VOLUME <caminho_destino>`. Imagem cria um volume *anônimo* automaticamente quando contêiner inicia (se nenhum volume for especificado no `run`).
*   **Listagem 6.1:** Trecho de Dockerfile usando `VOLUME /data`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    WORKDIR /app
    ENTRYPOINT ["dotnet", "ToDoList.dll"]
    VOLUME /data # Declara um ponto de montagem para volume
    COPY --from=builder /out/ .
    ```
*   Quando contêiner roda dessa imagem (sem `-v` explícito), Docker cria volume anônimo e monta em `/data` (ou `C:\data` Windows). Dados escritos lá persistem no volume.

🧠 Explicação detalhada e didática:

*   Esta seção introduz os **Volumes Docker** como a principal forma gerenciada pelo Docker para persistir dados.
*   A analogia do "pendrive para contêineres" é boa: um volume é uma unidade de armazenamento separada que você pode "plug" em um contêiner. O Docker cuida de onde esse "pendrive" é armazenado fisicamente no host, mas você interage com ele através de nomes.
*   O fluxo de uso para atualizações é explicado: o volume sobrevive à remoção do contêiner antigo e é re-anexado ao novo, preservando os dados.
*   Há duas maneiras principais de associar um volume a um contêiner:
    1.  **Explícita (`docker run -v`):** Você cria um volume com nome (`docker volume create meu-volume`) e o monta em um caminho específico dentro do contêiner (`docker run -v meu-volume:/app/data ...`). Esta é a forma **recomendada** para dados importantes, pois você controla o nome e o ciclo de vida do volume.
    2.  **Implícita (`VOLUME` no Dockerfile):** A instrução `VOLUME /data` no Dockerfile diz ao Docker: "Se alguém rodar um contêiner desta imagem *sem* montar explicitamente algo em `/data`, por favor, crie um volume *anônimo* (com um nome aleatório longo) e monte-o aqui". Isso garante que os dados escritos em `/data` sempre vão para *algum* volume, mas gerenciar volumes anônimos é mais difícil.
*   A Listagem 6.1 mostra um Dockerfile que usa a instrução `VOLUME`.

💻 Exemplos práticos (para meu ambiente Windows):

*   A instrução `VOLUME /data` no Dockerfile funciona para imagens baseadas em Linux ou Windows. O Docker mapeará para `C:\data` automaticamente se a imagem base for Windows.
*   Você executará um contêiner baseado na imagem com `VOLUME` no próximo passo para ver o volume anônimo sendo criado.

---

📄 Página 107

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar contêiner da imagem `diamol/ch06-todo-list` (que tem `VOLUME /data`) e verificar o volume criado.
    *   Comandos:
        *   `docker container run --name todo1 -d -p 8010:80 diamol/ch06-todo-list`
        *   `docker container inspect --format '{{.Mounts}}' todo1` (mostra detalhes da montagem do volume)
        *   `docker volume ls` (lista volumes existentes)
*   Saída esperada (Figura 6.5): `inspect` mostra um volume montado em `/data` (ou `C:\data`); `volume ls` mostra um volume com nome longo e aleatório (anônimo).
*   Volumes são transparentes para o app (vê apenas o diretório `/data`).
*   **Teste da Aplicação To-Do:**
    *   Acessar `http://localhost:8010` no navegador.
    *   App armazena dados em um arquivo no diretório `/data`.
    *   Itens adicionados via web são salvos no volume Docker.
*   Figura 6.6: Mostra a interface web do app To-Do. App permite adicionar itens, mas não remover (piada do autor).

🧠 Explicação detalhada e didática:

*   O exercício demonstra a criação automática de um volume **anônimo** devido à instrução `VOLUME /data` no Dockerfile da imagem `diamol/ch06-todo-list`.
*   Ao rodar o contêiner `todo1` sem especificar um volume com `-v`, o Docker cria um volume com um nome longo e aleatório (ex: `f8a...`) e o monta no caminho `/data` dentro do contêiner.
*   O comando `docker container inspect --format '{{.Mounts}}'` é usado para confirmar que um volume está montado e qual o seu nome/ID.
*   O comando `docker volume ls` lista todos os volumes gerenciados pelo Docker, incluindo o anônimo recém-criado.
*   Para a aplicação To-Do dentro do contêiner, o volume é invisível; ela simplesmente lê e escreve arquivos no diretório `/data` como faria em qualquer sistema de arquivos. O Docker garante que essas operações aconteçam no volume associado.
*   Acessar a aplicação no navegador e adicionar itens demonstra que os dados estão sendo salvos (no volume).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # Rodar o container (Docker cria volume anônimo)
    docker container run --name todo1 -d -p 8010:80 diamol/ch06-todo-list

    # Inspecionar as montagens do container
    docker container inspect --format '{{.Mounts}}' todo1
    # Observe a saída, deve mostrar um 'Type=volume', um 'Name' longo (ID), e 'Destination=/data' (ou C:\data)

    # Listar os volumes (procurar pelo volume com o ID longo)
    docker volume ls

    # Testar o App
    # Abra o navegador e acesse http://localhost:8010
    # Adicione alguns itens à lista (ex: "Aprender Volumes", "Fazer Lab 6")
    ```

---

📄 Página 108

🔍 Resumo dos principais pontos:

*   Volumes de imagem (anônimos) são criados *por contêiner*. Novo contêiner = novo volume anônimo.
*   Pode-se **compartilhar** volumes entre contêineres usando `docker run --volumes-from <outro_container>`.
    *   Anexa os volumes do `<outro_container>` ao novo contêiner.
*   **Exercício Try-it-now:** Demonstrar volumes dedicados vs. compartilhados.
    *   1. Rodar `todo2` (terá seu próprio volume anônimo, lista vazia).
        *   `docker container run --name todo2 -d diamol/ch06-todo-list`
        *   Verificar conteúdo de `/data` em `todo2` (usando `docker exec ... ls /data` ou `dir C:\data`). Deve estar vazio.
    *   2. Rodar `t3` usando volumes de `todo1` (terá os itens adicionados anteriormente).
        *   `docker container run -d --name t3 --volumes-from todo1 diamol/ch06-todo-list`
        *   Verificar conteúdo de `/data` em `t3`. Deve mostrar o arquivo de dados (ex: `todolist.db`).
*   Saída esperada (Figura 6.7): Mostra `todo2` vazio e `t3` com dados de `todo1`.

🧠 Explicação detalhada e didática:

*   A página explica que a criação automática de volumes via instrução `VOLUME` resulta em um volume *novo e separado* para cada contêiner iniciado (a menos que se use `-v`).
*   Introduz a flag `--volumes-from`. Ela permite que um novo contêiner (`t3` no exemplo) "pegue emprestado" e use exatamente os mesmos volumes que outro contêiner (`todo1`) está usando (ou usou, mesmo que `todo1` esteja parado). Isso permite que múltiplos contêineres acessem e modifiquem os mesmos dados armazenados em um volume.
*   O exercício demonstra isso:
    *   `todo2` é iniciado normalmente e recebe seu próprio volume anônimo vazio.
    *   `t3` é iniciado com `--volumes-from todo1`, então ele não cria um novo volume, mas monta o volume *de todo1* em `/data`. Como `todo1` já tinha dados (do exercício anterior), `t3` agora vê esses mesmos dados.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # 1. Rodar todo2
    docker container run --name todo2 -d diamol/ch06-todo-list

    # Verificar /data em todo2 (assumindo imagem Linux base)
    docker container exec todo2 ls /data
    # (Saída deve ser vazia)
    # Se for imagem Windows base:
    # docker container exec todo2 cmd /C "dir C:\data"

    # 2. Rodar t3 usando volumes de todo1
    docker container run -d --name t3 --volumes-from todo1 diamol/ch06-todo-list

    # Verificar /data em t3 (assumindo imagem Linux base)
    docker container exec t3 ls /data
    # (Saída deve mostrar o arquivo de dados, ex: 'todolist.db')
    # Se for imagem Windows base:
    # docker container exec t3 cmd /C "dir C:\data"
    ```

💡 Analogias ou observações úteis (se houver):

*   `--volumes-from`: É como dizer ao novo funcionário (`t3`) para usar exatamente a mesma gaveta de arquivos (`volume`) que o funcionário antigo (`todo1`) usava, em vez de pegar uma gaveta nova e vazia.

---

📄 Página 109

🔍 Resumo dos principais pontos:

*   Figura 6.7: Mostra a saída do exercício anterior (diretórios em `todo2` vs `t3`).
*   Compartilhar volumes (`--volumes-from`) pode não ser ideal.
    *   Apps podem esperar acesso exclusivo aos arquivos e falhar se outro contêiner modificar ao mesmo tempo.
*   Melhor uso de volumes: preservar estado entre atualizações de app.
*   Abordagem recomendada: **Volumes Nomeados (Named Volumes)**.
    *   Criar explicitamente um volume com um nome (`docker volume create <nome>`).
    *   Anexar o volume nomeado ao contêiner (`docker run -v <nome>:<caminho>`).
*   **Exercício Try-it-now:** Demonstrar atualização de app usando volume nomeado.
    *   1. Definir variável `target` para caminho no contêiner (`/data` ou `c:\data`).
    *   2. Criar volume nomeado `todo-list`: `docker volume create todo-list`.
    *   3. Rodar app v1 (`todo-v1`) usando o volume nomeado: `docker run ... -v todo-list:$target ... diamol/ch06-todo-list`.
    *   4. Adicionar dados via UI (`http://localhost:8011`).
    *   5. Remover contêiner v1: `docker rm -f todo-v1`.
    *   6. Rodar app v2 (`todo-v2`) usando o *mesmo* volume nomeado: `docker run ... -v todo-list:$target ... diamol/ch06-todo-list:v2`.

🧠 Explicação detalhada e didática:

*   A página adverte sobre os riscos de compartilhar o mesmo volume entre múltiplos contêineres simultaneamente (`--volumes-from`), pois muitas aplicações não são projetadas para lidar com escrita concorrente nos mesmos arquivos.
*   Introduz a abordagem preferida para persistência de dados entre atualizações: **Volumes Nomeados**. Em vez de deixar o Docker criar volumes anônimos (via `VOLUME` no Dockerfile ou sem `-v`), você cria um volume com um nome significativo (`docker volume create todo-list`) e o gerencia explicitamente.
*   Ao iniciar um contêiner, você usa a sintaxe `-v <nome_do_volume>:<caminho_no_container>` para montar esse volume específico.
*   O exercício demonstra o ciclo de atualização:
    1.  Cria-se o volume `todo-list`.
    2.  Inicia-se a versão 1 do app (`todo-v1`), montando `todo-list` em `/data`.
    3.  Adicionam-se dados (eles vão para o volume `todo-list`).
    4.  Remove-se o contêiner `todo-v1` (o volume `todo-list` permanece intacto).
    5.  Inicia-se a versão 2 do app (`todo-v2`), montando o *mesmo* volume `todo-list` em `/data`.
    6.  O `todo-v2` agora tem acesso aos dados criados pelo `todo-v1`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # 1. Definir variável de destino (ajuste C: se necessário para imagem Windows)
    $target='/data' # Para imagem Linux
    # $target='c:\data' # Para imagem Windows

    # 2. Criar volume nomeado
    docker volume create todo-list

    # 3. Rodar v1 com volume nomeado (nova porta 8011)
    docker container run -d -p 8011:80 -v todo-list:$target --name todo-v1 diamol/ch06-todo-list

    # 4. Adicionar dados
    # Abra o navegador em http://localhost:8011 e adicione itens.

    # 5. Remover v1
    docker container rm -f todo-v1

    # 6. Rodar v2 com o mesmo volume nomeado
    docker container run -d -p 8011:80 -v todo-list:$target --name todo-v2 diamol/ch06-todo-list:v2
    ```
*   O teste final será na próxima página (acessar a UI do v2).

---

📄 Página 110

🔍 Resumo dos principais pontos:

*   Figura 6.8: Mostra a saída do exercício anterior, confirmando que o volume nomeado persiste após remoção do contêiner v1 e é usado pelo v2.
*   **Teste:** Acessar `http://localhost:8011` (agora servido pelo v2).
    *   Deve-se ver a UI da versão 2 (Figura 6.9), mas com os dados adicionados na v1.
*   **Esclarecimento sobre `VOLUME` vs. `-v`:**
    *   Instrução `VOLUME` no Dockerfile e flag `-v` (ou `--volume`) no `docker run` são features separadas.
    *   Se imagem tem `VOLUME /data` e `docker run` *não* usa `-v ...:/data`: Docker cria volume *anônimo*.
    *   Se imagem tem `VOLUME /data` e `docker run` *usa* `-v meu-volume:/data`: Docker usa o volume *nomeado* `meu-volume`, *não* cria um anônimo.
    *   Se imagem *não* tem `VOLUME` e `docker run` usa `-v meu-volume:/data`: Docker usa o volume nomeado `meu-volume`.
*   **Recomendação:**
    *   Autor da imagem: Usar `VOLUME` como "fail-safe" para apps stateful (garante que dados vão para *algum* volume).
    *   Usuário da imagem: É melhor *não* depender do `VOLUME` implícito e usar sempre volumes *nomeados* (`docker volume create ...` e `docker run -v ...`) para clareza e controle.

🧠 Explicação detalhada e didática:

*   A página confirma o resultado esperado do exercício anterior: ao acessar a aplicação na porta 8011 (agora servida pelo contêiner `todo-v2`), você verá a nova interface (Figura 6.9), mas a lista de tarefas conterá os itens que você adicionou quando a v1 estava rodando. Isso prova que o volume nomeado persistiu os dados através da atualização.
*   Um ponto importante de esclarecimento é a interação entre a instrução `VOLUME` no Dockerfile e a flag `-v` no comando `docker run`:
    *   Se você *especifica* um volume com `-v` (seja nomeado ou anônimo criado na hora), ele tem **prioridade** sobre a instrução `VOLUME` no Dockerfile para aquele caminho específico. O Docker usará o que você mandou no `-v`.
    *   A instrução `VOLUME` só entra em ação (criando um volume anônimo) se o `docker run` *não* montar nada naquele caminho específico.
*   A recomendação final é clara: embora `VOLUME` no Dockerfile possa ser um backup, a **melhor prática** para gerenciar dados persistentes é usar **volumes nomeados** criados e montados explicitamente com `docker volume create` e `docker run -v <nome>:<caminho>`. Isso dá controle total sobre o ciclo de vida e a identificação dos seus dados.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Teste:** Abra o navegador em `http://localhost:8011`. Você deve ver a nova interface (Figura 6.9) e os itens que adicionou anteriormente.
2.  **Listar Volumes:** Execute `docker volume ls`. Você verá o volume `todo-list` na lista, junto com quaisquer volumes anônimos de execuções anteriores (que podem ser limpos com `docker volume prune`).

---

📄 Página 111

🔍 Resumo dos principais pontos:

*   Figura 6.9: Mostra a nova interface (v2) do app To-Do.
*   **6.3 Running containers with filesystem mounts** (Rodando contêineres com montagens de sistema de arquivos)
*   Volumes são gerenciados pelo Docker; localização física no host é abstraída.
*   Alternativa: **Bind Mounts**.
    *   Mapeia um diretório ou arquivo *diretamente* da máquina **host** para dentro do contêiner.
    *   Contêiner vê como parte normal do seu filesystem.
    *   Permite acesso a arquivos do host pelo contêiner e vice-versa.
*   **Casos de uso para Bind Mounts:**
    *   Usar armazenamento específico do host (SSD rápido, RAID, storage de rede). Se o host acessa, o contêiner pode usar via bind mount.
    *   Exemplo: Rodar BD em contêiner, mas armazenar dados em array RAID do host para confiabilidade.

🧠 Explicação detalhada e didática:

*   Esta seção introduz a segunda forma principal de fornecer armazenamento externo a contêineres: **Bind Mounts**.
*   A diferença fundamental em relação aos Volumes Docker é:
    *   **Volumes Docker:** São gerenciados *pelo Docker*. Você cria (`docker volume create`), lista (`docker volume ls`), remove (`docker volume rm`). O Docker decide onde armazená-los fisicamente no sistema de arquivos do host (geralmente em um local específico dentro da área de dados do Docker, como `/var/lib/docker/volumes` no Linux). Você geralmente não interage com o local físico.
    *   **Bind Mounts:** São um mapeamento *direto* de um caminho (diretório ou arquivo) que já existe na sua máquina **host** para um caminho dentro do contêiner. Você especifica o caminho exato no host.
*   Bind mounts são úteis quando você quer que o contêiner acesse ou modifique arquivos que estão em um local específico do seu sistema host, ou quando quer usar um tipo específico de armazenamento (como um disco SSD rápido ou um compartilhamento de rede montado no host) que o Docker não gerenciaria diretamente como um volume.

💻 Exemplos práticos (para meu ambiente Windows):

*   O comando para bind mount usa a flag `-v` com uma sintaxe diferente dos volumes nomeados, ou a flag mais moderna e explícita `--mount`.
    *   Sintaxe `-v`: `docker run -v <caminho_ABSULUTO_no_host>:<caminho_no_container> ...`
    *   Sintaxe `--mount`: `docker run --mount type=bind,source=<caminho_ABSOLUTO_no_host>,target=<caminho_no_container> ...`
*   **Importante para Windows:** Caminhos do host devem ser absolutos (ex: `C:\Users\Alexandre\MeuApp\data`) ou usar variáveis como `$(pwd)` (PowerShell) ou `%cd%` (CMD) para obter o diretório atual de forma absoluta. O Docker Desktop pode exigir configurações de compartilhamento de arquivos (File Sharing) nas Settings para permitir que os contêineres acessem diretórios fora do perfil do usuário.

💡 Analogias ou observações úteis (se houver):

*   Volume Docker: Docker te dá um armário trancado (volume) para guardar suas coisas; ele cuida de onde o armário fica.
*   Bind Mount: Você diz ao Docker para usar uma gaveta específica *da sua própria mesa* (diretório no host) para guardar as coisas do contêiner. Você sabe exatamente onde está, e pode acessar diretamente pela sua mesa ou pelo contêiner.

---

📄 Página 112

🔍 Resumo dos principais pontos:

*   Exercício usa bind mount para armazenar dados do app To-Do em um diretório local do host.
*   **Exercício Try-it-now:** Usar bind mount com o app To-Do.
    *   1. Definir variáveis `source` (caminho no host, ex: `$(pwd)\databases`) e `target` (caminho no contêiner, ex: `c:\data`). Ajustar para Windows/Linux.
    *   2. Criar diretório `databases` no host (`mkdir`).
    *   3. Rodar contêiner usando `--mount type=bind...`: `docker run --mount type=bind,source=$source,target=$target -d -p 8012:80 diamol/ch06-todo-list`.
    *   4. Fazer requisição `curl` para o app (para ele criar o arquivo de dados).
    *   5. Listar conteúdo do diretório `databases` *no host* (`ls ./databases`).
*   Resultado esperado (Figura 6.10): Arquivo de dados do app (ex: `todolist.db`) aparece no diretório `databases` da máquina *host*.

🧠 Explicação detalhada e didática:

*   O exercício demonstra o bind mount na prática.
*   Primeiro, definem-se variáveis para os caminhos no host (`source`) e no contêiner (`target`) para facilitar a portabilidade do comando entre Windows e Linux. O caminho `source` usa `$(pwd)` (ou similar) para referenciar um subdiretório (`databases`) dentro do diretório atual onde o comando está sendo executado no host.
*   Cria-se o diretório `databases` no host.
*   Inicia-se o contêiner `diamol/ch06-todo-list` usando a sintaxe `--mount type=bind,source=$source,target=$target`. Isso diz ao Docker: "pegue o diretório `$source` do host e faça ele aparecer como `$target` dentro do contêiner".
*   Uma requisição `curl` é feita apenas para garantir que a aplicação dentro do contêiner seja inicializada e crie seu arquivo de dados dentro do diretório `$target` (que, na verdade, é o diretório `$source` do host).
*   O passo final é listar o conteúdo do diretório `$source` (`databases`) *no host*. A presença do arquivo de dados da aplicação confirma que o bind mount funcionou e os dados do contêiner estão sendo escritos diretamente no sistema de arquivos do host.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # 1. Definir variáveis (estando no diretório C:\DockerLabs\diamol\ch06\exercises)
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "databases"
    $TargetPath = 'c:\data' # Assumindo imagem base Windows para consistência no exemplo
                            # Se a imagem for Linux, use $TargetPath = '/data'
                            # A imagem diamol/ch06-todo-list é multi-arch,
                            # no Windows ele puxará a variante Windows se disponível.

    # 2. Criar diretório no host
    mkdir $SourcePath -ErrorAction SilentlyContinue

    # 3. Rodar container com bind mount (porta 8012)
    docker container run --mount type=bind,source=$SourcePath,target=$TargetPath `
      -d -p 8012:80 --name todo-bind diamol/ch06-todo-list

    # 4. Fazer requisição (pode demorar um pouco para o app iniciar)
    Start-Sleep -Seconds 5 # Dar tempo para iniciar
    Invoke-WebRequest http://localhost:8012

    # 5. Listar conteúdo do diretório no HOST
    Get-ChildItem -Path $SourcePath
    # Você deve ver o arquivo de dados (ex: todolist.db) aqui
    ```
*   **Observação sobre compartilhamento:** Se `$SourcePath` estiver fora de `C:\Users`, o Docker Desktop pode pedir permissão para compartilhar o drive/diretório na primeira vez que você usar um bind mount nele. Verifique as configurações em Settings > Resources > File Sharing.

---

📄 Página 113

🔍 Resumo dos principais pontos:

*   Bind mount é bidirecional: mudanças no host refletem no contêiner e vice-versa.
*   **Implicações de Segurança:**
    *   Contêineres geralmente devem rodar com privilégios mínimos.
    *   Bind mounts podem exigir privilégios elevados no contêiner para ler/escrever no host.
    *   Imagem de exemplo usa `USER root` (Linux) ou `ContainerAdministrator` (Windows) para ter direitos administrativos.
*   Opção `readonly`: Montar diretório do host como somente leitura dentro do contêiner.
    *   Útil para injetar arquivos de configuração do host sem permitir que o contêiner os modifique.
*   **Exercício Try-it-now:** Usar bind mount `readonly` para injetar configuração.
    *   App To-Do procura por arquivo de config extra em `/app/config` (ou `C:\app\config`).
    *   1. Navegar para `ch06/exercises/todo-list`.
    *   2. Definir variáveis `source` (pasta `config` local) e `target` (`/app/config` ou `C:\app\config`).
    *   3. Rodar contêiner com mount `readonly`: `docker run --name todo-configured ... --mount type=bind,source=$source,target=$target,readonly diamol/ch06-todo-list`.
    *   4. Fazer requisição `curl`.
    *   5. Verificar logs do contêiner (`docker logs todo-configured`).
*   Arquivo de config no host (`config/appsettings.json` ou similar) ativa logging detalhado (Debug).

🧠 Explicação detalhada e didática:

*   A página destaca que a natureza bidirecional dos bind mounts traz considerações de **segurança**. Se o contêiner puder escrever em qualquer lugar do host mapeado, um contêiner comprometido poderia causar danos ao sistema host. Por isso, é comum que imagens que precisam de bind mounts rodem com usuários com mais privilégios (como `root` ou `Administrator`), o que por si só já é um risco de segurança.
*   Uma forma de mitigar o risco é usar a opção `readonly` no bind mount. Isso permite que o contêiner *leia* arquivos do host (ótimo para fornecer arquivos de configuração), mas impede que ele os *modifique*.
*   O exercício demonstra o uso de um bind mount `readonly` para injetar um arquivo de configuração que altera o nível de logging da aplicação To-Do. A aplicação é projetada para procurar por configurações em um local específico (`/app/config`). Montando uma pasta local do host nesse local *dentro* do contêiner (como `readonly`), a aplicação lerá o arquivo de configuração do host, ativando o logging detalhado.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # 1. Navegar para o diretório
    cd C:\DockerLabs\diamol\ch06\exercises\todo-list

    # 2. Definir variáveis (ajuste target se necessário)
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config"
    $TargetPath = 'C:\app\config' # Para imagem Windows
    # $TargetPath = '/app/config' # Para imagem Linux

    # 3. Rodar container com mount readonly (porta 8013)
    docker container run --name todo-configured -d -p 8013:80 `
      --mount type=bind,source=$SourcePath,target=$TargetPath,readonly `
      diamol/ch06-todo-list

    # 4. Fazer requisição
    Start-Sleep -Seconds 5
    Invoke-WebRequest http://localhost:8013

    # 5. Verificar logs (procurar por mensagens de nível 'Debug' ou mais detalhadas)
    docker container logs todo-configured
    ```
*   Abra a pasta `config` local e veja o arquivo `appsettings.json` (ou similar) que define o nível de logging como "Debug".

---

📄 Página 114

🔍 Resumo dos principais pontos:

*   Saída esperada do exercício anterior (Figura 6.11): Logs detalhados (nível Debug) no output do `docker logs`.
*   Pode-se usar bind mount com qualquer fonte acessível pelo host (ex: drive de rede montado como `/mnt/nfs` no Linux ou `X:` no Windows).
*   Útil para storage confiável ou distribuído, mas há limitações.
*   **6.4 Limitations of filesystem mounts** (Limitações das montagens de sistema de arquivos)
*   Cenários e limitações importantes:
    1.  **Ocultação (Shadowing):** O que acontece se montar algo em um diretório (`target`) que *já existe* e tem arquivos na imagem base?
        *   Intuição: talvez mesclasse os conteúdos? **Não.**
        *   Comportamento: O diretório montado (`source`) **substitui/oculta** completamente o diretório original da imagem no `target`. Arquivos da imagem naquele `target` ficam inacessíveis.

🧠 Explicação detalhada e didática:

*   A página confirma o sucesso do exercício anterior: o bind mount `readonly` injetou a configuração do host, e os logs do contêiner refletem isso.
*   Menciona a flexibilidade de usar bind mounts com armazenamento em rede mapeado no host.
*   Inicia a seção 6.4, que aborda **limitações e comportamentos específicos** de montagens que podem causar surpresas.
*   **Primeira limitação: Ocultação (Shadowing) de Diretórios.** Se a imagem Docker já contém um diretório com arquivos (ex: `/app/config/default.conf`) e você faz um bind mount de um diretório do host para `/app/config`, o conteúdo original de `/app/config` da imagem *desaparece* dentro do contêiner. O contêiner só verá o conteúdo vindo do host. Isso é diferente de uma mesclagem; é uma substituição completa daquele ponto de montagem. Isso pode ser problemático se você esperava que o contêiner usasse uma combinação de arquivos da imagem e do host naquele diretório.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício demonstrará o efeito de ocultação.
*   Pense no exemplo anterior: montamos a pasta `config` do host em `/app/config`. Se a imagem `diamol/ch06-todo-list` já tivesse algum arquivo dentro de `/app/config`, ele teria ficado inacessível para o contêiner `todo-configured`.

💡 Analogias ou observações úteis (se houver):

*   Ocultação com Bind Mount: É como colocar um tapete (diretório do host) sobre uma área do piso (diretório da imagem). Você só vê o tapete; o que estava no piso embaixo dele fica escondido.

---

📄 Página 115

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Demonstrar ocultação de diretório com bind mount.
    *   Imagem `diamol/ch06-bind-mount` lista conteúdo de `/init` (Linux) ou `C:\init` (Windows).
    *   1. Rodar contêiner sem mount: mostra arquivos da imagem (`abc.txt`, `def.txt`).
    *   2. Definir variáveis `source` (pasta `new` local) e `target` (`/init` ou `C:\init`).
    *   3. Rodar contêiner *com* bind mount da pasta `new`: mostra arquivos do host (`123.txt`, `456.txt`), *não* os da imagem.
*   Figura 6.12: Mostra a saída, confirmando a ocultação.
*   **Segunda limitação: Montagem de Arquivo Único.** O que acontece se montar um *arquivo* do host em um diretório (`target`) que existe na imagem?
    *   Comportamento Linux: Conteúdo é mesclado. Vê-se arquivos originais da imagem + o arquivo montado do host.
    *   Comportamento Windows Containers: **Não suportado.** Feature não implementada.

🧠 Explicação detalhada e didática:

*   O exercício demonstra o efeito de ocultação (shadowing). A imagem `diamol/ch06-bind-mount` foi construída para ter arquivos (`abc.txt`, `def.txt`) no diretório `/init`.
    *   Quando rodada sem mount, ela lista esses arquivos.
    *   Quando rodada com um bind mount que mapeia a pasta `new` do host (contendo `123.txt`, `456.txt`) para `/init`, o contêiner lista *apenas* `123.txt` e `456.txt`. Os arquivos `abc.txt` e `def.txt` originais da imagem foram ocultados.
*   A segunda limitação discutida é a montagem de **arquivos individuais** (em vez de diretórios) do host para dentro de um diretório no contêiner.
    *   No **Linux**, isso funciona como uma "mesclagem": o contêiner vê os arquivos que já estavam no diretório da imagem *mais* o arquivo que foi montado do host.
    *   No **Windows Containers**, tentar montar um arquivo individual do host para dentro de um diretório do contêiner **resulta em erro**. É uma limitação conhecida da plataforma.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais (Demonstrar Ocultação):**
    ```powershell
    # Navegar para o diretório
    cd C:\DockerLabs\diamol\ch06\exercises\bind-mount

    # 1. Rodar sem mount (ver abc.txt, def.txt)
    docker container run diamol/ch06-bind-mount

    # 2. Definir variáveis (ajuste target se imagem for Linux)
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "new"
    $TargetPath = 'C:\init' # Para imagem Windows
    # $TargetPath = '/init' # Para imagem Linux (Verifique a base da imagem diamol/ch06-bind-mount)

    # 3. Rodar com bind mount (ver 123.txt, 456.txt)
    docker container run --mount type=bind,source=$SourcePath,target=$TargetPath `
      diamol/ch06-bind-mount
    ```
*   **Sobre Montagem de Arquivo Único:** O próximo exercício na p.116 mostrará a diferença de comportamento. No Windows, você esperaria um erro.

---

📄 Página 116

🔍 Resumo dos principais pontos:

*   **Continuação: Limitações de Filesystem Mounts**
*   Filesystem do contêiner é área com diferenças entre Linux e Windows containers.
    *   Uso de paths estilo Linux (`/data`) funciona em Dockerfiles para ambos (vira `C:\data` no Windows).
    *   Mas *não* funciona para source/target em volume mounts/bind mounts (precisa usar paths nativos do OS - `/data` Linux, `C:\data` Windows).
    *   Limitação de montagem de arquivo único no Windows.
*   **Exercício Try-it-now:** Demonstrar diferença de montagem de arquivo único.
    *   Comandos separados para Linux e Windows, montando `new/123.txt` em `/init/123.txt` ou `C:\init\123.txt`.
*   Resultado esperado (Figura 6.13): Linux funciona (mostra `abc.txt`, `def.txt`, `123.txt`), Windows dá erro.

🧠 Explicação detalhada e didática:

*   A página reforça que, embora o Docker tente abstrair diferenças, o **sistema de arquivos** é uma área onde o comportamento entre contêineres Linux e Windows pode divergir.
    *   Especificamente, ao definir `source` e `target` para volumes ou bind mounts no comando `docker run` (ou Docker Compose), você *deve* usar a sintaxe de caminho correta para o sistema operacional *do contêiner* (e do host, para o `source` do bind mount). Usar `/` universalmente não funciona aqui como funciona em algumas instruções do Dockerfile.
    *   Reafirma a limitação de montar arquivos individuais no Windows.
*   O exercício visa comprovar experimentalmente a diferença na montagem de arquivos únicos:
    *   O comando para Linux monta `new/123.txt` em `/init/123.txt`. Como `/init` já continha `abc.txt` e `def.txt` na imagem, e montagem de arquivo único mescla no Linux, a saída esperada é a listagem dos três arquivos.
    *   O comando para Windows tenta montar `new/123.txt` em `C:\init\123.txt`. Como isso não é suportado, a expectativa é que o comando `docker run` falhe com um erro.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais (Executar no Windows):**
    ```powershell
    # (Estando em C:\DockerLabs\diamol\ch06\exercises\bind-mount)

    # Tentar montar arquivo único (assumindo imagem base Windows)
    $SourceFile = Join-Path -Path (Get-Location).Path -ChildPath "new\123.txt"
    $TargetFile = 'C:\init\123.txt'
    docker container run --mount type=bind,source=$SourceFile,target=$TargetFile `
      diamol/ch06-bind-mount
    ```
    *Você deve receber um erro do Docker, confirmando a limitação.*

*   **Se você estiver usando Docker Desktop no modo Linux:**
    ```powershell
    # Tentar montar arquivo único (imagem base Linux)
    $SourceFile = "$(pwd)/new/123.txt" # Docker Desktop converte o path
    $TargetFile = '/init/123.txt'
    docker container run --mount type=bind,source=$SourceFile,target=$TargetFile `
      diamol/ch06-bind-mount
    ```
    *Neste caso (modo Linux), deve funcionar e listar `abc.txt`, `def.txt`, `123.txt`.*

---

📄 Página 117

🔍 Resumo dos principais pontos:

*   Figura 6.13: Mostra a saída do exercício anterior (Linux funciona, Windows falha).
*   **Terceira limitação: Bind Mounts com Filesystems Distribuídos.**
    *   Cenário menos comum, difícil de reproduzir facilmente.
    *   Pergunta: O que acontece ao montar um filesystem distribuído (compartilhamento SMB, Azure Files, AWS S3) como bind mount? App no contêiner funcionará?
    *   Filesystems distribuídos podem *não suportar todas as operações* de um filesystem local padrão (ex: criar links simbólicos, bloqueios específicos).
    *   Aplicativos podem falhar se usarem operações não suportadas pelo storage distribuído subjacente.
*   Diagrama (Figura 6.14): Exemplo concreto - rodar banco de dados Postgres em contêiner na nuvem, usando Azure Files para os dados via bind mount.
    *   Postgres tenta criar um "file link" (operação de filesystem).
    *   Azure Files não suporta essa operação específica.
    *   Resultado: App (Postgres) trava/falha.

🧠 Explicação detalhada e didática:

*   A página discute uma terceira limitação, mais sutil, dos bind mounts: quando a origem (`source`) no host não é um disco local comum, mas sim um **sistema de arquivos distribuído** (como um compartilhamento de rede Windows/SMB, um mount NFS de um servidor Linux, ou um serviço de arquivos na nuvem como Azure Files ou AWS EFS).
*   Embora você possa montar esses armazenamentos distribuídos no seu host e depois fazer um bind mount deles para dentro de um contêiner, existe um risco: esses sistemas de arquivos distribuídos podem **não implementar todas as funcionalidades** que um sistema de arquivos local padrão oferece.
*   Aplicações complexas, especialmente bancos de dados como o Postgres (exemplo da Figura 6.14), podem depender de operações de filesystem específicas (como links simbólicos, tipos específicos de bloqueio de arquivo, etc.). Se o storage distribuído usado no bind mount não suportar exatamente a operação que a aplicação tenta realizar, a aplicação dentro do contêiner **falhará**, muitas vezes com erros obscuros.
*   A lição é: tenha cuidado ao usar bind mounts com storage distribuído para aplicações stateful complexas. Teste exaustivamente e esteja ciente das limitações do sistema de armazenamento subjacente.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você pode mapear um compartilhamento de rede no seu Windows (ex: `\\server\share` para `Z:`) e depois tentar usar `Z:\data` como `source` em um bind mount:
    ```powershell
    # Exemplo conceitual (requer compartilhamento SMB acessível)
    # New-PSDrive -Name "Z" -PSProvider "FileSystem" -Root "\\server\share"
    # $SourcePath = "Z:\MinhaAppDB"
    # $TargetPath = "C:\data"
    # mkdir $SourcePath -ErrorAction SilentlyContinue
    # docker run --mount type=bind,source=$SourcePath,target=$TargetPath ... <imagem_bd>
    ```
    O sucesso ou falha dependerá da aplicação (`<imagem_bd>`) e das funcionalidades suportadas pelo compartilhamento SMB (`\\server\share`).

---

📄 Página 118

🔍 Resumo dos principais pontos:

*   Cenário de falha com storage distribuído é exceção, mas importante saber. Storage de origem pode não ter todas as features esperadas pelo app. Não dá para planejar, só testando.
*   Storage distribuído também terá características de **performance** diferentes (mais lento que disco local devido à rede). Apps com I/O intenso podem ficar lentos.
*   **6.5 Understanding how the container filesystem is built** (Entendendo como o sistema de arquivos do contêiner é construído)
*   Recapitulação: storage em contêineres é diferente de máquinas físicas/VMs.
*   Consolidação dos conceitos do capítulo.
*   Contêiner tem disco único *virtual* (union filesystem).
*   Docker monta esse disco a partir de várias fontes.
*   Implementação do union filesystem varia (Docker escolhe a melhor para o SO).
*   App no contêiner vê disco único, mas partes podem vir de locais diferentes.
*   Diagrama (Figura 6.15): Mostra o union filesystem combinando múltiplas fontes (camadas da imagem, camada gravável, volumes, bind mounts).
*   **Diretrizes/Boas Práticas para Opções de Storage:**
    *   **Camada Gravável (Writable Layer):** Boa para dados de curta duração/transitórios (ex: cache). Única por contêiner, perdida com `docker rm`.
    *   **Bind Mounts Locais:** Compartilhar dados host <-> contêiner. Útil para desenvolvimento (montar código fonte local no contêiner para ver mudanças na hora).
    *   **Bind Mounts Distribuídos:** Compartilhar storage de rede com contêineres. Usar com cautela (performance, features). Bom para config read-only ou cache compartilhado.
    *   **Volume Mounts (Volumes Docker):** Compartilhar dados entre contêiner e storage gerenciado pelo Docker. **Ideal para dados persistentes** que precisam sobreviver a atualizações.

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre limitações, mencionando também o impacto na **performance** ao usar bind mounts com storage distribuído (rede é mais lenta que disco local).
*   A seção 6.5 serve como um **resumo e consolidação** de tudo que foi visto no Capítulo 6 sobre o filesystem do contêiner.
*   O conceito chave é o **Union Filesystem**: o Docker cria a ilusão de um disco único para o contêiner, "unindo" diferentes fontes de dados de forma transparente para a aplicação.
*   A Figura 6.15 é uma ótima representação visual disso, mostrando que o filesystem final pode ser uma combinação de:
    *   Camadas somente leitura da imagem base.
    *   A camada gravável única do contêiner.
    *   Um ou mais Volumes Docker montados.
    *   Um ou mais Bind Mounts (locais ou distribuídos) montados.
*   A página finaliza com um guia prático sobre **quando usar cada tipo de armazenamento**:
    *   **Camada Gravável:** Só para dados temporários que podem ser perdidos.
    *   **Bind Mount Local:** Principalmente para desenvolvimento (acesso ao código fonte) ou acesso a arquivos específicos do host.
    *   **Bind Mount Distribuído:** Para acesso a storage de rede, com cuidado quanto a performance e compatibilidade.
    *   **Volume Docker:** A solução **preferida** para dados que a aplicação no contêiner precisa que **persistam** (bancos de dados, uploads de usuários, etc.).

💻 Exemplos práticos (para meu ambiente Windows):

*   Revise os comandos usados no capítulo:
    *   Volumes: `docker volume create`, `docker run -v <volume>:<target>`
    *   Bind Mounts: `docker run --mount type=bind,source=<host_path>,target=<target>[,readonly]` ou `docker run -v <host_path>:<target>[:ro]`
*   Pense em um cenário seu:
    *   Precisa salvar dados de um banco de dados rodando em contêiner? Use **Volume Docker**.
    *   Quer editar código no VS Code e ver o resultado na hora em um servidor web rodando em contêiner? Use **Bind Mount Local** do seu diretório de código.
    *   Precisa ler um arquivo de configuração central que está em um compartilhamento de rede? Use **Bind Mount Distribuído** (read-only).

---

📄 Página 119

🔍 Resumo dos principais pontos:

*   **Diretrizes de Storage (continuação):**
    *   **Camadas da Imagem (Image Layers):** Apresentam o filesystem inicial. São empilhadas; camadas posteriores podem ocultar/sobrescrever arquivos de camadas anteriores. Read-only, compartilhadas.
*   **6.6 Lab** (Laboratório)
    *   Reúne os conceitos do capítulo.
    *   App: To-Do list novamente.
    *   Tarefa: Rodar o app (`diamol/ch06-lab`) que *já vem* com tarefas pré-criadas (na imagem). Modificar a forma como ele roda para:
        1.  Fazer a lista começar *vazia*.
        2.  Garantir que itens salvos *persistam* em um **Volume Docker**.
    *   Dica: Exercícios do capítulo devem ajudar.

🧠 Explicação detalhada e didática:

*   A página finaliza as diretrizes de storage, lembrando que as **camadas da imagem** formam a base inicial do filesystem do contêiner.
*   Introduz o **Lab do Capítulo 6**. O desafio envolve usar os conceitos de volumes/mounts para alterar o comportamento de armazenamento de uma imagem existente (`diamol/ch06-lab`).
*   A imagem base do lab já contém dados (tarefas pré-existentes). O objetivo é duplo:
    1.  **Impedir que os dados da imagem sejam usados:** Fazer a aplicação iniciar com uma lista vazia. Isso provavelmente envolverá montar um volume ou bind mount *vazio* no local onde a aplicação armazena seus dados, efetivamente *ocultando* os dados que vieram da imagem (lembra do "shadowing"?).
    2.  **Garantir persistência:** Certificar-se de que os *novos* itens adicionados sejam salvos em um **Volume Docker** nomeado, para que sobrevivam se o contêiner for recriado.
*   A chave é descobrir *onde* a aplicação `diamol/ch06-lab` armazena seus dados (provavelmente em `/data` ou similar, como nos exemplos anteriores) e montar um volume nomeado nesse local.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 6:**
    1.  **Investigar:** Rode `docker run diamol/ch06-lab` para ver as tarefas iniciais. Tente descobrir onde os dados são armazenados (pode ser `/data` ou talvez inspecionando o Dockerfile se disponível, ou por tentativa e erro). Assumiremos que é `/data` (ou `C:\data`).
    2.  **Criar Volume:** Crie um volume nomeado: `docker volume create lab6-data`.
    3.  **Rodar com Ocultação e Persistência:** Rode o contêiner montando o volume nomeado no local dos dados. Isso ocultará os dados da imagem e garantirá que os novos dados vão para o volume.
        ```powershell
        # Assumindo target /data para imagem Linux
        $target='/data'
        docker container run -d -p 8099:80 --name lab6-app -v lab6-data:$target diamol/ch06-lab
        ```
    4.  **Verificar:** Acesse `http://localhost:8099`. A lista deve estar *vazia*. Adicione alguns itens.
    5.  **Testar Persistência:** Remova o contêiner (`docker rm -f lab6-app`) e rode-o novamente com o *mesmo* comando do passo 3. Acesse `http://localhost:8099` novamente. Os itens que você adicionou devem estar lá.

---

📄 Página 120

🔍 Resumo dos principais pontos:

*   **Dicas do Lab 6 (continuação):**
    *   Lembrete: `docker rm -f $(docker ps -aq)` remove todos os contêineres.
    *   Rodar `diamol/ch06-lab` primeiro para ver as tarefas existentes.
    *   Precisará rodar novamente com opções de montagem (`-v` ou `--mount`).
    *   App usa arquivo de configuração; pode haver mais do que apenas settings de log nele (dica sutil sobre onde os dados podem ser configurados/localizados).
*   Link para solução no GitHub.

🧠 Explicação detalhada e didática:

*   A página oferece as dicas finais para o Lab 6. Relembra o comando de limpeza geral. Sugere rodar a imagem original primeiro para entender o estado inicial (com tarefas). Indica que a solução envolverá rodar o contêiner novamente com flags de montagem (`-v` ou `--mount`). A última dica é interessante: sugere que o *local* onde a aplicação salva os dados pode ser configurável através de um arquivo de configuração, o que poderia influenciar qual caminho (`target`) usar na montagem do volume.

💻 Exemplos práticos (para meu ambiente Windows):

*   A estratégia da página anterior (montar um volume nomeado em `/data` ou `C:\data`) é a mais provável, baseada nos exemplos do capítulo. A dica sobre o arquivo de configuração pode ser uma pista secundária ou uma forma alternativa de descobrir o caminho dos dados, caso `/data` não seja o correto para a imagem `diamol/ch06-lab`.

---

📄 Página 121

🔍 Resumo dos principais pontos:

*   **Início da Parte 2: Running distributed applications in containers**
*   Revisão: Poucas aplicações são um componente único; geralmente são distribuídas.
*   Foco da Parte 2: Usar Docker e **Docker Compose** para definir, rodar e gerenciar aplicações com múltiplos contêineres.
*   Tópicos:
    *   Definir apps multi-contêiner (Compose).
    *   Gerenciar apps multi-contêiner (Compose).
    *   Isolar workloads com redes Docker.
    *   Construir pipelines de Integração Contínua (CI) com Docker.
    *   Configurar múltiplos ambientes.
    *   Preparar para produção: health checks e observabilidade (métricas/logging).

🧠 Explicação detalhada e didática:

*   Esta página introduz a Parte 2 do livro, que move o foco de contêineres individuais para **aplicações distribuídas**, compostas por múltiplos contêineres que precisam trabalhar juntos (ex: um frontend web, uma API backend, um banco de dados).
*   A ferramenta central para gerenciar essas aplicações em um único host Docker será o **Docker Compose**.
*   Os capítulos desta parte ensinarão a usar o Compose para definir a estrutura da aplicação, configurar a comunicação entre contêineres (redes), gerenciar diferentes ambientes (dev, teste), automatizar builds e testes (CI), e adicionar robustez com verificações de saúde e monitoramento.

💻 Exemplos práticos (para meu ambiente Windows):

*   Antecipa o uso extensivo de arquivos `docker-compose.yml` e comandos `docker-compose ...` (como `up`, `down`, `build`, `ps`, `logs`, `exec`) no seu terminal PowerShell/VS Code.

---

📄 Página 122

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, separando a introdução da Parte 2 do início do Capítulo 7.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 123

🔍 Resumo dos principais pontos:

*   **Capítulo 7: Running multi-container apps with Docker Compose** (Rodando apps multi-contêiner com Docker Compose)
*   Revisão: A maioria dos apps não é um componente único. Docker é ideal para apps distribuídos (n-tier, microsserviços).
*   Docker Compose: ferramenta para definir e gerenciar apps multi-contêiner.
*   Foco do Cap. 7: Introdução ao Compose, revisitando apps anteriores para ver como Compose facilita o uso.
*   **7.1 The anatomy of a Docker Compose file** (A anatomia de um arquivo Docker Compose)
    *   Dockerfile empacota *um* componente. App distribuído pode ter vários Dockerfiles (web, API, BD).
    *   Como rodar? `docker run` para cada um é manual e propenso a erros (configuração, comunicação).
    *   Solução: Arquivo Docker Compose descreve a *estrutura* da aplicação.
    *   Arquivo Compose define o *estado desejado* do app.
    *   Coloca opções do `docker run` no arquivo Compose.
    *   Ferramenta `docker-compose` lê o arquivo e cria/gerencia os recursos Docker (contêineres, redes, volumes).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 7, focado no **Docker Compose**. Ele é apresentado como a solução para gerenciar aplicações que consistem em múltiplos contêineres interdependentes.
*   A seção 7.1 explica a necessidade do Compose: enquanto um `Dockerfile` define como construir a imagem de *um* serviço, o arquivo `docker-compose.yml` define como *múltiplos* serviços (cada um com sua imagem) devem ser executados juntos para formar a aplicação completa.
*   Em vez de executar vários comandos `docker run` complexos e garantir manualmente que os contêineres se conectem corretamente, você descreve toda a aplicação (serviços, redes, volumes, portas, variáveis de ambiente, etc.) em um único arquivo `docker-compose.yml`.
*   A ferramenta de linha de comando `docker-compose` então lê esse arquivo e interage com o Docker Engine para criar ou atualizar os recursos necessários para atingir o "estado desejado" descrito no arquivo.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará o VS Code para criar e editar arquivos `docker-compose.yml`.
*   Você usará comandos como `docker-compose up` (para iniciar a aplicação definida no arquivo) e `docker-compose down` (para parar e remover os recursos) no terminal PowerShell/VS Code.

💡 Analogias ou observações úteis (se houver):

*   Dockerfile: Receita para fazer um tipo de bolo (ex: bolo de chocolate).
*   docker-compose.yml: O plano completo de uma festa de aniversário, especificando quantos bolos de chocolate, quantos de baunilha (outros serviços), quais bebidas (volumes), onde colocar cada coisa (redes, portas).
*   `docker-compose up`: O comando para o organizador da festa (ferramenta Compose) ler o plano e montar tudo.

---

📄 Página 124

🔍 Resumo dos principais pontos:

*   **Listagem 7.1:** Exemplo de arquivo `docker-compose.yml` simples para o app To-Do.
    ```yaml
    version: '3.7' # Versão da especificação Compose
    services: # Define os componentes/contêineres
      todo-web: # Nome do serviço (será o hostname na rede)
        image: diamol/ch06-todo-list # Imagem a usar
        ports: # Mapeamento de portas host:container
          - "8020:80"
        networks: # Redes a que o serviço se conecta
          - app-net
    networks: # Define as redes usadas no arquivo
      app-net: # Nome lógico da rede dentro do Compose
        external: # Indica que a rede já existe fora do Compose
          name: nat # Nome real da rede externa a ser usada
    ```
*   **Formato YAML:** Legível por humanos, traduz fácil para JSON (usado por APIs). **Espaços/Indentação são importantes!**
*   **Estrutura do arquivo:**
    *   `version`: Especifica a versão do formato Compose (influencia features disponíveis).
    *   `services`: Lista os componentes (serviços) da aplicação. Compose pensa em "serviços" (podem ter múltiplas réplicas/contêineres) em vez de contêineres individuais.
    *   `networks`: Lista as redes Docker usadas pelos serviços.
*   **Análise do Exemplo:**
    *   Define 1 serviço (`todo-web`) e 1 rede (`app-net`).
    *   Serviço `todo-web`: usa imagem `diamol/ch06-todo-list`, publica porta `8020` do host para `80` do contêiner, conecta à rede `app-net`.
    *   Rede `app-net`: é definida como `external` com nome real `nat`. Isso significa que o Compose *espera* que uma rede chamada `nat` já exista no Docker e a usará; ele *não* tentará criar a rede `app-net`.
*   Resultado final é similar a rodar `docker run -p 8020:80 --name todo-web --network nat diamol/ch06-todo-list`.

🧠 Explicação detalhada e didática:

*   A página apresenta o primeiro exemplo concreto de um arquivo `docker-compose.yml` e explica sua estrutura e sintaxe.
*   **YAML:** É um formato de dados legível que usa indentação (espaços, não tabs!) para definir a estrutura hierárquica. É crucial prestar atenção à indentação correta.
*   **Seções Principais:**
    *   `version`: Informa ao Docker qual versão da especificação do Compose está sendo usada. Diferentes versões suportam diferentes funcionalidades. `'3.7'` é uma versão comum e moderna.
    *   `services`: O coração do arquivo. Define cada componente da sua aplicação como um "serviço". Cada serviço terá um nome (ex: `todo-web`) e suas configurações (imagem, portas, redes, volumes, variáveis de ambiente, etc.), que são muito parecidas com as opções do `docker run`. O nome do serviço também funciona como nome DNS dentro da rede Compose.
    *   `networks`, `volumes`, `configs`, `secrets`: Seções opcionais para definir ou referenciar outros recursos Docker que os serviços utilizarão.
*   **Exemplo `todo-web`:** Define um único serviço que mapeia diretamente para um contêiner. A configuração `networks` especifica que ele deve se conectar à rede `app-net`.
*   **Exemplo `app-net`:** A seção `networks` define `app-net`. A opção `external: name: nat` é importante: ela diz ao Compose para *não criar* uma rede chamada `app-net`, mas sim para usar uma rede *já existente* no Docker chamada `nat`. Isso foi feito para garantir compatibilidade com o exemplo anterior (onde criamos a rede `nat` manualmente) e também funciona bem no Windows onde `nat` é frequentemente a rede padrão.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar o arquivo:** No diretório `C:\DockerLabs\diamol\ch07\exercises\todo-list`, crie um arquivo chamado `docker-compose.yml` usando o VS Code.
2.  **Copiar Conteúdo:** Cole o conteúdo da Listagem 7.1 no arquivo. Preste muita atenção à indentação (geralmente 2 espaços).
3.  **Pré-requisito:** Certifique-se de que a rede `nat` existe (comando `docker network create nat` da p. 77, ignore erro se já existir).

---

📄 Página 125

🔍 Resumo dos principais pontos:

*   Propriedades do serviço (`image`, `ports`, `networks`) mapeiam para opções do `docker run`.
*   Nome do serviço (`todo-web`) vira nome do contêiner e nome DNS na rede Docker.
*   Definição de rede `external` significa que Compose espera que a rede `nat` já exista.
*   Gerenciamento com `docker-compose`: ferramenta separada da CLI `docker`.
*   Comando `docker-compose up`: Inicia a aplicação.
    *   Inspeciona arquivo Compose.
    *   Cria o que for necessário para atingir o estado desejado.
*   **Exercício Try-it-now:** Criar rede `nat` (se necessário) e rodar o app com Compose.
    *   Comandos:
        *   `docker network create nat` (ignorar erro se existir).
        *   `cd ./ch07/exercises/todo-list`.
        *   `docker-compose up`.
*   `docker-compose` procura por `docker-compose.yml` no diretório atual por padrão.
*   Como nenhum contêiner correspondente existe, Compose iniciará um.
*   `docker-compose up` (sem `-d`) roda em primeiro plano e agrega logs de todos os contêineres do app, prefixados pelo nome do serviço. Útil para dev/teste.

🧠 Explicação detalhada e didática:

*   A página continua explicando o arquivo Compose e introduz o comando principal para usá-lo: `docker-compose up`.
*   Este comando é o equivalente Compose ao `docker run`, mas aplicado a *toda* a aplicação definida no arquivo `docker-compose.yml`.
*   Ele lê o arquivo, compara o estado desejado com o que já existe rodando no Docker (para aquele projeto), e faz as alterações necessárias (criar contêineres, redes, volumes, etc.).
*   O exercício instrui a executar `docker-compose up` no diretório que contém o `docker-compose.yml` da Listagem 7.1.
*   **Importante:** Executar `docker-compose up` sem a flag `-d` (detach) faz com que a aplicação rode em **primeiro plano (foreground)**. Isso significa que:
    1.  Seu terminal ficará "preso" exibindo os logs da aplicação.
    2.  Os logs de *todos* os contêineres definidos no arquivo serão exibidos juntos (intercalados), com um prefixo indicando de qual serviço/contêiner veio cada linha. Isso é extremamente útil durante o desenvolvimento para ver a interação entre os serviços.
    3.  Para parar a aplicação e liberar o terminal, você precisará pressionar `Ctrl+C`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar/Criar Rede:**
    ```powershell
    docker network create nat # Ignore erro se já existir
    ```
2.  **Navegar para o Diretório:**
    ```powershell
    cd C:\DockerLabs\diamol\ch07\exercises\todo-list
    ```
3.  **Iniciar a Aplicação:**
    ```powershell
    docker-compose up
    ```
4.  **Observar Saída:** Você verá o Docker (possivelmente) baixar a imagem, depois criar a rede (ou usar a existente) e criar/iniciar o contêiner `todo-list_todo-web_1`. Em seguida, começará a ver os logs da aplicação To-Do sendo exibidos no terminal. O terminal ficará ativo mostrando esses logs.

---

📄 Página 126

🔍 Resumo dos principais pontos:

*   Figura 7.2: Mostra a saída do `docker-compose up`, incluindo criação de rede e contêiner, e logs da aplicação.
*   Após `docker-compose up` rodar (em foreground), pode-se acessar o app no navegador em `http://localhost:8020`.
*   Funciona igual ao Cap. 6, mas Compose oferece forma mais robusta de iniciar/definir o app.
*   Arquivo Compose serve como:
    *   Definição executável da aplicação.
    *   Documentação da configuração (imagem, porta, etc., não precisa de README separado para isso).
*   Compose pode registrar outros recursos (volumes, secrets).
*   Exemplo atual é simples (1 serviço), mas Compose brilha com apps multi-contêiner.
*   **7.2 Running a multi-container application with Compose** (Rodando uma aplicação multi-contêiner com Compose)
    *   Relembra app "image gallery" do Cap. 4 (Go web frontend, Java API, Node.js log API).
    *   Antes, rodamos cada contêiner separadamente, cuidando da rede/nomes. Compose simplifica isso.

🧠 Explicação detalhada e didática:

*   A página confirma que, enquanto `docker-compose up` está rodando em primeiro plano, a aplicação To-Do está acessível em `http://localhost:8020`.
*   Destaca os benefícios de usar o Docker Compose mesmo para um app de um só contêiner:
    *   **Robustez:** Define a configuração de forma declarativa e reprodutível.
    *   **Documentação:** O próprio arquivo `docker-compose.yml` documenta como a aplicação deve ser configurada e executada.
*   A seção 7.2 introduz o cenário onde o Compose realmente se destaca: gerenciar aplicações compostas por **múltiplos serviços** interdependentes, usando como exemplo a aplicação "Image Gallery" que foi construída no Capítulo 4. O Compose eliminará a necessidade de iniciar manualmente cada um dos três contêineres (Go, Java, Node.js) e configurar suas conexões.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Testar o App:** Com o `docker-compose up` ainda rodando no terminal, abra o navegador e acesse `http://localhost:8020`. Adicione alguns itens à lista.
2.  **Parar o App:** Volte ao terminal onde `docker-compose up` está rodando e pressione `Ctrl+C`. Isso interromperá e removerá o contêiner (comportamento padrão de `up` sem `-d` seguido de `Ctrl+C`).
3.  **Preparar para Próximo Exemplo:** Você precisará do `docker-compose.yml` para o app Image Gallery (provavelmente em `ch07\exercises\image-of-the-day`).

---

📄 Página 127

🔍 Resumo dos principais pontos:

*   **Listagem 7.2:** Seção `services` de um Compose file para o app Image Gallery (sem a config de rede, que é igual ao exemplo anterior - usa `nat`).
    ```yaml
    services:
      accesslog: # Serviço Node.js (log API)
        image: diamol/ch04-access-log
      iotd: # Serviço Java (NASA API)
        image: diamol/ch04-image-of-the-day
        ports:
          - "80" # Publica porta 80 do container em porta ALEATÓRIA do host
      image-gallery: # Serviço Go (Web UI)
        image: diamol/ch04-image-gallery
        ports:
          - "8010:80" # Publica porta 80 do container na 8010 do host
        depends_on: # Define dependências de inicialização
          - accesslog
          - iotd
    ```
*   **Análise da Configuração dos Serviços:**
    *   `accesslog`: Mais simples, só define a imagem. Não expõe portas ao host (serviço interno).
    *   `iotd`: Define imagem. `ports: - "80"` (sem porta do host) significa: publique a porta 80 do contêiner em uma porta *aleatória* disponível no host. Útil quando não se importa com a porta externa ou quer evitar conflitos.
    *   `image-gallery`: Define imagem, publica porta 80 interna na porta *fixa* 8010 do host. Contém seção `depends_on`.
*   **`depends_on`:** Indica que o serviço `image-gallery` depende dos serviços `accesslog` e `iotd`. O Compose tentará iniciar `accesslog` e `iotd` *antes* de iniciar `image-gallery`. (Nota: Isso só controla a *ordem de início* dos contêineres, *não* garante que os serviços dependentes estejam *prontos* para receber conexões - Cap. 8 abordará isso).

🧠 Explicação detalhada e didática:

*   A Listagem 7.2 mostra a definição dos três serviços para a aplicação Image Gallery dentro de um arquivo `docker-compose.yml`.
*   Cada serviço (`accesslog`, `iotd`, `image-gallery`) especifica a imagem Docker a ser usada.
*   **Configuração de Portas:**
    *   `accesslog`: Não tem `ports`, indicando que é um serviço puramente interno, não acessível diretamente do host.
    *   `iotd`: Usa `ports: - "80"`. Esta sintaxe (sem especificar a porta do host) instrui o Docker a mapear a porta 80 do contêiner para uma porta *aleatória* livre no host. Isso é útil para serviços internos que talvez precisem ser acessados ocasionalmente para depuração, sem se preocupar em escolher uma porta fixa que possa conflitar.
    *   `image-gallery`: Usa `ports: - "8010:80"`. Mapeia a porta 80 do contêiner para a porta *fixa* 8010 no host. É o ponto de entrada principal para o usuário acessar a aplicação web.
*   **`depends_on`:** Esta seção no serviço `image-gallery` informa ao Compose que ele não deve iniciar o contêiner `image-gallery` até que os contêineres para `accesslog` e `iotd` tenham sido iniciados. É um controle básico de ordem de inicialização.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar/Editar Arquivo:** No diretório `C:\DockerLabs\diamol\ch07\exercises\image-of-the-day`, crie/edite `docker-compose.yml`.
2.  **Adicionar Conteúdo:** Combine a seção `services` da Listagem 7.2 com as seções `version` e `networks` (usando a rede externa `nat`) da Listagem 7.1. O arquivo completo deve parecer com:
    ```yaml
    version: '3.7'
    services:
      accesslog:
        image: diamol/ch04-access-log
        networks:
          - app-net
      iotd:
        image: diamol/ch04-image-of-the-day
        ports:
          - "80"
        networks:
          - app-net
      image-gallery:
        image: diamol/ch04-image-gallery
        ports:
          - "8010:80"
        depends_on:
          - accesslog
          - iotd
        networks:
          - app-net
    networks:
      app-net:
        external:
          name: nat
    ```

---

📄 Página 128

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 7.3): Mostra a arquitetura da aplicação com 3 serviços conectados à mesma rede. Gerado por ferramenta `docker-compose-viz`.
*   Rodar app com `docker-compose`, mas agora em modo **detached** (`-d`).
    *   Logs ainda são coletados, mas contêineres rodam em background.
    *   Terminal fica livre para usar outros comandos Compose.
*   **Exercício Try-it-now:** Navegar para a pasta `image-of-the-day` e rodar o app em modo detached.
    *   Comandos: `cd ./ch07/exercises/image-of-the-day`, `docker-compose up --detach`. (A flag `-d` é abreviação de `--detach`).

🧠 Explicação detalhada e didática:

*   A Figura 7.3 visualiza a aplicação que será iniciada: três serviços (`accesslog`, `iotd`, `image-gallery`) todos conectados à rede `nat` (representada como `app-net` no Compose). O diagrama foi feito com uma ferramenta que lê o arquivo Compose, mostrando como ele pode servir de base para documentação automática.
*   Introduz a execução do `docker-compose up` com a flag `-d` (ou `--detach`). Diferente da execução anterior (foreground), o modo detached inicia os contêineres em **segundo plano** e retorna o controle do terminal imediatamente. Os logs não são exibidos automaticamente, mas o Docker continua a coletá-los (podemos vê-los depois com `docker-compose logs`). Este é o modo mais comum para rodar aplicações que não exigem interação direta constante.
*   O exercício instrui a iniciar a aplicação Image Gallery neste modo.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar (se necessário):**
    ```powershell
    cd C:\DockerLabs\diamol\ch07\exercises\image-of-the-day
    ```
2.  **Iniciar em Background:**
    ```powershell
    docker-compose up -d
    # ou docker-compose up --detach
    ```
3.  **Observar Saída:** O comando deve retornar rapidamente após criar/iniciar os contêineres (Figura 7.4 na próxima página), e o prompt do PowerShell estará disponível novamente. Você verá mensagens indicando a criação dos contêineres (ex: `Creating image-of-the-day_accesslog_1 ... done`).
4.  **Verificar:**
    ```powershell
    docker-compose ps
    # ou docker ps
    ```
    Isso mostrará os três contêineres (`accesslog`, `iotd`, `image-gallery`) em execução (estado `Up`).

---

📄 Página 129

🔍 Resumo dos principais pontos:

*   Figura 7.4: Mostra a saída do `docker-compose up -d`, indicando a ordem de criação respeitando `depends_on`.
*   App agora rodando; acessar web UI em `http://localhost:8010`. Funciona como no Cap. 4.
*   Benefício Compose: Definição clara da configuração e comunicação.
*   Gerenciamento da aplicação como um todo com Compose.
*   **Escalando Serviços:** Pode-se escalar serviços stateless (como a API `iotd`) para rodar múltiplos contêineres. Docker distribui requisições entre eles.
*   **Exercício Try-it-now:** Escalar o serviço `iotd` para 3 instâncias e verificar logs.
    *   Comando para escalar: `docker-compose up -d --scale iotd=3`.
    *   Acessar/refrescar `http://localhost:8010` algumas vezes para gerar tráfego para a API.
    *   Comando para ver o último log de cada contêiner `iotd`: `docker-compose logs --tail=1 iotd`.

🧠 Explicação detalhada e didática:

*   A página confirma que a aplicação está rodando e acessível.
*   Introduz o conceito de **escalar serviços** usando Docker Compose. Se um serviço é stateless (não guarda estado entre requisições), como a API `iotd`, podemos facilmente rodar múltiplas instâncias (réplicas/contêineres) dele para lidar com mais carga. O Docker (através da rede Compose) fará um balanceamento de carga básico (round-robin via DNS) entre as instâncias.
*   O comando `docker-compose up -d --scale <nome_servico>=<numero_instancias>` é usado para ajustar o número de contêineres para um serviço específico. Se o número for maior que o atual, o Compose cria novos contêineres. Se for menor, ele remove os excedentes.
*   O exercício escala o serviço `iotd` para 3 contêineres. Depois de gerar tráfego, o comando `docker-compose logs --tail=1 iotd` é usado para ver a última linha de log de *cada um* dos 3 contêineres `iotd`, permitindo verificar quais deles receberam as requisições da UI `image-gallery`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Escalar o Serviço `iotd`:** (No mesmo diretório do `docker-compose.yml`)
    ```powershell
    docker-compose up -d --scale iotd=3
    ```
    Observe a saída indicando a criação de `image-of-the-day_iotd_2` e `image-of-the-day_iotd_3`.
2.  **Verificar Contêineres:**
    ```powershell
    docker-compose ps
    ```
    Você deve ver 5 contêineres agora (1 accesslog, 3 iotd, 1 image-gallery).
3.  **Gerar Tráfego:** Abra `http://localhost:8010` no navegador e pressione F5 algumas vezes.
4.  **Verificar Logs:**
    ```powershell
    docker-compose logs --tail=1 iotd
    ```
    Você verá 3 linhas de log, uma de cada contêiner `iotd`, mostrando quais processaram as últimas requisições (a distribuição pode não ser perfeitamente igual em poucos requests).

---

📄 Página 130

🔍 Resumo dos principais pontos:

*   Figura 7.5: Mostra a saída do scaling e dos logs distribuídos.
*   Compose agora gerencia 5 contêineres.
*   Pode-se usar Compose para parar (`stop`) e iniciar (`start`) toda a aplicação.
*   Contêineres criados pelo Compose são contêineres Docker normais, gerenciáveis também via CLI `docker`.
*   Compose e CLI `docker` usam a mesma API Docker.
*   **Exercício Try-it-now:** Parar e iniciar o app com Compose, depois listar contêineres com CLI `docker`.
    *   Comandos:
        *   `docker-compose stop`
        *   `docker-compose start`
        *   `docker container ls` (ou `docker ps`)
*   Análise da saída (Figura 7.6):
    *   `stop` lista contêineres individuais sendo parados.
    *   `start` lista serviços sendo iniciados (respeitando `depends_on`).
    *   `docker container ls` mostra que os contêineres *existentes* foram reiniciados (CREATED time é antigo, STATUS Up é recente), não recriados.

🧠 Explicação detalhada e didática:

*   A página introduz os comandos `docker-compose stop` e `docker-compose start` para gerenciar o ciclo de vida da aplicação definida no arquivo Compose como um todo.
    *   `docker-compose stop`: Para todos os contêineres associados ao projeto, mas *não* os remove. Eles ficam no estado `Exited`.
    *   `docker-compose start`: Reinicia os contêineres que foram previamente parados com `stop`.
*   Um ponto importante é que os contêineres gerenciados pelo Compose são contêineres Docker comuns. Você ainda pode usar os comandos `docker ps`, `docker logs <nome_container>`, `docker exec`, etc., diretamente neles. O Compose é apenas uma ferramenta de orquestração que facilita o gerenciamento do *conjunto*.
*   O exercício demonstra o ciclo `stop`/`start` e confirma com `docker ps` que os mesmos contêineres foram reutilizados (não foram destruídos e recriados, preservando seus estados internos, se houver - embora estes serviços sejam stateless).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Parar a Aplicação:**
    ```powershell
    docker-compose stop
    ```
    Observe a saída. Verifique com `docker-compose ps` (mostrará estado `Exit`) ou `docker ps` (não mostrará nada).
2.  **Iniciar a Aplicação:**
    ```powershell
    docker-compose start
    ```
    Observe a saída.
3.  **Listar com CLI Docker:**
    ```powershell
    docker container ls
    # ou docker ps
    ```
    Confirme que os 5 contêineres estão rodando (estado `Up`). Compare o tempo em `CREATED` com o tempo em `STATUS`. `CREATED` deve ser mais antigo que `STATUS`.

---

📄 Página 131

🔍 Resumo dos principais pontos:

*   Figura 7.6: Mostra a saída dos comandos `stop`, `start`, `docker ps`.
*   Compose tem muitos outros comandos (`docker-compose --help`).
*   **Consideração Importante:** Docker Compose é uma ferramenta **client-side**.
    *   Lê o arquivo `.yml` local e envia instruções para a API Docker.
    *   O Docker Engine *não* sabe que os contêineres formam uma "aplicação"; só o Compose sabe, baseado no arquivo `.yml`.
    *   É preciso ter o arquivo `.yml` para gerenciar a aplicação com Compose.
*   Risco de dessincronização: Se o estado real (contêineres rodando) mudar (ex: escalar manualmente com `docker run`, ou arquivo `.yml` for alterado) sem usar o Compose, o próximo comando Compose pode ter comportamento inesperado.
*   Exemplo: Escalamos `iotd` para 3 réplicas (p. 129), mas isso *não* foi salvo no `.yml`. Se derrubarmos e subirmos a aplicação, ela voltará ao estado original do `.yml` (1 réplica para `iotd`).
*   **Exercício Try-it-now:** Demonstrar a perda do estado de escala.
    *   Comandos:
        *   `docker-compose down` (para e *remove* contêineres, redes, etc., definidos no Compose).
        *   `docker-compose up -d` (recria tudo do zero, baseado no `.yml`).
        *   `docker container ls` (ou `docker-compose ps`).

🧠 Explicação detalhada e didática:

*   A página destaca uma característica fundamental e limitação do Docker Compose: ele é uma ferramenta que opera do **lado do cliente (client-side)**. Isso significa que:
    *   Toda a inteligência sobre a estrutura da sua aplicação reside no arquivo `docker-compose.yml` que você tem na sua máquina (ou no servidor de CI).
    *   Quando você executa `docker-compose up/down/stop/start/ps`, a ferramenta lê esse arquivo e envia os comandos apropriados para a API do Docker Engine.
    *   O Docker Engine em si apenas executa as ordens (criar contêiner X, parar contêiner Y), ele não tem uma noção persistente de que X e Y pertencem à mesma "aplicação Compose".
*   **Implicações:**
    *   Você precisa do arquivo `docker-compose.yml` correto para gerenciar a aplicação consistentemente.
    *   Mudanças feitas manualmente via `docker run/rm/scale` ou alterações no arquivo `.yml` que não são aplicadas com `docker-compose up` podem levar a um estado onde o Compose não sabe mais exatamente qual é o estado real da aplicação, podendo causar resultados inesperados em comandos futuros.
*   O exemplo prático disso é a escala do serviço `iotd`. O comando `--scale iotd=3` foi uma instrução *imperativa* dada ao Compose naquele momento, mas ela não alterou o arquivo `docker-compose.yml` *declarativo*. O arquivo ainda descreve `iotd` com 1 réplica (implicitamente).
*   O comando `docker-compose down` destrói os recursos gerenciados pelo Compose. Quando `docker-compose up -d` é executado novamente, ele lê o arquivo `.yml` original e recria a aplicação com apenas 1 réplica de `iotd`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Derrubar e Remover:**
    ```powershell
    docker-compose down
    ```
    Observe a saída (removing network, removing containers...). Verifique com `docker-compose ps` (deve dar erro ou não mostrar nada).
2.  **Subir Novamente:**
    ```powershell
    docker-compose up -d
    ```
3.  **Verificar Contêineres:**
    ```powershell
    docker container ls
    # ou docker-compose ps
    ```
    Confirme que agora só há 3 contêineres no total (1 `accesslog`, 1 `iotd`, 1 `image-gallery`), pois a escala de `iotd` foi perdida e voltou ao padrão do arquivo `.yml`.

---

📄 Página 132

🔍 Resumo dos principais pontos:

*   Figura 7.7: Mostra a saída do `down`/`up`, confirmando que `iotd` voltou a ter 1 réplica.
*   Docker Compose é simples e poderoso, mas requer gerenciamento cuidadoso dos arquivos `.yml`.
*   Recursos criados pelo Compose são recursos Docker normais, mas a relação entre eles ("aplicação") só existe enquanto se tem o arquivo Compose para gerenciá-los.
*   **7.3 How Docker plugs containers together** (Como Docker conecta os contêineres)
    *   Como os componentes (contêineres) se comunicam?
    *   Contêineres têm IP virtual próprio na rede Docker.
    *   Contêineres na *mesma* rede Docker podem se alcançar pelo IP.
    *   Problema: IPs mudam quando contêineres são recriados.
    *   Solução Docker: **Descoberta de Serviço (Service Discovery) via DNS interno**.

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre a natureza client-side do Compose e a importância de manter os arquivos `.yml` como a fonte da verdade para a definição da aplicação.
*   A seção 7.3 aprofunda como funciona a **comunicação entre contêineres** gerenciados pelo Docker (e Compose).
*   A base é a **rede virtual Docker**. Contêineres conectados à mesma rede podem, em teoria, comunicar-se usando seus endereços IP internos (ex: `172.18.0.3`).
*   No entanto, usar IPs diretamente é frágil, pois um contêiner pode ser removido e recriado (durante uma atualização ou falha) e receber um IP *diferente*.
*   A solução robusta que o Docker oferece é um **servidor DNS embutido**. Quando um contêiner (`app-a`) tenta se conectar a outro usando seu **nome de serviço** (definido no `docker-compose.yml`, ex: `http://iotd/...`), a requisição DNS é interceptada pelo Docker. O DNS interno do Docker resolve o nome do serviço (`iotd`) para o endereço IP *atual* do(s) contêiner(es) que rodam aquele serviço naquela rede. Isso permite comunicação estável mesmo que os IPs dos contêineres mudem.

💻 Exemplos práticos (para meu ambiente Windows):

*   No Dockerfile do `image-gallery` (Listagem 4.4, p. 82), as variáveis de ambiente definem os URLs das APIs como `http://iotd/image` e `http://accesslog/access-log`. O app Go usa esses nomes. Como os três contêineres (`image-gallery`, `iotd`, `accesslog`) estão na mesma rede `nat` (via Compose), o DNS interno do Docker resolverá `iotd` e `accesslog` para os IPs corretos quando o app Go tentar acessá-los.

💡 Analogias ou observações úteis (se houver):

*   DNS Interno Docker: É como ter uma lista telefônica atualizada automaticamente dentro do condomínio (rede Docker). Em vez de decorar o ramal (IP) de cada apartamento (contêiner), que pode mudar se o morador trocar de apartamento, você simplesmente liga para o "Apartamento do Fulano" (nome do serviço), e a central telefônica do condomínio (DNS Docker) te conecta ao ramal certo naquele momento.

---

📄 Página 133

🔍 Resumo dos principais pontos:

*   **Funcionamento do DNS Docker:**
    *   App no contêiner faz lookup DNS (ex: para `blog.sixeyed.com` ou `iotd`).
    *   DNS interno Docker intercepta.
    *   Se nome é de outro contêiner/serviço na mesma rede Docker, retorna IP(s) do(s) contêiner(es).
    *   Se nome não é de contêiner, repassa consulta para DNS do host (rede da empresa ou internet).
*   **DNS e Escala:**
    *   Se serviço roda em 1 contêiner, DNS retorna 1 IP.
    *   Se serviço roda em múltiplos contêineres (escalado), DNS retorna **múltiplos IPs** (um para cada contêiner).
*   **Exercício Try-it-now:** Ver DNS em ação com `nslookup`.
    *   1. Escalar `iotd` para 3 réplicas: `docker-compose up -d --scale iotd=3`.
    *   2. Conectar a um terminal dentro do contêiner `image-gallery`.
        *   Comando Linux: `docker container exec -it image-of-the-day_image-gallery_1 sh`.
        *   Comando Windows: `docker container exec -it image-of-the-day_image-gallery_1 cmd`.
    *   3. Dentro do contêiner, usar `nslookup <nome_servico>` para consultar o DNS Docker.
        *   `nslookup accesslog` (deve retornar 1 IP).
    *   4. Sair do contêiner (`exit`).

🧠 Explicação detalhada e didática:

*   A página detalha o funcionamento do DNS interno do Docker, explicando como ele resolve nomes tanto para outros contêineres na mesma rede quanto para endereços externos (repassando para o DNS configurado no host).
*   Um ponto importante é como o DNS lida com serviços **escalados**: se um serviço (`iotd`) está rodando com 3 réplicas (contêineres), uma consulta DNS para `iotd` retornará os endereços IP de **todos os três** contêineres. Isso permite que o cliente (o contêiner `image-gallery`) escolha um dos IPs (ou use algum mecanismo de balanceamento) para se conectar.
*   O exercício usa o comando `docker exec -it <container> <comando>` para executar um comando (neste caso, `sh` ou `cmd` para obter um shell) *dentro de um contêiner que já está rodando*. Isso é diferente de `docker run -it`, que inicia um novo contêiner.
*   Dentro do contêiner `image-gallery`, a ferramenta `nslookup` (um utilitário padrão de consulta DNS) é usada para consultar o nome do serviço `accesslog`. Como `accesslog` está rodando com apenas 1 réplica, a expectativa é que `nslookup` retorne um único endereço IP.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Escalar `iotd`:** (No diretório do `docker-compose.yml`)
    ```powershell
    docker-compose up -d --scale iotd=3
    ```
2.  **Obter Nome Completo do Contêiner:**
    ```powershell
    docker ps # Encontre o nome completo na coluna NAMES, ex: image-of-the-day_image-gallery_1
    ```
3.  **Executar Shell Interativo no Contêiner:** (Substitua pelo nome real)
    ```powershell
    # Se a imagem base for Linux (provável):
    docker container exec -it image-of-the-day_image-gallery_1 sh
    # Se a imagem base for Windows:
    # docker container exec -it image-of-the-day_image-gallery_1 cmd
    ```
4.  **Dentro do Contêiner (prompt /app # ou C:\app>):**
    ```
    nslookup accesslog
    ```
    Observe a saída. Ignore a primeira mensagem de erro sobre o servidor DNS; procure pela(s) linha(s) `Address:` que mostram o IP resolvido (deve ser um da faixa 172.x.x.x). Anote o IP retornado.
5.  **Sair do Contêiner:**
    ```
    exit
    ```

---

📄 Página 134

🔍 Resumo dos principais pontos:

*   Figura 7.8: Mostra a saída do `nslookup accesslog` dentro do contêiner, retornando 1 IP.
*   IPs são da rede Docker (ex: 172.24.0.2).
*   DNS permite que app funcione mesmo se IP mudar após recriação do contêiner.
*   **Exercício Try-it-now:** Verificar mudança de IP (ou não) e resolução para serviço escalado.
    *   1. Remover manualmente o contêiner `accesslog`: `docker container rm -f image-of-the-day_accesslog_1`.
    *   2. Rodar `docker-compose up -d --scale iotd=3` novamente. Compose recriará `accesslog`.
    *   3. Conectar novamente ao `image-gallery` (`docker exec -it ... sh/cmd`).
    *   4. Rodar `nslookup accesslog` (verificar se IP mudou).
    *   5. Rodar `nslookup iotd` (verificar se retorna 3 IPs, um para cada réplica).
    *   6. Sair (`exit`).

🧠 Explicação detalhada e didática:

*   A página explica que o uso de nomes DNS garante a resiliência da comunicação: mesmo que um contêiner seja destruído e recriado (possivelmente com um novo IP), o DNS interno do Docker sempre fornecerá o IP *atual* quando outro contêiner consultar pelo nome do serviço.
*   O exercício demonstra isso e também como o DNS funciona para serviços escalados:
    1.  Força-se a recriação do contêiner `accesslog` removendo-o manualmente e rodando `docker-compose up` (que detecta a falta e o recria).
    2.  Ao consultar `nslookup accesslog` novamente de dentro do `image-gallery`, o IP retornado *pode* ser diferente do anterior (depende de como o Docker aloca IPs), mas a resolução funciona.
    3.  Ao consultar `nslookup iotd` (que está escalado para 3 réplicas), o DNS Docker retorna os **três** endereços IP correspondentes às réplicas.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Remover `accesslog`:** (Precisa do nome completo do contêiner)
    ```powershell
    docker container rm -f image-of-the-day_accesslog_1
    ```
2.  **Recriar/Garantir Estado:**
    ```powershell
    docker-compose up -d --scale iotd=3
    ```
3.  **Executar Shell no `image-gallery`:** (Use o nome completo)
    ```powershell
    docker container exec -it image-of-the-day_image-gallery_1 sh
    ```
4.  **Dentro do Contêiner:**
    ```
    nslookup accesslog
    # Anote o IP. Mudou em relação à p.133? (Pode não mudar em ambiente simples)

    nslookup iotd
    # Verifique se 3 IPs diferentes são retornados na seção 'Non-authoritative answer'.
    ```
5.  **Sair:**
    ```
    exit
    ```

---

📄 Página 135

🔍 Resumo dos principais pontos:

*   Figura 7.9: Mostra a saída do `nslookup` para `accesslog` (1 IP, talvez o mesmo de antes) e `iotd` (3 IPs).
*   Servidores DNS podem retornar múltiplos IPs para um nome.
*   Docker Compose usa isso para **balanceamento de carga simples**: retorna todos os IPs das réplicas.
*   Cliente decide como usar múltiplos IPs (alguns usam só o primeiro).
*   DNS Docker retorna a lista em **ordem diferente** a cada consulta (tentativa básica de distribuir carga - round-robin DNS). Pode-se verificar repetindo `nslookup iotd`.
*   **Conclusão:** Compose grava opções de inicialização, cuida da comunicação via DNS. Próximo: configuração de ambiente com Compose.
*   **7.4 Application configuration in Docker Compose** (Configuração de aplicação no Docker Compose)
    *   Revisita app To-Do (Cap. 6). Pode usar SQLite (arquivo local) ou BD remoto (Postgres).
    *   Rodar BD Postgres em outro contêiner.
    *   Imagem To-Do pode ser configurada para usar Postgres em vez de SQLite.
    *   Configuração aplicada via Docker Compose.

🧠 Explicação detalhada e didática:

*   A página explica o mecanismo de balanceamento de carga padrão do Docker (e Compose) quando um serviço é escalado: o DNS interno retorna *todos* os IPs das réplicas, geralmente embaralhando a ordem a cada consulta (Round-Robin DNS). Cabe à aplicação cliente decidir como usar essa lista (usar o primeiro, tentar outro se falhar, etc.). É um balanceamento simples, mas funcional para muitos casos.
*   A seção 7.4 introduz como usar o Docker Compose para gerenciar a **configuração das aplicações** nos contêineres, permitindo que a mesma imagem se comporte de maneira diferente dependendo de como é iniciada.
*   O exemplo será novamente o aplicativo To-Do, que pode ser configurado para usar ou um banco de dados de arquivo simples (SQLite) ou um servidor de banco de dados mais robusto (PostgreSQL), que também rodará em um contêiner separado. O Compose será usado para "dizer" ao contêiner do To-Do qual banco de dados usar e como se conectar a ele.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo, mas prepara para usar as diretivas `environment` e `secrets` no `docker-compose.yml` para passar configurações para os contêineres.

💡 Analogias ou observações úteis (se houver):

*   Balanceamento via DNS: Imagine pedir o telefone da pizzaria "Pizza Boa" (serviço `iotd`). A telefonista (DNS Docker) te dá três números diferentes (IPs das 3 réplicas), talvez em ordem diferente a cada vez que você liga. Você (contêiner `image-gallery`) escolhe um deles para fazer o pedido.

---

📄 Página 136

🔍 Resumo dos principais pontos:

*   Postgres: BD relacional open source popular, roda bem em Docker.
*   Imagem To-Do segue boas práticas: configuração padrão para dev (SQLite), mas permite overrides para outros ambientes (Postgres).
*   **Listagem 7.3:** Seção `services` de Compose file para rodar To-Do com Postgres.
    ```yaml
    services:
      todo-db: # Serviço do Banco de Dados Postgres
        image: diamol/postgres:11.5
        ports:
          - "5433:5432" # Publica porta do BD (opcional, para acesso externo/debug)
        networks:
          - app-net
      todo-web: # Serviço da Aplicação Web To-Do
        image: diamol/ch06-todo-list
        ports:
          - "8020:80" # Porta da aplicação web
        environment: # Define Variável de Ambiente
          - Database:Provider=Postgres # Configura app para usar Postgres
        depends_on: # Depende do BD para iniciar antes
          - todo-db
        networks:
          - app-net
        secrets: # Define uso de um Secret
          - source: postgres-connection # Nome lógico do secret
            target: /app/config/secrets.json # Onde o secret aparecerá como arquivo no container
    ```

🧠 Explicação detalhada e didática:

*   A página apresenta a configuração no `docker-compose.yml` (Listagem 7.3) para rodar a aplicação To-Do conectada a um banco de dados PostgreSQL, ambos em contêineres separados.
*   **Serviço `todo-db`:** Simplesmente inicia um contêiner usando uma imagem PostgreSQL (`diamol/postgres:11.5`) e o conecta à rede `app-net`. A porta 5432 (padrão do Postgres) é opcionalmente publicada na porta 5433 do host para permitir acesso direto ao banco de dados de fora (ex: com uma ferramenta de cliente SQL), mas a aplicação web se conectará a ele usando o nome `todo-db` através da rede interna `app-net`.
*   **Serviço `todo-web`:**
    *   Usa a imagem `diamol/ch06-todo-list`.
    *   **`environment:`**: Define a variável de ambiente `Database:Provider` com o valor `Postgres`. A aplicação To-Do lerá essa variável e saberá que deve usar o modo PostgreSQL em vez do modo SQLite padrão.
    *   **`depends_on:`**: Garante que o contêiner `todo-db` seja iniciado antes do `todo-web`.
    *   **`secrets:`**: Esta é uma nova diretiva. Ela instrui o Docker a pegar um "segredo" chamado `postgres-connection` (que conterá a string de conexão do banco de dados) e torná-lo disponível *dentro* do contêiner `todo-web` como um arquivo localizado em `/app/config/secrets.json`. A aplicação To-Do está programada para ler este arquivo e usar seu conteúdo para se conectar ao banco de dados.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você precisará de um arquivo `docker-compose.yml` (ex: em `ch07\exercises\todo-list-postgres`) contendo a Listagem 7.3, mais as seções `version`, `networks` (usando `nat` externo) e a definição do `secrets` (próxima página).
*   O uso de `secrets` é a forma recomendada para passar informações sensíveis (como senhas de banco de dados) para contêineres, em vez de usar variáveis de ambiente que podem ser inspecionadas mais facilmente.

---

📄 Página 137

🔍 Resumo dos principais pontos:

*   Continuação da análise da Listagem 7.3.
*   **Secrets em Cluster vs. Compose:**
    *   `secrets` são ideais para dados sensíveis (strings de conexão, chaves API, certificados).
    *   Em clusters (Swarm, Kubernetes), secrets são armazenados de forma segura (criptografados) no banco de dados do cluster e entregues aos contêineres.
    *   No Docker Compose (single-host), não há cluster DB. Secrets são carregados de **arquivos locais** no host.
*   **Listagem 7.4:** Seção `secrets` no final do Compose file para carregar de arquivo.
    ```yaml
    secrets:
      postgres-connection: # Nome lógico do secret (referenciado no serviço)
        file: ./config/secrets.json # Caminho do arquivo NO HOST que contém o segredo
    ```
*   **Como funciona:** `docker-compose` lê o conteúdo do arquivo `./config/secrets.json` do host e o injeta no contêiner `todo-web` no caminho `/app/config/secrets.json` (definido no serviço).
*   É similar a bind mount, mas usar a diretiva `secrets` permite migração futura para segredos reais de cluster.
*   **Benefício:** Permite usar a mesma imagem em diferentes ambientes, mudando a configuração (BD, logging, etc.) via Compose (variáveis de ambiente, secrets/arquivos).
*   Aplicação se comporta igual, mas dados agora vão para contêiner Postgres.

🧠 Explicação detalhada e didática:

*   A página explica como os `secrets` funcionam especificamente no contexto do **Docker Compose**. Enquanto em um cluster como Docker Swarm ou Kubernetes os segredos são objetos gerenciados e armazenados de forma segura pelo orquestrador, no Docker Compose (que roda em uma única máquina), a diretiva `secrets` serve como um mecanismo para ler o conteúdo de um **arquivo local** na máquina host e montá-lo como um arquivo dentro do contêiner, de forma similar a um bind mount, mas usando a semântica de "secret".
*   A **Listagem 7.4** mostra como definir a *origem* do secret `postgres-connection` referenciado no serviço `todo-web`. A chave `file:` indica o caminho relativo (`./config/secrets.json`) no host onde o Compose encontrará o arquivo contendo a string de conexão do Postgres.
*   **Vantagem:** Embora no Compose local isso seja apenas uma montagem de arquivo, usar a sintaxe `secrets:` torna o arquivo `docker-compose.yml` mais preparado para ser usado (com modificações mínimas ou via overrides) em um ambiente de cluster que suporte segredos nativos.
*   O resultado final é que a aplicação To-Do (`todo-web`), ao iniciar, será configurada via variável de ambiente para usar `Postgres` e lerá o arquivo `/app/config/secrets.json` (injetado pelo Compose a partir do arquivo local) para obter os detalhes de conexão para o contêiner `todo-db`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Completar o `docker-compose.yml`:** No diretório `ch07\exercises\todo-list-postgres`, adicione a seção `secrets:` da Listagem 7.4 ao arquivo que contém a Listagem 7.3 e as seções `version`/`networks`.
2.  **Criar o Arquivo do Secret:** No mesmo diretório, crie uma subpasta `config`. Dentro dela, crie um arquivo `secrets.json` com o seguinte conteúdo (string de conexão para o contêiner `todo-db`):
    ```json
    {
      "ConnectionStrings": {
        "ToDoDb": "Host=todo-db; Port=5432; Database=todo; Username=postgres; Include Error Detail=true"
      }
    }
    ```
    *(Nota: A imagem `diamol/postgres` provavelmente está configurada para aceitar conexões sem senha do usuário `postgres` para o banco `todo` a partir da mesma rede Docker).*

---

📄 Página 138

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar o app To-Do + Postgres com Compose.
    *   Navegar para `ch07/exercises/todo-list-postgres`.
    *   Arquivo `secrets.json` está lá.
    *   Comando Linux: `docker-compose up -d`.
    *   Comando Windows (usa `-f docker-compose-windows.yml` que ajusta paths): `docker-compose -f docker-compose-windows.yml up -d`.
    *   Verificar contêineres rodando: `docker-compose ps`.
*   Figura 7.10: Mostra saída do `up` e `ps`, com contêineres `todo-db` e `todo-web`.

🧠 Explicação detalhada e didática:

*   O exercício instrui a iniciar a aplicação completa (web + banco de dados Postgres) usando o arquivo `docker-compose.yml` que define ambos os serviços e a configuração via `environment` e `secrets`.
*   Uma observação importante é a diferença no comando para usuários Windows. Isso ocorre porque montar arquivos como `secrets` (ou em bind mounts/volumes) pode exigir sintaxe de caminho ligeiramente diferente ou tratamento especial no Windows. O autor forneceu um arquivo `docker-compose-windows.yml` que provavelmente ajusta o caminho na definição do `secret` (ou talvez em volumes, se houvesse) para funcionar corretamente no Windows. A flag `-f` permite especificar um arquivo Compose alternativo ou adicional.
*   Após rodar `docker-compose up -d`, os dois contêineres (`todo-db` e `todo-web`) devem estar em execução. O comando `docker-compose ps` é o equivalente a `docker ps` mas focado nos serviços definidos no arquivo Compose do projeto atual.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar para o Diretório:**
    ```powershell
    cd C:\DockerLabs\diamol\ch07\exercises\todo-list-postgres
    ```
2.  **Verificar Arquivos:** Confirme que `docker-compose.yml`, `docker-compose-windows.yml` e a pasta `config` com `secrets.json` existem.
3.  **Iniciar a Aplicação (usando o arquivo Windows):**
    ```powershell
    docker-compose -f docker-compose-windows.yml up -d
    ```
    *Nota: Se o arquivo principal já estiver ajustado para Windows, talvez `docker-compose up -d` funcione diretamente.*
4.  **Verificar Serviços:**
    ```powershell
    docker-compose ps
    ```
    Você deve ver dois serviços (`todo-db`, `todo-web`) no estado `Up`.

---

📄 Página 139

🔍 Resumo dos principais pontos:

*   Figura 7.10: Mostra a saída do `up` e `ps`.
*   Acessar a aplicação web em `http://localhost:8030`. (Porta 8030 - **Correção**: A Listagem 7.3 na p.136 mostra porta 8020. Assumirei 8020 conforme a listagem original).
*   Funcionalidade é a mesma, mas dados agora salvos no contêiner Postgres.
*   **Verificação:** Usar cliente de BD (ex: Sqlectron - open source, cross-platform) para conectar ao Postgres.
    *   Endereço: `localhost:5433` (porta do host mapeada para `todo-db`).
    *   DB: `todo`, User: `postgres`, Senha: (nenhuma).
*   Figura 7.11: Mostra dados adicionados via web sendo consultados no Postgres via Sqlectron.
*   **Benefício Chave:** Separação do pacote da aplicação (imagem Docker) da configuração de runtime (Compose file).
    *   Mesma imagem progride pelos ambientes (teste -> prod).
    *   Cada ambiente aplica sua config (via env vars, bind mounts, secrets no Compose).
    *   Garante que o binário/dependências testados são os mesmos em produção.

🧠 Explicação detalhada e didática:

*   Após iniciar os contêineres, a página instrui a testar a aplicação acessando a interface web (usarei a porta 8020 conforme Listagem 7.3). A aplicação deve funcionar como antes, mas agora, os dados inseridos estão sendo armazenados no banco de dados PostgreSQL que roda no contêiner `todo-db`.
*   Para comprovar isso, sugere-se conectar ao banco de dados diretamente usando uma ferramenta cliente SQL (como DBeaver, pgAdmin, ou o Sqlectron mencionado). A conexão é feita para `localhost:5433` porque essa é a porta do *host* que foi mapeada para a porta 5432 *dentro* do contêiner `todo-db`. As credenciais padrão (usuário `postgres`, banco `todo`, sem senha) são usadas. A Figura 7.11 mostra que é possível ver os dados inseridos pela aplicação web diretamente no banco de dados.
*   A página conclui reforçando um dos maiores benefícios dessa abordagem: a **separação entre a imagem da aplicação e sua configuração**. Você constrói a imagem Docker *uma vez*. Essa mesma imagem imutável é usada em todos os ambientes (desenvolvimento, teste, homologação, produção). O que muda em cada ambiente é apenas a *configuração* aplicada no momento da execução, geralmente definida nos arquivos Docker Compose (ou manifestos de orquestração), garantindo consistência e reduzindo erros de implantação.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Testar App:** Abra o navegador em `http://localhost:8020`. Adicione alguns itens.
2.  **(Opcional) Conectar ao BD:**
    *   Baixe e instale um cliente Postgres como DBeaver (https://dbeaver.io/) ou use a extensão PostgreSQL no VS Code.
    *   Crie uma nova conexão:
        *   Host: `localhost`
        *   Port: `5433`
        *   Database: `todo`
        *   Username: `postgres`
        *   Password: (deixe em branco)
    *   Conecte-se e execute uma query SQL: `SELECT * FROM public."ToDos";` Você deverá ver os itens que adicionou via web.

---

📄 Página 140

🔍 Resumo dos principais pontos:

*   Figura 7.11: Mostra consulta ao BD Postgres.
*   **7.5 Understanding the problem Docker Compose solves** (Entendendo o problema que Docker Compose resolve)
*   Compose é forma elegante de descrever apps complexos em arquivo simples (YAML).
*   Arquivo Compose = guia de deploy executável (melhor que docs manuais desatualizados).
*   Útil na toolkit Docker, mas entender suas limitações é importante.
*   **O que Compose faz:** Define app, aplica definição a *um único* host Docker. Compara estado desejado (YAML) com estado real (host), envia comandos API para criar/atualizar recursos.
*   **O que Compose NÃO faz (Limitações):**
    *   Não é plataforma completa como Docker Swarm ou Kubernetes.
    *   Não monitora continuamente o estado desejado.
    *   Se contêineres falham/são removidos manualmente, Compose *não* os reinicia/substitui automaticamente (só se rodar `docker-compose up` de novo).

🧠 Explicação detalhada e didática:

*   A seção 7.5 recapitula o valor do Docker Compose: ele fornece uma maneira **declarativa**, **legível** e **executável** de definir aplicações multi-contêiner, servindo tanto como ferramenta de implantação quanto como documentação viva.
*   No entanto, é crucial entender suas **limitações**, principalmente ao compará-lo com orquestradores de cluster como Docker Swarm ou Kubernetes:
    *   **Escopo:** Compose é projetado para gerenciar contêineres em um **único host Docker**. Orquestradores gerenciam contêineres em **múltiplos hosts (cluster)**.
    *   **Natureza:** Compose é uma ferramenta **client-side** que aplica um estado desejado *uma vez* (quando você roda `up`). Ele **não é um controlador ativo** que fica monitorando e corrigindo desvios continuamente.
    *   **Auto-recuperação (Self-healing):** Se um contêiner gerenciado pelo Compose falhar ou for parado por outra razão, o Compose *não fará nada* para reiniciá-lo ou substituí-lo automaticamente. Você precisaria intervir e rodar `docker-compose up` novamente. Orquestradores, por outro lado, monitoram ativamente e tentam manter o estado desejado, substituindo contêineres falhos.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Demonstrar Limitação:**
    ```powershell
    # Certifique-se que o app todo-list-postgres está rodando (via docker-compose ps)
    # Pare manualmente um dos containers
    docker stop todo-list-postgres_todo-web_1
    # Verifique o estado com Compose
    docker-compose ps
    # Você verá que o todo-web está 'Exit' ou 'Stopped'. O Compose não o reiniciou.
    # Para trazê-lo de volta:
    docker-compose up -d # Ou docker-compose start todo-web
    ```

💡 Analogias ou observações úteis (se houver):

*   Docker Compose: É como um arquiteto que entrega a planta da casa (YAML) e supervisiona a construção inicial (`up`). Depois que a casa está pronta, ele vai embora. Se uma parede cair (contêiner falhar), ele não volta para consertar sozinho.
*   Orquestrador (Swarm/K8s): É como um síndico que tem a planta (YAML), supervisiona a construção, e *continua morando no prédio*, monitorando tudo. Se uma lâmpada queima (contêiner falha), o síndico automaticamente troca a lâmpada para manter tudo funcionando conforme a planta.

---

📄 Página 141

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 7.12): Mostra onde Compose se encaixa no ciclo de vida: principalmente Desenvolvimento e Teste/CI, mas *pode* ser usado em Produção simples.
*   **Compose em Produção:**
    *   Não é ideal, mas pode ser ponto de partida se migrando de VMs individuais para contêineres.
    *   Não oferece alta disponibilidade, balanceamento real, failover (como Swarm/K8s).
    *   Mas *oferece* consistência de artefatos (Dockerfiles, Compose files) e ferramentas entre ambientes.
    *   Pode ser suficiente antes de adotar um cluster.

🧠 Explicação detalhada e didática:

*   A Figura 7.12 posiciona o Docker Compose no ciclo de vida de uma aplicação. Seu ponto forte é nos ambientes de **Desenvolvimento** e **Teste/Integração Contínua (CI)**, onde a facilidade de definir e rodar rapidamente aplicações multi-contêiner em uma única máquina é uma grande vantagem.
*   A página discute a viabilidade de usar Compose em **Produção**. Embora *não* seja a solução ideal para aplicações que exigem alta disponibilidade, escalabilidade real e recuperação automática de falhas (para isso servem Swarm e Kubernetes), ele *pode* ser um passo inicial válido em certos cenários:
    *   Se você está apenas começando com contêineres e migrando aplicações que antes rodavam em VMs isoladas (que também não tinham alta disponibilidade intrínseca).
    *   O benefício principal seria padronizar o processo de deploy e gerenciamento usando Docker e Compose, mesmo que rodando em um único servidor de produção por aplicação.
*   A mensagem principal é: Compose é ótimo para dev/teste, e um ponto de partida *possível* (mas limitado) para produção, antes de avançar para orquestradores de cluster.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você está usando o Compose exatamente como descrito para dev/teste no seu Docker Desktop. Todos os `docker-compose up/down` que você rodou simulam esse fluxo.

💡 Analogias ou observações úteis (se houver):

*   Usar Compose em Produção (simples): É como usar a planta da casa para construir uma única casa pré-fabricada em um terreno. Funciona, mas se a casa pegar fogo, não há outra.
*   Usar Orquestrador em Produção: É como usar a planta para construir um condomínio com várias casas idênticas, segurança e equipe de manutenção. Se uma casa tem problema, os moradores podem usar outra, e a equipe conserta a que falhou.

---

📄 Página 142

🔍 Resumo dos principais pontos:

*   **7.6 Lab** (Laboratório)
    *   Objetivo: Adicionar **confiabilidade** à execução do app To-Do web usando features do Compose.
    *   Cenário: Ambiente de teste.
*   **Requisitos:**
    1.  Contêineres da aplicação (`todo-db`, `todo-web`) devem reiniciar automaticamente se máquina/Docker reiniciar.
    2.  Contêiner do banco de dados (`todo-db`) deve usar **bind mount** para armazenar arquivos, permitindo `down`/`up` sem perder dados.
    3.  Aplicação web (`todo-web`) deve ouvir na porta **80 padrão** do host para o ambiente de teste.
*   **Dica:** Consultar a especificação do arquivo Docker Compose na documentação oficial (link fornecido) para encontrar as configurações necessárias.
*   Solução de exemplo no GitHub.

🧠 Explicação detalhada e didática:

*   O **Lab do Capítulo 7** foca em usar configurações do Docker Compose para tornar a execução da aplicação To-Do (com banco de dados Postgres) mais robusta para um ambiente de teste.
*   Os requisitos são:
    1.  **Reinício Automático:** Garantir que os contêineres voltem a rodar se o Docker ou a máquina forem reiniciados. Isso geralmente é feito com a diretiva `restart:` (ex: `restart: always` ou `restart: unless-stopped`) na definição do serviço no `docker-compose.yml`.
    2.  **Persistência com Bind Mount:** Em vez de usar um Volume Docker (gerenciado pelo Docker) para os dados do Postgres, o lab pede para usar um **Bind Mount**, mapeando um diretório do *host* para o local onde o Postgres armazena seus dados dentro do contêiner (geralmente `/var/lib/postgresql/data`). Isso garante que os dados fiquem diretamente no host e persistam entre `docker-compose down` e `up`. (Precisa encontrar o caminho de dados do Postgres e usar a sintaxe `volumes:` com caminho do host).
    3.  **Porta Padrão:** Configurar o serviço `todo-web` para mapear sua porta interna 80 para a porta **80** do host (`ports: - "80:80"`), em vez de portas altas como 8020.
*   A dica aponta para a documentação oficial, que é a fonte definitiva para todas as opções de configuração do Compose.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 7:**
    1.  Comece com o arquivo `docker-compose-windows.yml` (ou `.yml` principal) de `ch07\exercises\todo-list-postgres`.
    2.  **Reinício:** Adicione `restart: unless-stopped` a ambos os serviços (`todo-db` e `todo-web`).
    3.  **Bind Mount para BD:**
        *   Encontre o caminho de dados do Postgres dentro do contêiner (geralmente `/var/lib/postgresql/data`).
        *   Crie uma pasta no host (ex: `C:\DockerLabs\diamol\ch07\lab\pgdata`).
        *   No serviço `todo-db`, substitua a possível seção `volumes:` existente (se houver) ou adicione uma nova, usando a sintaxe de bind mount:
          ```yaml
          services:
            todo-db:
              # ... outras configs ...
              restart: unless-stopped
              volumes:
                - C:\DockerLabs\diamol\ch07\lab\pgdata:/var/lib/postgresql/data # Bind mount
          ```
          *(Nota: Use `\` ou `/` consistentes no caminho do host, ou use variáveis como no exercício da p.112)*.
    4.  **Porta Web:** No serviço `todo-web`, mude a seção `ports:` para:
        ```yaml
          ports:
            - "80:80"
        ```
    5.  Salve o arquivo como `docker-compose.yml` (ou outro nome) na pasta `ch07\lab`.
    6.  **Teste:** Navegue para a pasta `ch07\lab` e rode `docker-compose up -d`. Acesse `http://localhost` (porta 80). Adicione dados. Rode `docker-compose down`, depois `docker-compose up -d` novamente. Verifique se os dados ainda estão lá (acessando `http://localhost`). Verifique se a pasta `pgdata` no host contém arquivos.

---

📄 Página 143

🔍 Resumo dos principais pontos:

*   **Capítulo 8: Supporting reliability with health checks and dependency checks** (Suportando confiabilidade com verificações de saúde e dependência)
*   Jornada para produção: já vimos empacotamento, execução e Compose.
*   Produção usa orquestradores (Swarm, K8s) que têm features para **auto-recuperação (self-healing)**.
*   Plataforma usa informações fornecidas pela imagem para checar se app está saudável.
*   Se app falha, plataforma pode remover contêiner doente e substituí-lo.
*   Foco do Cap. 8: Como empacotar essas **verificações (checks)** nas imagens para habilitar auto-recuperação.
*   **8.1 Building health checks into Docker images** (Construindo verificações de saúde nas imagens Docker)
    *   Docker já monitora o básico: o processo principal do contêiner.
    *   Se processo principal morre, contêiner entra em `Exited`.
    *   Plataforma pode reiniciar/substituir contêiner `Exited`.
    *   **Limitação:** Checagem básica só garante que o *processo* está rodando, não que o *app* está respondendo corretamente.
    *   Exemplo: Web app travado retornando erro 503, mas processo rodando -> Docker acha que está saudável.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 8, focado em aumentar a **confiabilidade** das aplicações em contêineres, especialmente em ambientes orquestrados (produção).
*   A ideia central é que orquestradores como Docker Swarm e Kubernetes podem automaticamente detectar e corrigir problemas (ex: substituir um contêiner que não está respondendo), criando aplicações "auto-reparáveis" (self-healing). Mas, para isso, o orquestrador precisa saber *como* verificar se a aplicação dentro do contêiner está realmente funcionando bem.
*   A seção 8.1 começa explicando o monitoramento padrão e sua limitação: o Docker, por si só, apenas verifica se o processo principal que ele iniciou dentro do contêiner ainda está ativo. Se o processo morrer, o contêiner para. Isso é útil, mas insuficiente. Uma aplicação pode estar com o processo rodando, mas completamente travada ou respondendo com erros para todas as requisições. Para o Docker, ela pareceria "saudável".

💻 Exemplos práticos (para meu ambiente Windows):

*   Pense em um contêiner rodando um servidor web. Se o processo do servidor web (`nginx.exe`, `httpd.exe`, `dotnet run`, etc.) travar e fechar, `docker ps` mostrará o contêiner como `Exited`.
*   No entanto, se o servidor web continuar rodando mas, devido a um bug ou sobrecarga, começar a retornar apenas páginas de erro (HTTP 500 ou 503), `docker ps` ainda mostrará o contêiner como `Up`, pois o processo principal está ativo. O próximo passo é ensinar o Docker a detectar essa segunda situação.

💡 Analogias ou observações úteis (se houver):

*   Monitoramento básico do Docker: É como verificar se o funcionário (processo) bateu o ponto e está na empresa. Não garante que ele esteja realmente trabalhando ou atendendo bem os clientes (aplicação saudável).

---

📄 Página 144

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Demonstrar a limitação do monitoramento padrão.
    *   App: API REST simples (`diamol/ch08-numbers-api`) que retorna número aleatório.
    *   Bug: Após 3 chamadas, API entra em estado de falha e sempre retorna erro 500.
    *   1. Rodar contêiner da API: `docker run -d -p 8080:80 diamol/ch08-numbers-api`.
    *   2. Chamar API 3 vezes via `curl http://localhost:8080/rng` (funciona).
    *   3. Chamar 4ª vez (falha, retorna 500).
    *   4. Verificar status do contêiner: `docker container ls`.
*   Resultado esperado (Figura 8.1): Após a 4ª chamada, API falha, mas `docker ls` ainda mostra o contêiner como `Up`. Docker não sabe que o app interno falhou.
*   **Solução: Instrução `HEALTHCHECK` no Dockerfile.**
    *   Permite definir um comando que o Docker executa *dentro* do contêiner periodicamente para verificar a saúde *real* do app.
    *   Comando deve retornar código de status: 0 para sucesso (saudável), diferente de 0 para falha.
    *   Docker roda o comando em intervalos. Se falhar múltiplas vezes seguidas, marca contêiner como `unhealthy`.
*   **Listagem 8.1:** Trecho de Dockerfile com `HEALTHCHECK`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENTRYPOINT ["dotnet", "/app/Numbers.Api.dll"]
    # Define o comando de verificação de saúde:
    HEALTHCHECK CMD curl --fail http://localhost/health
    WORKDIR /app
    COPY --from=builder /out/ .
    ```
*   Explicação do `HEALTHCHECK`:
    *   Usa `curl --fail` para chamar um endpoint `/health` dentro do próprio contêiner.
    *   Endpoint `/health` (implementado no app) retorna status 200 (OK) se app está bom, 500 (Erro) se bug foi ativado.
    *   `--fail` faz `curl` retornar código de saída 0 se HTTP status for 2xx, e diferente de 0 caso contrário.

🧠 Explicação detalhada e didática:

*   O exercício prático comprova a limitação do monitoramento padrão: a API falha internamente, mas o contêiner continua `Up` para o Docker.
*   A solução é a instrução `HEALTHCHECK` no Dockerfile. Ela permite que *você* defina um teste mais inteligente para verificar a saúde da *sua aplicação*.
*   **Como funciona:**
    1.  Você adiciona `HEALTHCHECK CMD <comando_de_teste>` ao seu Dockerfile.
    2.  O `<comando_de_teste>` é algo que roda *dentro* do contêiner e verifica se a aplicação está respondendo corretamente. Pode ser chamar um endpoint HTTP especial (`/health`, `/ping`), executar uma query no banco de dados, verificar um arquivo, etc.
    3.  O comando deve terminar com um **código de saída**: `0` significa "saudável", qualquer outro valor significa "não saudável".
    4.  O Docker Engine executa esse comando periodicamente (configurável).
    5.  Se o comando falha por um número configurável de vezes consecutivas, o Docker muda o status do contêiner para `unhealthy`.
*   A Listagem 8.1 mostra um `HEALTHCHECK` que usa `curl` para verificar um endpoint `/health` da própria aplicação. Se o endpoint responder OK (HTTP 200), `curl --fail` retorna 0. Se responder com erro (HTTP 500), `curl --fail` retorna um código diferente de 0.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais (Demonstrar problema):**
    ```powershell
    # 1. Rodar API v1 (sem healthcheck)
    docker container run -d -p 8080:80 --name api-v1 diamol/ch08-numbers-api

    # 2. Chamar 3 vezes (OK)
    Invoke-WebRequest http://localhost:8080/rng
    Invoke-WebRequest http://localhost:8080/rng
    Invoke-WebRequest http://localhost:8080/rng

    # 3. Chamar 4ª vez (Erro 500)
    Invoke-WebRequest http://localhost:8080/rng -UseBasicParsing # -UseBasicParsing evita erro no PowerShell em caso de HTTP 500

    # 4. Verificar status (ainda 'Up')
    docker ps
    ```
*   **Analisar `HEALTHCHECK`:** Veja a Listagem 8.1. O comando `curl --fail http://localhost/health` será executado *pelo Docker Engine*, dentro do namespace do contêiner, em intervalos regulares.

---

📄 Página 145

🔍 Resumo dos principais pontos:

*   Resto do Dockerfile (Listagem 8.1) é padrão (.NET Core app). `ENTRYPOINT` roda `dotnet`, que é o processo monitorado pelo Docker para status `Up`/`Exited`.
*   `curl --fail` passa o status HTTP como código de saída para o Docker (0=sucesso, !=0=falha).
*   **Construindo imagem com `HEALTHCHECK`:**
    *   Exemplo usa estrutura diferente: Dockerfile (`Dockerfile.v2`) em pasta separada do código fonte.
    *   Necessário usar flag `-f` no `docker build` para especificar caminho do Dockerfile.
*   **Exercício Try-it-now:** Construir a imagem v2 com health check.
    *   Navegar para `ch08/exercises/numbers`.
    *   Comando: `docker image build -t diamol/ch08-numbers-api:v2 -f ./numbers-api/Dockerfile.v2 .`
    *   `.` no final ainda é o contexto (pasta `numbers`), `-f` especifica o Dockerfile a usar dentro desse contexto ou subpastas.

🧠 Explicação detalhada e didática:

*   A página conclui a análise do Dockerfile com `HEALTHCHECK`, explicando como o `curl --fail` traduz o status HTTP da aplicação em um código de saída que o Docker entende.
*   Introduz uma variação no comando `docker build`: a flag `-f` (ou `--file`). Ela é usada quando o arquivo Dockerfile não se chama `Dockerfile` ou não está na raiz do diretório de contexto. No exemplo, o arquivo se chama `Dockerfile.v2` e está em uma subpasta (`numbers-api`), enquanto o contexto (`.`) é a pasta pai (`numbers`, que contém o código fonte a ser copiado).
*   O exercício instrui a construir a imagem `v2` da API, que inclui a instrução `HEALTHCHECK`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal:**
    1.  No PowerShell/terminal, navegue para:
        ```powershell
        cd C:\DockerLabs\diamol\ch08\exercises\numbers
        ```
    2.  Execute o comando de build especificando o Dockerfile v2:
        ```powershell
        docker image build -t diamol/ch08-numbers-api:v2 -f ./numbers-api/Dockerfile.v2 .
        ```
    3.  Observe a saída do build. Será similar aos builds anteriores, mas a imagem resultante `diamol/ch08-numbers-api:v2` agora tem a capacidade de health check embutida.

---

📄 Página 146

🔍 Resumo dos principais pontos:

*   Imagem v2 construída, pronta para rodar com health check.
*   Configuração do `HEALTHCHECK` (intervalo, tentativas) pode ser ajustada.
    *   Padrões: checa a cada 30s, marca como `unhealthy` após 3 falhas seguidas.
*   **Exercício Try-it-now:** Rodar e testar o contêiner v2 com health check.
    *   1. Rodar contêiner v2: `docker run -d -p 8081:80 diamol/ch08-numbers-api:v2`.
    *   2. Esperar ~30s e listar contêineres: `docker container ls`. Status deve incluir `(healthy)`.
    *   3. Chamar API 4 vezes via `curl` (3 OK, 1 Falha).
    *   4. Esperar ~90s (3x 30s) e listar contêineres de novo: `docker container ls`. Status deve mudar para `(unhealthy)`.
*   Figura 8.2: Mostra a saída com status mudando de `healthy` para `unhealthy`.

🧠 Explicação detalhada e didática:

*   Agora que a imagem `v2` com `HEALTHCHECK` foi construída, esta página mostra como ela se comporta em tempo de execução.
*   O Docker Engine automaticamente detecta a instrução `HEALTHCHECK` na imagem e começa a executar o comando de teste nos intervalos padrão (a cada 30 segundos).
*   O comando `docker ps` (ou `docker container ls`) agora exibe o estado de saúde entre parênteses no final da coluna `STATUS`:
    *   `(starting)`: O contêiner iniciou, mas o primeiro health check ainda não rodou ou ainda não passou no `start_period` (ver p. 157).
    *   `(healthy)`: O último health check foi bem-sucedido.
    *   `(unhealthy)`: O health check falhou o número configurado de vezes consecutivas.
*   O exercício demonstra exatamente isso:
    *   Ao iniciar o contêiner `v2`, ele começa como `(healthy)`.
    *   Após causar o bug na API (chamando-a 4 vezes), os health checks subsequentes (que rodam a cada 30s) começarão a falhar.
    *   Após 3 falhas consecutivas (aproximadamente 90 segundos depois de causar o bug), o status do contêiner em `docker ps` mudará para `(unhealthy)`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Remover Contêiner v1 (se ainda existir):**
    ```powershell
    docker rm -f api-v1
    ```
2.  **Rodar API v2:**
    ```powershell
    docker container run -d -p 8081:80 --name api-v2 diamol/ch08-numbers-api:v2
    ```
3.  **Verificar Status Inicial:** Espere uns 30 segundos.
    ```powershell
    docker ps
    # Procure por api-v2. STATUS deve terminar com '(healthy)'
    ```
4.  **Causar o Bug:**
    ```powershell
    Invoke-WebRequest http://localhost:8081/rng # Repetir 4 vezes
    ```
5.  **Verificar Status Final:** Espere cerca de 90 segundos (ou um pouco mais).
    ```powershell
    docker ps
    # Procure por api-v2. STATUS agora deve terminar com '(unhealthy)'
    ```

---

📄 Página 147

🔍 Resumo dos principais pontos:

*   Figura 8.2: Mostra a mudança de status para `unhealthy`.
*   Status `unhealthy` é publicado como evento pela API Docker -> plataforma orquestradora é notificada e pode agir.
*   Docker também grava resultado dos últimos health checks. Pode ser visto com `docker inspect`.
*   **Exercício Try-it-now:** Inspecionar o contêiner `unhealthy` para ver detalhes do health check.
    *   Comando usa `docker ls --last 1 ...` para pegar ID do último contêiner (o `v2`).
    *   Comando: `docker container inspect $(docker container ls --last 1 --format '{{.ID}}')`.
*   Saída do `inspect`: Procurar pela seção `State.Health`.
    *   Contém `Status` (healthy/unhealthy), `FailingStreak` (número de falhas seguidas), e `Log` (histórico das últimas verificações com saída/erro).
*   Figura 8.3: Mostra trecho do `inspect` com detalhes do health check falhando (HTTP 500).

🧠 Explicação detalhada e didática:

*   A página explica o que acontece quando um contêiner é marcado como `unhealthy`. O Docker Engine não apenas muda o status visível em `docker ps`, mas também dispara um evento interno e armazena informações detalhadas sobre as falhas.
*   Essas informações podem ser acessadas usando `docker container inspect <container>`. Dentro do grande JSON de saída, a seção `State.Health` contém o histórico recente das execuções do comando `HEALTHCHECK`, incluindo o código de saída e a saída de texto (stdout/stderr) do comando de teste. Isso é muito útil para diagnosticar por que um health check está falhando.
*   O exercício mostra como usar `docker inspect` para ver esses detalhes no contêiner `api-v2` que ficou `unhealthy`. A sintaxe `$(docker container ls --last 1 --format '{{.ID}}')` é um truque para obter automaticamente o ID do último contêiner iniciado, evitando ter que copiá-lo manualmente.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Inspecionar Contêiner:** (Certifique-se que `api-v2` foi o último iniciado e está `unhealthy`)
    ```powershell
    docker container inspect $(docker container ls --last 1 --format '{{.ID}}')
    ```
2.  **Analisar Saída:** Role a longa saída JSON até encontrar a chave `"State"` e dentro dela, a chave `"Health"`. Examine os campos:
    *   `"Status"`: Deve ser `"unhealthy"`.
    *   `"FailingStreak"`: Deve ser 3 ou mais.
    *   `"Log"`: Será um array mostrando as últimas execuções do `curl`. As mais recentes devem indicar um código de saída diferente de 0 e talvez uma mensagem de erro HTTP 500 na saída (Output).

---

📄 Página 148

🔍 Resumo dos principais pontos:

*   Figura 8.3: Mostra detalhes do health check falho no `inspect`.
*   **Pergunta:** Por que o Docker *não* reinicia ou substitui o contêiner `unhealthy` automaticamente?
*   **Resposta:** Em um *único* servidor, Docker Engine não pode fazer isso com segurança.
    *   Reiniciar contêiner = downtime do app.
    *   Recriar contêiner = downtime + **perda de dados** (se app escreveu na camada gravável).
    *   Docker não sabe se a ação corretiva pioraria a situação.
*   **Comportamento Padrão:** Docker apenas *notifica* (via status e eventos) que está `unhealthy`, mas o deixa rodando.
*   Health check continua rodando; se falha for temporária e check passar depois, status volta para `healthy`.
*   **Benefício Real em Cluster (Swarm/Kubernetes):**
    *   Orquestrador é notificado sobre `unhealthy`.
    *   Pode agir com segurança: iniciar contêiner de substituição *antes* de parar o doente (se houver capacidade no cluster).
    *   Evita downtime da aplicação.
*   **8.2 Starting containers with dependency checks** (Iniciando contêineres com verificações de dependência)
    *   Health check: teste *contínuo* para manter app rodando.
    *   Clusters lidam com falhas temporárias iniciando novos contêineres.
    *   **Novo Desafio em Cluster:** Ordem de inicialização não é garantida. Contêiner pode iniciar *antes* de suas dependências (outros contêineres/serviços) estarem prontas.
    *   Exemplo: App Web (`numbers-web`) depende da API (`numbers-api`). Web pode iniciar antes da API.

🧠 Explicação detalhada e didática:

*   A página responde a uma pergunta importante: se o Docker sabe que o contêiner está `unhealthy`, por que ele não o conserta (reinicia/substitui)? A razão é que, rodando em uma **única máquina**, o Docker Engine não tem contexto suficiente para fazer isso sem potencialmente causar mais problemas (interrupção do serviço ou perda de dados). Ele adota uma abordagem conservadora: sinaliza o problema, mas não age.
*   O verdadeiro poder do `HEALTHCHECK` se manifesta em um **ambiente orquestrado (cluster)**. O orquestrador (Swarm ou Kubernetes) *pode* tomar ações corretivas com segurança, como iniciar uma réplica substituta em outro nó do cluster antes de remover a réplica doente, garantindo a continuidade do serviço.
*   A seção 8.2 introduz um problema relacionado, também mais pronunciado em clusters: **dependências de inicialização**. Em um cluster, você não pode garantir que o contêiner da sua API estará pronto antes do contêiner da sua aplicação web que a consome. O que acontece se a web iniciar primeiro?

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo, mas a explicação sobre a limitação do Docker Engine vs. Orquestrador é fundamental. O `HEALTHCHECK` que você configurou é útil mesmo no Docker Desktop (para saber o estado), mas a *ação automática* só ocorreria em um Swarm ou Kubernetes.

💡 Analogias ou observações úteis (se houver):

*   Docker Engine (sozinho) vendo `unhealthy`: É como um médico de plantão que diagnostica um paciente gravemente doente em casa, mas não tem recursos para operar ali mesmo. Ele apenas informa a condição.
*   Orquestrador (cluster) vendo `unhealthy`: É como um hospital. O médico diagnostica, e o hospital tem recursos (outras salas, médicos, enfermeiros) para mover o paciente, operar, e garantir que o atendimento continue sem interrupção.

---

📄 Página 149

🔍 Resumo dos principais pontos:

*   O que acontece se app iniciar antes de suas dependências depende do app. App `numbers-web` não lida bem.
*   **Exercício Try-it-now:** Demonstrar falha por dependência ausente.
    *   1. Remover todos os contêineres rodando: `docker container rm -f $(docker container ls -aq)`.
    *   2. Rodar *apenas* o contêiner web (`numbers-web`): `docker run -d -p 8082:80 diamol/ch08-numbers-web`.
    *   3. Verificar status: `docker container ls` (estará `Up`).
    *   4. Acessar `http://localhost:8082` no navegador. App carrega, mas botão "random number" dará erro (Figura 8.4).

🧠 Explicação detalhada e didática:

*   A página explica que o comportamento de uma aplicação quando suas dependências não estão disponíveis varia. Algumas podem ter lógica interna para esperar ou tentar reconectar, mas muitas (como o exemplo `numbers-web`) simplesmente assumem que as dependências estarão lá e falham quando tentam usá-las.
*   O exercício demonstra isso:
    1.  Garante-se que nenhum contêiner (especialmente a API `numbers-api`) esteja rodando.
    2.  Inicia-se *apenas* o contêiner da aplicação web (`numbers-web`).
    3.  O contêiner inicia normalmente (o processo web server está rodando), então `docker ps` o mostra como `Up`.
    4.  No entanto, ao tentar usar a funcionalidade principal da aplicação (clicar no botão que depende da API), ocorre um erro porque a API não está disponível para ser contatada.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ações principais:**
    ```powershell
    # 1. Limpar containers anteriores (incluindo api-v2)
    docker container rm -f $(docker container ls -aq)

    # 2. Rodar SÓ o web app (sem a API)
    docker container run -d -p 8082:80 --name web-no-api diamol/ch08-numbers-web

    # 3. Verificar status (estará 'Up')
    docker ps

    # 4. Testar no Navegador
    # Abra http://localhost:8082. A página carrega.
    # Clique no botão "Get Random Number >>". Você verá uma mensagem de erro.
    ```

💡 Analogias ou observações úteis (se houver):

*   É como tentar usar seu telefone (`web-no-api`) para ligar para um amigo (`numbers-api`) quando a torre de celular do amigo está desligada. Seu telefone funciona (`Up`), mas a chamada (`clicar no botão`) falha.

---

📄 Página 150

🔍 Resumo dos principais pontos:

*   Figura 8.4: Mostra o erro na UI web quando a API está indisponível.
*   Situação indesejada: Contêiner `Up`, mas app inutilizável.
*   Alguns apps verificam dependências na inicialização, mas muitos não (como `numbers-web`).
*   **Solução: Adicionar verificação de dependência *na inicialização* do contêiner.**
    *   Diferente de health check (contínuo), dependency check roda *antes* do app iniciar.
    *   Garante que tudo que o app precisa está disponível.
    *   Se OK -> app inicia.
    *   Se Falha -> check falha -> contêiner *sai* (`Exited`).
*   Docker não tem instrução como `HEALTHCHECK` para isso, mas pode-se colocar a lógica no **comando de inicialização** (`CMD` ou `ENTRYPOINT`).

🧠 Explicação detalhada e didática:

*   A página critica a situação onde um contêiner parece saudável (`Up`) mas a aplicação está quebrada devido a uma dependência ausente.
*   A solução proposta é implementar uma **Verificação de Dependência (Dependency Check)** que rode *antes* da aplicação principal ser iniciada dentro do contêiner.
*   **Fluxo do Dependency Check:**
    1.  O comando de inicialização do contêiner (`CMD` ou `ENTRYPOINT`) primeiro executa o script/comando de verificação.
    2.  Esse script tenta contatar/verificar todas as dependências essenciais (outros serviços, bancos de dados, etc.).
    3.  Se todas as dependências responderem OK, o script termina com sucesso (código 0).
    4.  Se alguma dependência falhar, o script termina com erro (código diferente de 0).
    5.  O comando de inicialização é encadeado (ex: com `&&`) para que a aplicação principal *só* seja iniciada se o script de verificação terminar com sucesso (código 0). Se o script falhar, a aplicação principal nunca inicia, e o contêiner termina (entra em `Exited`).
*   Isso implementa um padrão "fail-fast" (falhar rapidamente).

💻 Exemplos práticos (para meu ambiente Windows):

*   A próxima página mostrará como implementar isso no `CMD` do Dockerfile usando `curl` e `&&`.
*   Em PowerShell, o encadeamento `&&` não existe nativamente da mesma forma que no `cmd` ou `sh`. Dentro de um `CMD` de Dockerfile Windows, pode-se usar `&&`. Se fosse um script PowerShell, seria mais comum usar tratamento de erro explícito (`try/catch` ou checar `$?` ou `$LASTEXITCODE`).

💡 Analogias ou observações úteis (se houver):

*   Dependency Check: É como um piloto fazendo o checklist pré-voo *antes* de ligar os motores principais. Se algum item crítico falhar no checklist (ex: combustível baixo, asa faltando), o piloto não decola (contêiner sai). Se tudo estiver OK, ele prossegue para a decolagem (inicia o app).

---

📄 Página 151

🔍 Resumo dos principais pontos:

*   **Listagem 8.2:** Estágio final de um Dockerfile (`Dockerfile.v2`) para `numbers-web` com dependency check no `CMD`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENV RngApi:Url=http://numbers-api/rng # App usa esta ENV para saber URL da API
    # CMD agora tem duas partes, ligadas por &&
    CMD curl --fail http://numbers-api/rng && \ # 1. Verifica se API responde
        dotnet Numbers.Web.dll # 2. SÓ SE curl OK, roda o app .NET
    WORKDIR /app
    COPY --from=builder /out/ .
    ```
*   **Análise do `CMD`:**
    *   Usa `curl --fail` para fazer uma requisição à API no nome DNS `numbers-api`.
    *   `&&` (operador E lógico de shell): Executa o segundo comando *somente se* o primeiro (`curl`) for bem-sucedido (retornar código 0).
    *   Se `curl` falhar (API indisponível -> HTTP erro -> `curl --fail` retorna != 0), o comando `dotnet` *não é executado*, `CMD` termina, contêiner sai.
    *   Se `curl` sucesso (API disponível -> HTTP 200 -> `curl --fail` retorna 0), o comando `dotnet` é executado, iniciando a aplicação web. Docker então monitora o processo `dotnet`.
*   **Exercício Try-it-now:** Rodar contêiner da imagem v2 (`diamol/ch08-numbers-web:v2`). API ainda não está rodando.
    *   Comandos:
        *   `docker container run -d -p 8084:80 diamol/ch08-numbers-web:v2`
        *   `docker container ls --all` (verificar que saiu).

🧠 Explicação detalhada e didática:

*   A Listagem 8.2 mostra a implementação prática do dependency check diretamente na instrução `CMD` do Dockerfile para a aplicação web.
*   O `CMD` agora consiste em dois comandos unidos pelo operador `&&`:
    1.  `curl --fail http://numbers-api/rng`: Tenta acessar a API. Se a API estiver rodando e responder OK, este comando termina com sucesso (código 0). Se a API não estiver acessível ou retornar erro, o comando falha (código != 0).
    2.  `dotnet Numbers.Web.dll`: O comando para iniciar a aplicação web .NET.
*   Devido ao `&&`, o `dotnet` só será executado se o `curl` for bem-sucedido. Isso garante que a aplicação web só inicie se sua dependência (a API) estiver respondendo.
*   O exercício pede para rodar um contêiner desta nova imagem `v2` *sem* ter a API rodando. A expectativa é que o `curl` dentro do `CMD` falhe, o `dotnet` não seja executado, e o contêiner termine quase imediatamente, aparecendo como `Exited` no `docker ps -a`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Pré-requisito:** Construir a imagem v2 do web app (similar à p.145 para a API):
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch08\exercises\numbers
    docker image build -t diamol/ch08-numbers-web:v2 -f ./numbers-web/Dockerfile.v2 .
    ```
*   **Ação principal (Testar Dependency Check):**
    ```powershell
    # Garantir que API NÃO está rodando (se api-v2 ainda existe, pare/remova)
    # docker stop api-v2 && docker rm api-v2

    # Rodar web app v2
    docker container run -d -p 8084:80 --name web-v2-fail diamol/ch08-numbers-web:v2

    # Esperar alguns segundos e verificar TODOS os containers
    Start-Sleep -Seconds 5
    docker container ls --all
    ```
    Procure pelo contêiner `web-v2-fail`. O STATUS deve ser `Exited (...)` com um código de erro (não 0). O tempo em `STATUS` (ex: `Exited (1) 4 seconds ago`) será muito curto.

---

📄 Página 152

🔍 Resumo dos principais pontos:

*   Figura 8.5: Mostra a saída do exercício anterior, com contêiner v2 `Exited` rapidamente, enquanto v1 (sem check) continua `Up` mas quebrado.
*   **Fail-fast é melhor:** Contêiner `Exited` é preferível a `Up` mas não funcional.
*   Em ambientes escalados (cluster), contêiner `Exited` pode ser automaticamente substituído/reagendado pela plataforma.
    *   Ex: API demora a iniciar. Web inicia -> check falha -> web sai -> web é reagendado -> na segunda tentativa, API está pronta -> web inicia com sucesso.
*   Com health e dependency checks, app torna-se "bom cidadão" na plataforma.
*   Checks até agora foram básicos (HTTP com `curl`).
    *   Simplista, depende de ferramenta externa (`curl`).
*   **8.3 Writing custom utilities for application check logic** (Escrevendo utilitários customizados para lógica de verificação da aplicação)
    *   `curl` é útil, mas adiciona dependência extra na imagem que o app não precisa para rodar.
    *   Viola princípio de imagem mínima (Cap. 4). Aumenta tamanho, superfície de ataque, frequência de updates da imagem base.
    *   **Melhor abordagem:** Escrever utilitário de verificação na *mesma linguagem* do app (Java para app Java, etc.).

🧠 Explicação detalhada e didática:

*   A página argumenta que o comportamento "fail-fast" (falhar e sair rapidamente) do contêiner `v2` é, na verdade, mais desejável do que o contêiner `v1` que fica rodando (`Up`) mas está funcionalmente quebrado. Em um ambiente orquestrado, a plataforma pode detectar o contêiner `Exited` e tentar reiniciá-lo ou criar um substituto, potencialmente resolvendo o problema se a dependência estiver disponível na segunda tentativa.
*   No entanto, depender de uma ferramenta externa como `curl` para os checks não é ideal por razões de otimização e segurança (princípio da imagem mínima).
*   A seção 8.3 propõe uma solução mais robusta: criar pequenos **utilitários customizados**, escritos na mesma linguagem da aplicação principal, para realizar as verificações de saúde e dependência.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. A ideia é que, em vez de `HEALTHCHECK CMD curl ...` ou `CMD curl ... && dotnet ...`, você teria algo como:
    *   `HEALTHCHECK CMD ["dotnet", "MeuHealthCheckUtil.dll", "--url", "http://localhost/health"]`
    *   `CMD dotnet MeuDependencyCheckUtil.dll --url http://numbers-api/rng && dotnet MinhaAppPrincipal.dll`
    *   Onde `MeuHealthCheckUtil.dll` e `MeuDependencyCheckUtil.dll` seriam pequenos programas .NET compilados e incluídos na imagem junto com a aplicação principal.

💡 Analogias ou observações úteis (se houver):

*   Usar `curl`: É como pedir ao carteiro (ferramenta externa) para verificar se a loja (dependência) está aberta antes de você ir lá.
*   Usar utilitário customizado: É como você mesmo ter um pequeno app no seu telefone (utilitário na mesma linguagem) que verifica o status da loja online antes de você sair de casa. Você não depende do carteiro e usa a mesma tecnologia que já domina.

---

📄 Página 153

🔍 Resumo dos principais pontos:

*   **Vantagens de Utilitários Customizados:**
    1.  **Menos Dependências na Imagem:** Não precisa instalar `curl` ou outras ferramentas; utilitário usa runtime já presente para o app.
    2.  **Lógica Complexa:** Permite verificações mais sofisticadas (retentativas, condicionais) difíceis em scripts shell, especialmente cross-platform (Linux/Windows).
    3.  **Reuso de Config:** Utilitário pode ler a *mesma* configuração do app (ex: URLs de dependências), evitando duplicação e dessincronização.
    4.  **Testes Abrangentes:** Pode verificar conexões de BD, existência de arquivos (certificados, etc.) usando as mesmas bibliotecas do app.
*   Utilitários podem ser genéricos (ex: um checador HTTP reutilizável).
*   Autor criou utilitário HTTP em .NET Core para usar nos exemplos.
*   **Build com Utilitários:** Dockerfile multi-estágio pode ter:
    *   Estágio para compilar app.
    *   Estágio para compilar utilitário.
    *   Estágio final copia ambos (app + utilitário).
*   Diagrama (Figura 8.6): Ilustra fluxo multi-estágio com compilação separada do app e do utilitário.

🧠 Explicação detalhada e didática:

*   A página lista as vantagens significativas de escrever seus próprios utilitários para health/dependency checks:
    *   **Imagens Menores/Mais Seguras:** Você não precisa adicionar ferramentas extras como `curl` à sua imagem final. O utilitário usa o mesmo runtime (.NET, Java, Node.js) que sua aplicação já requer.
    *   **Checks Mais Inteligentes:** É mais fácil escrever lógica complexa (ex: "tente conectar à API 3 vezes com intervalo de 5 segundos antes de falhar") em C#, Java, Python, etc., do que em um script shell, especialmente se precisar funcionar em Windows e Linux.
    *   **Configuração Consistente:** O utilitário pode ler as mesmas variáveis de ambiente ou arquivos de configuração que a aplicação principal usa para descobrir os endereços das dependências. Isso evita ter que definir a mesma URL em dois lugares (no check e no app).
    *   **Verificações Profundas:** O utilitário pode ir além de um simples ping HTTP e usar as bibliotecas da linguagem para, por exemplo, tentar autenticar no banco de dados ou verificar a validade de um certificado.
*   O autor menciona que criou um utilitário genérico de checagem HTTP em .NET Core para os próximos exemplos.
*   A Figura 8.6 mostra como integrar a compilação desse utilitário no build multi-estágio: um estágio compila o app principal, outro compila o utilitário, e o estágio final copia ambos para a imagem de runtime.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você não precisará escrever o utilitário, ele já estará compilado e será copiado nos Dockerfiles `v3` dos exemplos.
*   O Dockerfile `v3` (próxima página) seguirá o padrão da Figura 8.6.

---

📄 Página 154

🔍 Resumo dos principais pontos:

*   **Listagem 8.3:** Estágio final do Dockerfile `v3` para a API (`Dockerfile.v3`), usando utilitário customizado (`Utilities.HttpCheck.dll`) no `HEALTHCHECK`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENTRYPOINT ["dotnet", "Numbers.Api.dll"]
    # HEALTHCHECK agora chama o utilitário .NET
    HEALTHCHECK CMD ["dotnet", "Utilities.HttpCheck.dll", "-u", "http://localhost/health"]
    WORKDIR /app
    COPY --from=http-check-builder /out/ . # Copia utilitário compilado
    COPY --from=builder /out/ . # Copia app compilado
    ```
*   Comportamento similar ao `curl`, mas sem precisar dele na imagem. Logs de health check no `inspect` serão menos verbosos.
*   App ainda deve reportar `healthy` inicialmente, depois `unhealthy` após bug ser ativado.
*   **Exercício Try-it-now:** Rodar e testar API v3 com utilitário customizado.
    *   1. Remover contêineres existentes.
    *   2. Rodar contêiner v3, especificando intervalo de health check menor (`--health-interval 5s`).
        *   `docker run -d -p 8080:80 --health-interval 5s diamol/ch08-numbers-api:v3`.
    *   3. Esperar ~5s, verificar `docker ps` (deve estar `healthy`).
    *   4. Chamar API 4x via `curl`.
    *   5. Esperar ~15s (3x 5s), verificar `docker ps` de novo (deve estar `unhealthy`).

🧠 Explicação detalhada e didática:

*   A Listagem 8.3 mostra como a instrução `HEALTHCHECK` é modificada para usar o utilitário .NET customizado (`Utilities.HttpCheck.dll`) em vez do `curl`. O comando agora executa o runtime `dotnet` com o DLL do utilitário e passa a URL a ser verificada (`-u ...`). Note também as duas instruções `COPY --from=...` no final, trazendo os resultados dos estágios de build do app e do utilitário para a imagem final.
*   O comportamento esperado é o mesmo da versão com `curl`, mas a imagem final não contém mais a dependência do `curl`.
*   O exercício instrui a rodar a imagem `v3` da API. A flag `--health-interval 5s` é adicionada ao `docker run` para *sobrescrever* o intervalo padrão (30s) e fazer os health checks rodarem a cada 5 segundos, acelerando o teste. (Outras opções como `--health-retries`, `--health-timeout`, `--health-start-period` também podem ser usadas no `docker run` para customizar o health check em tempo de execução). O teste segue a mesma lógica: verificar se fica `healthy`, causar o bug, verificar se fica `unhealthy`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Pré-requisito:** Construir a imagem v3 da API (similar à p.145, mas usando `Dockerfile.v3`):
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch08\exercises\numbers
    docker image build -t diamol/ch08-numbers-api:v3 -f ./numbers-api/Dockerfile.v3 .
    ```
*   **Ações principais:**
    ```powershell
    # 1. Limpar containers
    docker container rm -f $(docker container ls -aq)

    # 2. Rodar API v3 com intervalo customizado
    docker container run -d -p 8080:80 --health-interval 5s --name api-v3 diamol/ch08-numbers-api:v3

    # 3. Verificar status inicial (após ~5s)
    Start-Sleep -Seconds 5
    docker ps # Deve mostrar api-v3 (healthy)

    # 4. Causar bug (chamar 4x)
    Invoke-WebRequest http://localhost:8080/rng # Repetir 4 vezes

    # 5. Verificar status final (após ~15s)
    Start-Sleep -Seconds 15
    docker ps # Deve mostrar api-v3 (unhealthy)
    ```

---

📄 Página 155

🔍 Resumo dos principais pontos:

*   Figura 8.7: Mostra a saída do exercício anterior, com v3 mudando para `unhealthy`.
*   Utilitário HTTP é flexível (muitas opções). Usado também para dependency check no `Dockerfile.v3` do web app.
*   **Listagem 8.4:** Estágio final do Dockerfile v3 para `numbers-web`, usando utilitário no `CMD`.
    ```dockerfile
    FROM diamol/dotnet-aspnet
    ENV RngApi:Url=http://numbers-api/rng
    # CMD usa utilitário para checar API antes de rodar app
    CMD dotnet Utilities.HttpCheck.dll -c RngApi:Url -t 900 && \
        dotnet Numbers.Web.dll
    WORKDIR /app
    COPY --from=http-check-builder /out/ . # Copia utilitário
    COPY --from=builder /out/ . # Copia app
    ```
*   **Análise do `CMD`:**
    *   Chama `Utilities.HttpCheck.dll`.
    *   `-c RngApi:Url`: Diz ao utilitário para ler o URL da API (`http://numbers-api/rng`) da *mesma configuração* que o app usa (via `ENV RngApi:Url`).
    *   `-t 900`: Define um timeout (em segundos? milissegundos? - *Nota: precisa verificar a doc do utilitário, mas provavelmente ms*) para a checagem.
    *   `&& dotnet ...`: Inicia app principal só se check passar.
*   Remove necessidade de `curl` na imagem web também. Comportamento similar (sai se API não disponível).
*   **Exercício Try-it-now:** Rodar web app v3 (sem a API rodando). Esperado: sair rapidamente.
    *   Comandos:
        *   `docker run -d -p 8081:80 diamol/ch08-numbers-web:v3`
        *   `docker container ls --all`

🧠 Explicação detalhada e didática:

*   A página mostra como o mesmo utilitário customizado (`Utilities.HttpCheck.dll`) pode ser usado também para a **verificação de dependência** na inicialização do contêiner web (`numbers-web`).
*   A Listagem 8.4 apresenta o `CMD` modificado no `Dockerfile.v3` do web app. Em vez de `curl`, ele agora chama `dotnet Utilities.HttpCheck.dll`.
*   **Vantagens demonstradas:**
    *   Não precisa mais do `curl` na imagem.
    *   Reuso de configuração: A flag `-c RngApi:Url` instrui o utilitário a obter o endereço da API (`http://numbers-api/rng`) lendo a mesma variável de ambiente (`ENV RngApi:Url`) que a aplicação principal usa. Isso evita definir a URL em dois lugares.
    *   Flexibilidade: A flag `-t 900` (provavelmente 900ms) adiciona um timeout à verificação.
*   O comportamento "fail-fast" é mantido: se o check falhar, o `&&` impede a execução do `dotnet Numbers.Web.dll`, e o contêiner sai.
*   O exercício pede para rodar esta imagem `v3` do web app (que você precisará construir primeiro) sem a API, para confirmar que ela ainda sai rapidamente devido à falha no dependency check.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Pré-requisito:** Construir a imagem v3 do web app:
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch08\exercises\numbers
    docker image build -t diamol/ch08-numbers-web:v3 -f ./numbers-web/Dockerfile.v3 .
    ```
*   **Ação principal (Testar Dependency Check v3):**
    ```powershell
    # Garantir que API NÃO está rodando
    # docker stop api-v3 && docker rm api-v3 (se ainda existir)

    # Rodar web app v3
    docker container run -d -p 8081:80 --name web-v3-fail diamol/ch08-numbers-web:v3

    # Esperar alguns segundos e verificar TODOS os containers
    Start-Sleep -Seconds 5
    docker container ls --all
    ```
    Procure por `web-v3-fail`. O STATUS deve ser `Exited (...)` com um código de erro, e o tempo em `STATUS` deve ser curto.

---

📄 Página 156

🔍 Resumo dos principais pontos:

*   Figura 8.8: Mostra a saída do exercício anterior (web v3 `Exited`). API v3 (de antes) ainda rodando mas `unhealthy`.
*   Web v3 falhou ao procurar `numbers-api` via DNS, pois contêiner `api-v3` não foi nomeado assim. Mesmo se tivesse achado, API estava `unhealthy`.
*   **Benefício do utilitário customizado:** Torna imagem portável. Lógica de check está na imagem, funciona igual em Compose, Swarm, Kubernetes (que têm formas diferentes de *declarar* checks).
*   **8.4 Defining health checks and dependency checks in Docker Compose** (Definindo health checks e dependency checks no Docker Compose)
    *   Relembra: Contêiner sair quando dependência falha é bom para auto-recuperação em cluster.
    *   Docker Compose *pode* ajudar com confiabilidade, mas limitado (single host).
    *   Compose *não* substitui contêineres `unhealthy` automaticamente (mesma razão do Engine: risco de downtime/perda de dados).
    *   Mas Compose pode:
        *   Configurar contêineres para reiniciar se saírem (`restart: on-failure`).
        *   *Adicionar* um health check a um serviço cuja imagem *não* tenha `HEALTHCHECK` embutido.

🧠 Explicação detalhada e didática:

*   A página explica por que o `web-v3` falhou (não encontrou a API pelo nome DNS esperado) e reforça um benefício chave dos utilitários customizados: eles encapsulam a lógica de verificação *dentro* da imagem, tornando-a mais independente da plataforma de execução. Diferentes orquestradores (Compose, Swarm, K8s) têm sintaxes diferentes para *definir* health checks, mas se a *lógica* do check já está na sua imagem (via `HEALTHCHECK` ou `CMD` chamando seu utilitário), ela funcionará consistentemente em todos eles.
*   A seção 8.4 volta ao **Docker Compose** e como ele pode interagir com health/dependency checks.
*   **Limitação Reiterada:** Compose em um host único não faz substituição automática de contêineres `unhealthy`.
*   **O que Compose Pode Fazer:**
    1.  **Política de Reinício (`restart:`):** Você pode instruir o Compose a reiniciar automaticamente um contêiner se ele parar (sair). `restart: on-failure` reinicia apenas se o contêiner saiu com um código de erro (não 0). `restart: always` reinicia sempre que parar (a menos que parado manualmente via `docker stop` ou `docker-compose stop`). `restart: unless-stopped` é similar a `always`, mas não reinicia se foi parado manualmente. Isso ajuda a recuperar contêineres que falharam no dependency check.
    2.  **Definir/Sobrescrever Healthcheck:** O Compose permite definir um `healthcheck` na configuração do serviço no `.yml`. Se a imagem já tem `HEALTHCHECK`, a definição no Compose pode *sobrescrever* parâmetros como intervalo, retries, etc. Se a imagem *não* tem `HEALTHCHECK`, a definição no Compose *adiciona* um health check ao contêiner em tempo de execução.

💻 Exemplos práticos (para meu ambiente Windows):

*   A próxima página mostrará como usar `healthcheck:` e `restart:` no `docker-compose.yml`.
*   Exemplo de política de reinício:
    ```yaml
    services:
      my-service:
        image: my-image
        restart: on-failure # Reinicia se container sair com erro
    ```

---

📄 Página 157

🔍 Resumo dos principais pontos:

*   **Listagem 8.5:** Serviço API (`numbers-api`) no `docker-compose.yml` (em `ch08/exercises/numbers/`), usando imagem `v3` (com `HEALTHCHECK` embutido) e *customizando* parâmetros do health check via Compose.
    ```yaml
    numbers-api:
      image: diamol/ch08-numbers-api:v3
      ports:
        - "8087:80"
      healthcheck: # Sobrescreve/define parâmetros do healthcheck
        interval: 5s # Intervalo entre checks
        timeout: 1s # Tempo máximo para check rodar
        retries: 2 # Nº de falhas seguidas para ficar unhealthy
        start_period: 5s # Espera inicial antes do 1º check (dá tempo p/ app iniciar)
      networks:
        - app-net
    ```
*   Explica os parâmetros de `healthcheck` no Compose: `interval`, `timeout`, `retries`, `start_period`.
*   Balancear frequência vs. overhead (checks consomem recursos).
*   Pode-se *adicionar* health check no Compose para imagens sem `HEALTHCHECK`.
*   **Listagem 8.6:** Serviço Web (`numbers-web`) no mesmo Compose file, usando imagem `v3` (com check de dependência no `CMD`), adicionando `restart: on-failure` e definindo um `healthcheck` via Compose.
    ```yaml
    numbers-web:
      image: diamol/ch08-numbers-web:v3
      restart: on-failure # Reinicia se sair com erro (ex: falha no dependency check)
      ports:
        - "8088:80"
      healthcheck: # Adiciona healthcheck via Compose
        test: ["CMD", "dotnet", "Utilities.HttpCheck.dll", "-t", "150"] # Comando do check
        interval: 5s
        timeout: 1s
        retries: 2
        start_period: 10s
      networks:
        - app-net
    ```
*   Campo `test:` no `healthcheck` do Compose define o comando a ser executado (se imagem não tiver `HEALTHCHECK` ou se quiser sobrescrever o comando). Formato `["CMD", "comando", "arg1"...]` é um dos aceitos.

🧠 Explicação detalhada e didática:

*   As listagens mostram como configurar health checks e reinícios no `docker-compose.yml`.
*   **Listagem 8.5 (API):** A imagem `v3` já tem um `HEALTHCHECK CMD [...]`. A seção `healthcheck:` no Compose aqui *não* define o comando (`test:` está ausente), então ela usa o comando da imagem, mas *sobrescreve* os parâmetros de temporização (`interval`, `timeout`, `retries`, `start_period`). O `start_period` é útil para evitar que o health check falhe logo no início, enquanto a aplicação ainda está carregando.
*   **Listagem 8.6 (Web):**
    *   `restart: on-failure`: Instrui o Docker/Compose a reiniciar este contêiner automaticamente *se* ele sair com um código de erro (o que acontecerá se o dependency check no `CMD` falhar).
    *   `healthcheck:`: Como a imagem `web:v3` não tinha `HEALTHCHECK` no Dockerfile, esta seção *adiciona* um health check em tempo de execução. O campo `test:` especifica o comando a ser usado (neste caso, o mesmo utilitário .NET, mas sem a flag `-c` e com timeout diferente). Os outros campos (`interval`, etc.) definem a temporização.
*   **Combinação:** Agora o web app tem:
    1.  Um dependency check no `CMD` que o faz sair se a API não estiver pronta.
    2.  Uma política `restart: on-failure` que o reinicia se ele sair por causa do check.
    3.  Um health check adicionado pelo Compose para monitoramento contínuo após iniciar.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Arquivo Compose:** No diretório `C:\DockerLabs\diamol\ch08\exercises\numbers`, crie `docker-compose.yml` combinando Listagens 8.5 e 8.6 (mais `version:` e `networks:`).
    ```yaml
    version: '3.7'
    services:
      numbers-api:
        image: diamol/ch08-numbers-api:v3
        # ... (resto da Listagem 8.5) ...
        networks:
          - app-net
      numbers-web:
        image: diamol/ch08-numbers-web:v3
        # ... (resto da Listagem 8.6) ...
        networks:
          - app-net
    networks:
      app-net: # Cria rede gerenciada pelo Compose (não externa)
        name: numbers-app
    ```
    *(Nota: Usei `name: numbers-app` para a rede, em vez de `external: name: nat`)*

---

📄 Página 158

🔍 Resumo dos principais pontos:

*   Com `restart: on-failure` e dependency check, o sistema pode se auto-corrigir na inicialização.
    *   Compose pode iniciar em qualquer ordem (sem `depends_on`).
    *   Se Web iniciar antes, check falha, Web sai. API inicia. Web reinicia (devido a `restart:`), check passa, app funciona.
*   **Exercício Try-it-now:** Rodar o app completo com Compose e observar a auto-correção.
    *   Navegar para `ch08/exercises/numbers`.
    *   Remover contêineres existentes.
    *   Iniciar app: `docker-compose up -d`.
    *   Esperar ~5s, listar contêineres: `docker container ls`.
    *   Verificar logs do web: `docker container logs numbers_numbers-web_1`.

🧠 Explicação detalhada e didática:

*   A página explica o mecanismo de auto-correção que emerge da combinação do dependency check (que faz o contêiner sair se a dependência não está pronta) e da política `restart: on-failure` (que reinicia o contêiner que saiu com erro).
*   Como o `docker-compose.yml` neste exemplo *não* usa `depends_on`, o Compose pode iniciar os contêineres `numbers-api` e `numbers-web` em qualquer ordem, ou até simultaneamente.
*   **Cenário Provável:**
    1.  `numbers-web` inicia.
    2.  Seu dependency check (`dotnet Utilities.HttpCheck...`) roda.
    3.  `numbers-api` ainda não está pronto/respondendo.
    4.  O check falha, `numbers-web` sai com erro.
    5.  `numbers-api` termina de iniciar e fica pronto.
    6.  Docker/Compose detecta que `numbers-web` saiu com erro e o reinicia (devido a `restart: on-failure`).
    7.  `numbers-web` (reiniciado) roda seu dependency check novamente.
    8.  Desta vez, `numbers-api` está pronto, o check passa.
    9.  A aplicação web (`dotnet Numbers.Web.dll`) inicia.
*   O exercício instrui a rodar a aplicação com `docker-compose up -d` e verificar os logs do contêiner web para observar essa sequência de falha inicial e sucesso na retentativa.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Limpeza:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch08\exercises\numbers
    # Certifique-se que 'docker-compose.yml' existe aqui (criado na p.157)
    ```
3.  **Iniciar App:**
    ```powershell
    docker-compose up -d
    ```
4.  **Esperar e Listar:**
    ```powershell
    Start-Sleep -Seconds 10 # Dar tempo para possível reinício
    docker container ls
    # Ou docker-compose ps
    # Ambos os containers devem estar 'Up' e 'healthy' (após o start_period)
    ```
5.  **Verificar Logs Web:** (O nome do container será `numbers_numbers-web_1` por padrão)
    ```powershell
    docker container logs numbers_numbers-web_1
    ```
    Procure por mensagens indicando a execução do `HttpCheck`. Você *pode* ver uma falha inicial seguida por uma execução bem-sucedida (depende do timing exato da inicialização dos contêineres).

---

📄 Página 159

🔍 Resumo dos principais pontos:

*   Figura 8.9: Mostra a saída dos logs do web container, indicando falha inicial do check (timeout > 150ms) e sucesso na segunda tentativa (115ms) após reinício.
*   App agora funcional: Acessar `http://localhost:8088`.
*   Botão funciona (3 vezes), depois falha (bug da API). Figura 8.10 mostra sucesso.
*   **Por que dependency check no CMD vs. `depends_on` no Compose?**
    *   `depends_on`: Controla *ordem de início* dos contêineres, só funciona no Compose (single host).
    *   Dependency Check no CMD/ENTRYPOINT: Verifica se dependência está *realmente pronta* para responder. Funciona em qualquer plataforma (Compose, Swarm, K8s). Essencial para clusters onde ordem de início não é garantida.

🧠 Explicação detalhada e didática:

*   A página analisa a Figura 8.9, que confirma a sequência de auto-correção: o web container falhou na primeira tentativa de verificar a API (porque a API ainda estava iniciando), foi reiniciado automaticamente, e na segunda tentativa o check passou, permitindo que a aplicação web iniciasse corretamente.
*   Confirma que a aplicação está agora acessível em `http://localhost:8088` e funciona (pelas primeiras 3 requisições, até o bug da API ser ativado).
*   Finalmente, aborda uma questão importante: por que se dar ao trabalho de colocar o dependency check dentro do `CMD` da imagem, se o Docker Compose tem a opção `depends_on`?
    *   `depends_on`: Garante apenas que o contêiner B comece *depois* do contêiner A ter iniciado. **Não garante** que a *aplicação dentro* do contêiner A esteja pronta para receber conexões. Pode haver um tempo de inicialização interno do app.
    *   Dependency Check (no CMD/ENTRYPOINT): Verifica ativamente se a dependência está *respondendo*. É uma garantia mais forte.
    *   **Portabilidade:** `depends_on` é uma feature específica do Docker Compose. Orquestradores de cluster como Swarm e Kubernetes têm seus próprios mecanismos, mas não garantem ordem de início entre serviços diferentes da mesma forma. Colocar o check *dentro* da imagem (no `CMD` ou `ENTRYPOINT`) garante que essa lógica de verificação funcione consistentemente em *qualquer* ambiente onde o contêiner for executado.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Testar App:** Abra `http://localhost:8088`. Clique no botão. Deve funcionar (3 vezes). Na 4ª, deve dar erro.
2.  **(Opcional) Observar Recuperação da API:** Se você esperar o suficiente (dependendo do `health-interval` da API, ex: 15s+), a API `unhealthy` será eventualmente reiniciada pelo Docker Engine (se configurado com `restart:`) ou substituída em um cluster. Após a recuperação da API, o botão no `http://localhost:8088` voltaria a funcionar (por mais 3 vezes). *Nota: No Compose simples sem Swarm, a substituição de `unhealthy` não é automática.*

---

📄 Página 160

🔍 Resumo dos principais pontos:

*   Figura 8.10: Mostra a UI web funcionando (antes de ativar o bug).
*   **8.5 Understanding how checks power self-healing apps** (Entendendo como as verificações potencializam apps auto-reparáveis)
*   Apps distribuídos são complexos; tentador querer controlar ordem de início, mas não é ideal para escala.
*   Exemplo cluster: App com 20 APIs, 50 Webs. Forçar início de todas APIs antes das Webs é ineficiente e frágil (e se 1 API demorar?).
*   **Abordagem com Checks:**
    *   Plataforma inicia contêineres o mais rápido possível, sem ordem garantida entre serviços.
    *   Se contêiner não encontra dependência (dependency check falha) -> falha rápido (`Exited`).
    *   Se contêiner fica doente durante execução (health check falha) -> marcado como `unhealthy`.
    *   Plataforma (Swarm/K8s) detecta `Exited` ou `unhealthy` e reinicia/substitui o contêiner.
    *   App pode levar alguns minutos para estabilizar 100% em deploy grande, mas fica parcialmente online durante o processo.
*   Diagrama (Figura 8.11): Ciclo de vida de contêiner em cluster prod (Running -> Unhealthy -> Shutdown -> Starting -> Running).

🧠 Explicação detalhada e didática:

*   Esta seção resume a filosofia por trás do uso de health checks e dependency checks em ambientes orquestrados para alcançar a **auto-recuperação (self-healing)**.
*   A ideia principal é **abraçar a natureza dinâmica e potencialmente caótica** de um cluster. Em vez de tentar impor uma ordem de inicialização rígida (que é difícil e ineficiente em escala), você projeta seus contêineres para serem resilientes:
    *   Eles verificam suas próprias dependências na inicialização e **falham rapidamente** se algo estiver faltando.
    *   Eles expõem um health check que permite à plataforma monitorar sua saúde *real* durante a execução.
*   O **orquestrador** (Swarm/Kubernetes) então assume o papel de "curador":
    *   Ele lança todos os contêineres necessários.
    *   Se um contêiner sai (falha no dependency check), o orquestrador tenta iniciá-lo novamente (ou um substituto).
    *   Se um contêiner fica `unhealthy` (falha no health check), o orquestrador o remove e o substitui por um novo.
*   O resultado é um sistema que tende a se corrigir automaticamente de falhas transitórias (ex: dependência demorou a iniciar, bug temporário causou travamento). A Figura 8.11 ilustra esse ciclo de monitoramento e substituição.

💻 Exemplos práticos (para meu ambiente Windows):

*   Os conceitos aqui são mais relevantes para Swarm (Caps 12-14) e Kubernetes. No Docker Compose simples, você viu a parte do reinício com `restart: on-failure`, mas a substituição de `unhealthy` não acontece automaticamente.

💡 Analogias ou observações úteis (se houver):

*   Orquestração com Checks: É como um time de futebol onde cada jogador (contêiner) sabe sua função e a dos seus companheiros próximos (dependências). O técnico (orquestrador) coloca todos em campo. Se um jogador se machuca (unhealthy) ou não encontra a bola para começar (dependency fail), o técnico o substitui rapidamente, mantendo o time jogando. Ele não espera todos estarem perfeitamente posicionados antes de começar o jogo.

---

📄 Página 161

🔍 Resumo dos principais pontos:

*   **Cuidado com os Checks:**
    *   Health Checks (periódicos): Não devem ser muito pesados (CPU/recursos). Balancear frequência vs. overhead. Testar partes chave sem demorar muito.
    *   Dependency Checks (inicialização): Menos preocupação com recursos (só rodam uma vez), mas cuidado com o que verificar. Se dependência estiver fora do controle da plataforma (ex: serviço externo), falhar o contêiner pode não ajudar se a plataforma não puder consertar a dependência.
*   Lógica dos checks é a parte difícil; Docker facilita a execução. Checks corretos -> plataforma mantém app rodando.
*   **8.6 Lab** (Laboratório)
    *   App: Simulador de "Memory Hog" (consumidor de memória) em Node.js.
    *   **Tarefa:** Adicionar checks ao Dockerfile (`ch08/lab`).
        1.  **Dependency Check (Inicialização):** Verificar se há memória *suficiente* para rodar. Se não, sair.
        2.  **Health Check (Runtime):** A cada 5s, verificar se memória *alocada* (simulada) excedeu limite. Se sim, ficar `unhealthy`.
    *   Lógica dos testes já existe no script `memory-check.js`. Precisa conectar ao Dockerfile.
    *   Scripts e Dockerfile inicial na pasta `ch08/lab`.

🧠 Explicação detalhada e didática:

*   A página oferece conselhos importantes sobre como projetar boas verificações:
    *   **Health Checks:** Devem ser leves e rápidos, pois rodam repetidamente. Teste apenas o essencial para saber se o app está funcional.
    *   **Dependency Checks:** Podem ser mais completos, mas pense nas consequências. Se você verificar uma dependência externa (ex: uma API de terceiros) e ela estiver fora do ar, fazer seu contêiner sair e ser reiniciado continuamente não resolverá o problema. Verifique principalmente dependências *internas* que a plataforma *pode* gerenciar/reiniciar.
*   O **Lab do Capítulo 8** foca em implementar ambos os tipos de check em um Dockerfile para uma aplicação Node.js que simula consumir memória.
*   A lógica de verificação já está pronta em um script (`memory-check.js`). Sua tarefa é modificar o `Dockerfile` para:
    1.  Executar o `memory-check.js` com um parâmetro (provavelmente) para fazer a verificação inicial de memória disponível como parte do `CMD` ou `ENTRYPOINT`, antes de iniciar o app principal, e encadear com `&&`.
    2.  Adicionar uma instrução `HEALTHCHECK` que execute o mesmo `memory-check.js` com outro parâmetro (provavelmente) para verificar o uso atual de memória.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 8:**
    1.  Navegue para `C:\DockerLabs\diamol\ch08\lab`.
    2.  Examine `Dockerfile` e `memory-check.js`. O script provavelmente aceita argumentos para distinguir entre check inicial e health check.
    3.  **Modificar `Dockerfile`:**
        *   Encontre o `CMD` ou `ENTRYPOINT` atual que inicia o app Node.js (ex: `node server.js`).
        *   Altere-o para primeiro chamar o check inicial e depois o app, usando `&&`. Exemplo: `CMD ["node", "memory-check.js", "startup-check", "500"] && ["node", "server.js"]` (Argumentos `startup-check`, `500` são hipotéticos, veja o script real).
        *   Adicione uma instrução `HEALTHCHECK`. Configure o `interval` para 5s (ex: `--interval=5s`). O comando (`test:`) deve chamar o check de runtime. Exemplo: `HEALTHCHECK --interval=5s --retries=3 CMD ["node", "memory-check.js", "runtime-check", "1000"]` (Argumentos `runtime-check`, `1000` são hipotéticos).
    4.  **Construir e Testar:**
        *   `docker build -t lab8-memhog .`
        *   Rodar com limite de memória baixo (para falhar startup): `docker run --memory=10m lab8-memhog`. Verificar se sai (`docker ps -a`).
        *   Rodar com memória suficiente mas limite para health check: `docker run -d --memory=100m --name memhog-test lab8-memhog`. Verificar se fica `healthy` e depois `unhealthy` (`docker ps`). (Os limites de memória simulados no script podem não corresponder aos limites reais do Docker).

---

📄 Página 162

🔍 Resumo dos principais pontos:

*   **Nota sobre Simulação de Memória:** App do lab *simula* uso de memória, não aloca de verdade, pois gerenciamento real varia entre ambientes Docker (Desktop vs. Linux CE).
*   Solução de exemplo no GitHub.

🧠 Explicação detalhada e didática:

*   A página contém uma nota importante esclarecendo que a aplicação do Lab 8 apenas *finge* usar memória, para simplificar o exercício e torná-lo consistente entre diferentes setups Docker. O gerenciamento real de limites de memória em contêineres (`docker run --memory=...`) pode ter comportamentos diferentes dependendo de como e onde o Docker está rodando.
*   Fornece o link para a solução do lab no repositório do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Ao fazer o Lab 8, lembre-se que os limites de memória mencionados nos comandos (`startup-check 500`, `runtime-check 1000`) referem-se a valores *internos* da simulação do app, não necessariamente a MB reais que você define com `docker run --memory`.

---

📄 Página 163

🔍 Resumo dos principais pontos:

*   **Capítulo 9: Adding observability with containerized monitoring** (Adicionando observabilidade com monitoramento conteinerizado)
*   Apps autônomos (escalam/reparam sozinhos) parecem bons demais, e são. Plataforma ajuda, mas monitoramento humano e alertas ainda são necessários para problemas sérios.
*   Falta de insight (observabilidade) impede ir para produção.
*   **Observabilidade:** Crítica para apps em contêineres. Diz o que apps estão fazendo, como estão performando, ajuda a achar problemas.
*   Foco do Cap. 9: Abordagem estabelecida com **Prometheus** e **Grafana**.
    *   Expor **métricas** dos contêineres da aplicação.
    *   Usar **Prometheus** para coletar métricas.
    *   Usar **Grafana** para visualizar métricas em dashboards.
*   Ferramentas são open source, multiplataforma, rodam em contêineres.
*   Benefício: Mesmo insight em todos os ambientes (dev a prod).
*   **9.1 The monitoring stack for containerized applications** (A pilha de monitoramento para aplicações conteinerizadas)
    *   Monitoramento tradicional (lista de servidores, uso de CPU/disco) não funciona bem para contêineres (dinâmicos, curta duração, muitos).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 9, focado em **observabilidade** - a capacidade de entender o que está acontecendo dentro da sua aplicação e infraestrutura.
*   Argumenta que, apesar da automação e auto-recuperação oferecidas por contêineres e orquestradores, o monitoramento ativo ainda é essencial, especialmente para diagnosticar problemas complexos e garantir a prontidão para produção.
*   Apresenta a pilha de ferramentas open source popular para isso:
    1.  **Métricas:** As aplicações são instrumentadas para expor dados numéricos sobre seu desempenho e atividade (ex: número de requisições, tempo de resposta, uso de memória interna) em um formato padronizado (formato Prometheus).
    2.  **Prometheus:** Um sistema de monitoramento e alerta que *coleta* (scraping) essas métricas periodicamente de várias fontes (aplicações, infraestrutura), as armazena em um banco de dados de séries temporais e permite fazer consultas sobre elas.
    3.  **Grafana:** Uma plataforma de visualização que se conecta a fontes de dados como Prometheus e permite criar dashboards gráficos interativos para exibir as métricas de forma compreensível.
*   A grande vantagem é que Prometheus e Grafana rodam em contêineres, permitindo que você tenha o mesmo sistema de monitoramento robusto desde o ambiente de desenvolvimento local até a produção.
*   A seção 9.1 começa contrastando o monitoramento moderno (focado em serviços e métricas de aplicação) com o tradicional (focado em máquinas individuais), que é inadequado para a natureza dinâmica dos contêineres.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você rodará contêineres para Prometheus e Grafana usando `docker run` ou `docker-compose`.
*   Você modificará (ou usará versões modificadas) das aplicações de exemplo para expor um endpoint `/metrics` no formato Prometheus.
*   Você configurará o Prometheus para encontrar e coletar métricas desses endpoints.
*   Você acessará a interface web do Grafana para visualizar os dados coletados pelo Prometheus.

---

📄 Página 164

🔍 Resumo dos principais pontos:

*   Monitoramento de contêineres precisa ser "container-aware" (consciente de contêineres).
*   **Prometheus:** Open source, CNCF, descobre e coleta métricas de contêineres dinamicamente. Roda em contêiner.
*   Diagrama (Figura 9.1): Mostra Prometheus em contêiner coletando métricas de outros contêineres de aplicação e do próprio Docker Engine.
*   **Benefício Prometheus: Consistência.**
    *   Mesmo formato de métricas para todos os apps (ex: .NET/Windows, Node.js/Linux).
    *   Mesma linguagem de consulta (PromQL).
*   **Docker Engine também pode exportar métricas** no formato Prometheus.
    *   Dá insight sobre a plataforma/infraestrutura Docker.
    *   Precisa ser habilitado explicitamente na config do Docker Engine (`daemon.json` ou UI Docker Desktop).

🧠 Explicação detalhada e didática:

*   A página introduz formalmente o **Prometheus** como a ferramenta central para coleta de métricas no mundo dos contêineres. Sua capacidade de descobrir dinamicamente os contêineres que precisa monitorar (em vez de depender de uma lista estática de IPs) é crucial.
*   A Figura 9.1 ilustra o cenário: um contêiner Prometheus "observando" tanto os contêineres das aplicações quanto o próprio motor Docker (Docker Engine) que os está executando.
*   A **consistência** é um grande trunfo: Prometheus usa um formato de texto simples para métricas, que pode ser gerado por qualquer aplicação, independentemente da linguagem ou SO. Isso unifica o monitoramento.
*   Uma fonte importante de métricas é o próprio **Docker Engine**. Ele pode ser configurado para expor informações sobre seu próprio desempenho, uso de recursos e o estado dos contêineres que gerencia. Habilitar isso fornece uma visão valiosa da saúde da infraestrutura subjacente.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo passo será habilitar a exportação de métricas no seu Docker Desktop.

💡 Analogias ou observações úteis (se houver):

*   Prometheus: É como um agente de censo que visita regularmente cada "casa" (contêiner, Docker Engine) que foi instruído a monitorar, pergunta "quantas pessoas moram aí?", "quanta energia estão usando?" (coleta métricas), anota as respostas com a data e hora, e guarda tudo em um arquivo central.
*   Métricas do Docker Engine: São os dados do próprio "síndico" do prédio, informando quantos apartamentos estão ocupados, se o elevador está funcionando, etc.

---

📄 Página 165

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Habilitar métricas do Prometheus no Docker Engine.
    *   Editar `daemon.json` (Linux/Win Server) ou usar UI Docker Desktop (Settings > Daemon/Docker Engine).
    *   Adicionar/Garantir as seguintes configurações JSON:
        ```json
        "metrics-addr" : "0.0.0.0:9323",
        "experimental": true
        ```
    *   `metrics-addr`: Diz ao Engine para expor métricas na porta 9323, acessível de qualquer endereço IP (`0.0.0.0`).
    *   `experimental`: Necessário porque a feature de métricas ainda é considerada experimental (embora estável).
*   Figura 9.2: Mostra o arquivo de configuração JSON completo no editor do Docker Desktop.

🧠 Explicação detalhada e didática:

*   Este exercício ativa a funcionalidade do Docker Engine para expor suas próprias métricas internas no formato Prometheus.
*   Isso é feito adicionando duas linhas à configuração do Docker Engine (o `daemon.json`):
    *   `"metrics-addr" : "0.0.0.0:9323"`: Configura um pequeno servidor web dentro do Docker Engine para responder na porta `9323`. O endereço `0.0.0.0` significa que ele aceitará conexões de qualquer interface de rede da máquina (incluindo conexões de contêineres rodando na mesma máquina). Se usasse `127.0.0.1`, só seria acessível da própria máquina host.
    *   `"experimental": true`: Algumas funcionalidades do Docker, como as métricas do Prometheus, são liberadas primeiro sob a flag "experimental". É preciso habilitá-la para usar essas features.
*   Para usuários do Docker Desktop, a maneira mais fácil é editar o JSON diretamente na interface de Configurações (na seção "Docker Engine"). Após clicar em "Apply & Restart", o Docker Engine reiniciará com as novas configurações. Para usuários de servidor, seria preciso editar o arquivo `daemon.json` manualmente e reiniciar o serviço Docker.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Abrir Configurações:** Clique direito no ícone Docker > Settings.
2.  **Editar JSON:** Vá para a seção "Docker Engine".
3.  **Adicionar Linhas:** No editor JSON, adicione as duas linhas, garantindo que a sintaxe JSON permaneça válida (vírgulas entre itens, chaves `{}` corretas). Exemplo mínimo:
    ```json
    {
      "experimental": true,
      "metrics-addr": "0.0.0.0:9323"
    }
    ```
    *(Mantenha outras configurações que já existirem, como `builder`)*.
4.  **Aplicar:** Clique em "Apply & Restart". Aguarde o Docker Engine reiniciar (o ícone da baleia ficará amarelo e depois verde novamente).

---

📄 Página 166

🔍 Resumo dos principais pontos:

*   Métricas do Engine são experimentais, mas estáveis e úteis para dashboards (saúde do sistema).
*   **Teste:** Acessar `http://localhost:9323/metrics` no navegador.
    *   Deve mostrar saída em texto no formato Prometheus (Figura 9.3).
*   **Formato Prometheus:**
    *   Texto simples.
    *   Cada métrica: nome e valor.
    *   Precedida por linhas `# HELP` (descrição) e `# TYPE` (tipo de dado - gauge, counter, etc.).
*   Essas linhas de texto são a base da solução de monitoramento. Prometheus coleta, adiciona timestamp e armazena para consulta.

🧠 Explicação detalhada e didática:

*   Após habilitar as métricas e reiniciar o Engine, a página instrui a verificar se está funcionando acessando o endpoint `http://localhost:9323/metrics` diretamente no navegador.
*   O resultado esperado é uma página de texto (não HTML) contendo uma longa lista de métricas no formato específico do Prometheus.
*   O formato é explicado:
    *   Linhas começando com `# HELP` descrevem o que a métrica significa.
    *   Linhas começando com `# TYPE` indicam o tipo da métrica (os mais comuns são `gauge` - valor pode subir/descer, como uso de memória; e `counter` - valor só sobe ou reseta, como número total de requisições).
    *   A linha da métrica em si contém o `nome_da_metrica`, opcionalmente {`labels="valores"`} (pares chave-valor para diferenciar instâncias da mesma métrica), e o `valor` numérico atual.
*   Esses dados brutos expostos pelo Docker Engine (e pelas aplicações) serão a fonte para o Prometheus construir seu histórico e para o Grafana gerar os gráficos.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Acessar Métricas:** Abra seu navegador e vá para `http://localhost:9323/metrics`.
2.  **Observar Saída:** Você verá uma grande quantidade de texto similar à Figura 9.3. Procure por métricas como `engine_daemon_engine_version`, `engine_daemon_container_states_containers`, `process_resident_memory_bytes`. Observe as linhas `# HELP` e `# TYPE` acima delas.

💡 Analogias ou observações úteis (se houver):

*   Endpoint `/metrics`: É como um painel de instrumentos cheio de mostradores e contadores, mas em formato de texto, que o Prometheus sabe ler.

---

📄 Página 167

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar Prometheus em contêiner para coletar métricas do Docker host.
    *   Necessário obter IP da máquina host para passar ao contêiner Prometheus.
    *   Comandos para obter IP do host (Windows PowerShell, Linux, Mac).
    *   Rodar contêiner Prometheus (`diamol/prometheus:2.13.1`), passando IP do host via variável de ambiente `DOCKER_HOST`.
        *   `docker run -e DOCKER_HOST=$hostIP -d -p 9090:9090 diamol/prometheus:2.13.1`.
*   Configuração na imagem `diamol/prometheus` usa `DOCKER_HOST` para encontrar e coletar (`scrape`) métricas do Engine na porta 9323.
*   Acessar serviço no host a partir de contêiner é raro; geralmente usa-se nome DNS, mas IP funciona para dev local.
*   **Prometheus em execução:**
    *   Coleta métricas do host em intervalos.
    *   Armazena valores com timestamp em seu BD interno.
    *   Expõe UI web básica na porta 9090.
*   UI Prometheus permite ver métricas coletadas, filtrar

**You**

🧠 Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), página por página, a apartir da página 167.

Para cada página, siga este modelo de resposta:

📄 Página X

🔍 Resumo dos principais pontos:

\[bullet points com os tópicos abordados na página\]

🧠 Explicação detalhada e didática:

\[explique os conceitos em português claro, com boa didática, como um professor\]

💻 Exemplos práticos (para meu ambiente Windows):

\[comandos ou trechos de código aplicáveis no Windows, usando Docker Desktop e Visual Studio Code\]

💡 Analogias ou observações úteis (se houver):

\[comparações simples ou observações para reforçar o entendimento\]

✅ Importante: percorra automaticamente todas as páginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "próxima".

Quando terminar todas as páginas, diga:

"✅ Fim da leitura. Todas as páginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Okay, Alexandre, continuando a análise do PDF a partir da página 167.

---

📄 Página 167

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar Prometheus em contêiner para coletar métricas do Docker host.
    *   Necessário obter IP da máquina host para passar ao contêiner Prometheus.
    *   Comandos fornecidos para obter IP do host (Windows PowerShell, Linux, Mac).
    *   Rodar contêiner Prometheus (`diamol/prometheus:2.13.1`), passando IP do host via variável de ambiente `DOCKER_HOST`.
        *   Comando base: `docker container run -e DOCKER_HOST=$hostIP -d -p 9090:9090 diamol/prometheus:2.13.1`.
*   Configuração na imagem `diamol/prometheus` usa `DOCKER_HOST` para encontrar e coletar (`scrape`) métricas do Engine na porta 9323.
*   Acessar serviço no host a partir de contêiner é raro; geralmente usa-se nome DNS, mas IP funciona para dev local.
*   **Prometheus em execução:**
    *   Coleta métricas do host em intervalos.
    *   Armazena valores com timestamp em seu BD interno.
    *   Expõe UI web básica na porta 9090 (`http://localhost:9090`).
*   UI Prometheus permite ver métricas coletadas, filtrar, exibir em tabelas/gráficos.
*   **Exercício Try-it-now:** Verificar UI do Prometheus e consultar métrica.
    *   Acessar `http://localhost:9090`.
    *   Verificar Status > Targets (deve mostrar `DOCKER_HOST` como UP/verde).
    *   Ir para Graph, selecionar métrica `engine_daemon_container_actions_seconds_sum`, clicar Execute.
*   Figura 9.4: Mostra a UI do Prometheus com a consulta e resultado da métrica.

🧠 Explicação detalhada e didática:

*   Esta página mostra como iniciar o **Prometheus** em um contêiner para que ele comece a coletar as métricas que o Docker Engine está expondo (conforme configurado nas páginas anteriores).
*   Um detalhe técnico: o contêiner do Prometheus precisa saber o endereço IP da máquina host onde o Docker Engine está rodando para poder acessar `http://<ip_do_host>:9323/metrics`. O exercício fornece comandos para descobrir o IP do host e armazená-lo em uma variável (`$hostIP` no PowerShell). Essa variável é então passada para dentro do contêiner Prometheus usando a flag `-e DOCKER_HOST=$hostIP` no comando `docker run`. A imagem `diamol/prometheus` foi pré-configurada para usar essa variável `DOCKER_HOST` para encontrar seu alvo.
*   Uma vez que o contêiner Prometheus está rodando, ele começa seu ciclo de "scraping": periodicamente (a cada poucos segundos ou minutos, dependendo da configuração), ele acessa o endpoint `/metrics` do Docker Engine, coleta os dados, adiciona um timestamp e armazena em seu próprio banco de dados interno (um banco de dados de séries temporais).
*   Prometheus também oferece uma interface web simples na porta 9090 para visualização e consulta básica. O exercício guia você para:
    1.  Acessar a UI (`http://localhost:9090`).
    2.  Verificar a página "Targets" para confirmar que o Prometheus está conseguindo se conectar ao Docker Engine (`DOCKER_HOST` deve estar verde).
    3.  Usar a página "Graph" para selecionar uma métrica específica (ex: `engine_daemon_container_actions_seconds_sum`, que mede o tempo gasto em ações de contêiner) e visualizar seu valor atual.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Obter IP do Host (PowerShell):**
    ```powershell
    $hostIP = (Get-NetIPConfiguration | Where-Object {$_.IPv4DefaultGateway -ne $null -and $_.NetAdapter.Status -eq 'Up' } | Select-Object -First 1).IPv4Address.IPAddress
    echo $hostIP # Anote este IP
    # Alternativa (pode funcionar melhor com WSL2): Use 'host.docker.internal' em vez do IP
    # $hostIP = 'host.docker.internal'
    ```
    *Nota: Obter o IP correto do host que é visível de dentro do contêiner pode ser complicado, especialmente com WSL2. Usar o nome DNS especial `host.docker.internal` (que o Docker Desktop define) pode ser mais confiável.* Vamos tentar com ele.
2.  **Rodar Prometheus:**
    ```powershell
    docker container run -e DOCKER_HOST='host.docker.internal' -d -p 9090:9090 --name prometheus diamol/prometheus:2.13.1
    ```
3.  **Verificar Prometheus UI:**
    *   Abra o navegador em `http://localhost:9090`.
    *   Vá para Status > Targets. Verifique se `DOCKER_HOST` (ou o nome configurado) está com State "UP".
    *   Vá para a aba "Graph".
    *   No campo de expressão, comece a digitar `engine_daemon_container_actions_seconds_sum`. Selecione-o na lista suspensa.
    *   Clique em "Execute". Você deverá ver os valores atuais dessa métrica na aba "Table".

---

📄 Página 168

🔍 Resumo dos principais pontos:

*   Figura 9.4: Mostra a UI do Prometheus com a consulta.
*   UI Prometheus é simples, boa para explorar métricas disponíveis e rodar queries.
*   Docker Engine expõe muitas métricas:
    *   Nível alto: nº containers por estado, nº health checks falhos.
    *   Nível baixo: memória usada pelo Engine.
    *   Info estática: nº CPUs disponíveis.
*   Métricas do Engine são nível **infraestrutura**.
*   Aplicações expõem suas próprias métricas (nível **runtime** e **aplicação**).
*   Objetivo: Ter endpoint `/metrics` em cada contêiner de app, Prometheus coletando de todos.
*   Prometheus armazena histórico, permitindo dashboards que mostram saúde geral do sistema.
*   **9.2 Exposing metrics from your application** (Expondo métricas da sua aplicação)
    *   Métricas do Engine são começo; métricas das *aplicações* exigem mais esforço.
    *   Necessário código para capturar métricas e prover endpoint HTTP `/metrics`.
    *   **Bibliotecas Cliente Prometheus:** Facilitam isso para várias linguagens (Java, Go, Node.js, Python, etc.).

🧠 Explicação detalhada e didática:

*   A página conclui a introdução ao Prometheus e suas métricas do Docker Engine, classificando-as como métricas de **infraestrutura**.
*   A seção 9.2 introduz o próximo passo crucial: fazer com que as **suas próprias aplicações** exponham métricas sobre seu funcionamento interno. Isso requer instrumentar o código da aplicação.
*   A boa notícia é que não é preciso fazer tudo do zero. Existem **bibliotecas cliente oficiais e comunitárias do Prometheus** para a maioria das linguagens de programação populares. Essas bibliotecas oferecem funcionalidades prontas para:
    1.  Coletar métricas padrão do **runtime** da linguagem (ex: uso de memória da JVM em Java, número de Goroutines em Go, informações do event loop em Node.js).
    2.  Permitir que o desenvolvedor defina e atualize métricas **customizadas** da **aplicação** (ex: número de logins, tempo médio de processamento de pedido, itens no carrinho).
    3.  Expor todas essas métricas através de um endpoint HTTP (geralmente `/metrics`) no formato que o Prometheus entende.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nos próximos exercícios, você usará versões das aplicações de exemplo (Image Gallery) que já foram modificadas para incluir essas bibliotecas cliente Prometheus e expor o endpoint `/metrics`.
*   Se você fosse fazer isso em seu próprio projeto .NET Core, por exemplo, usaria pacotes NuGet como `prometheus-net` e `prometheus-net.AspNetCore`.

💡 Analogias ou observações úteis (se houver):

*   Métricas do Engine: Medem a saúde do "prédio" (infraestrutura Docker).
*   Métricas da Aplicação: Medem a saúde e atividade *dentro* de cada "apartamento" (contêiner de aplicação).
*   Bibliotecas Cliente Prometheus: São como kits de "sensores + painel de leitura" pré-fabricados que você instala no seu apartamento (aplicação) para medir coisas como temperatura interna, consumo de energia, quantas vezes a porta foi aberta (métricas de runtime e aplicação) e exibir tudo em um formato padrão na porta (`/metrics`).

---

📄 Página 169

🔍 Resumo dos principais pontos:

*   Autor revisitou app Image Gallery (Go, Java, Node.js) e adicionou métricas Prometheus usando bibliotecas cliente.
*   Diagrama (Figura 9.5): Mostra cada contêiner da aplicação agora com uma biblioteca cliente Prometheus expondo métricas.
*   Métricas das bibliotecas cliente = nível **runtime**.
    *   Info sobre o que o contêiner está fazendo, quão "quente" está rodando (CPU, memória, threads, etc.).
    *   Específicas da linguagem (ex: Goroutines em Go, memória JVM em Java).
    *   Geralmente obtidas "de graça" ao adicionar a biblioteca.
*   **Exercício Try-it-now:** Rodar nova versão do app Image Gallery (com métricas) e acessar endpoint `/metrics`.
    *   Navegar para `ch09/exercises`.
    *   Limpar contêineres antigos.
    *   Criar rede `nat` (ignorar erro se existir).
    *   Iniciar app: `docker-compose up -d`.
    *   Usar app: acessar `http://localhost:8010`.
    *   Ver métricas do frontend Go: acessar `http://localhost:8010/metrics`.

🧠 Explicação detalhada e didática:

*   A página explica que as aplicações de exemplo foram atualizadas para incluir as bibliotecas cliente Prometheus. A Figura 9.5 ilustra isso.
*   Essas bibliotecas fornecem automaticamente métricas valiosas sobre o **runtime** da aplicação (Go, Java, Node.js), como uso de memória específico da plataforma, atividade de threads/processos internos, etc. Isso já dá uma boa visão de como a aplicação está se comportando internamente, sem que o desenvolvedor precise escrever código extra para essas métricas específicas do runtime.
*   O exercício instrui a iniciar a versão "instrumentada" do aplicativo Image Gallery usando `docker-compose` (o arquivo `.yml` nesta pasta já definirá os 3 serviços - Go, Java, Node.js - usando as novas imagens com métricas). Após iniciar e usar um pouco a aplicação web para gerar atividade, o exercício pede para acessar diretamente o endpoint `/metrics` do serviço frontend Go (`http://localhost:8010/metrics`) para ver as métricas que ele está expondo.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch09\exercises
    ```
2.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Pode remover também o container prometheus, se quiser começar limpo
    # docker rm -f prometheus
    ```
3.  **Criar Rede:**
    ```powershell
    docker network create nat # Ignore erro se já existir
    ```
4.  **Iniciar App com Métricas:** (O `docker-compose.yml` nesta pasta define os serviços `image-gallery`, `iotd`, `accesslog` com as imagens corretas)
    ```powershell
    docker-compose up -d
    ```
5.  **Verificar:**
    ```powershell
    docker-compose ps # Deve mostrar os 3 serviços Up
    ```
6.  **Usar App:** Abra o navegador em `http://localhost:8010`. Clique algumas vezes se houver botões, ou apenas carregue a página.
7.  **Ver Métricas Go:** Abra uma nova aba no navegador e acesse `http://localhost:8010/metrics`. Você verá a saída de texto no formato Prometheus.

---

📄 Página 170

🔍 Resumo dos principais pontos:

*   Figura 9.6: Mostra a saída do endpoint `/metrics` do frontend Go.
*   Métricas obtidas "de graça" (sem código customizado) apenas adicionando a biblioteca cliente Go.
*   Pode-se ver métricas similares da API Java em `http://localhost:8011/actuator/prometheus`. (Nota: Porta do host para Java API pode variar se for aleatória, mas o path `/actuator/prometheus` é comum em Spring Boot).
*   Endpoints `/metrics` são "mar de texto", mas contêm dados chave para dashboards (ex: container rodando "quente" - CPU, memória, threads altos).
*   Métricas de **runtime** são úteis, mas não contam toda a história.
*   Necessário também: **métricas de aplicação** (customizadas).
    *   Capturam info específica sobre o app/negócio.
    *   Exemplos operacionais: nº eventos processados, tempo médio de resposta.
    *   Exemplos de negócio: nº usuários ativos, nº cadastros novos.

🧠 Explicação detalhada e didática:

*   A página mostra o resultado de acessar o endpoint `/metrics` da aplicação Go (Figura 9.6) e menciona o endpoint equivalente para a aplicação Java (que usa Spring Boot Actuator, uma dependência comum que expõe métricas e outros endpoints de gerenciamento).
*   Reforça que essas métricas de **runtime** são valiosas para entender o consumo de recursos e o estado interno da plataforma (Go, JVM), e que são obtidas com pouco ou nenhum esforço extra do desenvolvedor, apenas incluindo a biblioteca cliente apropriada.
*   No entanto, para uma observabilidade completa, são necessárias também as **métricas de aplicação** (ou métricas customizadas/de negócio). Estas são métricas que *o desenvolvedor* precisa definir e incrementar/decrementar explicitamente no código para rastrear eventos ou estados específicos do domínio da aplicação ou do negócio. Exemplos incluem contar quantas vezes uma função específica foi chamada, medir a duração de um processo de negócio, ou rastrear o número de itens em um carrinho de compras.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Ver Métricas Java:**
    *   Primeiro, descubra a porta externa da API Java (`iotd`):
        ```powershell
        docker-compose ps
        # Procure pela linha do serviço 'iotd'. A coluna PORTS mostrará algo como '0.0.0.0:32768->8080/tcp'. Anote a porta antes de '->'.
        # Ou use:
        # docker-compose port iotd 8080
        ```
    *   Acesse no navegador: `http://localhost:<porta_anotada>/actuator/prometheus`. Você verá as métricas da JVM e do Spring Boot.
2.  **Entender Métricas Customizadas:** O próximo passo mostrará como elas são definidas no código e como aparecem no endpoint `/metrics`.

💡 Analogias ou observações úteis (se houver):

*   Métricas de Runtime: Como os indicadores no painel do seu carro (velocímetro, RPM, temperatura do motor, nível de combustível) - informações padrão sobre o funcionamento da máquina.
*   Métricas de Aplicação: Como contadores ou anotações que *você* adiciona para rastrear coisas específicas da sua viagem (quantos pedágios passou, tempo gasto em cada parada, número de passageiros) - informações específicas do propósito da sua "viagem" (aplicação).

---

📄 Página 171

🔍 Resumo dos principais pontos:

*   Bibliotecas cliente Prometheus permitem gravar métricas customizadas (aplicação/negócio). Requer escrita de código explícito.
*   Não é difícil. **Listagem 9.1:** Exemplo em Node.js (do app `access-log`).
    ```javascript
    // Declara métricas customizadas:
    const accessCounter = new prom.Counter({ // Contador: só incrementa
      name: "access_log_total",
      help: "Access Log - total log requests"
    });
    const clientIpGauge = new prom.Gauge({ // Gauge: pode incrementar/decrementar
      name: "access_client_ip_current",
      help: "Access Log - current unique IP addresses"
    });

    // Depois, no código que lida com requests:
    accessCounter.inc(); // Incrementa contador a cada request
    clientIpGauge.set(countOfIpAddresses); // Define valor atual do gauge
    ```
*   Menciona que exemplos Go e Java no código fonte também adicionam métricas customizadas (usando APIs específicas de cada biblioteca cliente).
*   **Tipos de Métricas Prometheus (simples):**
    *   **Counter:** Valor numérico que só aumenta ou reseta para zero (ex: nº total de requisições HTTP).
    *   **Gauge:** Valor numérico que pode aumentar ou diminuir (ex: uso atual de memória, nº usuários logados).
*   Desenvolvedor escolhe tipo e atualiza valor; biblioteca/Prometheus cuidam do resto.

🧠 Explicação detalhada e didática:

*   A página mostra como é simples, conceitualmente, adicionar métricas customizadas usando as bibliotecas cliente Prometheus. O exemplo em Node.js (Listagem 9.1) ilustra os dois tipos básicos de métricas:
    *   **Counter:** Usado para contar eventos que ocorrem. Você o declara uma vez (dando nome e descrição) e depois simplesmente chama um método como `.inc()` toda vez que o evento acontece (ex: uma requisição HTTP chega). Prometheus armazena o valor acumulado.
    *   **Gauge:** Usado para representar um valor que pode variar para cima ou para baixo. Você o declara e depois usa um método como `.set(valor)` para atualizar seu valor atual (ex: número de conexões ativas no momento).
*   Embora a sintaxe varie entre as linguagens (Java, Go, Node.js), a ideia é a mesma: declarar a métrica e depois chamar métodos da biblioteca cliente nos pontos apropriados do código da aplicação para atualizar seus valores. A biblioteca então inclui essas métricas customizadas no endpoint `/metrics` junto com as métricas de runtime padrão.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Explorar Código (Opcional):** Se você clonou o repositório, pode abrir as pastas das aplicações em `ch09/exercises` (ex: `access-log/src/server.js`, `image-gallery/src/main.go`, `image-of-the-day/src/.../ImageController.java`) no VS Code e procurar pelos exemplos de código que definem e atualizam contadores (`Counter`) e medidores (`Gauge`) do Prometheus.
*   O próximo exercício mostrará o resultado dessas métricas customizadas sendo expostas.

---

📄 Página 172

🔍 Resumo dos principais pontos:

*   App Image Gallery está rodando (com métricas customizadas incluídas).
*   **Exercício Try-it-now:** Gerar carga e ver métricas customizadas do app Node.js (`access-log`).
    *   1. Gerar 5-10 requisições para o site (`http://localhost:8010`) usando loop `for` (comandos Windows/Linux fornecidos). Isso fará o site chamar a API `access-log`.
    *   2. Acessar endpoint de métricas do `access-log`: `http://localhost:8012/metrics` (Nota: porta 8012 pode precisar ser descoberta se for aleatória).
*   Figura 9.7: Mostra saída do `/metrics` do `access-log`.
    *   Destaca as métricas customizadas: `access_log_total` (counter) e `access_client_ip_current` (gauge).
    *   Também inclui métricas de runtime do Node.js padrão.
*   Métricas customizadas simples (counters/gauges) já permitem instrumentação detalhada.
*   **Diretrizes para Métricas Customizadas (O que medir?):**
    *   Chamadas para sistemas externos: medir duração e sucesso/falha.
    *   Eventos importantes (vale a pena logar?): talvez seja melhor/mais barato registrar como métrica (incrementar counter).
    *   Métricas de negócio (comportamento do usuário, KPIs): permitem dashboards em tempo real.

🧠 Explicação detalhada e didática:

*   O exercício tem como objetivo observar as métricas *customizadas* que foram adicionadas à aplicação `access-log` (Node.js).
    1.  Primeiro, gera-se tráfego para a aplicação web (`image-gallery`). Cada acesso à web causa uma chamada para a API `access-log`.
    2.  Depois, acessa-se diretamente o endpoint `/metrics` do contêiner `access-log` (na porta 8012, conforme definido no `docker-compose.yml` de `ch09/exercises`).
*   A Figura 9.7 mostra que, além das métricas padrão do Node.js, agora aparecem as métricas customizadas (`access_log_total`, `access_client_ip_current`) definidas na Listagem 9.1, com seus valores atuais refletindo a carga gerada.
*   A página finaliza com sugestões sobre quais tipos de métricas customizadas são geralmente úteis: monitorar interações com sistemas externos, contar eventos chave (alternativa/complemento a logs), e rastrear indicadores de negócio.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Porta `access-log`:** (No diretório `ch09/exercises`)
    ```powershell
    docker-compose ps
    # Verifique a porta mapeada para o serviço accesslog (deve ser 8012, conforme docker-compose.yml)
    # Se não for 8012, use a porta correta nos próximos passos.
    ```
2.  **Gerar Carga (Windows PowerShell):**
    ```powershell
    for ($i=1; $i -le 10; $i++) { Invoke-WebRequest -UseBasicParsing http://localhost:8010 | Out-Null }
    ```
3.  **Ver Métricas `access-log`:** Abra o navegador em `http://localhost:8012/metrics`. Procure pelas linhas `access_log_total` e `access_client_ip_current`. O valor de `access_log_total` deve ser pelo menos 10.

---

📄 Página 173

🔍 Resumo dos principais pontos:

*   **9.3 Running a Prometheus container to collect metrics** (Rodando um contêiner Prometheus para coletar métricas)
*   Modelo Prometheus: **Pull** (Prometheus busca dados das fontes), não Push (fontes enviam dados).
*   Prometheus "raspa" (scrapes) endpoints `/metrics` configurados.
*   Configuração do Prometheus: Define quais endpoints raspar.
    *   Em produção (cluster): pode descobrir contêineres automaticamente.
    *   No Docker Compose (single host): usa lista de nomes de serviço (DNS Docker).
*   **Listagem 9.2:** Configuração do Prometheus (`prometheus.yml`) para raspar apps Image Gallery.
    ```yaml
    global:
      scrape_interval: 10s # Intervalo padrão entre coletas
    scrape_configs:
      - job_name: "image-gallery" # Nome do trabalho/alvo
        metrics_path: /metrics # Path do endpoint
        static_configs: # Configuração estática de alvos
          - targets: ["image-gallery"] # Nome DNS do serviço a raspar
      - job_name: "iotd-api"
        metrics_path: /actuator/prometheus # Path diferente para Java/Spring
        static_configs:
          - targets: ["iotd"]
      - job_name: "access-log"
        metrics_path: /metrics
        dns_sd_configs: # Configuração via DNS Service Discovery
          - names:
              - accesslog # Nome DNS a procurar
            type: A # Tipo de registro DNS a esperar (endereço IPv4)
            port: 80 # Porta onde o endpoint /metrics está (no container)
    ```

🧠 Explicação detalhada e didática:

*   Agora que as aplicações estão expondo métricas, a seção 9.3 foca em configurar o **Prometheus** para coletá-las.
*   O modelo de coleta do Prometheus é **pull**: o servidor Prometheus é configurado com uma lista de "alvos" (targets) - os endpoints `/metrics` das suas aplicações, do Docker Engine, etc. - e ele periodicamente *acessa* esses alvos para buscar (scrape) os valores atuais das métricas.
*   A **Listagem 9.2** mostra um exemplo de arquivo de configuração para o Prometheus (`prometheus.yml`).
    *   `global`: Define configurações gerais, como `scrape_interval` (aqui, coletar a cada 10 segundos).
    *   `scrape_configs`: Uma lista de "trabalhos" (jobs) de coleta. Cada job define um conjunto de alvos a serem raspados.
        *   `job_name`: Um nome para o grupo de alvos (ex: "image-gallery", "iotd-api").
        *   `metrics_path`: O caminho URL onde as métricas estão expostas (padrão `/metrics`, mas pode ser diferente, como no caso do `iotd-api` Java/Spring).
        *   `static_configs`: Define uma lista *fixa* de alvos usando seus nomes DNS (que o Docker resolverá). Usado aqui para `image-gallery` e `iotd`.
        *   `dns_sd_configs`: Define alvos usando **descoberta de serviço via DNS**. Prometheus consultará o DNS para o nome `accesslog`, esperando múltiplos endereços IP (registros tipo `A`), e tentará raspar o `/metrics` na porta `80` de cada IP encontrado. Isso é ideal para serviços escalados.

💻 Exemplos práticos (para meu ambiente Windows):

*   Este arquivo `prometheus.yml` será usado para configurar um contêiner Prometheus que coletará métricas dos contêineres da aplicação Image Gallery. Você não precisa criar o arquivo manualmente agora; ele provavelmente será fornecido ou embutido em uma imagem Prometheus customizada nos próximos exercícios.

💡 Analogias ou observações úteis (se houver):

*   Configuração do Prometheus: É a lista de "endereços" que o agente do censo (Prometheus) deve visitar e o caminho da "porta" (`/metrics`) onde ele deve bater para coletar os dados. `static_configs` é uma lista fixa de endereços. `dns_sd_configs` é como dizer "visite todas as casas na rua 'accesslog'".

---

📄 Página 174

🔍 Resumo dos principais pontos:

*   Análise da Configuração Prometheus (Listagem 9.2):
    *   Poll a cada 10s.
    *   Usa DNS para achar IPs.
    *   `static_configs` para `image-gallery`/`iotd` espera 1 container; comportamento inesperado se escalar. Prometheus usa só o 1º IP da lista DNS se receber vários.
    *   `dns_sd_configs` para `access-log` suporta múltiplos IPs (escala); Prometheus raspa todos.
*   Diagrama (Figura 9.8): Ilustra Prometheus raspando métricas dos 3 contêineres da aplicação via rede Docker.
*   Imagem Prometheus customizada (`diamol/ch09-prometheus`) foi criada pelo autor.
    *   Baseada na imagem oficial Prometheus.
    *   Copia o arquivo de configuração (Listagem 9.2) para dentro da imagem.
    *   Permite rodar Prometheus pré-configurado, mas pode sobrescrever config se necessário.
*   Métricas ficam mais interessantes com múltiplos contêineres (escala).
*   Pode-se escalar componente Node.js (`access-log`) e Prometheus coletará de todos.

🧠 Explicação detalhada e didática:

*   A página detalha as implicações da configuração do Prometheus:
    *   Usar `static_configs` (como feito para `image-gallery` e `iotd`) é simples, mas assume que haverá apenas um contêiner para aquele serviço. Se você escalar o serviço, o Prometheus ainda usará apenas o primeiro IP retornado pelo DNS, levando a métricas incompletas ou enganosas.
    *   Usar `dns_sd_configs` (como feito para `access-log`) é a maneira correta de lidar com serviços que podem ser escalados. Prometheus consultará o DNS, obterá a lista de todos os IPs das réplicas e configurará um alvo de scraping para *cada um* deles.
*   A Figura 9.8 mostra o Prometheus "puxando" dados dos outros contêineres.
*   O autor menciona que criou uma imagem Prometheus customizada (`diamol/ch09-prometheus`) que já embute a configuração da Listagem 9.2. Isso simplifica os próximos exercícios, pois não será preciso montar o arquivo de configuração manualmente.
*   Prepara para o próximo exercício, onde o serviço `access-log` será escalado para demonstrar a coleta de métricas de múltiplas instâncias usando `dns_sd_configs`.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício usará um `docker-compose.yml` que provavelmente inclui um serviço `prometheus` usando a imagem `diamol/ch09-prometheus`, conectado à mesma rede `nat` (ou `app-net`) dos serviços da aplicação.

---

📄 Página 175

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Escalar `access-log` e verificar métricas no Prometheus.
    *   Usar arquivo `docker-compose-scale.yml` (provavelmente define `access-log` para usar porta aleatória, permitindo escala).
    *   1. Escalar `access-log` para 3 instâncias: `docker-compose -f docker-compose-scale.yml up -d --scale accesslog=3`.
    *   2. Gerar carga (loop `for` com 10 requests `curl` ou `iwr` para `http://localhost:8010`).
*   Website (`image-gallery`) chama `access-log` a cada request; carga deve ser balanceada entre as 3 instâncias `access-log`.
*   Como verificar balanceamento? Usando **labels** nas métricas Prometheus.
    *   Métricas do `access-log` incluem label `hostname` (que será o ID do contêiner Docker).
*   **Exercício Try-it-now:** Consultar métrica `access_log_total` no Prometheus UI (`http://localhost:9090/graph`).
    *   Selecionar `access_log_total`, clicar Execute.

🧠 Explicação detalhada e didática:

*   O exercício visa demonstrar como o Prometheus coleta métricas de um serviço escalado (`access-log` com 3 réplicas).
*   Usa-se `docker-compose --scale` para criar as réplicas extras.
*   Gera-se tráfego para a aplicação web, que por sua vez chamará a API `access-log`. Espera-se que o balanceamento de carga do Docker distribua essas chamadas entre as 3 réplicas de `access-log`.
*   A questão é: como confirmar isso usando as métricas? A resposta está nas **labels** do Prometheus. A biblioteca cliente Node.js (e muitas outras) automaticamente adiciona labels às métricas, como `instance` (o IP:porta do alvo raspado) e, neste caso, o autor parece ter configurado para incluir `hostname` (o ID do contêiner).
*   Ao consultar a métrica `access_log_total` (que conta o total de requests recebidas) na interface do Prometheus, como há 3 instâncias sendo raspadas e cada uma tem um `hostname` diferente, o Prometheus mostrará **três resultados separados**, um para cada contêiner, cada um com seu valor atual do contador. Comparando os valores, pode-se ter uma ideia de quão bem a carga foi distribuída.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar (se necessário):**
    ```powershell
    cd C:\DockerLabs\diamol\ch09\exercises
    ```
2.  **Rodar/Escalar App + Prometheus:** (Assume que `docker-compose-scale.yml` existe e define `prometheus` também)
    ```powershell
    # Parar/Remover versão anterior se houver conflito de nome/porta
    # docker-compose down
    # Rodar com escala e (presumivelmente) Prometheus
    docker-compose -f docker-compose-scale.yml up -d --scale accesslog=3
    ```
3.  **Verificar:**
    ```powershell
    docker-compose -f docker-compose-scale.yml ps
    # Deve mostrar accesslog (x3), iotd, image-gallery, prometheus
    ```
4.  **Gerar Carga:**
    ```powershell
    for ($i=1; $i -le 20; $i++) { Invoke-WebRequest -UseBasicParsing http://localhost:8010 | Out-Null } # Aumentei para 20
    ```
5.  **Consultar Prometheus:**
    *   Abra `http://localhost:9090/graph`.
    *   Digite e selecione `access_log_total`.
    *   Clique "Execute".
    *   Na aba "Table", você deverá ver 3 linhas, cada uma com um `{hostname="..."}` diferente e um valor para `access_log_total`. Compare os valores.

---

📄 Página 176

🔍 Resumo dos principais pontos:

*   Figura 9.9: Mostra a saída do Prometheus com 3 resultados para `access_log_total`, um por contêiner, diferenciados pela label `hostname`.
*   Valores podem não ser iguais devido a fatores de rede (DNS caching, HTTP keep-alive), especialmente em single host.
*   **Labels** são feature poderosa do Prometheus: permitem granularidade.
*   Pode-se agregar métricas usando **PromQL (Prometheus Query Language)**.
    *   Exemplo: `sum(access_log_total) without(hostname, instance)`.
    *   `sum()`: Agrega (soma) os valores.
    *   `without(label1, label2)`: Agrupa desconsiderando essas labels (soma todos os hostnames/instances).
*   **Exercício Try-it-now:** Adicionar nova query no Prometheus UI para agregar `access_log_total`.
    *   Clicar "Add Graph".
    *   Colar a query `sum(...)`.
    *   Clicar Execute.
    *   Resultado: Gráfico de linha mostrando o *total* de requests ao longo do tempo (Figura 9.10).

🧠 Explicação detalhada e didática:

*   A página analisa a saída do Prometheus (Figura 9.9), confirmando que as labels permitem ver dados por instância.
*   Introduz a **agregação** usando a linguagem de consulta **PromQL**. Em vez de ver os contadores individuais de cada réplica `access-log`, podemos querer ver o **total** de requisições processadas por todas elas juntas.
*   A query `sum(access_log_total) without(hostname, instance)` faz exatamente isso:
    *   `access_log_total`: Seleciona a métrica base.
    *   `sum(...)`: Aplica a função de soma.
    *   `without(hostname, instance)`: Diz ao `sum` para agrupar os resultados ignorando as labels `hostname` e `instance`. Como essas são as labels que diferenciam as réplicas, o resultado é a soma de todas elas.
*   O exercício instrui a executar essa query agregada no Prometheus. O resultado não será mais uma tabela com múltiplas linhas, mas um único valor agregado (ou, na visualização de gráfico, uma única linha mostrando a evolução desse total ao longo do tempo).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Continuar no Prometheus UI:** (Com a consulta `access_log_total` ainda visível).
2.  **Adicionar Gráfico/Query:** Clique no botão "Add graph" (ou similar, pode variar ligeiramente na UI).
3.  **Inserir Query Agregada:** No novo campo de expressão, cole:
    ```promql
    sum(access_log_total) without(hostname, instance)
    ```
4.  **Executar:** Clique "Execute".
5.  **Observar:** Agora você deve ter dois gráficos/tabelas. O primeiro mostra os valores individuais por contêiner. O segundo mostra um único valor (na tabela) ou uma única linha (no gráfico) representando a soma total. Gere mais carga (passo 4 da p.175) e veja o gráfico agregado subir.

💡 Analogias ou observações úteis (se houver):

*   Labels: Como etiquetas extras na sua conta de luz (ex: consumo por andar, consumo por aparelho).
*   PromQL (agregação): Permite calcular totais ("consumo total da casa") ou médias ("consumo médio por aparelho") a partir dos dados detalhados.

---

📄 Página 177

🔍 Resumo dos principais pontos:

*   Figura 9.10: Mostra gráfico do PromQL `sum(access_log_total)`.
*   PromQL é poderosa (funções estatísticas, taxas de mudança, subqueries), mas não precisa de complexidade para dashboards úteis.
*   Estrutura Prometheus (métricas + labels) permite visualizações úteis com queries simples (filtrar por label, agregar com `sum`).
*   Figura 9.11: Exemplo de query simples para dashboard: `sum(image_gallery_requests_total{code="200"}) without(instance)`.
    *   Métrica: `image_gallery_requests_total`.
    *   Filtro por label: `{code="200"}` (só requests OK).
    *   Agregação: `sum(...) without(instance)` (soma total entre instâncias).
    *   Resultado: Nº total de respostas HTTP 200 do serviço `image-gallery`.
*   Prometheus UI é bom para explorar e testar queries, mas não é dashboard final. Para isso, usa-se **Grafana**.
*   **9.4 Running a Grafana container to visualize metrics** (Rodando um contêiner Grafana para visualizar métricas)
    *   Monitoramento é core, mas detalhes dependem da aplicação (o que medir, como capturar).
    *   Após dados estarem no Prometheus, visualização é mais padrão.
    *   Usar UI Prometheus para achar métricas e criar queries.
    *   Rodar Grafana e plugar queries em dashboards visuais.

🧠 Explicação detalhada e didática:

*   A página enfatiza que, embora PromQL seja capaz de consultas muito complexas, você pode obter grande valor com consultas relativamente simples, principalmente combinando **filtragem por labels** e **agregação (como `sum`)**.
*   A Figura 9.11 dá um exemplo prático: contar o número total de requisições HTTP que o serviço `image-gallery` respondeu com sucesso (código 200). Isso é feito selecionando a métrica `image_gallery_requests_total`, filtrando apenas as que têm a label `code` igual a `"200"`, e depois somando os resultados de todas as instâncias (`without(instance)`). Esse tipo de query é a base para construir painéis em um dashboard.
*   Introduz a ferramenta **Grafana** como a solução padrão para criar dashboards bonitos e funcionais a partir dos dados armazenados no Prometheus. Enquanto o Prometheus foca na coleta e armazenamento, o Grafana foca na visualização.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Testar Query de Exemplo:** No Prometheus UI (`http://localhost:9090/graph`), execute a query:
    ```promql
    sum(image_gallery_requests_total{code="200"}) without(instance)
    ```
    Você verá o total de requests bem-sucedidas para o serviço web.
*   O próximo passo será iniciar o Grafana e conectá-lo ao Prometheus.

💡 Analogias ou observações úteis (se houver):

*   Prometheus UI: A planilha bruta do agente do censo, com todos os dados e capacidade de fazer filtros e somas simples.
*   Grafana: Uma empresa de design gráfico que pega a planilha do censo e a transforma em gráficos de pizza, barras, mapas de calor bonitos e fáceis de entender para apresentar ao público (o dashboard).

---

📄 Página 178

🔍 Resumo dos principais pontos:

*   Figura 9.12: Mostra o dashboard final no Grafana para o app Image Gallery.
    *   Visão geral com info dos componentes e do runtime Docker.
    *   Queries suportam escala (funcionam em cluster).
*   Dashboard parece complexo, mas cada visualização usa query PromQL simples (filtros, agregações).
*   Autor empacotou o dashboard em imagem Grafana customizada (`diamol/ch09-grafana`) para facilitar execução.

🧠 Explicação detalhada e didática:

*   A página apresenta o objetivo final do capítulo: um dashboard completo no Grafana (Figura 9.12) que consolida informações chave sobre a aplicação Image Gallery e a infraestrutura Docker subjacente.
*   Embora o dashboard pareça visualmente rico e complexo, a página reforça que ele é construído a partir de consultas PromQL relativamente simples, como as vistas anteriormente (filtros por label + agregações como `sum`). O poder vem da combinação de múltiplas dessas queries simples em visualizações apropriadas (gráficos de linha, medidores, tabelas, etc.) dentro do Grafana.
*   Para simplificar o exercício, o autor já criou uma imagem Docker customizada para o Grafana (`diamol/ch09-grafana`) que vem com:
    1.  O Grafana instalado.
    2.  A conexão com o Prometheus (que também estará rodando) pré-configurada.
    3.  O dashboard da Figura 9.12 já importado e pronto para usar.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício usará um `docker-compose.yml` que define não apenas os serviços da aplicação (`image-gallery`, `iotd`, `accesslog`) e o `prometheus`, mas também um serviço `grafana` usando a imagem `diamol/ch09-grafana`.

💡 Analogias ou observações úteis (se houver):

*   Imagem Grafana customizada com dashboard: É como receber um relatório pré-formatado e conectado à planilha do censo. Você só precisa abrir o relatório (acessar a UI do Grafana) e os gráficos já estarão lá, atualizados com os dados mais recentes.

---

📄 Página 179

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar app completo com Prometheus e Grafana pré-configurado.
    *   1. Obter IP do host novamente (para Prometheus acessar Docker Engine) e salvar em variável de ambiente `HOST_IP` (diferente de `$hostIP` usado antes).
        *   Comandos PowerShell/Linux fornecidos.
    *   2. Rodar stack completa com `docker-compose -f ./docker-compose-with-grafana.yml ...`.
        *   Usa arquivo Compose que inclui Grafana.
        *   Mantém escala de `accesslog=3`.
    *   3. Gerar carga (loop `for` com 20 requests).
    *   4. Acessar Grafana UI: `http://localhost:3000`.
*   Login no Grafana: user `admin`, password `admin`. Pedirá para trocar senha (pode pular - Skip).
*   Navegar no Grafana:
    *   Tela inicial -> Clicar link "Home" no topo esquerdo -> Lista de dashboards (Figura 9.13).
    *   Clicar em "Image Gallery" para abrir o dashboard da aplicação.

🧠 Explicação detalhada e didática:

*   Este exercício reúne tudo: inicia a aplicação Image Gallery (com `accesslog` escalado), o Prometheus (configurado para raspar tudo, incluindo o Docker Engine usando o IP do host via `HOST_IP`) e o Grafana (pré-configurado com a conexão ao Prometheus e o dashboard "Image Gallery").
*   O passo 1 repete a necessidade de obter o IP do host, mas desta vez o armazena em uma variável de ambiente (`HOST_IP` ou `$env:HOST_IP`) que o arquivo `docker-compose-with-grafana.yml` espera encontrar para configurar corretamente o Prometheus.
*   O passo 2 usa `docker-compose` com um arquivo `.yml` específico que define todos os serviços (app, prometheus, grafana).
*   O passo 3 gera tráfego para popular as métricas.
*   O passo 4 instrui a acessar a interface web do Grafana na porta 3000. É necessário fazer login com as credenciais padrão (`admin`/`admin`) e então navegar até o dashboard pré-criado "Image Gallery".

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Definir HOST_IP (PowerShell):** (Use `host.docker.internal` se o IP direto não funcionar bem)
    ```powershell
    # $env:HOST_IP = (Get-NetIPConfiguration | Where-Object {$_.IPv4DefaultGateway -ne $null -and $_.NetAdapter.Status -eq 'Up' } | Select-Object -First 1).IPv4Address.IPAddress
    $env:HOST_IP = 'host.docker.internal' # Mais confiável com Docker Desktop/WSL2
    echo $env:HOST_IP
    ```
2.  **Navegar e Iniciar Stack Completa:** (Estando em `ch09/exercises`)
    ```powershell
    # Parar/Remover stack anterior, se houver
    # docker-compose -f docker-compose-scale.yml down
    docker-compose -f ./docker-compose-with-grafana.yml up -d --scale accesslog=3
    ```
3.  **Verificar:**
    ```powershell
    docker-compose -f ./docker-compose-with-grafana.yml ps
    # Deve mostrar todos os serviços (app, prometheus, grafana) Up.
    ```
4.  **Gerar Carga:**
    ```powershell
    for ($i=1; $i -le 30; $i++) { Invoke-WebRequest -UseBasicParsing http://localhost:8010 | Out-Null } # Aumentei para 30
    ```
5.  **Acessar Grafana:**
    *   Abra `http://localhost:3000`.
    *   Login: `admin` / `admin`.
    *   Clique "Skip" na tela de mudança de senha (ou mude, se preferir).
    *   Clique no título "Home" no canto superior esquerdo.
    *   Na lista que aparece, clique em "Image Gallery".

---

📄 Página 180

🔍 Resumo dos principais pontos:

*   Figura 9.13: Mostra a navegação no Grafana para encontrar o dashboard "Image Gallery".
*   Dashboard "Image Gallery" é exemplo razoável para produção.
*   Baseado nos "Golden Signals" do Google SRE (Site Reliability Engineering): Latency, Traffic, Errors, Saturation.
*   Análise detalhada da primeira linha de visualizações (Figura 9.14) para o serviço `image-gallery` (Web UI Go).

🧠 Explicação detalhada e didática:

*   A página começa mostrando como encontrar o dashboard no Grafana (Figura 9.13).
*   Afirma que o dashboard "Image Gallery" fornecido é um bom ponto de partida, inspirado nos "Quatro Sinais de Ouro" do monitoramento definidos pelo Google SRE, que são métricas essenciais para entender a saúde de um serviço:
    *   **Latência (Latency):** Tempo para servir requisições.
    *   **Tráfego (Traffic):** Quão demandado está o serviço (ex: requisições por segundo).
    *   **Erros (Errors):** Taxa de requisições que falham.
    *   **Saturação (Saturation):** Quão "cheio" está o serviço (quão perto do limite de capacidade - ex: uso de CPU, memória, disco).
*   Começa a analisar a primeira linha de painéis do dashboard (Figura 9.14), focada no serviço web `image-gallery`.

💻 Exemplos práticos (para meu ambiente Windows):

*   Com o dashboard "Image Gallery" aberto no Grafana (`http://localhost:3000/...`), observe a primeira linha de gráficos. A próxima página detalhará o que cada um representa e a query PromQL por trás dele.

---

📄 Página 181

🔍 Resumo dos principais pontos:

*   Figura 9.14: Detalhe da primeira linha do dashboard (serviço Web `image-gallery`).
*   **Análise dos 4 painéis:** (Relacionados aos Golden Signals)
    1.  **HTTP 200 Responses (Respostas HTTP 200):**
        *   Gráfico de linha mostrando nº total de respostas "OK" ao longo do tempo. (Relacionado a Traffic/Errors - sucesso).
        *   Query PromQL: `sum(image_gallery_requests_total{code="200"}) without(instance)`. (Soma o contador `_requests_total` filtrando por `code="200"`).
        *   Poderia adicionar gráfico similar para `code="500"` (Errors).
    2.  **In-Flight Requests (Requisições em Andamento):**
        *   Gráfico de linha mostrando nº de requisições ativas no momento. (Relacionado a Saturation/Traffic).
        *   Métrica Prometheus tipo `gauge`.
        *   Query PromQL: `sum(image_gallery_in_flight_requests) without(instance)`. (Soma o gauge entre instâncias).
    3.  **Memory In Use (Memória em Uso):**
        *   Gráfico de barras mostrando memória usada por cada contêiner do serviço. (Relacionado a Saturation).
        *   Query PromQL: `go_memstats_stack_inuse_bytes{job="image-gallery"}`. (Métrica padrão Go; filtro `{job=...}` necessário para distinguir de métricas com mesmo nome de outras fontes, como Docker Engine).
    4.  **Active Goroutines (Goroutines Ativas):**
        *   Gráfico de linha mostrando nº de Goroutines (unidades de trabalho concorrente em Go). Indicador de quão "ocupado" está o serviço. (Relacionado a Saturation).
        *   Query PromQL: `sum(go_goroutines{job="image-gallery"}) without(instance)`. (Soma métrica padrão Go entre instâncias).

🧠 Explicação detalhada e didática:

*   Esta página detalha as quatro primeiras visualizações do dashboard do Grafana, explicando o que cada uma mede e qual consulta PromQL simples é usada para obter os dados do Prometheus.
*   **Respostas 200:** Usa um `counter` (total de requests) e filtra por código de status 200 para mostrar o tráfego bem-sucedido.
*   **Requisições em Andamento:** Usa um `gauge` para mostrar quantas requisições estão sendo processadas *agora*, um indicador de carga e possível saturação.
*   **Memória em Uso:** Usa uma métrica padrão do runtime Go (`go_memstats...`) para mostrar o consumo de memória. O filtro `job="image-gallery"` é importante para selecionar apenas a métrica vinda dos contêineres desse serviço específico (já que o Prometheus pode estar coletando métricas com nomes similares de outras fontes). O gráfico de barras é útil para comparar múltiplas instâncias lado a lado se o serviço estivesse escalado.
*   **Goroutines Ativas:** Outra métrica padrão Go, indicando o nível de concorrência interna. Um aumento súbito pode indicar um problema ou sobrecarga.
*   A ideia é que essas quatro métricas juntas dão uma boa visão instantânea da saúde e carga do serviço web, relacionando-se com os "Sinais de Ouro" do Google SRE.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Explorar no Grafana:** No dashboard "Image Gallery" (`http://localhost:3000/...`):
    *   Passe o mouse sobre cada um desses 4 gráficos.
    *   Clique no título de um gráfico e selecione "Edit".
    *   Na tela de edição, você pode ver a(s) query(ies) PromQL exatas usadas na aba "Query".
    *   Explore as opções de visualização ("Visualization") para ver como o gráfico de linha ou barra é configurado.
    *   Cancele ou volte para o dashboard (sem salvar mudanças, a menos que queira experimentar).

---

📄 Página 182

🔍 Resumo dos principais pontos:

*   Visualizações nas outras linhas do dashboard usam queries similares. Não precisa de PromQL complexo. Escolher métricas e visualizações certas é chave.
*   **Tendências** são mais importantes que valores absolutos. Anomalias (picos, quedas) são mais fáceis de ver.
*   Conjunto de métricas deve ajudar a ver anomalias e **correlações** rapidamente (ex: erros subindo e Goroutines subindo -> saturação?).
*   Grafana é ferramenta poderosa, mas simples de usar para o básico.
*   Dashboards podem ser editados (adicionar/editar painéis, redimensionar, mover) e **exportados** como JSON.
*   **Exercício Try-it-now:** Adicionar painel de Erros HTTP ao dashboard.
    *   Métrica de erro (HTTP 500) está faltando.
    *   1. No dashboard, clicar ícone "Add Panel" (gráfico de barras com '+'). (Figura 9.15).

🧠 Explicação detalhada e didática:

*   A página reforça que dashboards eficazes podem ser construídos com queries PromQL simples, focando em selecionar as métricas corretas e a melhor forma de visualizá-las (linhas para séries temporais, barras para comparações, medidores para valores atuais, etc.).
*   Enfatiza a importância de observar **tendências e anomalias** nos gráficos, em vez de se fixar em valores absolutos. Um pico repentino no uso de memória é mais informativo do que saber que o uso médio é X MB.
*   A capacidade de **correlacionar** diferentes métricas no mesmo dashboard é vital para diagnóstico (ex: ver que um aumento nos erros coincide com um aumento na latência de uma dependência externa).
*   Menciona que os dashboards do Grafana são customizáveis (editar painéis) e podem ser salvos/exportados como arquivos JSON, facilitando o versionamento e compartilhamento.
*   O exercício instrui a adicionar um novo painel ao dashboard existente para visualizar as requisições que resultaram em erro HTTP 500 para o serviço `image-gallery`, completando os "Sinais de Ouro" (Tráfego, Erros, Saturação, Latência - embora latência não esteja explicitamente mostrada nesses exemplos).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Iniciar Edição:** No dashboard "Image Gallery" no Grafana, localize e clique no ícone "Add panel" (geralmente no canto superior direito, parece um gráfico com um '+').

💡 Analogias ou observações úteis (se houver):

*   Observar tendências vs. valores: É como monitorar a febre de um paciente. Saber que a temperatura *subiu* de 37°C para 39°C rapidamente é mais importante do que apenas saber que *agora* está 39°C.
*   Correlação: Ver no painel do carro que a temperatura do motor está subindo *ao mesmo tempo* que a luz de óleo acendeu ajuda a diagnosticar o problema mais rapidamente.

---

📄 Página 183

🔍 Resumo dos principais pontos:

*   Figura 9.15: Mostra a barra de ferramentas do Grafana com ícone "Add Panel" destacado.
*   **Continuação do Exercício (Adicionar Painel de Erros):**
    *   2. Clicar "Add Query" na janela do novo painel.
    *   3. Selecionar "Prometheus" como data source.
    *   4. No campo "Metrics" (ou Query), colar a expressão PromQL: `sum(image_gallery_requests_total{code="500"}) without(instance)`.
    *   5. Painel deve se parecer com Figura 9.16. Pode precisar gerar alguns erros (difícil no app de exemplo?) para ver dados.
    *   6. Pressionar `Esc` para voltar ao dashboard.
*   Pode-se redimensionar (arrastar canto) e mover (arrastar título) painéis.
*   Salvar/Exportar dashboard: Ícone "Share Dashboard" (Figura 9.15) > Export > Save to file.
*   **Empacotando Imagem Grafana Customizada:**
    *   Imagem `diamol/ch09-grafana` já faz isso.
*   **Listagem 9.3:** Dockerfile para criar imagem Grafana customizada (`diamol/ch09-grafana`).
    ```dockerfile
    FROM diamol/grafana:6.4.3 # Baseia-se em imagem Grafana específica
    # Copia arquivos de provisionamento (configuração) para locais esperados pelo Grafana
    COPY datasource-prometheus.yaml ${GF_PATHS_PROVISIONING}/datasources/
    COPY dashboard-provider.yaml ${GF_PATHS_PROVISIONING}/dashboards/
    # Copia o arquivo JSON do dashboard exportado para pasta de dashboards
    COPY dashboard.json /var/lib/grafana/dashboards/
    ```
*   Explicação: Imagem começa de uma base Grafana, depois copia arquivos de configuração. Grafana usa **provisioning** para configurar data sources e dashboards automaticamente na inicialização a partir de arquivos em locais específicos.

🧠 Explicação detalhada e didática:

*   A página guia através da adição do novo painel no Grafana para mostrar erros HTTP 500. A query PromQL é similar à de sucessos (HTTP 200), mas filtra por `code="500"`.
*   Menciona como reorganizar e salvar/exportar o dashboard resultante como um arquivo JSON. Exportar é útil para colocar a definição do dashboard sob controle de versão (Git) junto com o código da aplicação e os arquivos Docker/Compose.
*   Explica como a imagem `diamol/ch09-grafana` (usada no exercício da p.179) foi criada. A Listagem 9.3 mostra um Dockerfile que:
    1.  Começa com uma imagem base oficial do Grafana (`diamol/grafana` que por sua vez vem de `grafana/grafana`).
    2.  Usa a funcionalidade de **provisioning** do Grafana. Isso permite pré-configurar o Grafana copiando arquivos YAML e JSON para diretórios específicos dentro da imagem.
    3.  `datasource-prometheus.yaml`: Arquivo YAML que diz ao Grafana como se conectar ao Prometheus (endereço, etc.).
    4.  `dashboard-provider.yaml`: Arquivo YAML que diz ao Grafana para procurar por arquivos de dashboard em um diretório específico.
    5.  `dashboard.json`: O arquivo JSON do dashboard (que poderia ter sido exportado do Grafana UI) é copiado para o diretório que o provider está monitorando.
*   Quando um contêiner é iniciado a partir desta imagem customizada, o Grafana automaticamente lê esses arquivos de provisionamento e configura a conexão com o Prometheus e carrega o dashboard "Image Gallery", tudo sem intervenção manual.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Completar Adição do Painel:** Siga os passos no Grafana UI para adicionar a query `sum(image_gallery_requests_total{code="500"}) without(instance)`. Nomeie o painel (ex: "HTTP 500 Errors"). Pressione `Esc`.
2.  **Reorganizar/Salvar (Opcional):** Arraste o novo painel para a posição desejada. Clique no ícone de disquete (Save dashboard) no topo direito. Dê uma mensagem de salvamento (ex: "Added 500 errors panel") e clique Save.
3.  **Exportar (Opcional):** Clique no ícone de compartilhamento (ao lado do save) > Aba "Export" > Clique "Save to file". Isso baixa o `dashboard.json`.
4.  **Entender Provisioning:** Se você explorar o código fonte do livro em `ch09/images/grafana`, encontrará os arquivos `datasource-prometheus.yaml`, `dashboard-provider.yaml`, `dashboard.json` e o `Dockerfile` (similar à Listagem 9.3) usados para construir `diamol/ch09-grafana`.

---

📄 Página 184

🔍 Resumo dos principais pontos:

*   Figura 9.16: Mostra o painel de erros HTTP 500 sendo adicionado no Grafana.
*   Provisioning do Grafana pode fazer mais (criar usuários, playlists de dashboards).
*   Exemplo: Criar imagem com múltiplos dashboards e usuário read-only para exibir em tela grande no escritório, rotacionando automaticamente (playlist).
*   **9.5 Understanding the levels of observability** (Entendendo os níveis de observabilidade)
    *   Observabilidade é requisito chave para produção.
    *   Introdução a Prometheus/Grafana também mostra poder do ecossistema Docker e padrões emergentes.
    *   Monitoramento era dor de cabeça inicial com contêineres; hoje há caminho testado.

🧠 Explicação detalhada e didática:

*   A página conclui a parte do Grafana, mencionando que o sistema de provisioning é bastante flexível, permitindo automatizar outras configurações além de data sources e dashboards.
*   A seção 9.5 faz uma recapitulação da importância da **observabilidade** (ter insights sobre o sistema) ao usar contêineres em produção.
*   Enfatiza que a pilha Prometheus + Grafana representa um padrão bem estabelecido e eficaz no ecossistema de contêineres para alcançar essa observabilidade, superando os desafios iniciais que existiam quando os contêineres surgiram.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. Consolida a importância das ferramentas apresentadas no capítulo.

---

📄 Página 185

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 9.17): Sumariza a arquitetura de monitoramento com Prometheus no centro.
*   Dashboard apresentado é visão geral; produção teria dashboards adicionais mais detalhados:
    *   Dashboard de Infraestrutura (uso de disco/CPU/memória/rede dos nós do cluster).
    *   Dashboards por Componente (detalhes específicos de cada serviço, ex: tempo por endpoint de API).
*   Dashboard sumário é crítico: deve mostrar info chave rapidamente para identificar problemas.
*   **9.6 Lab** (Laboratório)
    *   Objetivo: Adicionar monitoramento (Prometheus/Grafana) ao app **To-Do List** (de capítulos anteriores).
    *   Ponto de Partida: Nova imagem `diamol/ch09-todo-list` já expõe métricas (`/metrics`).
*   **Tarefa:**
    1.  Criar arquivo Docker Compose para rodar:
        *   App To-Do (`diamol/ch09-todo-list`).
        *   Contêiner Prometheus.
        *   Contêiner Grafana.
    2.  Configurar Prometheus para raspar métricas do app To-Do.
    3.  Configurar Grafana com dashboard para mostrar 3 métricas chave do To-Do: nº tarefas criadas, total requisições HTTP, requisições HTTP em andamento.

🧠 Explicação detalhada e didática:

*   A Figura 9.17 resume a arquitetura de monitoramento discutida: aplicações expõem métricas, Prometheus coleta, Grafana visualiza.
*   A página sugere que, em um cenário real, você provavelmente teria *múltiplos* dashboards no Grafana: um de alto nível (como o "Image Gallery"), outros focados na infraestrutura do cluster, e talvez dashboards ainda mais detalhados para cada microsserviço individual.
*   O **Lab do Capítulo 9** pede para aplicar o que foi aprendido a uma aplicação diferente: o To-Do List.
*   Você precisará criar um `docker-compose.yml` que orquestre o To-Do app, o Prometheus e o Grafana. O desafio principal estará em:
    1.  **Configurar o Prometheus:** Criar um arquivo `prometheus.yml` (ou usar uma imagem Prometheus customizada) que saiba como encontrar e raspar o endpoint `/metrics` do serviço `todo-list`.
    2.  **Configurar o Grafana:** Criar um dashboard (seja manualmente na UI ou via provisioning com JSON) que se conecte ao Prometheus e exiba as métricas específicas do To-Do mencionadas (nº tarefas, total requests, in-flight requests). Você precisará primeiro rodar o To-Do e o Prometheus, explorar as métricas disponíveis na UI do Prometheus para encontrar os nomes corretos dessas métricas, e então construir as queries PromQL para o Grafana.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 9:**
    1.  **Criar `docker-compose.yml`:** Baseado no `docker-compose-with-grafana.yml` dos exercícios, mas:
        *   Remova os serviços `iotd`, `accesslog`, `image-gallery`.
        *   Adicione o serviço `todo-list` usando a imagem `diamol/ch09-todo-list`, conecte à rede, publique uma porta (ex: 8090:80).
        *   Mantenha os serviços `prometheus` e `grafana`.
    2.  **Configurar Prometheus:**
        *   Crie um arquivo `prometheus.yml` (ou modifique o existente se estiver construindo imagem customizada).
        *   Adicione um `job` para o `todo-list`, especificando `job_name: "todo-list"`, `metrics_path: /metrics`, e `static_configs: - targets: ["todo-list"]`. (Assumindo que o serviço se chama `todo-list` no Compose).
        *   Se usar imagem Prometheus padrão, monte este arquivo como volume: `-v ./prometheus.yml:/etc/prometheus/prometheus.yml`.
    3.  **Configurar Grafana:**
        *   **Opção A (Manual):** Rode `docker-compose up -d`. Acesse Prometheus (`:9090`), encontre os nomes das métricas do To-Do (ex: `todo_items_created_total`, `http_requests_total{app="todo"}`, `http_requests_in_flight{app="todo"}` - nomes hipotéticos). Acesse Grafana (`:3000`), configure Data Source Prometheus (se não provisionado), crie novo dashboard, adicione painéis com as queries PromQL corretas.
        *   **Opção B (Provisioning):** Exporte o dashboard criado manualmente como JSON. Crie um `Dockerfile` para Grafana (similar à Listagem 9.3) que copie seu `datasource.yaml`, `provider.yaml` e `dashboard.json`. Atualize o `docker-compose.yml` para construir e usar esta imagem Grafana customizada.

---

📄 Página 186

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 9 no GitHub, com imagem do dashboard final.

🧠 Explicação detalhada e didática:

*   Fornece o link para a solução completa do Lab 9 no repositório do livro, caso o leitor precise de ajuda ou queira comparar sua solução.

💻 Exemplos práticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 9, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch09/lab/README.md

---

📄 Página 187

🔍 Resumo dos principais pontos:

*   **Capítulo 10: Running multiple environments with Docker Compose** (Rodando múltiplos ambientes com Docker Compose)
*   Revisão Cap. 7: Usou Compose para definir e gerenciar apps multi-contêiner.
*   Revisão Caps. 8-9: Adicionou features de produção (health checks, monitoramento).
*   Foco Cap. 10: Retornar ao Compose para gerenciar **múltiplos ambientes** (dev, teste, prod). Nem todas as features de produção são necessárias em todos os ambientes.
*   **Portabilidade Docker:** Benefício maior é rodar a *mesma* imagem em qualquer lugar, eliminando **drift** (diferenças) entre ambientes.
*   Drift: Causado por processos manuais, leva a falhas de deploy. Docker (imagem única) resolve isso.
*   Mas ainda precisa de **flexibilidade** para comportamentos diferentes por ambiente.
*   Compose oferece isso via features avançadas (overrides, etc.).
*   **10.1 Deploying many applications with Docker Compose** (Implantando muitas aplicações com Docker Compose)
    *   Compose é ótimo para devs e ambientes não-produção (teste, UAT, etc.).
    *   Permite rodar múltiplas versões/ambientes do mesmo app em hardware compartilhado (máxima utilização).
    *   Necessidade de **diferenças** entre instâncias (ex: portas publicadas, arquivos de dados).
    *   Como Compose identifica apps: **Nome do Projeto (Project Name)**.
        *   Usado para prefixar nomes de recursos (contêineres, redes, volumes) e em labels.
        *   Padrão: Nome do diretório onde `docker-compose.yml` está.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 10, que retorna ao Docker Compose com foco em gerenciar **múltiplos ambientes** (desenvolvimento, teste, homologação, etc.) usando a mesma base de código e imagens Docker.
*   Relembra o problema do "drift" de configuração entre ambientes em setups tradicionais e como o Docker (com sua imagem única) ajuda a eliminar isso. No entanto, ainda precisamos de maneiras de configurar *comportamentos* diferentes (ex: conectar a um banco de dados de teste vs. produção, habilitar logging detalhado em dev) para a mesma imagem em ambientes distintos. O Compose oferece mecanismos para isso.
*   A seção 10.1 aborda um pré-requisito para rodar múltiplos ambientes na mesma máquina Docker: como o Compose distingue uma instância de aplicação da outra. A chave é o **Nome do Projeto**. Por padrão, o Compose usa o nome da pasta onde o arquivo `docker-compose.yml` reside como o nome do projeto. Ele então usa esse nome como prefixo ao criar contêineres, redes, etc. (ex: se a pasta é `myapp`, o serviço `web` vira contêiner `myapp_web_1`).

💻 Exemplos práticos (para meu ambiente Windows):

*   Se você rodar `docker-compose up` na pasta `C:\DockerLabs\App1`, os contêineres serão nomeados como `app1_servicename_1`. Se rodar `docker-compose up` na pasta `C:\DockerLabs\App2`, os contêineres serão `app2_servicename_1`. Eles são considerados projetos diferentes.
*   O próximo exercício mostrará o que acontece se tentarmos rodar o *mesmo* projeto duas vezes do *mesmo* diretório.

---

📄 Página 188

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Demonstrar conflito ao tentar rodar mesmo projeto duas vezes.
    *   Navegar para `ch10/exercises`.
    *   1. Rodar app "numbers": `docker-compose -f ./numbers/docker-compose.yml up -d`.
    *   2. Rodar app "todo-list": `docker-compose -f ./todo-list/docker-compose.yml up -d`. (Funciona, pois são diretórios/projetos diferentes).
    *   3. Tentar rodar "todo-list" *novamente* do mesmo diretório/arquivo: `docker-compose -f ./todo-list/docker-compose.yml up -d`.
*   Resultado esperado (Figura 10.1): Terceiro comando não faz nada (ou diz que já está rodando/atualizado). Compose acha que app já está no estado desejado.
*   **Nomeação de Recursos Compose:**
    *   Padrão: `<project_name>_<service_name>_<index>`.
    *   `<project_name>` padrão = nome do diretório.
*   Diagrama (Figura 10.2): Ilustra nome do contêiner `todo-list_todo-web_1`.
*   **Solução para rodar múltiplas cópias:** Sobrescrever nome do projeto padrão usando flag `-p` (ou `--project-name`).
*   **Exercício Try-it-now:** Rodar segunda instância do To-Do usando `-p`.
    *   Comandos:
        *   `docker-compose -f ./todo-list/docker-compose.yml -p todo-test up -d`.
        *   `docker container ls` (verificar ambos os contêineres `todo-list...` e `todo-test...`).
        *   `docker container port todo-test_todo-web_1 80` (descobrir porta aleatória da instância 'test').
    *   O `.yml` do To-Do usado aqui provavelmente publica porta 80 em porta aleatória do host (como `- "80"`), por isso a necessidade de descobrir a porta com `docker port`.

🧠 Explicação detalhada e didática:

*   O primeiro exercício confirma que, se você rodar `docker-compose up` para o mesmo projeto (mesmo diretório/nome de projeto padrão) múltiplas vezes, ele não cria cópias adicionais; ele apenas garante que a aplicação esteja rodando conforme definido no arquivo (ou a atualiza se o arquivo mudou).
*   Para rodar *instâncias separadas* da *mesma* aplicação (definida pelo mesmo `docker-compose.yml`) na mesma máquina Docker, você precisa dar a cada instância um **nome de projeto diferente**. Isso é feito usando a flag `-p <novo_nome_projeto>` no comando `docker-compose`.
*   O segundo exercício demonstra isso:
    *   Já existe uma instância do To-Do rodando (com nome de projeto padrão `todo-list`, derivado do nome da pasta).
    *   O comando `docker-compose -f ./todo-list/docker-compose.yml -p todo-test up -d` inicia uma *segunda* instância, mas informa ao Compose para usar `todo-test` como nome do projeto.
    *   O Compose agora cria um novo contêiner chamado `todo-test_todo-web_1`, separado do `todo-list_todo-web_1`.
*   Como o arquivo `todo-list/docker-compose.yml` provavelmente usa publicação de porta aleatória (`ports: - "80"`), o exercício usa `docker container port <nome_container> 80` para descobrir em qual porta do host a segunda instância (`todo-test_...`) ficou acessível.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch10\exercises
    ```
2.  **Rodar Apps (Passos 1 e 2):**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml up -d
    docker-compose -f ./todo-list/docker-compose.yml up -d
    ```
3.  **Tentar Rodar To-Do Novamente (Passo 3):**
    ```powershell
    docker-compose -f ./todo-list/docker-compose.yml up -d
    # Observe que a saída diz algo como 'network ... is up-to-date', 'container ... is up-to-date'
    ```
4.  **Rodar Segunda Instância To-Do (com -p):**
    ```powershell
    docker-compose -f ./todo-list/docker-compose.yml -p todo-test up -d
    ```
5.  **Verificar Contêineres:**
    ```powershell
    docker container ls
    # Você deve ver containers com prefixo 'numbers_', 'todo-list_', e 'todo-test_'
    ```
6.  **Descobrir Porta da Instância 'test':**
    ```powershell
    docker container port todo-test_todo-web_1 80
    # Anote a porta retornada (ex: 0.0.0.0:32770)
    ```
    Acesse `http://localhost:<porta_anotada>` para ver a segunda instância.

---

📄 Página 189

🔍 Resumo dos principais pontos:

*   Figura 10.3: Mostra a saída do exercício anterior, com novo contêiner `todo-test_...` criado e porta descoberta.
*   Usar `-p` permite rodar múltiplas cópias de muitos apps diferentes.
*   Problema: Porta aleatória não é ideal para ops/testes.
*   Para suportar setups diferentes (ex: portas fixas diferentes por ambiente), pode-se duplicar Compose files e editar, *mas* isso leva a duplicação e drift.
*   **Solução Melhor:** Arquivos de **Override** do Docker Compose.
*   **10.2 Using Docker Compose override files** (Usando arquivos de override do Docker Compose)
    *   Evita duplicação e manutenção difícil de múltiplos arquivos quase idênticos.
    *   Compose permite **mesclar** múltiplos arquivos `.yml`. Propriedades de arquivos posteriores na mesclagem *sobrescrevem* as de arquivos anteriores.
*   Diagrama (Figura 10.4): Ilustra estrutura com overrides.
    *   **Core `docker-compose.yml`:** Contém estrutura base e configurações comuns a *todos* os ambientes.
    *   **Override `.yml` por ambiente (ex: `dev.yml`, `test.yml`, `prod.yml`):** Contém *apenas* as configurações específicas daquele ambiente (ex: portas diferentes, variáveis de ambiente diferentes, volumes diferentes). Não duplica o conteúdo do core.

🧠 Explicação detalhada e didática:

*   A página conclui que usar `-p` resolve o problema de rodar múltiplas instâncias, mas a questão de ter configurações *diferentes* para cada instância (ex: portas fixas diferentes, conexão a bancos de dados diferentes) ainda persiste. A solução óbvia, mas ruim, seria copiar e colar o `docker-compose.yml` para cada ambiente (`docker-compose-dev.yml`, `docker-compose-test.yml`) e editar as partes necessárias. Isso leva a muita duplicação e dificulta a manutenção (uma mudança comum precisa ser feita em vários arquivos).
*   A seção 10.2 introduz a solução elegante do Docker Compose: **arquivos de override**. A ideia é:
    1.  Ter um arquivo base (`docker-compose.yml`) com toda a estrutura e configurações comuns.
    2.  Ter arquivos adicionais (`docker-compose.override.yml` por padrão, ou arquivos com nomes específicos como `docker-compose.prod.yml`) que contêm *apenas* as definições que precisam ser diferentes ou adicionadas para um ambiente específico.
    3.  Ao executar `docker-compose`, você pode instruí-lo (usando múltiplas flags `-f`) a ler *ambos* os arquivos. O Compose inteligentemente **mescla** as definições, com as configurações dos arquivos posteriores na linha de comando tendo prioridade e sobrescrevendo as dos arquivos anteriores.
*   A Figura 10.4 ilustra essa separação: um arquivo core + arquivos específicos por ambiente.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará a flag `-f` múltiplas vezes nos comandos `docker-compose`:
    ```powershell
    # Exemplo conceitual para rodar ambiente de produção
    docker-compose -f docker-compose.yml -f docker-compose.prod.yml -p myapp-prod up -d
    ```
    Aqui, o Compose primeiro lê `docker-compose.yml`, depois lê `docker-compose.prod.yml`. Se ambos definirem, por exemplo, a porta para o serviço `web`, a definição de `docker-compose.prod.yml` será a utilizada.

💡 Analogias ou observações úteis (se houver):

*   Arquivos Override: É como ter um manual de instruções base (`docker-compose.yml`) e depois add-ons separados (`dev.yml`, `test.yml`) que dizem "Na página X, ignore o passo Y e faça Z" ou "Adicione o passo W extra aqui". Você lê o manual base junto com o add-on relevante para montar a versão correta.

---

📄 Página 190

🔍 Resumo dos principais pontos:

*   Abordagem com overrides é **sustentável**:
    *   Mudanças comuns (ex: atualizar imagem base) são feitas *uma vez* no arquivo core.
    *   Mudanças específicas de ambiente só afetam o arquivo daquele ambiente.
    *   Arquivos override documentam claramente as diferenças entre ambientes.
*   **Listagem 10.1:** Exemplo simples de override para mudar a tag da imagem.
    *   Arquivo core (`docker-compose.yml`): define `image: diamol/ch06-todo-list` (implícito `:latest`).
    *   Arquivo override (`docker-compose-v2.yml`): define `image: diamol/ch06-todo-list:v2`.
    *   Override só precisa especificar a propriedade a ser mudada, mantendo a estrutura (`services > todo-web > image`).
*   Comando `docker-compose config`: Valida e **mostra o resultado final da mesclagem** dos arquivos Compose, sem implantar. Útil para verificar o efeito do override.
*   **Exercício Try-it-now:** Usar `config` para ver a mesclagem dos arquivos da Listagem 10.1.
    *   Comando: `docker-compose -f ./todo-list/docker-compose.yml -f ./todo-list/docker-compose-v2.yml config`.

🧠 Explicação detalhada e didática:

*   A página reforça os benefícios da abordagem de overrides para manutenção a longo prazo.
*   A Listagem 10.1 mostra um exemplo mínimo: o arquivo override (`docker-compose-v2.yml`) contém apenas o necessário para mudar a `image` do serviço `todo-web` para a versão `:v2`. Note que a estrutura (`services:` > `todo-web:`) precisa ser mantida para que o Compose saiba onde aplicar a mudança.
*   Introduz o comando `docker-compose config`. Ele é extremamente útil ao trabalhar com múltiplos arquivos Compose. Ele executa o mesmo processo de leitura e mesclagem que o `docker-compose up` faria, mas em vez de aplicar as mudanças, ele apenas **imprime o arquivo YAML resultante final** no console (após validação). Isso permite que você veja exatamente qual configuração será usada antes de realmente implantar a aplicação.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch10\exercises
    ```
2.  **Verificar Mesclagem:**
    ```powershell
    docker-compose -f ./todo-list/docker-compose.yml -f ./todo-list/docker-compose-v2.yml config
    ```
3.  **Analisar Saída:** Procure pela seção `services:` > `todo-web:`. Você verá que todas as propriedades (`ports`, `environment`, `networks`) vieram do primeiro arquivo, mas a propriedade `image:` mostrará `diamol/ch06-todo-list:v2`, que veio do segundo arquivo, sobrescrevendo a original. (A ordem das seções na saída pode ser alfabética, não a original).

---

📄 Página 191

🔍 Resumo dos principais pontos:

*   Figura 10.5: Mostra a saída do `docker-compose config`, destacando a tag `:v2` da imagem que foi sobrescrita.
*   Ordem dos arquivos `-f` importa: arquivos à direita sobrescrevem os da esquerda. `config` ajuda a verificar.
*   Saída do `config` ordena alfabeticamente (networks, services, version), útil para comparar (`diff`) entre versões.
*   Exemplo mais realista: App Random Number (`./numbers/`) com múltiplos overrides:
    *   `docker-compose.yml`: Core (serviços web/api, sem portas/redes).
    *   `docker-compose-dev.yml`: Para dev (cria rede dev, adiciona portas, desabilita healthchecks).
    *   `docker-compose-test.yml`: Para teste (cria rede test, healthchecks, publica porta web, API interna).
    *   `docker-compose-uat.yml`: Para UAT (cria rede uat, porta 80 web, restart always, healthchecks rigorosos).
*   **Listagem 10.2:** Conteúdo do override `dev.yml`. Mostra que só contém as diferenças/adições (sem `image:`).

🧠 Explicação detalhada e didática:

*   A página confirma a análise da saída do `docker-compose config`. Enfatiza que a **ordem** em que você lista os arquivos com `-f` é crucial, pois determina qual arquivo tem prioridade para sobrescrever.
*   Introduz um exemplo mais completo e realista usando a aplicação Random Number, com arquivos Compose separados para os ambientes `dev`, `test` e `uat`, além do arquivo `core`. Cada arquivo override define apenas as configurações específicas daquele ambiente (portas, redes, configurações de healthcheck, políticas de reinício, etc.), aproveitando a definição base do arquivo core.
*   A Listagem 10.2 mostra o conteúdo do `docker-compose-dev.yml`. Note que ele não repete a definição da `image`, apenas adiciona ou modifica outras propriedades como `ports`, `healthcheck` (desabilitando-o para dev) e define uma rede específica para o ambiente de desenvolvimento.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Explorar Arquivos:** Navegue para `C:\DockerLabs\diamol\ch10\exercises\numbers` no seu explorador de arquivos ou VS Code. Abra e compare `docker-compose.yml`, `docker-compose-dev.yml`, `docker-compose-test.yml`, `docker-compose-uat.yml`. Observe como eles se complementam.
2.  **Verificar Mesclagem para Dev:**
    ```powershell
    # Estando na pasta ch10\exercises
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-dev.yml config
    # Observe como as definições foram combinadas.
    ```
3.  **Verificar Mesclagem para UAT:**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-uat.yml config
    # Compare a saída com a do Dev. Note as diferenças nas portas, redes, restart, healthcheck.
    ```

---

📄 Página 192

🔍 Resumo dos principais pontos:

*   Arquivos override (`test.yml`, `uat.yml`) seguem mesmo padrão (só diferenças).
*   Cada ambiente usa portas e redes diferentes -> podem rodar simultaneamente na mesma máquina.
*   **Exercício Try-it-now:** Rodar o app Random Number em múltiplos ambientes (dev, test, uat) simultaneamente.
    *   1. Remover contêineres existentes: `docker container rm -f $(docker container ls -aq)`.
    *   2. Rodar Dev: `docker-compose -f ./numbers/... -f ./numbers/dev.yml -p numbers-dev up -d`.
    *   3. Rodar Test: `docker-compose -f ./numbers/... -f ./numbers/test.yml -p numbers-test up -d`.
    *   4. Rodar UAT: `docker-compose -f ./numbers/... -f ./numbers/uat.yml -p numbers-uat up -d`.
*   Resultado: 3 cópias do app rodando, isoladas por redes Docker diferentes. Figura 10.6 mostra saída.

🧠 Explicação detalhada e didática:

*   A página confirma que os arquivos de override para os ambientes de Teste e UAT (User Acceptance Testing) seguem o mesmo princípio do `dev.yml`, apenas definindo as configurações específicas para cada um. A chave para permitir que todos rodem ao mesmo tempo na mesma máquina Docker é que cada override define:
    *   Um **nome de rede diferente** (ex: `numbers-dev`, `numbers-test`, `numbers-uat`).
    *   **Portas publicadas diferentes** para os serviços expostos (ex: web em 8088 no dev, 8080 no test, 80 no uat).
*   O exercício instrui a iniciar as três instâncias da aplicação, uma para cada ambiente. Note que para cada comando `docker-compose up`, são usadas:
    *   Múltiplas flags `-f`: para carregar o arquivo core *e* o override do ambiente específico.
    *   A flag `-p`: para dar um nome de projeto único a cada instância (`numbers-dev`, `numbers-test`, `numbers-uat`).
*   O resultado será a execução simultânea de três conjuntos de contêineres (web + api), cada conjunto isolado em sua própria rede Docker, acessíveis através de portas diferentes no host.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Opcional: remover redes antigas se houver conflito
    # docker network rm numbers-dev numbers-test numbers-uat numbers-app (ignore erros se não existirem)
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch10\exercises
    ```
3.  **Rodar Ambientes:**
    ```powershell
    # Dev
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-dev.yml -p numbers-dev up -d
    # Test
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-test.yml -p numbers-test up -d
    # UAT
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-uat.yml -p numbers-uat up -d
    ```
4.  **Verificar:**
    ```powershell
    docker ps
    # Você deve ver 6 containers:
    # numbers-dev_numbers-web_1, numbers-dev_numbers-api_1
    # numbers-test_numbers-web_1, numbers-test_numbers-api_1
    # numbers-uat_numbers-web_1, numbers-uat_numbers-api_1
    docker network ls
    # Você deve ver as redes numbers-dev, numbers-test, numbers-uat
    ```

---

📄 Página 193

🔍 Resumo dos principais pontos:

*   Figura 10.6: Mostra a saída da criação dos 3 ambientes.
*   Agora existem 3 deployments isolados:
    *   UAT: `http://localhost` (porta 80).
    *   Test: `http://localhost:8080`.
    *   Dev: `http://localhost:8088`.
*   Mesma aplicação, mas cada instância web só vê a API na *sua própria rede*.
*   Isolamento: Causar bug na API do Dev não afeta Test/UAT.
*   Contêineres usam DNS (`numbers-api`) para comunicação, mas Docker restringe tráfego à rede do contêiner.
*   Diagrama (Figura 10.7): Ilustra isolamento via redes Docker separadas para cada ambiente.

🧠 Explicação detalhada e didática:

*   A página confirma que as três instâncias da aplicação estão rodando isoladamente, acessíveis em portas diferentes no `localhost`.
*   O ponto crucial é o **isolamento proporcionado pelas redes Docker separadas**. Embora a aplicação web em todos os ambientes tente se conectar ao serviço chamado `numbers-api` (definido no `docker-compose.yml`), o DNS interno do Docker resolverá esse nome *apenas* para o IP do contêiner `numbers-api` que está *na mesma rede* que o contêiner web que fez a consulta.
*   Portanto, o `numbers-dev_numbers-web_1` só consegue falar com o `numbers-dev_numbers-api_1`, o `numbers-test_numbers-web_1` só fala com o `numbers-test_numbers-api_1`, e assim por diante. Isso garante que os ambientes não interfiram uns nos outros. A Figura 10.7 visualiza essa separação.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Testar Acesso:**
    *   Abra `http://localhost` no navegador (UAT).
    *   Abra `http://localhost:8080` (Test).
    *   Abra `http://localhost:8088` (Dev).
    *   Todas devem mostrar a mesma UI inicial do Random Number.
2.  **Testar Isolamento:**
    *   Vá para a aba do ambiente Dev (`:8088`). Clique no botão "Get Random Number" 4 ou mais vezes para causar o bug na API *deste ambiente*. O botão deve parar de funcionar ou mostrar erro.
    *   Vá para as abas dos ambientes Test (`:8080`) e UAT (`:80`). Clique no botão. Eles devem continuar funcionando normalmente (pelas primeiras 3 vezes), pois a falha na API do Dev não os afetou.

💡 Analogias ou observações úteis (se houver):

*   Redes Docker separadas por ambiente: É como ter sistemas de interfone completamente separados para prédios diferentes (Dev, Test, UAT) que por acaso estão no mesmo terreno (máquina host). Ligar para o "Apartamento API" do prédio Dev só conecta com a API do Dev, não com as dos outros prédios.

---

📄 Página 194

🔍 Resumo dos principais pontos:

*   Relembra: Compose é client-side, precisa dos arquivos `.yml` corretos e nome do projeto (`-p`) para gerenciar apps.
*   Se quiser derrubar (`down`) um ambiente específico (ex: Test), precisa usar o mesmo comando `docker-compose` (com os mesmos `-f` e `-p`) que usou para subir (`up`).
*   **Exercício Try-it-now:** Tentar derrubar o ambiente 'test' de formas incorretas e depois da forma correta.
    *   1. `docker-compose down` (sem `-f`, `-p`): Não remove nada (procura projeto 'numbers' padrão, não acha).
    *   2. `docker-compose -f ... -f test.yml down` (sem `-p`): Tenta remover projeto 'numbers' com config de teste. *Pode causar erros* ao tentar remover rede (nome explícito) que tem containers de 'numbers-test' ainda ligados.
    *   3. `docker-compose -f ... -f test.yml -p numbers-test down` (Correto): Encontra e remove recursos do projeto 'numbers-test'.

🧠 Explicação detalhada e didática:

*   Esta página aborda como **parar e remover** os recursos de um ambiente específico que foi iniciado com múltiplos arquivos `-f` e a flag `-p`.
*   A regra é: o comando `docker-compose down` (ou `stop`, `start`, `ps`, etc.) precisa receber exatamente as **mesmas informações de contexto** (arquivos `-f` na ordem correta e nome do projeto `-p`) que foram usadas no comando `docker-compose up` para aquele ambiente específico.
*   O exercício demonstra isso por tentativa e erro:
    1.  Tentar `down` sem argumentos falha porque o Compose procura pelo projeto padrão do diretório atual (`numbers`), que não corresponde aos projetos que estão rodando (`numbers-dev`, `numbers-test`, `numbers-uat`).
    2.  Tentar `down` com os arquivos `-f` corretos mas *sem* o `-p numbers-test` também falha de forma confusa. O Compose usa o projeto padrão (`numbers`) mas aplica a configuração de rede do `test.yml` (que define a rede `numbers-test` explicitamente). Ele não encontra contêineres do projeto `numbers` para remover, mas tenta remover a rede `numbers-test` (achando que pertence ao projeto `numbers`). Isso falha porque os contêineres do projeto *real* (`numbers-test`) ainda estão conectados a ela.
    3.  Apenas o comando `down` que inclui *todos* os `-f` originais *e* o `-p numbers-test` correto consegue identificar e remover com sucesso os contêineres e a rede associados à instância de teste.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Tentativa 1 (Falha):** (Estando em `ch10/exercises`)
    ```powershell
    docker-compose down # Não deve remover nada dos 3 ambientes
    ```
2.  **Tentativa 2 (Erro/Confusão):**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-test.yml down
    # Deve mostrar erros sobre remover a rede 'numbers-test'
    docker ps # Containers de 'numbers-test' ainda devem estar rodando
    ```
3.  **Tentativa 3 (Sucesso):**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/docker-compose-test.yml -p numbers-test down
    # Deve remover containers e rede 'numbers-test' com sucesso
    docker ps # Containers de 'numbers-test' não devem mais aparecer
    # docker network ls # Rede 'numbers-test' não deve mais aparecer
    ```

---

📄 Página 195

🔍 Resumo dos principais pontos:

*   Figura 10.8: Mostra a saída das tentativas de `down`, incluindo o erro da segunda tentativa e o sucesso da terceira.
*   Explicação do erro na segunda tentativa:
    *   Sem `-p`, usou projeto padrão `numbers`.
    *   Não achou contêineres `numbers_...`, achou que já tinham sido removidos.
    *   Achou rede `numbers-test` (nome explícito no `test.yml`, sem prefixo de projeto padrão).
    *   Tentou remover a rede, mas falhou (Docker impede remover rede com contêineres `numbers-test_...` ainda conectados).
*   Lição: Precisa ter cuidado com Compose. É ótimo para não-produção, mas requer gerenciamento (scripts, automação) para deploy/teardown de múltiplos ambientes definidos por múltiplos arquivos e `-p`.

🧠 Explicação detalhada e didática:

*   A página analisa em detalhe por que a segunda tentativa de `docker-compose down` falhou de forma estranha. O problema foi a mistura: usar os arquivos de configuração que definiam a rede `numbers-test` explicitamente, mas não fornecer o nome do projeto (`-p numbers-test`) fez o Compose pensar que essa rede pertencia ao projeto padrão (`numbers`). Como não havia contêineres *desse* projeto (`numbers`) conectados, ele tentou remover a rede, mas o Docker Engine corretamente impediu, pois contêineres do projeto *real* (`numbers-test`) ainda estavam usando-a.
*   A conclusão reforça que, embora poderoso para rodar múltiplos ambientes, gerenciar esses ambientes com Compose (especialmente quando se usa múltiplos `-f` e `-p`) exige disciplina. É altamente recomendável **automatizar** os comandos `up` e `down` para cada ambiente em scripts, para garantir que as flags `-f` e `-p` corretas sejam sempre usadas consistentemente.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Exemplo de Script PowerShell para Gerenciar Ambientes:**
    ```powershell
    # Script: manage-env.ps1
    param(
        [Parameter(Mandatory=$true)]
        [ValidateSet('dev', 'test', 'uat')]
        [string]$Environment,

        [Parameter(Mandatory=$true)]
        [ValidateSet('up', 'down', 'ps', 'logs')]
        [string]$Action
    )

    $baseDir = "C:\DockerLabs\diamol\ch10\exercises\numbers" # Ajuste o caminho
    $coreFile = Join-Path -Path $baseDir -ChildPath "docker-compose.yml"
    $overrideFile = Join-Path -Path $baseDir -ChildPath "docker-compose-$Environment.yml"
    $projectName = "numbers-$Environment"

    Write-Host "Executando Ação '$Action' para Ambiente '$Environment' (Projeto '$projectName')"

    docker-compose -f $coreFile -f $overrideFile -p $projectName $Action
    ```
*   **Uso:**
    ```powershell
    .\manage-env.ps1 -Environment test -Action up -d # Inicia Teste
    .\manage-env.ps1 -Environment dev -Action logs # Vê logs do Dev
    .\manage-env.ps1 -Environment test -Action down # Derruba Teste
    ```
    *(Este script é apenas um exemplo, pode precisar de ajustes)*.

---

📄 Página 196

🔍 Resumo dos principais pontos:

*   **10.3 Injecting configuration with environment variables and secrets** (Injetando configuração com variáveis de ambiente e secrets)
*   Isolamento (redes) e diferenças estruturais (overrides) resolvidos. Próximo: **configuração da aplicação** entre ambientes.
*   Apps leem config de variáveis de ambiente ou arquivos. Compose suporta ambos.
*   Opções serão cobertas com exemplos.
*   Exemplo: App To-Do novamente. Configurações que variam:
    *   Nível de Logging (verbose em dev, menos em prod).
    *   Provedor de BD (SQLite local ou BD separado/containerizado).
    *   String de Conexão do BD (se não usar SQLite).
*   Uso de overrides para injetar config, mostrando diferentes abordagens do Compose.
*   **Listagem 10.3:** Arquivo Compose core (`docker-compose.yml`) para To-Do configurado.
    *   Define serviço `todo-web`, imagem, e uso de um `secret`.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        secrets: # Aplicação usará este secret
          - source: todo-db-connection # Nome lógico do secret
            target: /app/config/secrets.json # Onde aparece como arquivo no container
    ```
*   `secrets`: Suportado por Compose, Swarm, K8s.
*   `source`: De onde o runtime carrega o secret. `target`: Onde ele é montado no contêiner.

🧠 Explicação detalhada e didática:

*   Esta seção foca em como passar **configurações específicas da aplicação** (não apenas a estrutura dos serviços/redes) para os contêineres usando Docker Compose, permitindo que a mesma imagem se comporte diferentemente em cada ambiente.
*   O exemplo do To-Do App é usado novamente, identificando três tipos de configuração que tipicamente variam: nível de log, tipo de banco de dados e detalhes de conexão.
*   A Listagem 10.3 mostra um arquivo Compose base que já define que o serviço `todo-web` precisará de um `secret` chamado `todo-db-connection`, que será disponibilizado como um arquivo em `/app/config/secrets.json` dentro do contêiner. A aplicação lerá este arquivo para encontrar, por exemplo, a string de conexão do banco de dados. Como vimos antes, no Compose, a *definição* de onde esse secret vem (de um arquivo local) estará em outra seção do arquivo ou em um override.

💻 Exemplos práticos (para meu ambiente Windows):

*   Este arquivo `docker-compose.yml` (Listagem 10.3) será a base. Os próximos arquivos override (dev, test) adicionarão as configurações específicas e a definição da origem do `secret`.
*   O caminho `target` para o secret (`/app/config/secrets.json`) usa barras normais (`/`), que geralmente funcionam bem mesmo em contêineres Windows para a maioria das aplicações modernas que lidam com paths de forma abstrata.

---

📄 Página 197

🔍 Resumo dos principais pontos:

*   Arquivo core (Listagem 10.3) sozinho é inválido (falta seção `secrets:` e definição da origem).
*   **Listagem 10.4:** Override para ambiente de **Desenvolvimento** (`docker-compose-dev.yml`).
    ```yaml
    services:
      todo-web:
        ports:
          - 8089:80 # Porta específica para dev
        environment: # Via variável de ambiente
          - Database:Provider=Sqlite # Configura app para usar SQLite
        env_file: # Via arquivo .env no host
          - ./config/logging.debug.env # Carrega variáveis deste arquivo (para logging)
    secrets: # Define a origem do secret referenciado no serviço
      todo-db-connection:
        file: ./config/empty.json # Usa um JSON vazio, pois SQLite não precisa de conexão
    ```
*   **Três abordagens para injetar config neste override:**
    1.  **`environment:`**: Define variável de ambiente diretamente no YAML (`Database:Provider=Sqlite`). Fácil, claro para valores simples não sensíveis.
    2.  **`env_file:`**: Aponta para um arquivo (`.env`) no host. Compose lê cada linha (`CHAVE=VALOR`) desse arquivo e as define como variáveis de ambiente no contêiner. Bom para compartilhar um conjunto de settings entre serviços. Aqui, usado para configurar logging detalhado (Debug).
    3.  **`secrets:` (definição):** Define a origem do secret `todo-db-connection`. Usa `file:` para apontar para um arquivo local (`empty.json`). Como o modo é SQLite, um arquivo JSON vazio é suficiente (não precisa de string de conexão).
*   **Exercício Try-it-now:** Rodar app em config Dev usando core + override. Verificar logs.
    *   Remover contêineres.
    *   Comando Linux: `docker-compose -f ...core.yml -f ...dev.yml -p todo-dev up -d`.
    *   Comando Windows (precisa de 3º override `dev-windows.yml` para ajustar path do secret?): `docker-compose -f ...core.yml -f ...dev.yml -f ...dev-windows.yml -p todo-dev up -d`.
    *   `curl http://localhost:8089/list`.
    *   `docker logs --tail 4 todo-dev_todo-web_1`.

🧠 Explicação detalhada e didática:

*   A Listagem 10.4 mostra o arquivo de override para o ambiente de desenvolvimento, demonstrando **três maneiras** diferentes de injetar configuração usando o Compose:
    1.  **Diretamente via `environment`:** Para configurações simples e não sensíveis, definir a variável de ambiente diretamente no YAML é o mais claro. Aqui, `Database:Provider=Sqlite` configura o app.
    2.  **Via `env_file`:** Para um conjunto de configurações relacionadas (como níveis de log para diferentes partes do app), colocá-las em um arquivo `.env` separado no host (`logging.debug.env`) e referenciá-lo com `env_file:` mantém o YAML principal mais limpo. O Compose lê esse arquivo e injeta as variáveis no contêiner.
    3.  **Via `secrets` (definindo a fonte):** Completa a definição do `secret` `todo-db-connection` referenciado no serviço. Como em dev estamos usando SQLite (configurado via `environment`), não precisamos de uma string de conexão real, então usamos um arquivo JSON vazio (`empty.json`) como fonte.
*   O exercício instrui a rodar a aplicação combinando o arquivo core (Listagem 10.3) e este override de dev (Listagem 10.4), usando `-f` e `-p`. A menção a um terceiro arquivo (`dev-windows.yml`) sugere que o path em `secrets: file:` pode precisar de ajuste para Windows, similar ao exercício da p.138. O objetivo é verificar se a configuração (SQLite, logging Debug) foi aplicada corretamente olhando os logs.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar/Criar Arquivos:** No diretório `ch10\exercises\todo-list-configured`, verifique/crie:
    *   `docker-compose.yml` (Listagem 10.3).
    *   `docker-compose-dev.yml` (Listagem 10.4).
    *   `docker-compose-dev-windows.yml` (Verifique seu conteúdo, provavelmente só ajusta o path em `secrets: file:` para `./config/empty.json` ou `.\config\empty.json`).
    *   Pasta `config` contendo `logging.debug.env` e `empty.json`.
2.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    ```
3.  **Rodar Ambiente Dev:**
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-dev.yml -f ./docker-compose-dev-windows.yml -p todo-dev up -d
    ```
4.  **Testar e Ver Logs:**
    ```powershell
    Invoke-WebRequest http://localhost:8089/list
    docker logs --tail 10 todo-dev_todo-web_1 # Aumentei tail para 10
    ```
    Procure por logs detalhados (nível Debug) e talvez alguma indicação de SQLite nos logs.

---

📄 Página 198

🔍 Resumo dos principais pontos:

*   Figura 10.9: Mostra saída do exercício anterior, com logs detalhados (SQL do SQLite) confirmando config Debug.
*   Deployment Dev usou variáveis de ambiente e secrets (carregados de arquivos).
*   **Abordagem Alternativa:** Usar variáveis de ambiente *do host* para fornecer valores *dentro* do Compose file.
    *   Sintaxe: `${VARIAVEL_HOST}` ou `${VARIAVEL_HOST:-valor_padrao}`.
    *   Compose substitui o token pelo valor da variável de ambiente da máquina onde `docker-compose` está rodando.
    *   Se variável não existir, usa `valor_padrao` (se fornecido).
    *   Torna deploy mais portável (pode mudar config alterando variáveis do host, sem mudar arquivos Compose). Útil para múltiplos servers de teste com configs diferentes.
*   **Listagem 10.5:** Trecho de override para Teste (`docker-compose-test.yml`), usando substituição de variável para a porta.
    ```yaml
    todo-web:
      ports:
        - "${TODO_WEB_PORT}:80" # Porta do host vem da variável de ambiente
      environment:
        - Database:Provider=Postgres # Configura para Postgres
      env_file:
        - ./config/logging.information.env # Logging menos verboso
      networks:
        - app-net
    ```
*   Arquivo `test.yml` também inclui serviço `todo-db` e `secret` para conexão (não mostrados na listagem).

🧠 Explicação detalhada e didática:

*   A página introduz outra técnica de configuração flexível no Docker Compose: **substituição de variáveis de ambiente**.
*   Em vez de fixar um valor diretamente no arquivo `.yml` (como `ports: - "8089:80"`), você pode usar a sintaxe `${NOME_VARIAVEL_HOST}`. Quando `docker-compose` processar o arquivo, ele procurará por uma variável de ambiente chamada `NOME_VARIAVEL_HOST` na máquina onde o comando `docker-compose` está sendo executado. Se encontrar, ele substituirá `${NOME_VARIAVEL_HOST}` pelo valor dessa variável.
*   A sintaxe `${NOME_VARIAVEL_HOST:-valor_padrao}` adiciona um valor padrão a ser usado caso a variável de ambiente não esteja definida no host.
*   **Vantagem:** Permite controlar partes da configuração do Compose (como portas publicadas, tags de imagem, talvez até nomes de arquivos de config) através de variáveis de ambiente externas, sem precisar modificar os arquivos `.yml`. Isso é útil para pipelines de CI/CD ou para configurar o mesmo conjunto de arquivos `.yml` de forma ligeiramente diferente em máquinas diferentes.
*   A Listagem 10.5 mostra isso sendo usado para a porta do serviço `todo-web` no ambiente de teste. A porta externa será definida pelo valor da variável `TODO_WEB_PORT` no host. Este arquivo também configura o app para usar Postgres e um nível de log diferente (Information).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Definir Variável no Host (PowerShell - temporário para sessão atual):**
    ```powershell
    $env:TODO_WEB_PORT = 8877
    echo $env:TODO_WEB_PORT # Verificar
    ```
*   **Verificar Substituição com `docker-compose config`:** (Assumindo que `docker-compose-test.yml` existe em `todo-list-configured`)
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-test.yml -f ./docker-compose-test-windows.yml config
    ```
    Procure pela seção `ports:` do `todo-web`. Deve mostrar `- "8877:80"`, confirmando que a variável do host foi usada. Se você *não* definir `TODO_WEB_PORT` antes, e o YAML usar `${TODO_WEB_PORT}`, o Compose dará um aviso e usará uma string vazia (provavelmente causando erro ou porta aleatória). Se usar `${TODO_WEB_PORT:-8080}`, usaria 8080 como padrão.

---

📄 Página 199

🔍 Resumo dos principais pontos:

*   **Arquivo `.env`:** Feature do Compose para facilitar gerenciamento de variáveis de ambiente.
    *   Se Compose encontra arquivo chamado `.env` no diretório atual, ele automaticamente carrega as definições `CHAVE=VALOR` desse arquivo como variáveis de ambiente *antes* de processar o comando (`up`, `config`, etc.).
    *   Variáveis do `.env` podem ser usadas para substituição (`${VAR}`) dentro dos arquivos `.yml`.
    *   Variáveis do `.env` também podem configurar o *próprio* `docker-compose` (ex: quais arquivos `-f` usar por padrão, qual `-p` usar por padrão).
*   **Exercício Try-it-now:** Rodar ambiente Teste *sem* especificar `-f` ou `-p`, usando `.env`.
    *   Navegar para `ch10/exercises/todo-list-configured` (ou `...-windows`).
    *   Rodar: `docker-compose up -d`.

🧠 Explicação detalhada e didática:

*   A página introduz o arquivo `.env`, uma convenção usada pelo `docker-compose` para simplificar a configuração e execução.
*   **Como funciona:**
    1.  Quando você executa `docker-compose` (qualquer comando como `up`, `down`, `config`) em um diretório.
    2.  Ele automaticamente procura por um arquivo chamado `.env` nesse diretório.
    3.  Se encontrado, ele lê as linhas no formato `CHAVE=VALOR` e as carrega como variáveis de ambiente para o processo do `docker-compose`.
*   **Usos do `.env`:**
    1.  **Substituição em `.yml`:** As variáveis carregadas do `.env` podem ser usadas com a sintaxe `${VAR}` dentro dos seus arquivos `docker-compose.yml` (como visto com `TODO_WEB_PORT` na página anterior).
    2.  **Configuração do Compose:** Variáveis especiais no `.env` podem controlar o comportamento padrão do `docker-compose`:
        *   `COMPOSE_PROJECT_NAME`: Define o nome do projeto padrão (equivalente a usar `-p`).
        *   `COMPOSE_FILE`: Define uma lista de arquivos Compose a serem usados por padrão (equivalente a usar múltiplos `-f`). O separador da lista é definido por `COMPOSE_PATH_SEPARATOR` (geralmente `:` no Linux/Mac, `;` no Windows).
*   O exercício instrui a ir para a pasta `todo-list-configured` (que contém um `.env` pré-configurado para o ambiente de Teste) e simplesmente rodar `docker-compose up -d`. O `.env` fornecerá o nome do projeto e a lista de arquivos a serem usados, além das variáveis para substituição.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    # Se você estava em ch10\exercises
    cd .\todo-list-configured\
    # Ou para a versão Windows se existir:
    # cd .\todo-list-configured-windows\
    ```
2.  **Examinar `.env`:** Abra o arquivo `.env` nesta pasta com VS Code. Você verá as variáveis sendo definidas (próxima página mostra o conteúdo).
3.  **Rodar com `.env`:**
    ```powershell
    docker-compose up -d
    ```
    Observe a saída (próxima página). O Compose deve indicar qual nome de projeto e quais arquivos está usando (baseado no `.env`) e iniciar os contêineres para o ambiente de Teste (incluindo `todo-db`).

---

📄 Página 200

🔍 Resumo dos principais pontos:

*   Figura 10.10: Mostra a saída do `docker-compose up -d` usando `.env`. Indica criação de contêineres web e db, e uso do projeto `todo_ch10`.
*   Explica que `.env` configurou o Compose para usar arquivos `test.yml` e nome do projeto `todo_ch10` por padrão.
*   **Listagem 10.6:** Conteúdo completo do arquivo `.env`.
    ```dotenv
    # container configuration - ports to publish:
    TODO_WEB_PORT=8877
    TODO_DB_PORT=5432

    # compose configuration - files and project name:
    COMPOSE_PATH_SEPARATOR=; # Separador para Windows
    COMPOSE_FILE=docker-compose.yml;docker-compose-test.yml # Arquivos a usar
    COMPOSE_PROJECT_NAME=todo_ch10 # Nome do projeto
    ```
*   Explica as variáveis no `.env`:
    *   `TODO_WEB_PORT`, `TODO_DB_PORT`: Serão usadas para substituição (`${...}`) nos arquivos `.yml`.
    *   `COMPOSE_PATH_SEPARATOR`: Define o caractere (`;` para Windows) usado para separar múltiplos arquivos em `COMPOSE_FILE`.
    *   `COMPOSE_FILE`: Lista os arquivos Compose a serem lidos por padrão.
    *   `COMPOSE_PROJECT_NAME`: Define o nome do projeto padrão.

🧠 Explicação detalhada e didática:

*   A página analisa a Figura 10.10, confirmando que o `docker-compose up -d` (sem flags extras) usou as configurações do arquivo `.env` para iniciar o ambiente de Teste com o nome de projeto `todo_ch10`.
*   A Listagem 10.6 revela o conteúdo do arquivo `.env` que tornou isso possível:
    *   Define valores para `TODO_WEB_PORT` e `TODO_DB_PORT`, que serão substituídos nos `.yml` onde `${TODO_WEB_PORT}` ou `${TODO_DB_PORT}` forem usados.
    *   Define `COMPOSE_PATH_SEPARATOR` como `;` (correto para Windows; seria `:` para Linux/Mac).
    *   Define `COMPOSE_FILE` para usar `docker-compose.yml` e `docker-compose-test.yml` por padrão.
    *   Define `COMPOSE_PROJECT_NAME` como `todo_ch10` por padrão.
*   O arquivo `.env` é uma maneira conveniente de definir um "ambiente padrão" para um diretório, simplificando os comandos `docker-compose` para o caso de uso mais comum naquele diretório.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Serviços:** (Após `up` da página anterior)
    ```powershell
    docker-compose ps
    # Deve mostrar 'todo_ch10_todo-db_1' e 'todo_ch10_todo-web_1' rodando.
    # Note o prefixo 'todo_ch10_' vindo do COMPOSE_PROJECT_NAME.
    ```
2.  **Verificar Porta:**
    ```powershell
    docker-compose port todo-web 80
    # Deve retornar a porta definida em TODO_WEB_PORT no .env (ex: 8877).
    ```
3.  **Experimentar (Opcional):**
    *   Edite o arquivo `.env`. Mude `COMPOSE_FILE` para usar `docker-compose-dev.yml` em vez de `test.yml`. Mude `COMPOSE_PROJECT_NAME` para `todo-dev-default`.
    *   Rode `docker-compose down`.
    *   Rode `docker-compose up -d`.
    *   Verifique com `docker-compose ps`. Agora deve iniciar o ambiente Dev com o novo nome de projeto.

---

📄 Página 201

🔍 Resumo dos principais pontos:

*   Arquivo `.env` captura settings padrão, pode ser modificado para mudar o padrão (ex: de Teste para Dev).
*   Limitacão: Compose só procura por arquivo chamado `.env`. Não pode especificar nome diferente, dificultando *troca fácil* entre defaults de ambientes diferentes via múltiplos arquivos `.env`.
*   **Sumário das Opções de Configuração no Compose:**
    1.  **`environment:` (direto no YAML):** Mais simples, claro para settings não-sensíveis.
    2.  **`secrets:` (carregado de arquivo):** Mais flexível (fonte pode ser arquivo local no Compose, ou secret real em Swarm/K8s), ideal para dados sensíveis.
    3.  **`env_file:`:** Útil para agrupar settings compartilhadas entre serviços. Lê arquivo local, injeta como variáveis de ambiente no container.
    4.  **`.env` + Substituição (`${VAR}`):** Define defaults/configura Compose. Bom para definir ambiente padrão ou permitir overrides externos.
*   **10.4 Reducing duplication with extension fields** (Reduzindo duplicação com campos de extensão)
    *   Mesmo com overrides, pode haver duplicação *dentro* de um mesmo arquivo Compose, se múltiplos serviços compartilham configurações idênticas (ex: mesma config de logging, mesmas labels padrão).

🧠 Explicação detalhada e didática:

*   A página resume as quatro principais maneiras de injetar configuração usando Docker Compose, destacando os prós e contras de cada uma:
    1.  `environment:` direto no YAML: Simples e legível para poucos valores não-sensíveis.
    2.  `secrets:`: Padrão recomendado para dados sensíveis, portável entre Compose e orquestradores.
    3.  `env_file:`: Bom para organizar conjuntos de variáveis, mas valores ainda são injetados como variáveis de ambiente (menos seguro que `secrets` para dados sensíveis).
    4.  Substituição `${VAR}` (com ou sem `.env`): Permite parametrização externa, mas pode tornar o YAML menos auto-contido.
*   A seção 10.4 introduz uma técnica para lidar com outro tipo de duplicação: quando você tem **blocos de configuração idênticos** que precisam ser repetidos para vários serviços *dentro do mesmo arquivo* `docker-compose.yml` (ou dentro de um conjunto de arquivos que estão sendo mesclados).

💻 Exemplos práticos (para meu ambiente Windows):

*   Revise as Listagens 10.3, 10.4, 10.5 e 10.6 para ver exemplos de `environment`, `secrets`, `env_file` e substituição `${VAR}`.
*   O próximo passo mostrará como usar "extension fields" para evitar repetir, por exemplo, a mesma configuração de `logging:` ou `labels:` para múltiplos serviços.

---

📄 Página 202

🔍 Resumo dos principais pontos:

*   **Extension Fields (Campos de Extensão YAML):**
    *   Forma de definir blocos YAML reutilizáveis.
    *   Definidos fora das seções top-level (`services`, `networks`), nomeados com `&` (âncora YAML).
    *   Prefixo `x-` é convenção (não obrigatório).
    *   Reutilizados dentro do arquivo usando `<<: *` (merge YAML).
*   **Listagem 10.7:** Exemplo de definição de extension fields (em `image-gallery/docker-compose-prod.yml`).
    ```yaml
    # Define bloco 'logging' com config de logging
    x-logging: &logging # Âncora '&logging'
      logging:
        driver: "fluentd" # Exemplo
        options:
          max-size: "10m"
    # Define bloco 'labels' com label padrão
    x-labels: &labels # Âncora '&labels'
      app-name: image-gallery
    ```
*   Diferença nas definições: `x-logging` inclui a chave `logging:`, `x-labels` não inclui a chave `labels:`. Isso afeta como são usados.

🧠 Explicação detalhada e didática:

*   Esta página introduz os **Campos de Extensão (Extension Fields)**, que são uma funcionalidade do próprio **YAML** (não específica do Docker Compose, mas suportada por ele) para definir blocos de código reutilizáveis. Isso ajuda a seguir o princípio DRY (Don't Repeat Yourself).
*   **Definição:**
    *   Você cria um bloco YAML em um nível superior (fora de `services`, `networks`, etc.).
    *   Por convenção, o nome da chave começa com `x-` (ex: `x-logging`, `x-labels`).
    *   Você associa uma **âncora** YAML a esse bloco usando `&<nome_ancora>` (ex: `&logging`, `&labels`).
*   **Listagem 10.7** mostra dois exemplos:
    *   `x-logging`: Define um bloco completo de configuração de `logging:`, incluindo `driver:` e `options:`. A âncora é `&logging`.
    *   `x-labels`: Define *apenas* um par chave-valor (`app-name: image-gallery`) que se destina a ser *parte* de uma seção `labels:` maior. A âncora é `&labels`.
*   A diferença na estrutura (incluir ou não a chave principal como `logging:` ou `labels:` na definição da extensão) impacta como você a utiliza com a sintaxe de merge (próxima página).

💻 Exemplos práticos (para meu ambiente Windows):

*   Abra o arquivo `ch10\exercises\image-gallery\docker-compose-prod.yml` no VS Code. Localize essas seções `x-logging:` e `x-labels:` no início do arquivo (ou em algum nível superior).
*   Note a estrutura de cada uma.

💡 Analogias ou observações úteis (se houver):

*   Extension Fields + Âncoras YAML: É como criar um "carimbo" (`&logging`) com um conjunto de configurações padrão.
*   Uso com Merge (próxima página): É como pegar o carimbo (`*logging`) e aplicá-lo (`<<:`) em vários lugares do seu documento (definições de serviço).

---

📄 Página 203

🔍 Resumo dos principais pontos:

*   **Uso de Extension Fields:**
    *   Usa sintaxe de merge YAML `<<: *<nome_ancora>`.
*   **Listagem 10.8:** Exemplo de uso das extensões dentro de um serviço (`iotd`).
    ```yaml
    services:
      iotd:
        image: diamol/ch18-image-of-the-day # Imagem
        ports: # Propriedade específica do serviço
          - 8080:80
        <<: *logging # MERGE 1: Insere todo o bloco 'logging:' definido em &logging
        labels: # Propriedade específica do serviço
          <<: *labels # MERGE 2: Insere o conteúdo de &labels DENTRO da seção 'labels:'
          public: api # Outra label específica
    ```
*   **Explicação dos Merges:**
    *   `<<: *logging`: Como `&logging` já continha a chave `logging:`, ele insere toda essa seção diretamente sob `iotd:`.
    *   `<<: *labels`: Como `&labels` continha apenas `app-name: image-gallery`, ele insere esse par chave-valor *dentro* da seção `labels:` já existente do serviço, junto com a label `public: api`.
*   Resultado: Compose processa o arquivo, substituindo as referências `<<: *...` pelo conteúdo das âncoras correspondentes.
*   **Exercício Try-it-now:** Usar `docker-compose config` para ver o resultado da mesclagem com extensões.
    *   Navegar para `../image-gallery` (relativo a `numbers`).
    *   Comando: `docker-compose -f ./docker-compose.yml -f ./docker-compose-prod.yml config`.

🧠 Explicação detalhada e didática:

*   Esta página mostra como **usar** os campos de extensão definidos anteriormente. A sintaxe chave é `<<: *<nome_ancora>`. O `<<:` é o operador de merge do YAML, e `*<nome_ancora>` é uma referência à âncora definida com `&<nome_ancora>`.
*   A Listagem 10.8 ilustra os dois casos:
    *   **Merge de Bloco Inteiro:** `<<: *logging` insere todo o bloco `logging:` que foi definido na âncora `&logging`. Isso funciona porque a definição da extensão (`x-logging`) já incluía a chave principal (`logging:`). É útil para aplicar um conjunto completo de configurações (como logging, healthcheck, deploy) a vários serviços.
    *   **Merge Dentro de um Bloco:** `<<: *labels` está *dentro* da seção `labels:` do serviço. Isso insere apenas o *conteúdo* da âncora `&labels` (o par `app-name: image-gallery`) naquela posição, permitindo combinar labels reutilizáveis com labels específicas do serviço (`public: api`). Isso funciona porque a definição da extensão (`x-labels`) *não* incluía a chave principal (`labels:`).
*   O comando `docker-compose config` é novamente usado para visualizar o resultado final *após* o Compose realizar essas substituições/merges.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    # Se estava em ch10\exercises\numbers
    cd ..\image-gallery
    # Se estava em ch10\exercises
    cd .\image-gallery
    # Caminho completo: C:\DockerLabs\diamol\ch10\exercises\image-gallery
    ```
2.  **Verificar Configuração Mesclada:**
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-prod.yml config
    ```
3.  **Analisar Saída:** Encontre a definição do serviço `iotd` na saída. Confirme que ela agora contém a seção `logging:` completa (vinda de `*logging`) e que a seção `labels:` contém tanto `app-name: image-gallery` (vinda de `*labels`) quanto `public: api`. Verifique se outros serviços (como `accesslog`, `image-gallery`) também receberam as configurações de logging/labels via merge.

---

📄 Página 204

🔍 Resumo dos principais pontos:

*   Figura 10.11: Mostra trechos da saída do `config`, confirmando que os campos de extensão foram mesclados corretamente nos serviços.
*   Extension fields são úteis para garantir boas práticas (ex: settings de log padrão, labels padrão) consistentemente entre serviços.
*   Não são usados em todos os apps, mas bom ter na "caixa de ferramentas" para evitar copiar/colar YAML.
*   **Limitação Importante:** Extension fields (âncoras/referências YAML) **não funcionam entre arquivos separados**.
    *   Não se pode definir `&ancora` em `docker-compose.yml` e usar `*ancora` em `override.yml`.
    *   Restrição do YAML, não do Compose.
*   **10.5 Understanding the configuration workflow with Docker** (Entendendo o fluxo de trabalho de configuração com Docker)
    *   Valor de ter toda a config de deploy em artefatos versionados (Git).
    *   Permite deploy de qualquer versão, devs podem replicar stack prod localmente.
    *   Variações entre ambientes são necessárias; Compose permite capturá-las mantendo artefatos em source control.
    *   Recapitulação das áreas de variação gerenciadas com Compose:
        1.  **Composição da Aplicação:** Nem todo ambiente roda tudo (ex: monitoramento só em prod?). Overrides adicionam/removem serviços.
        2.  **Configuração do Contêiner:** Propriedades que mudam (portas, paths de volume). Overrides + redes isoladas permitem rodar múltiplos ambientes em um host.
        3.  **Configuração da Aplicação:** Comportamento interno (logging, features, conexões BD). Overrides + env_files + secrets + substituição ${VAR}.

🧠 Explicação detalhada e didática:

*   A página confirma que a mesclagem com extension fields funcionou como esperado (Figura 10.11). Reitera seu valor para padronização e evitar repetição.
*   Aponta uma **limitação crucial**: âncoras e referências YAML (`&` e `*`) só funcionam *dentro do mesmo arquivo YAML* ou dentro do conjunto de arquivos que está sendo processado *simultaneamente* pelo parser YAML. Você não pode definir uma âncora em `docker-compose.yml` e referenciá-la em `docker-compose.prod.yml` quando eles são mesclados pelo Compose usando `-f` (pois o Compose processa e mescla as *estruturas de dados* resultantes, não o texto YAML bruto com as âncoras).
*   A seção 10.5 faz um **resumo geral** do capítulo, conectando as diferentes técnicas de configuração do Compose (overrides, `-p`, redes, `environment`, `secrets`, `env_file`, substituição `${VAR}`, extension fields) ao objetivo maior de gerenciar múltiplos ambientes de forma consistente e sustentável.
*   O fluxo ideal é ter a definição da aplicação versionada no Git, permitindo que qualquer versão seja implantada e que os desenvolvedores possam facilmente rodar uma configuração similar à de produção localmente. O Compose, com suas funcionalidades de override e parametrização, é a ferramenta que permite que a *mesma imagem Docker* seja usada em todos esses cenários, apenas com configurações diferentes aplicadas em tempo de execução. As três áreas principais onde a configuração varia (composição, configuração do contêiner, configuração da aplicação) são bem cobertas pelas features do Compose.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. É uma seção de recapitulação e consolidação dos conceitos e técnicas vistas no Capítulo 10.

---

📄 Página 205

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 10.12): Ilustra app To-Do rodando em config Dev (SQLite) e Test (Postgres) no mesmo host, usando redes isoladas e portas diferentes.
*   **Takeaway Importante:** Fluxo de configuração usa a *mesma imagem Docker* em todos os ambientes.
    *   Pipeline CI/CD produz imagem testada (release candidate).
    *   Deploy em smoke-test usa imagem + config smoke.
    *   Se passar, deploy em UAT usa *mesma imagem* + config UAT.
    *   Se passar, deploy em Prod (Swarm/K8s) usa *mesma imagem* + config Prod (via manifestos do orquestrador).
    *   Garante que o software testado é o mesmo que vai para produção.
*   **10.6 Lab** (Laboratório)

🧠 Explicação detalhada e didática:

*   A Figura 10.12 visualiza o resultado prático das técnicas do capítulo: rodar instâncias funcionalmente diferentes (Dev com SQLite, Teste com Postgres) da *mesma aplicação* (baseada na mesma imagem `diamol/ch06-todo-list`) lado a lado na mesma máquina, graças ao isolamento de rede e configuração via Compose.
*   O ponto mais crucial do capítulo é reforçado: o objetivo final desse gerenciamento de configuração é permitir que a **mesma imagem Docker, construída e testada uma única vez, seja promovida através de todos os ambientes até a produção**. Apenas a *configuração externa* (fornecida via Docker Compose para dev/teste, ou via manifestos de Swarm/Kubernetes para produção) muda em cada estágio. Isso elimina o "drift" e aumenta a confiança nas implantações.
*   Prepara para o Lab do Capítulo 10.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. Consolida o fluxo de trabalho ideal habilitado pelo Docker e Compose.

---

📄 Página 206

🔍 Resumo dos principais pontos:

*   **Continuação do Lab 10:**
    *   Objetivo: Construir definições de ambiente (Dev e Test) para o app To-Do, garantindo que ambos possam rodar no mesmo host.
*   **Requisitos Ambiente Dev:**
    1.  Deve ser o **padrão** (rodar com `docker-compose up`).
    2.  Usar banco de dados de arquivo local (SQLite).
    3.  Publicar na porta **8089**.
    4.  Usar imagem `v2` do app To-Do (`diamol/ch06-todo-list:v2`).
*   **Requisitos Ambiente Test:**
    1.  Deve ser rodado com arquivos `-f` específicos e nome de projeto `-p`.
    2.  Usar contêiner de banco de dados (Postgres) separado.
    3.  Usar um **volume** (Docker Volume) para persistir dados do Postgres.
    4.  Publicar na porta **8080**.
    5.  Usar imagem `latest` do app To-Do (`diamol/ch06-todo-list` ou `:latest`).
*   **Dicas:**
    *   Similar aos exercícios `todo-list-configured`.
    *   Diferença principal: uso de Volume Docker para Postgres no Teste.
    *   Contêiner Postgres usa variável de ambiente `PGDATA` para definir onde salvar dados; usar isso com definição de volume no Compose.

🧠 Explicação detalhada e didática:

*   Esta página detalha o **Lab do Capítulo 10**. A tarefa é criar um conjunto de arquivos Docker Compose para gerenciar dois ambientes (Dev e Test) para a aplicação To-Do.
*   **Ambiente Dev (Padrão):** Deve ser configurado para ser o padrão ao rodar `docker-compose up` no diretório do lab. Isso provavelmente envolverá o uso de um arquivo `.env` (como nas p.199-200) para especificar os arquivos Compose (`core.yml` + `dev.yml`) e o nome do projeto Dev por padrão. As configurações específicas são: usar SQLite (via `environment`), porta 8089, e a imagem `:v2`.
*   **Ambiente Test (Explícito):** Deve ser iniciado usando flags `-f` e `-p` explícitas. Configurações: usar Postgres em outro contêiner, usar um Volume Docker nomeado para os dados do Postgres (montado no path definido por `PGDATA` dentro do contêiner Postgres), porta 8080, e a imagem `:latest` do To-Do.
*   A dica sobre `PGDATA` é chave para o ambiente de Teste. Você precisará definir um serviço `postgres` (baseado em `diamol/postgres` ou `postgres:alpine`), definir um Volume Docker nomeado na seção `volumes:` do Compose, e montar esse volume no caminho que o Postgres usa para dados (que pode ser influenciado pela variável `PGDATA`, ou usar o padrão `/var/lib/postgresql/data`).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 10:**
    1.  Crie uma pasta para o lab (ex: `ch10\lab`).
    2.  **Arquivo Core (`docker-compose.yml`):** Defina o serviço `todo-web` básico (só `image`, talvez `networks`). Defina o serviço `todo-db` básico (só `image`, `networks`). Defina a seção `networks`. Defina a seção `volumes` (vazia por enquanto). Defina a seção `secrets` (referenciando `todo-db-connection`).
    3.  **Arquivo Override Dev (`docker-compose.dev.yml`):**
        *   No `todo-web`: adicione `ports: ["8089:80"]`, `image: diamol/ch06-todo-list:v2`, `environment: [Database:Provider=SQLite]`.
        *   Adicione `secrets: todo-db-connection: file: ./config/empty.json`.
        *   Adicione `networks: app-net: name: todo-dev-net`.
    4.  **Arquivo Override Test (`docker-compose.test.yml`):**
        *   No `todo-web`: adicione `ports: ["8080:80"]`, `image: diamol/ch06-todo-list:latest`, `environment: [Database:Provider=Postgres]`, `depends_on: [todo-db]`.
        *   No `todo-db`: adicione `restart: unless-stopped`, `volumes: [todo-db-data:/var/lib/postgresql/data]`. (Verifique o path correto do PGDATA se necessário). Adicione `environment` se precisar definir user/pass/db (pode não ser necessário com a imagem `diamol/postgres`).
        *   Adicione `volumes: todo-db-data: {}` (para criar volume nomeado gerenciado pelo Compose).
        *   Adicione `secrets: todo-db-connection: file: ./config/secrets.json` (crie este arquivo com a string de conexão correta, usando `Host=todo-db`).
        *   Adicione `networks: app-net: name: todo-test-net`.
    5.  **Arquivo `.env`:**
        ```dotenv
        COMPOSE_PATH_SEPARATOR=;
        COMPOSE_FILE=docker-compose.yml;docker-compose.dev.yml
        COMPOSE_PROJECT_NAME=todo-dev
        ```
        *(Ajuste `COMPOSE_FILE` se usar arquivos -windows.yml)*.
    6.  **Arquivos de Config:** Crie a pasta `config` com `empty.json` e `secrets.json`.
    7.  **Testar Dev:** `cd ch10\lab`, `docker-compose up -d`. Acesse `:8089`.
    8.  **Testar Test:** `docker-compose down`, depois `docker-compose -f ...core.yml -f ...test.yml [-f ...test-windows.yml] -p todo-test up -d`. Acesse `:8080`. Verifique persistência com `down`/`up`.

---

📄 Página 207

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 10 no GitHub.

🧠 Explicação detalhada e didática:

*   Fornece o link para a solução completa do Lab 10 no repositório do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 10, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch10/lab/README.md

---

📄 Página 208

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, separando o Lab 10 do início do Capítulo 11.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 209

🔍 Resumo dos principais pontos:

*   **Capítulo 11: Building and testing applications with Docker and Docker Compose** (Construindo e testando aplicações com Docker e Docker Compose)
*   **Automação** é central no Docker (Dockerfiles, Compose files, CLIs).
*   CLIs se encaixam bem em processos automatizados (jobs agendados, gatilhos de código).
*   Compatível com qualquer servidor de automação que rode scripts.
*   Foco Cap. 11: **Integração Contínua (CI)** com Docker.
    *   **CI:** Processo automatizado (build + testes) que roda regularmente.
    *   Resultado saudável = código mais recente está bom, empacotado, pronto para deploy (release candidate).
    *   CI tradicional era complexo (gerenciar servidor de build, ferramentas).
    *   Docker simplifica cada parte do processo CI.
*   **11.1 How the CI process works with Docker** (Como o processo de CI funciona com Docker)
    *   CI é pipeline: Código Fonte -> Passos (Build, Teste) -> Artefato testado/implantável.
    *   Desafio CI tradicional: Pipelines únicos por projeto/tecnologia, servidor de build complexo.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 11, que conecta o Docker ao mundo da **automação**, especificamente à **Integração Contínua (CI)**.
*   A ideia da CI é automatizar o processo de pegar o código mais recente do controle de versão, compilá-lo, executar testes automatizados e, se tudo passar, gerar um pacote pronto para ser implantado (um "release candidate"). Isso é feito frequentemente (a cada commit, ou diariamente) para detectar problemas o mais cedo possível.
*   O capítulo argumenta que o Docker **simplifica enormemente** a CI. Servidores de CI tradicionais eram difíceis de manter, precisando ter todas as ferramentas (SDKs, runtimes, etc.) para todos os projetos da empresa instaladas e atualizadas. Com o Docker, o próprio processo de build e teste pode rodar *dentro* de contêineres, eliminando a necessidade de instalar essas ferramentas no servidor de CI.
*   A seção 11.1 começa descrevendo o fluxo básico de um pipeline de CI e os desafios da abordagem tradicional.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker build` (geralmente via `docker-compose build`) para a etapa de build no pipeline.
*   Você usará `docker run` ou `docker-compose up` para executar testes (unitários dentro do build, ou de integração/end-to-end com a aplicação rodando).
*   O artefato final do pipeline será uma ou mais **imagens Docker** testadas, prontas para serem enviadas a um registro.

💡 Analogias ou observações úteis (se houver):

*   Pipeline de CI: Uma linha de montagem automatizada na fábrica de software. Entra matéria-prima (código), passa por etapas de montagem (build) e controle de qualidade (testes), e sai um produto acabado e inspecionado (imagem Docker).
*   CI com Docker vs. Tradicional: É a diferença entre ter uma linha de montagem que usa ferramentas manuais específicas para cada produto (tradicional, difícil de manter) e uma linha que usa robôs flexíveis que carregam suas próprias ferramentas de uma biblioteca central (Docker, fácil de manter e adaptar).

---

📄 Página 210

🔍 Resumo dos principais pontos:

*   Docker traz **consistência** ao CI: mesmos passos, mesmo tipo de artefato (imagem Docker) para todos os projetos.
*   Diagrama (Figura 11.1): Passos típicos de pipeline CI com Docker.
    *   Trigger (Code Change / Schedule) -> Get Code -> Build Image (inclui Unit Tests) -> Push Image -> Run Integration Tests (com Compose) -> Run End-to-End Tests (com Compose) -> (Implícito: Imagem testada pronta).
*   **Como Docker é usado:**
    *   Build/Compilação em contêineres (sem SDKs no server CI).
    *   Testes unitários rodam *dentro* do `docker build` (se falham, build falha).
    *   Testes mais complexos (integração, E2E): usar `docker-compose` para subir aplicação + contêiner de teste separado.
*   **Infraestrutura CI Necessária:**
    *   Controle de Versão centralizado (ex: GitHub, GitLab, Gogs).
    *   Registro Docker (ex: Docker Hub, registro privado, registry container).
    *   Servidor de Automação (ex: Jenkins, GitLab CI, GitHub Actions, Azure DevOps).
*   Opções: Serviços gerenciados (misturar ou all-in-one) OU rodar sua própria infra CI em contêineres Docker.

🧠 Explicação detalhada e didática:

*   A página detalha como um pipeline de CI funciona especificamente com Docker, destacando a consistência que ele traz. A Figura 11.1 mostra um fluxo comum:
    1.  **Gatilho:** Uma mudança no código ou um agendamento inicia o pipeline.
    2.  **Obter Código:** O código fonte é baixado do sistema de controle de versão.
    3.  **Construir Imagem:** `docker build` (provavelmente multi-estágio) é executado. Testes unitários rápidos podem (e devem) ser executados *dentro* deste estágio de build. Se eles falharem, a imagem não é construída e o pipeline para.
    4.  **Enviar Imagem:** Se o build for bem-sucedido, a imagem resultante é enviada (`push`) para um Registro Docker (pode ser um registro intermediário de CI).
    5.  **Testes de Integração/E2E:** A aplicação (possivelmente multi-contêiner) é iniciada usando `docker-compose up` a partir das imagens recém-construídas. Um outro contêiner, contendo os scripts de teste, também é iniciado e executa testes contra a aplicação rodando.
    6.  **Resultado:** Se todos os testes passarem, a imagem (ou conjunto de imagens) é considerada um "release candidate" válido.
*   A página também lista os componentes de infraestrutura necessários para suportar esse pipeline (Git, Registro, Servidor de CI) e menciona que você pode usar serviços gerenciados na nuvem ou hospedar tudo você mesmo, inclusive rodando a própria infraestrutura de CI em contêineres Docker.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você verá como configurar um `Dockerfile` para incluir testes unitários.
*   Você usará `docker-compose` para definir não apenas a aplicação, mas também um serviço de "teste" que executa scripts contra os outros serviços.
*   O próximo exercício mostrará como rodar a infraestrutura de CI (Gogs, Registry, Jenkins) localmente usando Docker Compose.

---

📄 Página 211

🔍 Resumo dos principais pontos:

*   **11.2 Spinning up build infrastructure with Docker** (Criando infraestrutura de build com Docker)
*   Rodar infra própria pode ser útil (soberania de dados, velocidade local, backup de serviços).
*   Componentes (Git server, Docker Registry, CI server) podem rodar em contêineres com software open source.
    *   **Gogs:** Servidor Git leve.
    *   **Docker Registry (Open Source):** Registro básico.
    *   **Jenkins:** Servidor de automação popular.
*   Arquivo Docker Compose (`ch11/exercises/infrastructure`) define essa infraestrutura.
*   Setup difere ligeiramente entre Linux/Windows (provavelmente devido a montagem de volumes/pipes).
*   Necessário alias `registry.local` no arquivo `hosts` (se não feito no Cap. 5).
*   **Exercício Try-it-now:** Iniciar a infraestrutura de build com Compose.
    *   Navegar para `ch11/exercises/infrastructure`.
    *   Comando Linux: `docker-compose -f ...yml -f ...linux.yml up -d`.
    *   Comando Windows: `docker-compose -f ...yml -f ...windows.yml up -d`.
    *   Adicionar `registry.local` ao `hosts` (se necessário).
    *   Verificar contêineres: `docker container ls`.

🧠 Explicação detalhada e didática:

*   Esta seção mostra como é viável (e às vezes desejável) rodar sua própria infraestrutura de CI/CD localmente, usando contêineres Docker para hospedar as ferramentas necessárias. As ferramentas open source escolhidas são:
    *   **Gogs:** Uma alternativa leve e auto-hospedada ao GitHub/GitLab para gerenciamento de código fonte Git.
    *   **Docker Registry:** O mesmo contêiner de registro básico visto no Capítulo 5, para armazenar as imagens construídas.
    *   **Jenkins:** Um servidor de automação extremamente popular e flexível para executar os pipelines de CI/CD.
*   Um arquivo `docker-compose.yml` (com overrides específicos para Linux/Windows) é fornecido para iniciar esses três serviços como contêineres interconectados.
*   A necessidade de ter o alias `registry.local` no arquivo `hosts` é relembrada, pois Jenkins e Gogs podem precisar se referir ao registro por esse nome.
*   O exercício instrui a iniciar essa pilha de infraestrutura usando `docker-compose up -d` com os arquivos apropriados.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch11\exercises\infrastructure
    ```
2.  **Verificar/Adicionar `registry.local` ao Hosts:** (Se não fez na p.93)
    *   Abra PowerShell como Admin.
    *   `Add-Content -Value "127.0.0.1 registry.local" -Path C:\Windows\System32\drivers\etc\hosts`
3.  **Iniciar Infraestrutura:**
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-windows.yml up -d
    ```
4.  **Verificar Contêineres:**
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-windows.yml ps
    # Ou docker ps
    # Você deve ver containers para gogs, registry, jenkins rodando.
    ```
    Anote as portas publicadas (Gogs: 3000, Jenkins: 8080, Registry: 5000).

---

📄 Página 212

🔍 Resumo dos principais pontos:

*   Figura 11.2: Mostra a saída do `up` da infraestrutura e `docker ps` listando os 3 contêineres (gogs, jenkins, registry).
*   Níveis diferentes de automação no setup das ferramentas:
    *   **Registry:** Funciona sem setup extra.
    *   **Jenkins:** Usa plugins e pode ser configurado via scripts na inicialização do contêiner (imagem `diamol/jenkins` já faz isso).
    *   **Gogs:** Requer configuração manual inicial via web UI.
*   **Configuração Manual do Gogs:**
    *   **Exercício Try-it-now:** Acessar Gogs (`http://localhost:3000`).
        *   Tela de instalação inicial (Figura 11.3). Valores pré-configurados devem estar OK.
        *   Clicar "Install Gogs".

🧠 Explicação detalhada e didática:

*   A página confirma que os três contêineres da infraestrutura de CI/CD (Gogs, Jenkins, Registry) estão rodando.
*   Explica que as imagens Docker usadas para essas ferramentas têm níveis diferentes de automação de configuração inicial:
    *   O Registry é simples e não precisa de configuração.
    *   O Jenkins usado aqui (`diamol/jenkins`) é mais sofisticado e inclui scripts que rodam na primeira inicialização para instalar plugins, criar usuários e configurar jobs automaticamente.
    *   O Gogs (`diamol/gogs` provavelmente) requer uma configuração manual inicial através de sua interface web na primeira vez que é executado.
*   O exercício guia através dessa configuração inicial do Gogs: acessar a UI web na porta 3000, aceitar as configurações padrão na tela de instalação e clicar em "Install".

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Acessar Gogs:** Abra o navegador em `http://localhost:3000`.
2.  **Instalar Gogs:** Na página de instalação que aparece (similar à Figura 11.3):
    *   Verifique se os caminhos e configurações parecem corretos (geralmente os padrões funcionam se o Compose foi bem feito).
    *   Role para baixo e clique no botão "Install Gogs".
    *   Aguarde a instalação (deve ser rápido). Você será redirecionado para a página de login/registro.

---

📄 Página 213

🔍 Resumo dos principais pontos:

*   Figura 11.3: Mostra a tela de instalação do Gogs.
*   Após instalação, Gogs redireciona para login. Sem conta default.
*   **Exercício Try-it-now:** Registrar usuário no Gogs.
    *   Clicar "Register".
    *   Criar usuário com **username `diamol`** (importante, pois Jenkins espera esse nome).
    *   Email/senha podem ser quaisquer. (Figura 11.4).

🧠 Explicação detalhada e didática:

*   Após a instalação rápida do Gogs, a interface web mostra a página de login. Como nenhum usuário foi criado ainda, é preciso registrar um novo usuário.
*   O exercício instrui a criar um usuário com o **nome de usuário `diamol`**. Isso é crucial porque o job pré-configurado no Jenkins que rodaremos mais tarde está configurado para interagir com o Gogs usando este nome de usuário específico. A senha e o email podem ser quaisquer que você escolha e lembre.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Registrar no Gogs:** Na página de login/registro do Gogs (`http://localhost:3000/user/sign_up` ou similar):
    *   Username: `diamol`
    *   Email: (qualquer um, ex: `diamol@example.com`)
    *   Password: (crie e lembre-se de uma senha)
    *   Confirm Password: (repita a senha)
    *   Clique "Create New Account".

---

📄 Página 214

🔍 Resumo dos principais pontos:

*   Figura 11.4: Mostra a tela de registro do Gogs.
*   Após registro, fazer login com `diamol` / sua senha.
*   **Exercício Try-it-now:** Criar um repositório Git no Gogs.
    *   Acessar `http://localhost:3000/repo/create`.
    *   Nome do Repositório: `diamol`.
    *   Deixar outros detalhes vazios. (Figura 11.5).
*   Frustração com setup manual em Docker; ideal é automação total, mas nem sempre possível. Autor poderia ter pré-configurado imagem Gogs, mas quis mostrar a realidade.
*   **Jenkins:** Experiência melhor, pois imagem `diamol/jenkins` usa scripts para automação.
    *   Instala plugins, cria usuários, cria pipeline job automaticamente.

🧠 Explicação detalhada e didática:

*   Após criar o usuário `diamol`, você faz login no Gogs. O próximo passo manual é criar um repositório Git vazio onde o código fonte do livro será posteriormente enviado (`push`). O exercício instrui a criar um repositório chamado `diamol`.
*   O autor faz uma pausa para comentar sobre a inconveniência dessa configuração manual do Gogs, contrastando-a com a experiência mais automatizada que se espera do Jenkins (que será visto a seguir). Isso serve para ilustrar que nem todas as aplicações empacotadas em Docker são igualmente fáceis de automatizar na inicialização.
*   Antecipa que a imagem Jenkins usada (`diamol/jenkins`) já vem com um job de pipeline pré-configurado, graças a scripts de inicialização.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Login no Gogs:** Faça login com `diamol` e a senha que você criou.
2.  **Criar Repositório:**
    *   Clique no '+' no canto superior direito e selecione "New Repository", OU acesse `http://localhost:3000/repo/create`.
    *   Repository Name: `diamol`
    *   Description: (opcional)
    *   Visibility: Deixe "Private" ou mude para "Public".
    *   Initialize Repository: Deixe desmarcado (criaremos um repo vazio).
    *   Clique "Create Repository". Você será levado à página do repositório vazio.

---

📄 Página 215

🔍 Resumo dos principais pontos:

*   Figura 11.5: Mostra a tela de criação de repositório no Gogs.
*   **Exercício Try-it-now:** Acessar Jenkins (`http://localhost:8080`).
    *   UI do Jenkins (Figura 11.6) mostra job `diamol` pré-configurado, com status falho.
    *   Fazer login: clicar "Log In", usar user `diamol`, pass `diamol`.
*   Job Jenkins falhou porque foi configurado para buscar código do Gogs, que estava vazio.
*   Repositório do livro já é Git; adicionar Gogs local como `remote` e fazer `push`.

🧠 Explicação detalhada e didática:

*   Com o repositório `diamol` criado no Gogs, agora podemos verificar o Jenkins.
*   O exercício instrui a acessar a interface web do Jenkins na porta 8080. Como esperado (devido aos scripts de automação na imagem `diamol/jenkins`), já existe um job chamado `diamol`. Ele está com status de falha porque, na primeira execução (automática ou agendada), ele tentou buscar o código do repositório `diamol/diamol` no Gogs, mas o repositório estava vazio ou não existia completamente configurado naquele momento.
*   É preciso fazer login no Jenkins usando as credenciais padrão `diamol`/`diamol` (também pré-configuradas pelos scripts).
*   O próximo passo será popular o repositório Gogs com o código fonte do livro, que já está no seu disco local como um repositório Git (clonado do GitHub no Cap. 1). Faremos isso adicionando o Gogs local como um novo "destino remoto" (remote) ao repositório Git local e enviando (`push`) o código para ele.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Acessar Jenkins:** Abra o navegador em `http://localhost:8080`.
2.  **Login Jenkins:** Clique em "Log in" (canto superior direito). Use `diamol` / `diamol`.
3.  **Observar Job:** Você verá o job `diamol` na lista, provavelmente com um ícone vermelho ou cinza indicando falha ou não execução recente.

---

📄 Página 216

🔍 Resumo dos principais pontos:

*   Figura 11.6: Mostra a UI do Jenkins com o job `diamol` falho.
*   **Exercício Try-it-now:** Adicionar Gogs como remote e enviar código.
    *   Comandos (no terminal, na pasta raiz do código clonado, ex: `C:\DockerLabs\diamol`):
        *   `git remote add local http://localhost:3000/diamol/diamol.git` (Adiciona remote chamado 'local')
        *   `git push local` (Envia branch atual - `master` ou `main` - para o remote 'local')
    *   Gogs pedirá login (usar `diamol` / senha do Gogs).
*   Agora código fonte está no Gogs local.
*   Job Jenkins verifica mudanças a cada minuto; se houver, dispara pipeline.
*   Primeira execução falhou, Jenkins pode ter pausado agendamento. Precisa rodar manualmente agora.

🧠 Explicação detalhada e didática:

*   Esta página executa o passo crucial de enviar o código fonte do livro (que você tem localmente) para o servidor Gogs (rodando no contêiner).
*   Os comandos Git são executados a partir do diretório raiz onde você clonou o repositório `diamol` do GitHub.
    *   `git remote add local <url>`: Adiciona um "apelido" (`local`) para um novo servidor Git remoto. A URL é o endereço HTTP do repositório `diamol` que você criou no Gogs.
    *   `git push local`: Envia o branch atual do seu repositório local para o servidor remoto associado ao apelido `local`. O Gogs exigirá autenticação (usuário `diamol` e a senha que você definiu para o Gogs).
*   Após o `push`, o código estará no Gogs. O Jenkins está configurado para verificar esse repositório Gogs periodicamente. No entanto, como o primeiro build falhou, pode ser necessário iniciar o job manualmente para "reativar" o processo.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar para Raiz do Código:** Abra PowerShell/terminal e vá para a pasta onde clonou o repositório.
    ```powershell
    cd C:\DockerLabs\diamol
    ```
2.  **Adicionar Remote Gogs:**
    ```powershell
    git remote add local http://localhost:3000/diamol/diamol.git
    # Verificar se foi adicionado (opcional): git remote -v
    ```
3.  **Enviar Código para Gogs:**
    ```powershell
    git push local
    ```
    Quando pedir Username e Password, use `diamol` e a senha que você criou para o *Gogs*. A saída deve indicar que os objetos foram enviados e o branch foi configurado no remoto `local`.
4.  **(Opcional) Verificar no Gogs:** Atualize a página do repositório `diamol` no Gogs (`http://localhost:3000/diamol/diamol`). Você deve ver os arquivos e pastas do livro agora.

---

📄 Página 217

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Iniciar build manualmente no Jenkins.
    *   Acessar página do job: `http://localhost:8080/job/diamol`.
    *   Tela da Figura 11.7.
    *   Clicar "Build Now" no menu esquerdo (requer estar logado como `diamol`).
*   Aguardar build completar (1 min aprox.). Página atualiza.
*   Figura 11.8: Mostra job `diamol` com build #2 bem-sucedido.
*   **Como funciona? Docker-in-Docker "Trick":**
    *   Pipeline Jenkins roda *dentro* do contêiner Jenkins.
    *   Esse contêiner pode executar comandos `docker` e `docker-compose`.
    *   Comandos são enviados para a API do Docker Engine *do host* (sua máquina Windows).
    *   Isso é habilitado montando o canal de comunicação da API do host para dentro do contêiner Jenkins.

🧠 Explicação detalhada e didática:

*   Com o código no Gogs, o exercício instrui a iniciar manualmente o build no Jenkins. Acessando a página do job `diamol` e clicando em "Build Now", o Jenkins buscará o código mais recente do Gogs e executará os passos definidos no pipeline (que está em um arquivo `Jenkinsfile` dentro do código fonte).
*   Após a conclusão bem-sucedida (indicada por um status verde ou azul na Figura 11.8), a página explica *como* o Jenkins, rodando ele mesmo em um contêiner, consegue executar comandos `docker` que afetam o host.
*   A técnica envolve montar o "canal de comunicação" que a CLI `docker` local usa para falar com o Docker Engine do host (um *socket* no Linux, um *named pipe* no Windows) como um volume dentro do contêiner Jenkins. Quando o Jenkins (ou um script executado por ele) roda o comando `docker` *dentro* do contêiner, esse comando se conecta ao socket/pipe montado, que na verdade o direciona para a API do Docker Engine *do host*. Isso permite que o contêiner Jenkins controle o Docker na máquina onde ele mesmo está rodando.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Iniciar Build:** No Jenkins (`http://localhost:8080/job/diamol`), clique em "Build Now".
2.  **Observar:** Veja o "Build History" no canto inferior esquerdo. Um novo build (provavelmente #2) aparecerá e começará a piscar. Clique nele para ver o progresso. Ele passará por estágios (Checkout, Verify, Build, Test, Push). Aguarde até que fique verde (Success).
3.  **Entender a Montagem:** Relembre o `docker-compose-windows.yml` da infraestrutura (p. 211). Ele provavelmente tinha uma seção `volumes:` para o serviço `jenkins` que montava `\\.\pipe\docker_engine` (o named pipe do Docker no Windows).

💡 Analogias ou observações úteis (se houver):

*   Docker-in-Docker "Trick": É como dar ao seu contêiner Jenkins um "telefone direto" (o socket/pipe montado) para a "central de controle" (API do Docker Engine do host), permitindo que ele dê ordens como se estivesse do lado de fora.

---

📄 Página 218

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 11.9): Ilustra como o contêiner Jenkins se conecta à API Docker do host via socket (Linux) ou named pipe (Windows) montado como volume.
*   CLI Docker conecta à API local via socket (Linux) ou named pipe (Windows).
*   Canal pode ser usado como bind mount para contêineres.
*   Permite cenários onde app em contêiner consulta ou controla outros contêineres no mesmo host.
*   **Preocupação de Segurança:** Contêiner ganha acesso *total* ao Docker do host. Usar apenas com imagens confiáveis (`diamol` são confiáveis).
*   **Listagem 11.1:** Trechos dos Compose files (`docker-compose.yml`, `...linux.yml`, `...windows.yml`) mostrando a montagem do volume para o serviço `jenkins`.
    *   Linux: `source: /var/run/docker.sock`, `target: /var/run/docker.sock`.
    *   Windows: `type: npipe`, `source: \\.\pipe\docker_engine`, `target: \\.\pipe\docker_engine`.

🧠 Explicação detalhada e didática:

*   A Figura 11.9 visualiza a técnica "Docker-out-of-Docker" (o contêiner Jenkins controla o Docker do host, não rodando um Docker aninhado): o socket (Linux) ou named pipe (Windows) do host é mapeado para o mesmo caminho dentro do contêiner Jenkins.
*   A Listagem 11.1 mostra as definições exatas nos arquivos `docker-compose.yml` (com overrides) que habilitam essa conexão:
    *   No Linux, o socket Unix padrão `/var/run/docker.sock` do host é montado no mesmo caminho dentro do contêiner.
    *   No Windows, o named pipe `\\.\pipe\docker_engine` do host é montado (usando `type: npipe`) no mesmo caminho dentro do contêiner.
*   Quando a CLI `docker` dentro do contêiner Jenkins tenta se conectar ao Docker Engine (usando seu endereço padrão), ela encontra esse socket/pipe montado e acaba falando com o Engine do host.
*   **Implicação de Segurança:** Isso é muito poderoso, mas também perigoso. O contêiner Jenkins (e qualquer processo rodando dentro dele) agora tem o mesmo nível de controle sobre o Docker que você tem no host. Ele pode iniciar, parar, remover qualquer contêiner, volume, rede, etc. Por isso, só se deve usar essa técnica com imagens de contêiner (como a `diamol/jenkins` aqui) em que se confia completamente.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você já rodou o `docker-compose... up -d` que usou essa configuração na Listagem 11.1 (parte do `docker-compose-windows.yml`). O build bem-sucedido no Jenkins confirma que a conexão com o Docker Engine do host está funcionando.

---

📄 Página 219

🔍 Resumo dos principais pontos:

*   Infraestrutura CI pronta: Jenkins conecta ao Engine Docker host, e também conecta ao Gogs e Registry (via DNS Docker, pois estão na mesma rede Compose).
*   Pipeline CI roda comando único para build; complexidade está nos Dockerfiles e Compose files.
*   **11.3 Capturing build settings with Docker Compose** (Capturando configurações de build com Docker Compose)
    *   Job Jenkins construiu nova versão do app Random Number (Cap. 8).
    *   App usa overrides Compose para capturar settings de build.
*   **Listagem 11.2:** Arquivo Compose core (`docker-compose.yml` em `ch11/exercises`) para Random Number.
    *   Usa **substituição de variáveis de ambiente** no nome da imagem.
    ```yaml
    services:
      numbers-api:
        image: ${REGISTRY:-docker.io}/diamol/ch11-numbers-api:v3-build-${BUILD_NUMBER:-local}
        networks:
          - app-net
      numbers-web:
        image: ${REGISTRY:-docker.io}/diamol/ch11-numbers-web:v3-build-${BUILD_NUMBER:-local}
        # ... resto ...
    ```
    *   Sintaxe `${VAR:-default}`: Usa valor da variável `VAR` do host/ambiente. Se `VAR` não existir ou vazia, usa `default`.
    *   `REGISTRY`: Permite direcionar push/pull para registro local (`registry.local:5000`) ou Docker Hub (`docker.io`).
    *   `BUILD_NUMBER`: Permite incluir número do build CI na tag da imagem. Padrão `local` para builds de dev.

🧠 Explicação detalhada e didática:

*   A página confirma que a infraestrutura de CI está funcional e que o pipeline do Jenkins executará comandos Docker/Compose. A complexidade real do build está encapsulada nos Dockerfiles (multi-estágio) e nos arquivos Compose que definem como construir.
*   A seção 11.3 foca em como o **Docker Compose** é usado *dentro* do pipeline de CI para gerenciar o processo de **build** das imagens da aplicação (neste caso, o app Random Number).
*   A Listagem 11.2 mostra um arquivo `docker-compose.yml` inteligente que usa **substituição de variáveis de ambiente** (lembra do Cap. 10?) diretamente nos nomes das imagens.
    *   `image: ${REGISTRY:-docker.io}/diamol/ch11-numbers-api:v3-build-${BUILD_NUMBER:-local}`
    *   Isso torna o mesmo arquivo Compose utilizável tanto localmente quanto no CI:
        *   **Localmente (Dev):** As variáveis `REGISTRY` e `BUILD_NUMBER` provavelmente não estarão definidas. O Compose usará os defaults, resultando em nomes como `docker.io/diamol/ch11-numbers-api:v3-build-local` (que é o mesmo que `diamol/ch11-numbers-api:v3-build-local` pois `docker.io` é o padrão).
        *   **No CI (Jenkins):** O pipeline definirá as variáveis de ambiente antes de chamar `docker-compose`. Por exemplo, `REGISTRY=registry.local:5000` e `BUILD_NUMBER=2`. O Compose então usará esses valores, resultando em nomes como `registry.local:5000/diamol/ch11-numbers-api:v3-build-2`.
*   Essa técnica permite versionar e direcionar imagens de forma flexível usando o mesmo arquivo Compose base.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Verificar Substituição Local:**
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch11\exercises
    docker-compose -f docker-compose.yml config | Select-String "image:"
    # Deve mostrar as imagens com '-local' no final da tag.
    ```
*   **Simular CI:**
    ```powershell
    $env:REGISTRY="registry.local:5000"
    $env:BUILD_NUMBER="99"
    docker-compose -f docker-compose.yml config | Select-String "image:"
    # AGORA deve mostrar 'registry.local:5000' e '-99' nos nomes das imagens.
    Remove-Variable REGISTRY # Limpar variáveis
    Remove-Variable BUILD_NUMBER
    ```

---

📄 Página 220

🔍 Resumo dos principais pontos:

*   Variáveis de ambiente com defaults (`${VAR:-default}`) são padrão útil para suportar CI e build local com mesmos artefatos.
*   Comando `docker-compose build`: Constrói imagens para serviços que têm seção `build:` definida.
    *   Geralmente em arquivo override (`docker-compose-build.yml`).
*   É boa prática construir com Compose (mesmo para 1 imagem) para usar nome/tag definido no Compose.
*   **Labels de Imagem:** Metadados chave/valor adicionados à imagem no build (`LABEL` no Dockerfile).
    *   Ficam embutidos na imagem (viajam com push/pull).
    *   Úteis para **rastreabilidade/auditoria** no CI/CD: incluir info do build (número, job, commit Git) como labels.
*   **Listagem 11.3:** Trecho de Dockerfile (`numbers-api/Dockerfile.v4`) usando `ARG` e `LABEL`.
    ```dockerfile
    # ...
    FROM diamol/dotnet-aspnet
    ARG BUILD_NUMBER=0 # Define argumento de build com valor default
    ARG BUILD_TAG=local # Define outro argumento de build
    LABEL version="3.0" # Label fixa
    LABEL build_number=${BUILD_NUMBER} # Label usa valor do ARG
    LABEL build_tag=${BUILD_TAG} # Label usa valor do ARG
    ENTRYPOINT ["dotnet", "Numbers.Api.dll"]
    # ...
    ```
*   Instrução `ARG`: Similar a `ENV`, mas só existe durante o **build**. Não fica disponível no contêiner em runtime. Usada para passar parâmetros para o build.
*   Instrução `LABEL`: Adiciona metadados à imagem. Pode usar variáveis `ARG` para valores dinâmicos.

🧠 Explicação detalhada e didática:

*   A página introduz o comando `docker-compose build`. Assim como `docker-compose up` inicia serviços, `docker-compose build` constrói as imagens para os serviços que têm uma seção `build:` definida em seus arquivos Compose (geralmente em um arquivo override específico para build).
*   Enfatiza a importância de adicionar **Labels** às imagens durante o build. Labels são metadados (como `autor="Alexandre"`, `git-commit="abc1234"`) que ficam permanentemente associados à imagem. No contexto de CI/CD, é crucial usar labels para registrar informações sobre *como* e *quando* a imagem foi construída (ex: qual job, qual número de build, qual commit do código fonte). Isso cria uma trilha de auditoria essencial.
*   A Listagem 11.3 mostra como definir labels no Dockerfile usando a instrução `LABEL`.
*   Introduz a instrução `ARG`. Ela é usada para definir variáveis que podem ser passadas para o processo de build (via `docker build --build-arg NOME=valor` ou via seção `args:` no Compose). A diferença principal para `ENV` é que `ARG` só existe *durante o build* e não persiste no ambiente do contêiner final. É ideal para passar informações como números de build ou credenciais temporárias necessárias apenas para construir a imagem.
*   No exemplo, `ARG BUILD_NUMBER=0` define um argumento `BUILD_NUMBER` com valor padrão `0`. A instrução `LABEL build_number=${BUILD_NUMBER}` usa o valor desse argumento (que pode ser sobrescrito no build) para definir a label.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Analisar Dockerfile:** Abra `ch11\exercises\numbers\numbers-api\Dockerfile.v4` no VS Code. Veja as instruções `ARG` e `LABEL`.
*   **Build com `docker-compose build`:** (Próximo exercício).
*   **Build com `docker build` e `--build-arg`:**
    ```powershell
    # Estando em C:\DockerLabs\diamol\ch11\exercises\numbers
    # Build passando argumentos para sobrescrever defaults do ARG
    docker image build -f numbers-api/Dockerfile.v4 `
      --build-arg BUILD_NUMBER=77 `
      --build-arg BUILD_TAG="manual-test" `
      -t numbers-api-manual .

    # Inspecionar labels da imagem criada
    docker image inspect -f '{{.Config.Labels}}' numbers-api-manual
    # Deve mostrar build_number:77 e build_tag:manual-test
    ```

---

📄 Página 221

🔍 Resumo dos principais pontos:

*   Como settings do ambiente CI chegam aos `ARG`s do Dockerfile via `docker-compose build`: através da seção `build: args:` no arquivo Compose override.
*   **Listagem 11.4:** Conteúdo do `docker-compose-build.yml`.
    ```yaml
    # Define extensão 'args' reutilizável
    x-args: &args
      args: # Seção 'args' dentro do build:
        BUILD_NUMBER: ${BUILD_NUMBER:-0} # Usa variável de ambiente do host/CI
        BUILD_TAG: ${BUILD_TAG:-local} # Usa variável de ambiente do host/CI

    services:
      numbers-api:
        build:
          context: numbers # Diretório relativo ao Compose file com código/Dockerfile
          dockerfile: numbers-api/Dockerfile.v4 # Path relativo ao context
          <<: *args # Mescla a extensão 'args' aqui
      numbers-web:
        build:
          context: numbers
          dockerfile: numbers-web/Dockerfile.v4
          <<: *args # Mescla a extensão 'args' aqui
    ```
*   **Explicação da Seção `build:`:**
    *   `context:`: Path para o diretório de build (contém Dockerfile e arquivos a serem copiados).
    *   `dockerfile:`: Path para o Dockerfile (relativo ao `context`).
    *   `args:`: Define os argumentos (`ARG`) a serem passados para o `docker build`. Usa substituição `${VAR}` para pegar valores do ambiente do host (onde `docker-compose build` roda).
*   Uso de extension field (`x-args: &args`) e merge (`<<: *args`) para evitar repetir a seção `args:` para ambos os serviços.
*   Valores default (`:-0`, `:-local`) garantem que build funcione mesmo fora do CI (quando variáveis não estão setadas).

🧠 Explicação detalhada e didática:

*   Esta página conecta tudo: como as variáveis de ambiente definidas no ambiente de CI (Jenkins) são passadas para os argumentos `ARG` dentro do Dockerfile quando se usa `docker-compose build`.
*   A chave está na seção `build: args:` dentro da definição do serviço no arquivo `docker-compose-build.yml` (Listagem 11.4).
*   **Fluxo:**
    1.  O pipeline de CI (Jenkins) define variáveis de ambiente (ex: `BUILD_NUMBER=2`, `BUILD_TAG="jenkins-job-2"`).
    2.  O pipeline executa `docker-compose -f ... -f docker-compose-build.yml build`.
    3.  `docker-compose` lê o `docker-compose-build.yml`.
    4.  Na seção `build: args:`, ele encontra `BUILD_NUMBER: ${BUILD_NUMBER:-0}`.
    5.  Ele substitui `${BUILD_NUMBER}` pelo valor da variável de ambiente do CI (`2`). O valor resultante `BUILD_NUMBER: 2` é passado como argumento para o `docker build`.
    6.  Dentro do `Dockerfile`, a instrução `ARG BUILD_NUMBER=0` recebe o valor `2` passado pelo build.
    7.  A instrução `LABEL build_number=${BUILD_NUMBER}` usa esse valor `2`.
*   O uso de campos de extensão (`x-args`) é apenas uma técnica YAML para não repetir a seção `args:` para os serviços `numbers-api` e `numbers-web`, já que ambos usam os mesmos argumentos.
*   Os valores default (`:-0`, `:-local`) na seção `args:` do Compose garantem que o `docker-compose build` funcione mesmo se as variáveis de ambiente não estiverem definidas (ex: rodando localmente), passando os defaults para o `docker build`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Verificar Build com Compose:** (Estando em `ch11/exercises`)
    ```powershell
    # Limpar variáveis de ambiente (simular build local)
    Remove-Variable BUILD_NUMBER -ErrorAction SilentlyContinue
    Remove-Variable BUILD_TAG -ErrorAction SilentlyContinue

    # Construir usando Compose (deve usar defaults '0' e 'local')
    docker-compose -f docker-compose.yml -f docker-compose-build.yml build

    # Verificar labels (deve mostrar 0, local)
    docker image inspect -f '{{.Config.Labels}}' diamol/ch11-numbers-api:v3-build-local

    # Simular ambiente CI e construir novamente
    $env:BUILD_NUMBER="5"
    $env:BUILD_TAG="ci-run-5"
    docker-compose -f docker-compose.yml -f docker-compose-build.yml build

    # Verificar labels (deve mostrar 5, ci-run-5)
    docker image inspect -f '{{.Config.Labels}}' diamol/ch11-numbers-api:v3-build-5
    ```

---

📄 Página 222

🔍 Resumo dos principais pontos:

*   Defaults em múltiplos lugares (Compose `args:`, Dockerfile `ARG`) garantem que build funcione em diferentes cenários (CI, local com Compose, local só com `docker build`).
*   **Exercício Try-it-now:** Construir imagem API usando `docker image build` diretamente (sem Compose), passando um `--build-arg`.
    *   Navegar para `ch11/exercises/numbers`.
    *   Comando: `docker image build -f numbers-api/Dockerfile.v4 --build-arg BUILD_TAG=ch11 -t numbers-api .`
    *   Verificar labels: `docker image inspect -f '{{.Config.Labels}}' numbers-api`.
*   Resultado esperado (Figura 11.11): Label `build_tag` usa valor passado (`ch11`), label `build_number` usa default `0` do `ARG` no Dockerfile (pois não foi passado no comando).
*   Conclusão sobre `ARG`/`LABEL`: Importante para rastreabilidade (imagem -> build CI -> commit código).
*   **11.4 Writing CI jobs with no dependencies except Docker** (Escrevendo jobs CI sem dependências exceto Docker)
    *   Reafirma benefício: Builds rodam em containers (via multi-stage), server CI só precisa de Docker (+Compose). Não precisa de .NET SDK, Java SDK, etc. instalados no host CI.

🧠 Explicação detalhada e didática:

*   A página demonstra que, graças aos valores padrão definidos na instrução `ARG` dentro do Dockerfile, é possível construir a imagem usando o comando `docker image build` diretamente, mesmo sem usar o Docker Compose.
*   O exercício mostra como passar um valor para um argumento (`--build-arg BUILD_TAG=ch11`) e omitir outro (`BUILD_NUMBER`). O resultado (Figura 11.11) confirma que o valor passado é usado, e o omitido usa o default definido no `ARG` do Dockerfile. Isso garante flexibilidade máxima no processo de build.
*   A seção 11.4 retorna ao ponto central do capítulo: a simplicidade dos **jobs de CI** quando se usa Docker. Como toda a complexidade de compilação e empacotamento está encapsulada nos Dockerfiles (geralmente multi-estágio), o servidor de CI (Jenkins, GitLab CI, GitHub Actions, etc.) não precisa ter nenhuma ferramenta de desenvolvimento específica instalada. Sua única dependência real é ter o Docker (e talvez o Docker Compose) disponível para executar os comandos `docker build` ou `docker-compose build`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação principal (Build sem Compose):**
    ```powershell
    # Navegar para
    cd C:\DockerLabs\diamol\ch11\exercises\numbers

    # Construir com docker build, passando um arg
    docker image build -f numbers-api/Dockerfile.v4 --build-arg BUILD_TAG=ch11 -t numbers-api-direct .

    # Verificar labels
    docker image inspect -f '{{.Config.Labels}}' numbers-api-direct
    # Confirme build_tag=ch11 e build_number=0
    ```
*   O servidor Jenkins que você rodou anteriormente só tem Docker e Java (para rodar o próprio Jenkins) instalados; ele consegue construir o app .NET Core porque o build .NET ocorre *dentro* de um contêiner iniciado pelo `docker build`, usando a imagem `golden/dotnetcore-sdk`.

---

📄 Página 223

🔍 Resumo dos principais pontos:

*   Benefício de CI só com Docker: Server CI simples, scripts de build simples, fácil migrar entre serviços CI (Docker Hub, GitHub Actions, Azure DevOps).
*   Exemplo Jenkins: Job configurado via arquivo `Jenkinsfile` no source control.
*   **Listagem 11.5:** Trecho do `Jenkinsfile` e do script `01-build.bat` chamado por ele.
    *   **Jenkinsfile (Estágio 'Build'):**
        *   `dir('ch11/exercises')`: Muda diretório para onde estão os Compose files.
        *   `sh 'chmod +x ./ci/01-build.bat'`: Torna script executável (Linux).
        *   `sh './ci/01-build.bat'`: Executa o script de build.
    *   **Script `01-build.bat`:**
        *   Contém apenas o comando `docker-compose` para construir as imagens, usando os arquivos `-f` core e de build, e a flag `--pull`.
        ```bash
        docker-compose \
        -f docker-compose.yml \
        -f docker-compose-build.yml \
        build --pull
        ```
*   Flag `--pull`: Faz `docker-compose build` (e `docker build`) tentar baixar versões mais recentes das imagens base (`FROM ...`) antes de construir. Boa prática para CI.

🧠 Explicação detalhada e didática:

*   A página explica como o job Jenkins (`diamol`) que rodou anteriormente é configurado. Ele usa um arquivo chamado `Jenkinsfile` (uma convenção do Jenkins para definir pipelines como código, armazenado junto com o código fonte).
*   A Listagem 11.5 mostra um estágio ('Build') desse `Jenkinsfile`. A lógica importante é que ele simplesmente **executa um script externo** (`./ci/01-build.bat`).
*   O conteúdo do script `01-build.bat` é revelado: é apenas o comando `docker-compose build` que já usamos, combinando os arquivos `core` e `build` e adicionando a flag `--pull`.
*   **Abstração:** A complexidade do build está no `docker-compose build` (que por sua vez usa os Dockerfiles). O pipeline Jenkins em si é muito simples, apenas orquestrando a execução de scripts. Isso torna o pipeline fácil de entender e portável para outras ferramentas de CI (que também podem executar scripts).
*   A flag `--pull` é recomendada para builds CI para garantir que você esteja sempre usando as últimas atualizações de segurança e patches das suas imagens base, detectando problemas de compatibilidade o mais cedo possível.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Explorar Arquivos:**
    *   Abra `ch11\exercises\Jenkinsfile` (ou `Jenkinsfile.windows`) no VS Code. Veja a estrutura do pipeline com múltiplos `stage`s.
    *   Abra `ch11\exercises\ci\01-build.bat`. Veja o comando `docker-compose build`.
    *   Abra outros scripts em `ci\` (00-verify, 02-test, 03-push) para ver os comandos Docker/Compose que eles executam para os outros estágios do pipeline.
*   O build que você rodou manualmente com `docker-compose ... build` na p.221 executa essencialmente o mesmo que o estágio 'Build' do Jenkins.

---

📄 Página 224

🔍 Resumo dos principais pontos:

*   Script `01-build.bat` roda `docker-compose build`. Saída é capturada nos logs do Jenkins.
*   **Exercício Try-it-now:** Ver logs do build no Jenkins UI.
    *   Acessar job `diamol` (`http://localhost:8080/job/diamol`).
    *   Clicar no build #2 (o bem-sucedido).
    *   Clicar em "Logs" (ou "Console Output").
    *   Expandir passos; verá saída padrão do `docker build`. (Figura 11.12).
*   Pipeline segue padrão: chama script que roda comando Docker/Compose. Portável entre ferramentas CI.
*   **Outros Estágios do Pipeline Jenkins:**
    *   **Verify (`00-verify.bat`):** Imprime versões `docker` e `docker-compose` (verifica dependências, registra versões usadas).
    *   **Build (`01-build.bat`):** Roda `docker-compose build --pull`. Variável `REGISTRY` definida no Jenkinsfile para tag local (`registry.local:5000`).
    *   **Test (`02-test.bat`):** Roda `docker-compose up -d`, `docker ps`, `docker-compose down`. Teste simples (app sobe/desce sem erro). Real seria rodar testes E2E em outro container.
    *   **Push (`03-push.bat`):** Roda `docker-compose push`. Envia imagens (com tag do registro local) para `registry.local:5000`.

🧠 Explicação detalhada e didática:

*   A página mostra como visualizar os logs detalhados de um build específico no Jenkins, confirmando que a saída é a mesma que você veria ao rodar `docker-compose build` localmente.
*   Detalha os outros estágios do pipeline `diamol` definido no `Jenkinsfile`, explicando o propósito de cada script correspondente na pasta `ci/`:
    *   `00-verify`: Um passo inicial para garantir que `docker` e `docker-compose` estão disponíveis e registrar suas versões.
    *   `01-build`: Constrói as imagens (como já visto).
    *   `02-test`: Executa um teste de "fumaça" muito básico: consegue iniciar a aplicação completa com `docker-compose up` e depois derrubá-la com `down` sem erros? Um teste real aqui seria mais complexo, envolvendo a execução de testes de integração ou end-to-end contra a aplicação rodando.
    *   `03-push`: Se os estágios anteriores (Build, Test) passaram, envia as imagens construídas (que foram tagueadas para `registry.local:5000` devido à variável `REGISTRY` no Jenkinsfile) para o registro Docker local (rodando no contêiner `registry`).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Ver Logs Jenkins:**
    *   Vá para `http://localhost:8080/job/diamol/`.
    *   Clique no número do último build bem-sucedido (ex: #2) na seção "Build History".
    *   No menu esquerdo da página do build, clique em "Console Output".
    *   Examine os logs, procurando pela saída dos comandos `docker-compose build`.
2.  **Verificar Imagens no Registro Local:** (Próximo exercício fará isso via API, mas você pode listar localmente também)
    ```powershell
    # Imagens construídas pelo Jenkins (build #2) devem ter a tag '-build-2'
    docker image ls registry.local:5000/diamol/ch11-numbers-api*
    docker image ls registry.local:5000/diamol/ch11-numbers-web*
    ```

---

📄 Página 225

🔍 Resumo dos principais pontos:

*   Figura 11.12: Mostra logs do estágio Build no Jenkins UI.
*   Estágios CI são sequenciais; falha em um para o job.
*   Implicação: Registro só contém imagens que passaram no build e teste (release candidates).
*   **Exercício Try-it-now:** Verificar imagens no registro local via API REST.
    *   Build #1 falhou (sem código), build #2 sucesso. Esperado: só tags `-build-2` no registro.
    *   Comandos `curl` para API do registro local (`http://registry.local:5000/v2/...`):
        *   `/_catalog` (listar repositórios).
        *   `/<repo>/tags/list` (listar tags de um repositório específico).
*   Figura 11.13: Mostra saída do `curl` confirmando apenas tags `-build-2` presentes.

🧠 Explicação detalhada e didática:

*   A página reforça que, como os estágios do pipeline são sequenciais e uma falha interrompe o processo, o estágio `Push` só é alcançado se os estágios `Build` e `Test` forem bem-sucedidos. Isso significa que qualquer imagem que chegue ao registro Docker pode ser considerada um "release candidate" que passou pelas verificações automatizadas básicas.
*   O exercício usa a API REST do registro Docker local (a mesma explorada no Lab do Cap. 5) para verificar programaticamente quais imagens e tags foram efetivamente enviadas pelo pipeline do Jenkins. São usados dois endpoints:
    *   `/_catalog`: Lista todos os repositórios (nomes de imagem base) no registro.
    *   `/<namespace>/<nome_repo>/tags/list`: Lista todas as tags para um repositório específico.
*   A expectativa é encontrar os repositórios `diamol/ch11-numbers-api` e `diamol/ch11-numbers-web`, e que cada um contenha apenas a tag `v3-build-2` (assumindo que o último build bem-sucedido foi o #2), confirmando que o build #1 falhou antes do push.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Consultar API do Registro (PowerShell):**
    ```powershell
    # Listar repositórios
    Invoke-WebRequest http://registry.local:5000/v2/_catalog | ConvertFrom-Json

    # Listar tags da API
    Invoke-WebRequest http://registry.local:5000/v2/diamol/ch11-numbers-api/tags/list | ConvertFrom-Json

    # Listar tags da Web
    Invoke-WebRequest http://registry.local:5000/v2/diamol/ch11-numbers-web/tags/list | ConvertFrom-Json
    ```
    Confirme que a saída corresponde à expectativa (apenas tags `-build-2`).

---

📄 Página 226

🔍 Resumo dos principais pontos:

*   Figura 11.13: Mostra saída da consulta à API do registro.
*   Pipeline CI apresentado é simples, mas mostra estágios chave e boas práticas.
*   Chave: Docker faz trabalho pesado, pipeline só orquestra scripts. Portável entre ferramentas CI.
*   **11.5 Understanding containers in the CI process** (Entendendo contêineres no processo CI)
    *   Compilar/rodar apps em containers é só o começo.
    *   Consistência Docker permite adicionar mais features ao pipeline.
*   Diagrama (Figura 11.14): Pipeline CI mais extenso ("production-grade").
    *   Adiciona estágios: **Scan Image** (varredura de segurança) e **Sign Image** (assinatura digital).

🧠 Explicação detalhada e didática:

*   A página conclui a descrição do pipeline CI básico, enfatizando sua simplicidade e portabilidade devido à abstração fornecida pelo Docker.
*   A seção 11.5 introduz a ideia de **pipelines de CI mais avançados**, que aproveitam ainda mais o ecossistema Docker para adicionar etapas de segurança e garantia de qualidade.
*   A Figura 11.14 ilustra um pipeline que inclui:
    *   **Scan Image:** Após construir a imagem (e antes de enviá-la), uma ferramenta de varredura de segurança (como Anchore - visto no Cap. 17, ou outras como Trivy, Snyk, Clair) é usada para analisar a imagem em busca de vulnerabilidades conhecidas (CVEs) em seus componentes (SO base, bibliotecas, dependências). Se vulnerabilidades críticas forem encontradas, o pipeline pode ser configurado para falhar.
    *   **Sign Image:** Se a imagem passar na varredura e nos testes, ela pode ser digitalmente assinada. A assinatura atesta que a imagem foi produzida por um pipeline confiável e não foi adulterada. Ambientes de produção podem ser configurados para *somente* executar imagens que possuam uma assinatura válida de uma fonte confiável.

💻 Exemplos práticos (para meu ambiente Windows):

*   Embora o pipeline Jenkins atual não inclua Scan/Sign, você pode explorar ferramentas como:
    *   **Trivy:** Ferramenta de scan open source fácil de usar. Pode rodar como contêiner: `docker run aquasec/trivy image <nome_imagem>:<tag>` para escanear uma imagem localmente.
    *   **Docker Content Trust (Notary):** Mecanismo do Docker para assinatura e verificação de imagens. Requer configuração mais complexa.

💡 Analogias ou observações úteis (se houver):

*   Scan Image: Controle de qualidade que verifica se há peças defeituosas ou perigosas no produto antes de embalar.
*   Sign Image: Colocar um selo de autenticidade/lacre na embalagem do produto, garantindo que ele veio da fábrica certa e não foi violado.

---

📄 Página 227

🔍 Resumo dos principais pontos:

*   Esse fluxo avançado é chamado de **Secure Software Supply Chain** (Cadeia de Suprimentos de Software Segura).
*   Importante para garantir confiança no software implantado.
*   Ferramentas de scan podem falhar build se problemas forem encontrados.
*   Ambiente de produção pode ser configurado para só rodar imagens assinadas.
*   Garante que containers em prod vieram do pipeline confiável, foram testados e escaneados.
*   Checks (scan, sign) operam em containers/imagens -> aplicam-se consistentemente a todas as tecnologias. Builds/Dockerfiles podem variar, mas pipeline CI é o mesmo.
*   **11.6 Lab** (Laboratório)
    *   Objetivo: Construir seu próprio pipeline CI (simples).
    *   Base: App To-Do (Cap. 6), código em `ch11/lab`.
    *   Infra: Jenkinsfile, scripts CI, core Compose file já existem.
*   **Sua Tarefa:**
    1.  Escrever arquivo override `docker-compose-build.yml` com settings de build (context, dockerfile, args).
    2.  Criar novo job Jenkins para rodar pipeline do lab (`Jenkinsfile` em `ch11/lab`).
    3.  Fazer push das mudanças (novo `.yml`, talvez `Jenkinsfile` ajustado) para o repo `diamol` no Gogs local.

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre segurança no pipeline, chamando essa abordagem de "Secure Software Supply Chain". O objetivo é ter alta confiança de que as imagens Docker implantadas são seguras e autênticas. A consistência do Docker permite aplicar essas verificações de segurança (scan, sign) da mesma forma para qualquer aplicação, independentemente da tecnologia interna.
*   O **Lab do Capítulo 11** pede para você configurar um pipeline CI básico para a aplicação To-Do, similar ao que foi feito para o app Random Number.
*   A maior parte já está pronta (Jenkinsfile, scripts). Suas tarefas são:
    1.  **Criar o `docker-compose-build.yml`:** Baseado na Listagem 11.4 (p. 221), você precisa definir a seção `build:` para o serviço `todo-web` (a imagem do To-Do). Especifique o `context` (provavelmente `.`, se o build for rodado da pasta `ch11/lab/todo-list`), o `dockerfile` (o Dockerfile do To-Do), e a seção `args:` para passar `BUILD_NUMBER` e `BUILD_TAG` (usando substituição `${...:-...}` como no exemplo).
    2.  **Criar Job Jenkins:** No Jenkins UI (`http://localhost:8080`), criar um novo item (job), provavelmente do tipo "Pipeline". Você pode usar a opção "Copy from" para copiar a configuração do job `diamol` existente. A principal mudança será apontar para o `Jenkinsfile` correto (o que está na pasta `ch11/lab`, não o de `ch11/exercises`).
    3.  **Enviar Código para Gogs:** Adicionar o novo `docker-compose-build.yml` ao Git, fazer commit, e fazer `push` para o `remote local` (Gogs). Isso (após o próximo poll do Jenkins) deve disparar o novo job.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 11:**
    1.  **Criar `docker-compose-build.yml`:** Na pasta `ch11\lab`, crie o arquivo. Adapte a Listagem 11.4. O serviço será `todo-web`, o `context` provavelmente será `./todo-list`, e o `dockerfile` será algo como `todo-list/Dockerfile` (verifique os nomes reais). Mantenha a extensão `x-args` e o merge `<<: *args`.
    2.  **Criar Job Jenkins:**
        *   Em `http://localhost:8080`, clique "New Item".
        *   Nome: `todo-pipeline` (ou similar).
        *   Selecione "Pipeline".
        *   Pode haver opção "Copy from": digite `diamol` e OK.
        *   Na configuração do job, vá para a seção "Pipeline". Mude o "Script Path" de `ch11/exercises/Jenkinsfile` para `ch11/lab/Jenkinsfile` (ou `Jenkinsfile.windows`). Salve.
    3.  **Commit & Push:**
        ```powershell
        # Estando na raiz C:\DockerLabs\diamol
        git add .\ch11\lab\docker-compose-build.yml # Adiciona novo arquivo
        git commit -m "Add build config for todo-list lab"
        git push local # Push para Gogs
        ```
    4.  **Verificar Jenkins:** Após o push, vá para o Jenkins e veja se o job `todo-pipeline` inicia automaticamente (ou inicie manualmente com "Build Now"). Monitore a execução.

---

📄 Página 228

🔍 Resumo dos principais pontos:

*   Dica para Lab 11: Pode haver falhas iniciais; verificar logs e ajustar. Ninguém acerta job Jenkins de primeira.
*   Link para solução no GitHub.

🧠 Explicação detalhada e didática:

*   Oferece uma dica realista e encorajadora para o Lab 11: é normal que pipelines de CI não funcionem perfeitamente na primeira tentativa. A depuração envolve olhar os logs do Jenkins para entender onde falhou e ajustar a configuração (no `Jenkinsfile`, nos scripts `ci/`, ou nos arquivos Compose) até funcionar.
*   Fornece o link para a solução de referência.

💻 Exemplos práticos (para meu ambiente Windows):

*   Se o job `todo-pipeline` falhar no Jenkins, clique no build falho, vá para "Console Output" e analise os logs de erro para descobrir o que deu errado (ex: caminho de arquivo incorreto, comando falhou, permissões, etc.).

---

📄 Página 229

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, separando o Lab 11 do início da Parte 3.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 230

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 231

🔍 Resumo dos principais pontos:

*   **Início da Parte 3: Running at scale with a container orchestrator** (Rodando em escala com um orquestrador de contêineres)
*   **Orquestração:** Gerenciar aplicações conteinerizadas em **múltiplos servidores** (um cluster).
*   Usa as mesmas imagens Docker e formato de arquivo similar (Docker Compose).
*   Diferença: Você *declara* o estado desejado ao cluster, e o *orquestrador* gerencia os contêineres para você (criação, distribuição, monitoramento, substituição).
*   Foco da Parte 3: **Docker Swarm** (orquestrador simples e poderoso, embutido no Docker).
*   Tópicos: Deploy de stacks (apps) no Swarm, upgrades/rollbacks, conexão do pipeline CI/CD para deploy contínuo.

🧠 Explicação detalhada e didática:

*   Esta página introduz a Parte 3, que eleva o nível do gerenciamento de contêineres do single-host (com Docker e Compose) para múltiplos hosts (clusters) usando **orquestradores**.
*   A ideia chave da orquestração é a **abstração e automação em escala**. Em vez de você decidir manualmente em qual servidor rodar qual contêiner, você entrega a definição da sua aplicação (geralmente em um formato YAML, como o Docker Compose estendido para Swarm, ou os manifestos do Kubernetes) para o orquestrador. O orquestrador, então, automaticamente:
    *   Distribui os contêineres pelos servidores disponíveis no cluster (agendamento/scheduling).
    *   Monitora a saúde dos contêineres (usando health checks).
    *   Substitui contêineres que falham.
    *   Gerencia redes para comunicação entre contêineres no cluster.
    *   Facilita atualizações e rollbacks.
*   Esta parte do livro focará no **Docker Swarm**, que é o orquestrador nativo incluído com o Docker Engine, conhecido por sua simplicidade em comparação com o Kubernetes.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará comandos `docker swarm ...` para inicializar e gerenciar um cluster Swarm (pode ser simulado no seu Docker Desktop ou usando Play With Docker).
*   Você usará `docker stack deploy` para implantar aplicações definidas em arquivos Compose no cluster Swarm.
*   Você usará `docker service ...` para gerenciar os serviços rodando no Swarm.

---

📄 Página 232

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, separando a introdução da Parte 3 do início do Capítulo 12.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 233

🔍 Resumo dos principais pontos:

*   **Capítulo 12: Understanding orchestration: Docker Swarm and Kubernetes** (Entendendo orquestração: Docker Swarm e Kubernetes)
*   Revisão: Meio do livro, confortável com Docker/Compose. Próximo passo: produção (múltiplas máquinas).
*   Orquestração: Camada de gerenciamento que coordena máquinas e containers.
*   Principais orquestradores: **Docker Swarm** e **Kubernetes (K8s)**.
*   Capítulo focará em **Docker Swarm** (embutido, poderoso).
*   Kubernetes: Mais complexo, livro "Learn K8s in a Month of Lunches" recomendado.
*   Começar com Swarm é bom mesmo se objetivo for K8s (curva aprendizado mais suave).
*   **12.1 What is a container orchestrator?** (O que é um orquestrador de contêineres?)
    *   Compose é bom para single machine, mas não para produção (sem alta disponibilidade - HA).
    *   Orquestrador: Agrupa muitas máquinas em **cluster**. Gerencia containers, distribui trabalho, balanceia carga, substitui falhas.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 12, que introduz formalmente a **orquestração de contêineres**.
*   Contrapõe o Docker Compose (ótimo para desenvolvimento e ambientes simples em um único host) com a necessidade de **alta disponibilidade (HA)** e **escalabilidade** em produção, que exigem múltiplos servidores gerenciados por um orquestrador.
*   Apresenta os dois principais orquestradores atuais: **Docker Swarm** e **Kubernetes**. O capítulo focará no Swarm por ser mais simples e integrado ao Docker, mas reconhece a popularidade e complexidade do Kubernetes, sugerindo um aprendizado sequencial (Swarm primeiro pode facilitar K8s depois).
*   A seção 12.1 define o que um orquestrador faz: transforma um conjunto de servidores individuais em um **pool unificado de recursos (cluster)** e automatiza o ciclo de vida dos contêineres sobre esse pool (implantação, escalonamento, balanceamento de carga, recuperação de falhas).

💻 Exemplos práticos (para meu ambiente Windows):

*   O Docker Desktop tem um modo Swarm embutido que pode ser ativado para simular um cluster de um nó único, permitindo rodar os comandos `docker swarm`, `docker stack`, `docker service` localmente.
*   Alternativamente, pode-se usar playgrounds online como Play With Docker (pwd.docker.com) para criar clusters Swarm multi-nó temporários gratuitamente.

💡 Analogias ou observações úteis (se houver):

*   Orquestrador: Pense em um maestro regendo uma grande orquestra (o cluster de servidores). O maestro (orquestrador) tem a partitura (definição da aplicação), decide quais músicos (servidores) tocam quais partes (contêineres), garante que todos estejam sincronizados, e substitui um músico que desmaiar (falha de nó/contêiner) por outro reserva, sem parar a música (aplicação).

---

📄 Página 234

🔍 Resumo dos principais pontos:

*   Criação do cluster: Instalar Docker em cada máquina, uni-las com Swarm/K8s.
*   Gerenciamento remoto via CLI ou UI.
*   Diagrama (Figura 12.1): Mostra múltiplos servidores Docker sendo gerenciados como um único cluster pelo Orquestrador, que roda os contêineres.
*   **Capacidades extras do Orquestrador:**
    *   Banco de dados distribuído no cluster (armazena definições de apps, estado).
    *   Agendador (Scheduler): Decide onde rodar contêineres (baseado em recursos, constraints).
    *   Sistema de Heartbeat: Monitora saúde dos nós do cluster.
*   **Fluxo de Deploy no Orquestrador:**
    1.  Envia YAML para cluster.
    2.  Cluster armazena definição.
    3.  Scheduler aloca contêineres em nós com capacidade.
    4.  Cluster monitora: se nó/contêiner falha, substitui em outro nó.

🧠 Explicação detalhada e didática:

*   A página explica como um cluster é formado (instalando Docker e usando comandos do orquestrador para unir os nós) e como ele é gerenciado (remotamente). A Figura 12.1 ilustra que, para o usuário/administrador, o cluster aparece como uma entidade única, escondendo a complexidade dos servidores individuais.
*   Detalha os componentes internos chave de um orquestrador que permitem a automação:
    *   **Banco de Dados Distribuído:** Armazena de forma confiável (replicada) o "estado desejado" das aplicações e o estado atual do cluster.
    *   **Agendador (Scheduler):** O "cérebro" que decide em qual nó (servidor) um novo contêiner deve ser executado, considerando fatores como recursos disponíveis, afinidade/anti-afinidade, etc.
    *   **Monitoramento (Heartbeat):** Mecanismo pelo qual os nós se comunicam e o gerenciamento central sabe quais nós estão ativos e saudáveis.
*   O fluxo básico de implantação é descrito: você envia a definição da sua aplicação (YAML) para o cluster, ele a armazena e o agendador cuida de criar os contêineres nos nós apropriados. O sistema então trabalha continuamente para manter a aplicação rodando conforme definido, substituindo automaticamente contêineres ou nós que falharem.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker stack deploy -c meu-app.yml my-app-stack` para enviar a definição ao cluster Swarm.
*   Você usará `docker service ls`, `docker service ps <service>` para ver onde o Swarm decidiu rodar seus contêineres (em qual nó).

---

📄 Página 235

🔍 Resumo dos principais pontos:

*   Orquestrador abstrai detalhes de servidores/redes/storage. Trabalha-se com cluster como unidade única via API/CLI.
*   **Features extras do Orquestrador:** Networking, Configuração, Storage (além do gerenciamento de containers).
*   Diagrama (Figura 12.2): Mostra cluster com:
    *   **Ingress Networking:** Tráfego externo chega a qualquer nó, cluster roteia para container correto (mesmo em outro nó).
    *   **Overlay Networking:** Rede virtual que conecta containers através de múltiplos nós.
    *   **Config Objects / Secrets:** Armazenamento seguro de configuração no cluster, disponibilizado para containers.
    *   **Shared Storage / Volumes:** Volumes que podem ser acessados por containers em diferentes nós (requer driver de volume específico).

🧠 Explicação detalhada e didática:

*   A página reforça a abstração fornecida pelo orquestrador: você define o que quer rodar, e ele cuida de onde e como, escondendo os detalhes dos servidores individuais.
*   A Figura 12.2 introduz funcionalidades adicionais importantes que os orquestradores oferecem para suportar aplicações distribuídas em cluster:
    *   **Rede (Networking):**
        *   **Ingress:** Como o tráfego *externo* (de usuários, outras redes) entra no cluster e chega ao serviço correto, não importa em qual nó o usuário se conectou ou em qual nó o contêiner está rodando. O cluster faz o roteamento interno.
        *   **Overlay:** Como contêineres de serviços *diferentes* (rodando em nós diferentes) se comunicam entre si de forma transparente, como se estivessem na mesma rede local.
    *   **Configuração (Config Objects / Secrets):** Mecanismos para armazenar dados de configuração (normais ou sensíveis/secretos) centralmente no cluster e montá-los de forma segura dentro dos contêineres que precisam deles.
    *   **Armazenamento (Shared Storage / Volumes):** Soluções (geralmente via plugins/drivers) para que volumes possam ser acessados por contêineres independentemente do nó em que estão rodando, essencial para aplicações stateful escaladas ou com alta disponibilidade.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará redes `overlay` (já viu `docker network create --driver overlay` no Cap. 12).
*   Você usará a rede `ingress` implicitamente ao publicar portas de serviços no Swarm (`ports:` na definição do serviço no Compose).
*   Você usará `docker config create` e `docker secret create` e as seções `configs:` e `secrets:` no Compose para gerenciamento de configuração no Swarm (Cap. 13).
*   Volumes em Swarm serão discutidos no Cap. 13.

---

📄 Página 236

🔍 Resumo dos principais pontos:

*   **12.2 Setting up a Docker Swarm cluster** (Configurando um cluster Docker Swarm)
*   Deploy de orquestrador com Swarm é super fácil (features embutidas no Docker Engine).
*   **Inicialização:** Comando `docker swarm init` ativa modo Swarm.
    *   **Exercício Try-it-now:** Rodar `docker swarm init`.
    *   Pode dar erro se máquina tiver múltiplos IPs; Docker pede para especificar qual usar (`--advertise-addr <ip>`).
*   Figura 12.3: Mostra saída do `swarm init`.
    *   Informa que Swarm foi inicializado.
    *   Máquina atual tornou-se **Manager**.
    *   Mostra comando (`docker swarm join --token <TOKEN> <IP>:<PORT>`) para outras máquinas se juntarem como **Workers**.
*   **Roles no Cluster:**
    *   **Managers:** Gerenciam o cluster (BD, API, scheduling, monitoramento).
    *   **Workers:** Apenas rodam contêineres agendados pelos managers (e reportam status). Managers também podem rodar contêineres.

🧠 Explicação detalhada e didática:

*   Esta seção inicia a configuração prática de um cluster **Docker Swarm**. A grande vantagem é que as funcionalidades do Swarm já vêm incluídas no Docker Engine; você só precisa "ativá-las".
*   O comando `docker swarm init` transforma um Docker Engine standalone em um **nó Manager** de um novo cluster Swarm. Este primeiro nó se torna o líder inicial do cluster.
*   O comando é muito simples. Se a máquina tiver apenas uma interface de rede principal, ele geralmente funciona sem argumentos. Se tiver múltiplas, você precisa especificar qual endereço IP os outros nós devem usar para se conectar a este manager usando a flag `--advertise-addr`.
*   A saída do `swarm init` é importante: ela confirma a criação do Swarm e fornece o **comando exato**, incluindo um **token de segurança secreto**, que você deve executar em *outras* máquinas Docker para que elas se juntem a este Swarm como nós **Workers**.
*   A página introduz os dois papéis principais dos nós em um cluster Swarm:
    *   **Managers:** São o cérebro do Swarm. Eles mantêm o estado do cluster, recebem comandos, decidem onde rodar os contêineres e monitoram tudo. Para alta disponibilidade, você deve ter múltiplos managers (3 ou 5 é o recomendado).
    *   **Workers:** São os músculos do Swarm. Sua função principal é executar os contêineres conforme instruído pelos managers.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ativar Modo Swarm no Docker Desktop (Simula Cluster de 1 Nó):**
    ```powershell
    docker swarm init
    ```
    *   Você verá uma saída similar à Figura 12.3. Seu Docker Desktop agora está rodando como um Manager de um Swarm de um nó só. Você pode ignorar o comando `docker swarm join` por enquanto, pois não temos outros nós para adicionar neste setup simples.
*   **Verificar Status:**
    ```powershell
    docker info # Procure pela seção 'Swarm: active'
    docker node ls # Deve listar um nó, com 'Leader' como Manager Status
    ```

---

📄 Página 237

🔍 Resumo dos principais pontos:

*   `swarm init` só precisa ser feito uma vez (no primeiro manager).
*   Nós se juntam usando `docker swarm join` com token do manager. Token funciona como senha.
*   Pode-se obter tokens para workers ou novos managers a partir de um manager existente.
*   **Exercício Try-it-now:** Obter tokens e listar nós.
    *   Comandos (executar em um manager):
        *   `docker swarm join-token worker` (mostra comando/token para adicionar worker).
        *   `docker swarm join-token manager` (mostra comando/token para adicionar *outro* manager).
        *   `docker node ls` (lista todos os nós no Swarm).
*   Figura 12.4: Mostra saída dos comandos, incluindo os tokens (omitidos por segurança no livro) e a lista com um único nó (o manager atual).

🧠 Explicação detalhada e didática:

*   A página explica como adicionar mais nós a um Swarm existente. O processo requer:
    1.  Acesso a um nó que já é Manager no Swarm.
    2.  Obter o **token de junção** apropriado nesse manager. Existem tokens separados para adicionar novos Workers e para adicionar novos Managers. Esses tokens são segredos que autorizam o novo nó a se juntar ao cluster.
    3.  Executar o comando `docker swarm join --token <TOKEN> <IP_DO_MANAGER>:<PORTA>` na máquina que você quer adicionar ao Swarm.
*   O exercício demonstra como obter esses comandos/tokens usando `docker swarm join-token worker` e `docker swarm join-token manager` a partir do nó manager que você inicializou. Também mostra novamente `docker node ls` para visualizar os membros atuais do cluster.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Executar no seu Docker Desktop (que é o manager):**
    ```powershell
    # Obter comando/token para adicionar workers (apenas para ver)
    docker swarm join-token worker

    # Obter comando/token para adicionar managers (apenas para ver)
    docker swarm join-token manager

    # Listar nós (deve mostrar apenas seu nó local como Leader)
    docker node ls
    ```
*   **Importante:** Mantenha os tokens secretos! Se alguém tiver o token e acesso à rede do seu manager, pode adicionar nós não autorizados ao seu cluster. Os tokens podem ser rotacionados se comprometidos (`docker swarm join-token --rotate ...`).

---

📄 Página 238

🔍 Resumo dos principais pontos:

*   Swarm de nó único funciona igual a multi-nó, mas sem HA ou escalabilidade real.
*   Diagrama (Figura 12.5): Compara Swarm de 1 nó (dev/test) vs. Swarm multi-nó (produção com managers/workers).
*   Vantagem Swarm vs. K8s: Simplicidade de setup/gerenciamento do cluster.
    *   Swarm: `docker swarm init` + `docker swarm join` em cada nó.
    *   Mesmo processo simples para produção e teste.
*   Com Swarm de 1 nó pronto, pode-se explorar como apps rodam sob orquestração.
*   **12.3 Running applications as Docker Swarm services** (Rodando aplicações como serviços Docker Swarm)
    *   No Swarm, não se roda "containers", mas sim **deploy de "services"**.
    *   Swarm roda contêineres *para* o serviço.
    *   **Serviço:** Abstração sobre contêineres; pode ter múltiplas **réplicas** (contêineres idênticos).

🧠 Explicação detalhada e didática:

*   A página destaca que um Swarm de um nó único (como o que você tem rodando no Docker Desktop agora) é funcionalmente idêntico a um cluster maior para fins de aprendizado dos comandos e conceitos de serviço, embora obviamente não forneça os benefícios de alta disponibilidade (HA) ou capacidade de escalar para além dos recursos da sua máquina. A Figura 12.5 ilustra a diferença de arquitetura entre teste (1 nó) e produção (múltiplos nós com redundância de managers).
*   Reafirma a simplicidade da configuração do cluster Swarm como uma vantagem sobre o Kubernetes.
*   A seção 12.3 introduz o conceito central de como as aplicações são executadas no Swarm: através de **Serviços (Services)**. Em vez de usar `docker run` para criar um contêiner individual, você usa `docker service create` para definir um serviço. O Swarm então se encarrega de criar e gerenciar o número desejado de contêineres (chamados **réplicas**) para executar aquele serviço. Um serviço é a unidade de aplicação gerenciada pelo orquestrador.

💻 Exemplos práticos (para meu ambiente Windows):

*   Seu Docker Desktop em modo Swarm está pronto.
*   O próximo comando será `docker service create`.

💡 Analogias ou observações úteis (se houver):

*   Serviço vs. Contêiner: Pense no serviço como a "descrição do trabalho" (ex: "precisamos de 3 caixas no supermercado"). Os contêineres/réplicas são os "funcionários" individuais que efetivamente realizam aquele trabalho. O Swarm (gerente) lê a descrição do trabalho e contrata/demite/substitui os funcionários conforme necessário para cumprir a meta (ex: manter 3 caixas abertos).

---

📄 Página 239

🔍 Resumo dos principais pontos:

*   Definição de serviço inclui info similar a `docker run` (imagem, env vars, portas, nome).
*   Nome do serviço vira nome DNS na rede overlay do Swarm.
*   Serviço pode ter múltiplas **réplicas**.
*   **Exercício Try-it-now:** Criar um serviço simples.
    *   Comando: `docker service create --name timecheck --replicas 1 diamol/ch12-timecheck:1.0`.
    *   `--name timecheck`: Nome do serviço (e DNS).
    *   `--replicas 1`: Número desejado de contêineres/réplicas.
    *   `diamol/ch12-timecheck:1.0`: Imagem a ser usada.
    *   Listar serviços: `docker service ls`.
*   Figura 12.6: Mostra saída da criação e listagem do serviço. `service ls` mostra nome, modo (replicated), réplicas (desejado/atual - 1/1), imagem, portas.
*   Réplicas são contêineres Docker normais.

🧠 Explicação detalhada e didática:

*   A página introduz o comando `docker service create` para implantar uma aplicação como um serviço no Swarm. As opções são familiares, parecidas com `docker run`:
    *   `--name`: Define o nome do serviço. Este nome será usado para referenciar o serviço em outros comandos (`docker service ls`, `docker service rm`, etc.) e também funcionará como o nome DNS para outros serviços na mesma rede Swarm se comunicarem com ele.
    *   `--replicas`: Especifica quantos contêineres idênticos o Swarm deve manter rodando para este serviço.
    *   `<imagem>:<tag>`: A imagem Docker a ser usada para criar as réplicas.
*   O exercício cria um serviço chamado `timecheck` com 1 réplica, usando uma imagem simples que provavelmente imprime a hora periodicamente.
*   O comando `docker service ls` lista os serviços definidos no Swarm, mostrando seu estado (quantas réplicas estão rodando vs. quantas deveriam estar). A Figura 12.6 mostra que o serviço `timecheck` foi criado e tem 1/1 réplicas rodando.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Serviço:**
    ```powershell
    docker service create --name timecheck --replicas 1 diamol/ch12-timecheck:1.0
    ```
    Você verá um ID longo sendo impresso, confirmando a criação. O Docker baixará a imagem se necessário.
2.  **Listar Serviços:**
    ```powershell
    docker service ls
    ```
    Você deve ver o serviço `timecheck` na lista com `REPLICAS 1/1`.

---

📄 Página 240

🔍 Resumo dos principais pontos:

*   Réplicas (containers) rodam em algum nó; pode-se usar `docker container` nesse nó, *mas* não é recomendado.
*   Swarm gerencia containers; se você interferir (ex: `docker rm`), Swarm corrige.
*   **Exercício Try-it-now:** Demonstrar auto-recuperação do Swarm.
    *   1. Listar réplicas/tarefas do serviço: `docker service ps timecheck`. (Mostra em qual nó está rodando, status).
    *   2. Listar contêineres no nó: `docker container ls`. (Confirma que réplica é um contêiner).
    *   3. Remover o contêiner da réplica: `docker container rm -f $( docker container ls --last 1 -q)`.
    *   4. Listar réplicas novamente: `docker service ps timecheck`.
*   Resultado esperado (Figura 12.7):
    *   `service ps` inicial mostra 1 réplica `Running`.
    *   Após `rm`, o `service ps` mostra a réplica original como `Failed` e uma *nova* réplica `Running` (iniciada há poucos segundos). Swarm substituiu o contêiner removido.
*   **Gerenciamento no Swarm:** Usar comandos `docker service ...`, deixar Swarm gerenciar contêineres.
*   Comandos `docker service` operam no nível do Swarm (agregam info de todos os nós/réplicas).
    *   Ex: `docker service logs` mostra logs de *todas* as réplicas.
    *   Ex: `docker service inspect` mostra definição completa do serviço armazenada no cluster.

🧠 Explicação detalhada e didática:

*   A página explica que, embora as réplicas de um serviço sejam contêineres Docker normais, você **não deve** gerenciá-los diretamente usando comandos `docker container stop/rm`. O **Swarm** é o responsável por manter o número desejado de réplicas rodando.
*   O exercício demonstra isso de forma prática:
    1.  `docker service ps <serviço>` (ps = process status, ou tasks) lista as tarefas (réplicas) do serviço, mostrando seu status e em qual nó estão rodando.
    2.  Identifica-se o contêiner correspondente à réplica no nó local.
    3.  Remove-se *manualmente* esse contêiner.
    4.  Ao verificar novamente com `docker service ps`, observa-se que o Swarm detectou a ausência da réplica e **automaticamente iniciou uma nova** para restaurar o estado desejado (1 réplica). A réplica antiga é marcada como `Failed`. Isso demonstra a capacidade de auto-recuperação básica do Swarm.
*   A lição é: interaja com suas aplicações no Swarm usando os comandos `docker service ...` (e `docker stack ...`, como veremos), e não os comandos `docker container ...` diretamente nas réplicas. Os comandos `service` são projetados para operar no nível do cluster, agregando informações de todas as réplicas, onde quer que estejam rodando.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Listar Tarefas/Réplicas:**
    ```powershell
    docker service ps timecheck
    # Anote o ID da tarefa e o NOME (ex: timecheck.1.xyz...)
    ```
2.  **Listar Contêineres Locais:**
    ```powershell
    docker container ls
    # Encontre o container cujo NOME corresponde ao da tarefa acima. Anote seu CONTAINER ID.
    ```
3.  **Remover Contêiner Manualmente:** (Use o CONTAINER ID)
    ```powershell
    docker container rm -f <CONTAINER_ID_da_replica>
    ```
4.  **Listar Tarefas Novamente:** Espere alguns segundos.
    ```powershell
    docker service ps timecheck
    ```
    Você deverá ver a tarefa antiga como `Failed` ou `Shutdown`, e uma nova tarefa (com nome diferente, ex: `timecheck.1.abc...`) no estado `Running`.

---

📄 Página 241

🔍 Resumo dos principais pontos:

*   Figura 12.7: Mostra a saída do `service ps` antes e depois da remoção manual do contêiner, evidenciando a substituição automática.
*   **Exercício Try-it-now:** Usar comandos `docker service` para obter logs e inspecionar.
    *   Logs das últimas 10s: `docker service logs --since 10s timecheck`.
    *   Inspecionar serviço (mostrar só imagem): `docker service inspect timecheck -f '{{.Spec.TaskTemplate.ContainerSpec.Image}}'`.
*   Figura 12.8: Mostra saída dos logs agregados e da imagem inspecionada.
*   Definição completa do serviço (`inspect` sem `-f`) é armazenada no banco de dados do cluster (replicado entre managers).
*   **Diferença Swarm vs. Compose:** Swarm armazena definição no cluster; Compose depende do arquivo `.yml` local. Pode-se gerenciar serviços Swarm sem o YAML original.
*   **Atualizando Serviços:** Pode-se usar `docker service update` para mudar propriedades (ex: imagem). Não precisa repetir info inalterada.
*   Processo de deploy de atualizações no cluster. Swarm faz **rolling update**: substitui réplicas gradualmente (remove antiga, inicia nova).

🧠 Explicação detalhada e didática:

*   A página demonstra o uso dos comandos `docker service logs` e `docker service inspect`.
    *   `service logs`: Agrega os logs de **todas as réplicas** de um serviço. Flags como `--since` ou `--tail` funcionam como no `container logs`.
    *   `service inspect`: Mostra a **definição completa** do serviço como ela está armazenada no cluster Swarm. A flag `-f` (format) usa a sintaxe de templates Go para extrair apenas partes específicas da saída JSON.
*   Um ponto crucial de distinção entre Swarm e Compose é destacado: o Swarm **armazena a definição do serviço** em seu banco de dados distribuído interno. Isso significa que, uma vez que um serviço é criado, você pode gerenciá-lo (inspecionar, atualizar, remover) de qualquer nó manager, mesmo que você não tenha mais o arquivo YAML original que o definiu. O Compose, sendo client-side, sempre precisa do arquivo YAML.
*   Introduz o comando `docker service update` como a forma de aplicar mudanças a um serviço existente (ex: usar uma nova versão da imagem). O Swarm gerencia essa atualização através de um **rolling update**: ele substitui as réplicas uma por uma (ou em lotes, configurável) para tentar manter o serviço disponível durante a atualização.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Ver Logs do Serviço:**
    ```powershell
    docker service logs --since 30s timecheck # Ver logs dos últimos 30s
    ```
    Você verá logs prefixados com o nome da réplica (ex: `timecheck.1.abc... | ...log message...`).
2.  **Inspecionar Imagem do Serviço:**
    ```powershell
    docker service inspect timecheck -f '{{.Spec.TaskTemplate.ContainerSpec.Image}}'
    # Deve mostrar 'diamol/ch12-timecheck:1.0@sha256:...'
    ```
3.  **Inspecionar Tudo (Opcional):**
    ```powershell
    docker service inspect timecheck
    # Veja a estrutura JSON completa da definição do serviço no Swarm.
    ```

---

📄 Página 242

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Atualizar o serviço `timecheck` para usar imagem `:2.0`.
    *   Comando: `docker service update --image diamol/ch12-timecheck:2.0 timecheck`.
    *   Listar réplicas: `docker service ps timecheck`.
    *   Verificar logs: `docker service logs --since 20s timecheck`.
*   Saída do `service ps` mostra duas instâncias: a antiga (`:1.0`) e a nova (`:2.0`) que a substituiu.
*   Logs do serviço incluem ID da réplica, permitindo ver logs da versão antiga e da nova.
*   Figura 12.9: Mostra a saída do `update`, `ps` (com réplica antiga `Shutdown` e nova `Running`), e `logs` mostrando logs de ambas as versões.

🧠 Explicação detalhada e didática:

*   O exercício demonstra o processo de atualização de um serviço no Swarm usando `docker service update`. A flag `--image` especifica a nova imagem a ser usada (`diamol/ch12-timecheck:2.0`).
*   Ao executar o update, o Swarm inicia o rolling update:
    1.  Ele pode parar a réplica antiga (rodando `:1.0`).
    2.  Ele inicia uma nova réplica usando a imagem `:2.0`.
    3.  Ele monitora a nova réplica (verificando se ela inicia e, se houver health check, se fica saudável).
    4.  Se tudo OK, a atualização é considerada completa para essa réplica. (Como só temos 1 réplica, o processo termina aqui).
*   O comando `docker service ps timecheck` mostra o histórico das réplicas. Você verá a réplica original com status `Shutdown` (ou similar) e a nova réplica com status `Running`.
*   O comando `docker service logs` agora mostrará logs de ambas as réplicas (a antiga antes de parar, e a nova depois de iniciar), prefixados com seus respectivos IDs, permitindo ver a transição. A imagem `:2.0` foi feita para logar a versão, confirmando a atualização.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Atualizar Serviço:**
    ```powershell
    docker service update --image diamol/ch12-timecheck:2.0 timecheck
    ```
    Observe a saída (ex: `timecheck updated`).
2.  **Listar Tarefas/Réplicas:** Espere alguns segundos.
    ```powershell
    docker service ps timecheck
    ```
    Você verá a tarefa antiga (`.1`) como `Shutdown` e uma nova (`.2`, ou talvez `.1` reutilizado dependendo do Swarm) como `Running`, usando a imagem `:2.0`.
3.  **Verificar Logs:**
    ```powershell
    docker service logs --since 30s timecheck
    ```
    Procure por logs da réplica antiga (`:1.0`) e da nova réplica (que deve logar `version: 2.0`).

---

📄 Página 243

🔍 Resumo dos principais pontos:

*   Figura 12.9: Mostra logs confirmando update para v2.0.
*   Orquestradores usam **staged rollouts** (atualizações em etapas) para manter app online.
*   Swarm substitui réplicas uma por uma (padrão) ou em lotes. Se múltiplas réplicas, sempre há algumas rodando.
*   Comportamento do rolling update é configurável (será visto depois).
    *   Ex: 10 réplicas, atualizar 2 por vez, checar saúde antes de prosseguir.
*   Diagrama (Figura 12.10): Ilustra rolling update em progresso. Algumas réplicas rodam versão antiga, outras rodam a nova.
*   Durante rollout, ambas as versões estão ativas; gerenciar experiência do usuário é responsabilidade do app/dev.

🧠 Explicação detalhada e didática:

*   A página explica o conceito de **Staged Rollout** (lançamento em etapas) ou **Rolling Update**, que é o mecanismo padrão usado por orquestradores (Swarm, K8s) para atualizar aplicações sem causar downtime (ou minimizando-o).
*   A ideia é substituir as réplicas que rodam a versão antiga pelas que rodam a versão nova **gradualmente**, em vez de parar todas as antigas e depois iniciar todas as novas.
*   O comportamento padrão do Swarm é substituir uma réplica por vez, mas isso é configurável (ex: atualizar X réplicas em paralelo, esperar Y segundos entre lotes, verificar health check por Z segundos).
*   A Figura 12.10 visualiza um momento *durante* o rollout, onde o cluster tem réplicas de ambas as versões (antiga e nova) rodando simultaneamente. O tráfego de entrada pode ser direcionado para qualquer uma delas.
*   **Implicação:** A aplicação precisa ser projetada para lidar com essa coexistência temporária de versões diferentes, especialmente se houver mudanças no esquema do banco de dados ou formato da API entre as versões.

💻 Exemplos práticos (para meu ambiente Windows):

*   O update que você fez na página anterior foi um rolling update muito simples (1 réplica -> 1 réplica). Se você tivesse escalado para `--replicas 3` antes do update, veria o Swarm substituindo as réplicas uma após a outra (no comportamento padrão).
*   O Capítulo 14 detalhará como configurar o processo de `update_config` no Compose file para controlar o paralelismo, monitoramento e ordem do rollout.

---

📄 Página 244

🔍 Resumo dos principais pontos:

*   Rolling updates automatizados são grande melhoria sobre releases manuais.
*   Suportam auto-recuperação: update checa saúde de novas réplicas; se falharem, update pode pausar automaticamente.
*   Swarm armazena especificação *anterior* do serviço.
*   **Rollback Manual:** Se update causar problema não detectado pelo health check, pode-se reverter para versão anterior com um comando.
*   **Exercício Try-it-now:** Reverter (rollback) o update do `timecheck` para a versão anterior (1.0).
    *   Comando: `docker service update --rollback timecheck`.
    *   Listar réplicas: `docker service ps timecheck`.
    *   Verificar logs: `docker service logs --since 25s timecheck`.
*   Rollback funciona como update (staged rollout), mas usa a spec *anterior* armazenada no Swarm (não precisa especificar imagem `:1.0`).
*   Útil se update quebrar app de forma sutil (não pega no health check).

🧠 Explicação detalhada e didática:

*   A página destaca mais benefícios dos rolling updates gerenciados pelo orquestrador:
    *   **Segurança:** O processo pode incorporar checagens de saúde. Se as novas réplicas falharem nos health checks durante o rollout, o processo pode ser pausado automaticamente, prevenindo uma falha total da aplicação.
    *   **Reversibilidade (Rollback):** O Swarm (e K8s) mantém um histórico das configurações do serviço. Se você implantar uma atualização (ex: v2.0) e depois perceber que ela introduziu um bug não detectado pelos testes automáticos ou health checks, você pode facilmente acionar um **rollback** para a configuração *anterior* (ex: v1.0) com um único comando (`docker service update --rollback <serviço>`).
*   O exercício demonstra esse rollback manual. O comando `--rollback` instrui o Swarm a aplicar a especificação que o serviço tinha *antes* do último `update`. O Swarm então executa outro rolling update, substituindo as réplicas v2.0 por novas réplicas v1.0. Os comandos `ps` e `logs` confirmarão a reversão.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Executar Rollback:**
    ```powershell
    docker service update --rollback timecheck
    ```
2.  **Listar Tarefas/Réplicas:** Espere alguns segundos.
    ```powershell
    docker service ps timecheck
    ```
    Você verá a tarefa v2.0 (`.2` ou `.1`) como `Shutdown` e uma nova tarefa (`.3` ou `.2`) como `Running`, usando a imagem `:1.0` novamente.
3.  **Verificar Logs:**
    ```powershell
    docker service logs --since 30s timecheck
    ```
    Você verá logs da réplica v2.0 sendo parada e da nova réplica v1.0 iniciando e logando `version: 1.0`.

---

📄 Página 245

🔍 Resumo dos principais pontos:

*   Figura 12.11: Mostra a saída do rollback, com réplica v2 Shutdown e nova réplica v1 Running, e logs confirmando a reversão.
*   Gerenciamento no Swarm é via `services`. Outros recursos (networks, ports) funcionam de forma similar.
*   **12.4 Managing network traffic in the cluster** (Gerenciando tráfego de rede no cluster)
    *   Comunicação app-a-app no Swarm é TCP/IP padrão (para os apps).
    *   Apps usam DNS (nome do serviço) -> Docker DNS retorna IP -> App conecta ao IP.
    *   Containers podem estar em nós diferentes; comunicação transparente.
    *   Swarm faz "mágica" de rede por baixo dos panos.
    *   **Rede Overlay (Overlay Network):** Tipo de rede Docker para Swarm.
        *   Rede virtual que se estende por *todos* os nós do cluster.
        *   Serviços anexados a mesma overlay podem se comunicar usando nome do serviço como DNS.

🧠 Explicação detalhada e didática:

*   A página conclui a parte de updates/rollbacks e inicia a discussão sobre **redes no Docker Swarm** (Seção 12.4).
*   Para as aplicações rodando nos contêineres, a comunicação entre serviços parece simples: um serviço `web` apenas tenta se conectar a `http://api/...`. Ele usa o nome do serviço (`api`) como um nome DNS normal.
*   O Swarm fornece a infraestrutura de rede para que isso funcione, mesmo que os contêineres `web` e `api` estejam rodando em servidores físicos diferentes dentro do cluster. A principal ferramenta para isso é a **Rede Overlay**.
*   Uma rede overlay é um tipo especial de rede Docker (`--driver overlay`) que cria uma rede virtual privada que "cobre" (overlays) todos os nós participantes do Swarm. Quando você conecta serviços a essa rede, o Swarm garante que:
    *   Cada réplica receba um IP nessa rede virtual.
    *   O DNS interno do Swarm resolva os nomes dos serviços para os IPs apropriados dentro dessa rede.
    *   O tráfego possa fluir entre contêineres em nós diferentes de forma transparente através dessa rede virtual.

💻 Exemplos práticos (para meu ambiente Windows):

*   O comando para criar uma rede overlay é `docker network create --driver overlay <nome_rede>`. (Você usará isso no próximo exercício).
*   Ao definir serviços (com `docker service create` ou em um Compose file para `docker stack deploy`), você os conecta à rede overlay usando a flag `--network <nome_rede>`.

💡 Analogias ou observações úteis (se houver):

*   Rede Overlay: É como criar uma rede Wi-Fi privada (`<nome_rede>`) que magicamente cobre todos os prédios do seu campus (cluster Swarm). Qualquer dispositivo (contêiner) conectado a essa Wi-Fi pode falar com outros dispositivos na mesma Wi-Fi usando seus nomes, não importa em qual prédio eles estejam fisicamente.

---

📄 Página 246

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 12.12): Mostra duas redes overlay separadas (`App A`, `App B`) em um cluster multi-nó. Cada rede conecta os serviços de sua aplicação, mas isola `App A` de `App B`.
*   Rede overlay permite comunicação intra-app e isolamento inter-apps.
*   **Diferença DNS: Compose vs. Swarm Overlay:**
    *   Compose (rede bridge padrão): DNS retorna *todos* IPs das réplicas escaladas. Cliente escolhe/balanceia. Não escala bem para centenas de réplicas.
    *   Swarm (rede overlay): DNS retorna *um único IP Virtual (VIP)* para o serviço, compartilhado por todas as réplicas.
*   **Exercício Try-it-now:** Criar rede overlay e serviços API do app Image Gallery.
    *   1. Remover serviço `timecheck`: `docker service rm timecheck`.
    *   2. Criar rede overlay: `docker network create --driver overlay iotd-net`.
    *   3. Criar serviço API `iotd` (3 réplicas) na rede: `docker service create --detach --replicas 3 --network iotd-net --name iotd diamol/ch09-image-of-the-day`.
    *   4. Criar serviço API `accesslog` (2 réplicas) na rede: `docker service create --detach --replicas 2 --network iotd-net --name accesslog diamol/ch09-access-log`.
    *   5. Verificar serviços: `docker service ls`.

🧠 Explicação detalhada e didática:

*   A Figura 12.12 ilustra que você pode ter múltiplas redes overlay em um mesmo cluster, e elas funcionam como redes privadas separadas, garantindo isolamento entre diferentes aplicações.
*   A página aponta uma diferença importante no funcionamento do DNS entre as redes padrão do Compose (tipo `bridge`) e as redes `overlay` do Swarm, especialmente para serviços escalados:
    *   **Compose/Bridge:** Retorna múltiplos IPs.
    *   **Swarm/Overlay:** Retorna um único **IP Virtual (VIP)**. Este VIP atua como um endereço de balanceador de carga interno para o serviço. O Swarm automaticamente distribui o tráfego enviado para o VIP entre as réplicas saudáveis do serviço. Isso é mais eficiente e escalável do que o DNS round-robin do Compose.
*   O exercício prepara o cenário para observar o VIP: remove o serviço antigo, cria uma rede overlay (`iotd-net`), e inicia os dois serviços de backend (`iotd` e `accesslog`) da aplicação Image Gallery, conectando ambos à rede `iotd-net` e escalando-os para múltiplas réplicas.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Remover Serviço Antigo:**
    ```powershell
    docker service rm timecheck
    ```
2.  **Criar Rede Overlay:**
    ```powershell
    docker network create --driver overlay iotd-net
    ```
3.  **Criar Serviço `iotd`:**
    ```powershell
    docker service create --detach --replicas 3 --network iotd-net --name iotd diamol/ch09-image-of-the-day
    ```
4.  **Criar Serviço `accesslog`:**
    ```powershell
    docker service create --detach --replicas 2 --network iotd-net --name accesslog diamol/ch09-access-log
    ```
5.  **Verificar Serviços:**
    ```powershell
    docker service ls
    # Você deve ver iotd (3/3) e accesslog (2/2)
    ```

---

📄 Página 247

🔍 Resumo dos principais pontos:

*   Figura 12.13: Mostra saída do `service ls` com os novos serviços `iotd` e `accesslog` rodando com múltiplas réplicas na rede overlay.
*   Comandos rodam igual em Swarm de 1 nó ou 500 nós (exceto que réplicas estariam distribuídas).
*   **Verificando VIP:** Conectar a um terminal de uma réplica e usar `nslookup`.
*   **Exercício Try-it-now:** Executar `nslookup` dentro de uma réplica.
    *   1. Executar shell interativo no último contêiner criado (será uma réplica `accesslog` ou `iotd`).
        *   Comando Linux: `docker container exec -it $(...) sh`.
        *   Comando Windows: `docker container exec -it $(...) cmd`.
    *   2. Dentro do contêiner, rodar `nslookup iotd` e `nslookup accesslog`.

🧠 Explicação detalhada e didática:

*   A página confirma que os serviços foram criados com sucesso no Swarm (Figura 12.13).
*   O objetivo agora é verificar experimentalmente o **IP Virtual (VIP)** retornado pelo DNS do Swarm para serviços em redes overlay.
*   A maneira de fazer isso é entrar em um dos contêineres (réplicas) que estão conectados à rede overlay (`iotd-net`) e usar uma ferramenta de consulta DNS como `nslookup` a partir de lá.
*   O exercício instrui a usar `docker exec -it ...` para obter um shell dentro de uma das réplicas e, em seguida, executar `nslookup` para os nomes dos serviços (`iotd` e `accesslog`). A expectativa (baseada na explicação anterior) é que, mesmo com múltiplas réplicas, o `nslookup` retorne apenas **um** endereço IP para cada serviço - o VIP.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Obter ID do Último Container:**
    ```powershell
    $lastContainer = $(docker container ls --last 1 -q)
    echo $lastContainer
    ```
2.  **Executar Shell no Container:** (Assumindo imagem base Linux)
    ```powershell
    docker container exec -it $lastContainer sh
    ```
3.  **Dentro do Container (prompt / # ou /app #):**
    ```
    # Instalar dnsutils se nslookup não estiver presente (comum em Alpine)
    # apk update && apk add bind-tools
    # Ou apt-get update && apt-get install dnsutils (se Debian/Ubuntu)

    # Executar nslookup
    nslookup iotd
    nslookup accesslog
    ```
4.  **Analisar Saída:** Para cada `nslookup`, procure a(s) linha(s) `Address:` na seção "Non-authoritative answer". Você deve ver apenas **UM** endereço IP para `iotd` e **UM** endereço IP para `accesslog` (provavelmente diferentes entre si, mas apenas um por serviço). Anote esses IPs.
5.  **Sair:**
    ```
    exit
    ```

---

📄 Página 248

🔍 Resumo dos principais pontos:

*   Figura 12.14: Mostra saída do `nslookup` dentro do contêiner, confirmando que apenas 1 IP (o VIP) é retornado para cada serviço, mesmo com múltiplas réplicas.
*   **VIP Networking:** Usado em overlay (Linux/Windows) para load balancing eficiente.
    *   DNS retorna 1 VIP constante (mesmo se escalar).
    *   Cliente envia tráfego para VIP.
    *   Camada de rede do SO (kernel) intercepta e distribui tráfego para uma das réplicas reais (ex: via IPVS no Linux).
*   Swarm usa VIP para comunicação interna confiável e balanceada. Transparente para apps (usam DNS normal).
*   **Tráfego Externo (Ingress):** Swarm também simplifica como tráfego de fora entra no cluster. Problema complexo (qual nó recebe? onde estão as réplicas?).

🧠 Explicação detalhada e didática:

*   A página confirma (Figura 12.14) que o `nslookup` para serviços em rede overlay retorna apenas um IP, o **VIP (Virtual IP)**.
*   Explica como o VIP funciona para balanceamento de carga interno:
    1.  O cliente DNS obtém apenas o VIP.
    2.  O cliente envia pacotes para o VIP.
    3.  O kernel do sistema operacional em *cada nó* do Swarm é configurado (pelo Swarm) para saber que aquele VIP corresponde a múltiplas réplicas reais (possivelmente em outros nós).
    4.  O kernel intercepta o tráfego destinado ao VIP e o redireciona para uma das réplicas reais saudáveis, usando mecanismos eficientes de balanceamento de carga em nível de rede (como IPVS no Linux).
*   Isso fornece um balanceamento de carga mais robusto e escalável do que o DNS round-robin do Compose. Para a aplicação, tudo é transparente.
*   Introduz o próximo desafio de rede: como o tráfego vindo de **fora do cluster** (usuários na internet) chega ao serviço correto, considerando que o usuário pode se conectar a qualquer nó do cluster, e as réplicas do serviço podem estar em nós diferentes.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. A compreensão do VIP é conceitual.

💡 Analogias ou observações úteis (se houver):

*   VIP (Balanceamento Interno): É como ter um número de telefone único para o "Departamento de Vendas" (o serviço). Quando você liga para esse número (envia tráfego para o VIP), a central telefônica do prédio (kernel+Swarm) automaticamente direciona sua chamada para um vendedor disponível (réplica saudável), não importa em qual sala (nó) ele esteja.

---

📄 Página 249

🔍 Resumo dos principais pontos:

*   **Rede Ingress (Ingress Network):** Solução do Swarm para tráfego externo.
    *   Diagrama (Figura 12.15): Ilustra a rede Ingress.
    *   Todos os nós no Swarm escutam na porta publicada pelo serviço (ex: porta 80).
    *   Quando um nó recebe tráfego nessa porta, a Rede Ingress (que também é uma overlay) roteia a requisição para uma réplica saudável do serviço, *mesmo que a réplica esteja em outro nó*.
*   Ingress = balanceador de carga distribuído embutido no Swarm.

🧠 Explicação detalhada e didática:

*   A página explica a **Rede Ingress** do Docker Swarm, que resolve o problema do acesso externo.
*   **Como funciona (Figura 12.15):**
    1.  Você cria um serviço e publica uma porta (ex: `-p 80:8080`).
    2.  O Swarm automaticamente configura **todos os nós** do cluster para escutar naquela porta externa (porta 80 do host, neste exemplo).
    3.  Quando um usuário acessa `http://<IP_de_QUALQUER_NÓ>:80`, aquele nó recebe a requisição.
    4.  A rede Ingress (uma rede overlay especial gerenciada pelo Swarm) entra em ação. Ela sabe onde estão todas as réplicas saudáveis do serviço que publicou a porta 80.
    5.  A requisição é roteada (através da rede Ingress) para uma das réplicas saudáveis, não importa em qual nó ela esteja rodando.
    6.  A réplica processa a requisição e a resposta retorna pelo mesmo caminho.
*   **Benefício:** Você pode direcionar seus usuários (ou seu balanceador de carga externo) para o endereço IP de *qualquer* nó no seu Swarm, e o tráfego chegará ao serviço correto. A rede Ingress atua como um balanceador de carga distribuído na camada 4 (TCP/UDP) para portas publicadas.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você já usou a rede Ingress sem saber! Quando você usou `-p` (ou `ports:`) com `docker service create` ou `docker stack deploy` em modo Swarm, o Swarm configurou a rede Ingress para você.
*   O próximo exercício completará o deploy do Image Gallery adicionando o serviço web com uma porta publicada, que usará a rede Ingress.

💡 Analogias ou observações úteis (se houver):

*   Rede Ingress: É como se todos os porteiros de todos os prédios do campus (todos os nós do Swarm) tivessem o mesmo número de interfone público (porta publicada, ex: 80). Quando um visitante liga para esse número em qualquer portaria, o sistema central (Rede Ingress) sabe para qual apartamento específico (réplica saudável) em qual prédio (nó) encaminhar a chamada.

---

📄 Página 250

🔍 Resumo dos principais pontos:

*   Rede Ingress é padrão no Swarm ao publicar portas (`--publish` ou `ports:`). Complexo, mas fácil de usar.
*   **Exercício Try-it-now:** Implantar serviço web (`image-gallery`) do app, publicando porta via Ingress.
    *   Comando: `docker service create --detach --name image-gallery --network iotd-net --publish 8010:80 --replicas 2 diamol/ch09-image-gallery`.
    *   `--publish 8010:80`: Publica porta 80 interna na porta 8010 de *todos* os nós via Ingress.
    *   `--replicas 2`: Roda 2 instâncias do web app.
    *   Listar serviços: `docker service ls`.
*   Resultado: Serviço rodando com múltiplas réplicas, ouvindo em porta única via Ingress. Impossível no Compose (conflito de porta), possível no Swarm.
*   Figura 12.16: Mostra o `service ls` com o novo serviço `image-gallery` (2/2 réplicas) e a porta 8010 publicada.

🧠 Explicação detalhada e didática:

*   A página confirma que usar a rede Ingress é automático no Swarm: basta publicar uma porta para o serviço.
*   O exercício finaliza a implantação da aplicação Image Gallery no Swarm, criando o serviço frontend `image-gallery`.
    *   Ele é conectado à mesma rede overlay (`iotd-net`) que as APIs para comunicação interna.
    *   Ele publica a porta 8010 (`--publish 8010:80`). Isso automaticamente configura a rede Ingress para que o acesso a `http://<qualquer_nó>:8010` seja roteado para uma das réplicas do `image-gallery`.
    *   Ele é iniciado com `--replicas 2`. O Swarm garantirá que duas instâncias desse contêiner estejam rodando, e a rede Ingress fará o balanceamento de carga entre elas para requisições vindas de fora.
*   Um ponto importante é destacado: no Swarm, múltiplas réplicas do *mesmo serviço* podem (e devem) ser acessadas através da *mesma porta publicada* via Ingress. Isso é diferente do Docker Compose em um host único, onde você não pode ter múltiplos contêineres tentando usar a mesma porta do host.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Serviço Web:**
    ```powershell
    docker service create --detach --name image-gallery --network iotd-net --publish 8010:80 --replicas 2 diamol/ch09-image-gallery
    ```
2.  **Verificar Serviços:**
    ```powershell
    docker service ls
    ```
    Você deve ver os 3 serviços (`accesslog`, `iotd`, `image-gallery`) rodando com o número correto de réplicas e a porta `*:8010->80/tcp` listada para `image-gallery`.

---

📄 Página 251

🔍 Resumo dos principais pontos:

*   Figura 12.16: Saída do `service ls`.
*   **Acesso ao App:**
    *   **Linux/Mac/DockerDesktop-Linux:** Acessar `http://localhost:8010`. Funciona.
    *   **Windows Containers (Nativo/Server/DockerDesktop-Windows):** **NÃO** funciona acessar via `localhost`. Limitação de rede do Windows para Ingress Swarm. Precisa acessar pelo IP do nó a partir de *outra máquina* na rede, ou usar workarounds.
*   É uma das poucas grandes diferenças/limitações de Windows Containers. Funciona em cenários remotos (teste/prod), mas não localmente via `localhost`.
*   Autor muda para Linux containers para o resto do capítulo.
*   Figura 12.17: Mostra app Image Gallery acessado via navegador (presumivelmente de um host Linux/Mac). Tráfego roteado via Ingress para uma réplica web, que chama API via rede overlay.

🧠 Explicação detalhada e didática:

*   Com o serviço web rodando e a porta 8010 publicada via rede Ingress, a página explica como acessá-lo. E aqui surge uma **incompatibilidade significativa** para usuários de **Windows Containers nativos**.
*   **Linux/Mac:** A rede Ingress no Linux/Mac permite que você acesse o serviço publicado usando `localhost:<porta_publicada>` na própria máquina onde o Swarm (mesmo de 1 nó) está rodando.
*   **Windows Containers:** Devido a limitações na implementação da rede overlay/ingress no Windows, acessar `localhost:<porta_publicada>` **não funciona** para serviços Swarm. O tráfego não é roteado corretamente pela rede Ingress nesse caso específico. Para acessar um serviço rodando em um Swarm de Windows Containers, você precisa usar o endereço IP real do nó Swarm e acessá-lo de *outra máquina* na mesma rede, ou usar configurações de rede mais complexas (como modo de host direto, que tem outras implicações).
*   O autor reconhece que isso é uma limitação frustrante do Windows e informa que usará Linux containers para os screenshots/exemplos seguintes onde o acesso via `localhost` é necessário para a demonstração.
*   A Figura 12.17 ilustra o fluxo completo do tráfego em um ambiente funcional (Linux): navegador -> nó Swarm:8010 -> Rede Ingress -> réplica `image-gallery` -> Rede Overlay -> réplica `iotd`.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Se você estiver usando Docker Desktop no MODO LINUX:**
    *   Abra o navegador e acesse `http://localhost:8010`. Deve funcionar e mostrar a aplicação Image Gallery.
*   **Se você estiver usando Docker Desktop no MODO WINDOWS (ou Windows Server):**
    *   Acessar `http://localhost:8010` **NÃO** funcionará.
    *   Para testar, você precisaria:
        1.  Encontrar o endereço IP da sua máquina Windows na sua rede local (ex: via `ipconfig`).
        2.  Acessar `http://<seu_ip_windows>:8010` de *outro dispositivo* (outro PC, celular) na mesma rede Wi-Fi/LAN. (Pode precisar ajustar firewall do Windows para permitir tráfego na porta 8010).
    *   *Devido a essa complexidade, pode ser mais fácil seguir os próximos exemplos que dependem de acesso `localhost` trocando o Docker Desktop para o modo Linux.*

---

📄 Página 252

🔍 Resumo dos principais pontos:

*   Figura 12.17: Mostra o app rodando.
*   Reafirmação: Tamanho do cluster (1 nó vs 50 nós) não muda a forma de deploy/gerenciamento via Swarm. Comandos são os mesmos.
*   **12.5 Understanding the choice between Docker Swarm and Kubernetes** (Entendendo a escolha entre Docker Swarm e Kubernetes)
    *   Swarm foi projetado para ser simples, estendendo conceitos do Compose.
    *   Outros orquestradores surgiram, mas hoje a escolha é basicamente Swarm vs. Kubernetes (K8s).

🧠 Explicação detalhada e didática:

*   A página conclui a parte prática do Swarm, reforçando que os comandos e conceitos (`service create`, `service update`, `network create --driver overlay`, rede Ingress) funcionam da mesma forma independentemente do número de nós no cluster.
*   A seção 12.5 inicia uma discussão comparativa importante: **Docker Swarm versus Kubernetes (K8s)**.
*   Contextualiza que o Swarm evoluiu naturalmente do Docker e do Compose, buscando simplicidade. O Kubernetes surgiu como um projeto separado (originado no Google), com um escopo mais amplo e complexo. Outros orquestradores existiram, mas Swarm e K8s se consolidaram como as principais opções. O objetivo desta seção é ajudar o leitor a entender as diferenças e quando escolher um ou outro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. A discussão é conceitual e estratégica.
*   O Docker Desktop permite habilitar um cluster Kubernetes local (single-node) também (em Settings > Kubernetes > Enable Kubernetes), caso você queira experimentar K8s futuramente.

---

📄 Página 253

🔍 Resumo dos principais pontos:

*   **Kubernetes:**
    *   Mais popular, especialmente na nuvem (oferecido como serviço gerenciado por AWS, Azure, GCP).
    *   Nuvens cuidam do setup complexo do cluster K8s e gerenciamento dos nós (VMs).
    *   Extensível, integra bem com outros serviços da nuvem (load balancers, storage).
*   **Docker Swarm:**
    *   Não oferecido como serviço gerenciado nas nuvens principais.
    *   Para rodar Swarm na nuvem, precisa provisionar VMs e inicializar Swarm manualmente (mais trabalho).
*   Diagrama (Figura 12.18): Mostra a complexidade de recursos na nuvem (VMs, Load Balancer, Discos Gerenciados, Rede Virtual, etc.) que precisam ser gerenciados manualmente para rodar um Swarm em produção no Azure.

🧠 Explicação detalhada e didática:

*   A página compara Swarm e K8s sob a ótica da **disponibilidade e facilidade de uso na nuvem pública**.
*   **Kubernetes (K8s)** tem a vantagem clara aqui: todos os grandes provedores (AWS com EKS, Azure com AKS, Google com GKE) oferecem **Kubernetes como Serviço Gerenciado**. Isso significa que você pode criar um cluster K8s complexo e pronto para produção com poucos cliques ou comandos, e o provedor de nuvem cuida da instalação, gerenciamento e atualização

**You**

🧠 Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), página por página, a apartir da página 253.

Para cada página, siga este modelo de resposta:

📄 Página X

🔍 Resumo dos principais pontos:

\[bullet points com os tópicos abordados na página\]

🧠 Explicação detalhada e didática:

\[explique os conceitos em português claro, com boa didática, como um professor\]

💻 Exemplos práticos (para meu ambiente Windows):

\[comandos ou trechos de código aplicáveis no Windows, usando Docker Desktop e Visual Studio Code\]

💡 Analogias ou observações úteis (se houver):

\[comparações simples ou observações para reforçar o entendimento\]

✅ Importante: percorra automaticamente todas as páginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "próxima".

Quando terminar todas as páginas, diga:

"✅ Fim da leitura. Todas as páginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

OK, Alexandre, retomando a análise a partir da página 253.

---

📄 Página 253

🔍 Resumo dos principais pontos:

*   **Kubernetes:**
    *   Mais popular, especialmente na nuvem (oferecido como serviço gerenciado por AWS, Azure, GCP).
    *   Nuvens cuidam do setup complexo do cluster K8s e gerenciamento dos nós (VMs).
    *   Extensível, integra bem com outros serviços da nuvem (load balancers, storage).
*   **Docker Swarm:**
    *   Não oferecido como serviço gerenciado nas nuvens principais.
    *   Para rodar Swarm na nuvem, precisa provisionar VMs e inicializar Swarm manualmente (mais trabalho).
*   Diagrama (Figura 12.18): Mostra a complexidade de recursos na nuvem (VMs, Load Balancer, Discos Gerenciados, Rede Virtual, etc.) que precisam ser gerenciados manualmente para rodar um Swarm em produção no Azure.

🧠 Explicação detalhada e didática:

*   A página compara Swarm e K8s sob a ótica da **disponibilidade e facilidade de uso na nuvem pública**.
*   **Kubernetes (K8s)** tem a vantagem clara aqui: todos os grandes provedores (AWS com EKS, Azure com AKS, Google com GKE) oferecem **Kubernetes como Serviço Gerenciado**. Isso significa que você pode criar um cluster K8s complexo e pronto para produção com poucos cliques ou comandos, e o provedor de nuvem cuida da instalação, gerenciamento e atualização da infraestrutura do Kubernetes (os "nós mestres" ou "control plane") e dos nós trabalhadores (VMs). Além disso, esses serviços K8s gerenciados integram-se facilmente com outros recursos da nuvem, como balanceadores de carga, armazenamento persistente, etc.
*   **Docker Swarm**, por outro lado, não possui essa mesma oferta de serviço gerenciado nas principais nuvens. Se você quiser rodar Swarm na AWS, Azure ou GCP, você terá que:
    1.  Criar as máquinas virtuais (VMs) manualmente.
    2.  Instalar o Docker Engine em cada uma.
    3.  Executar `docker swarm init` e `docker swarm join` para formar o cluster.
    4.  Configurar manualmente a integração com balanceadores de carga externos, armazenamento compartilhado, etc.
*   A Figura 12.18 ilustra essa complexidade adicional ao rodar Swarm na nuvem, mostrando todos os componentes de infraestrutura que você mesmo teria que configurar e gerenciar no Azure.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Experimentar K8s local:** O Docker Desktop (nas configurações > Kubernetes) permite habilitar um cluster Kubernetes de nó único localmente para aprendizado.
*   **Experimentar Swarm na Nuvem (Conceitual):** Envolveria usar o portal ou CLI do Azure/AWS/GCP para criar VMs Linux ou Windows, conectar-se a elas (via SSH ou RDP), instalar o Docker e rodar os comandos `swarm init/join`. Ferramentas como Terraform ou Pulumi podem automatizar isso.

💡 Analogias ou observações úteis (se houver):

*   K8s Gerenciado na Nuvem: Comprar um carro pronto da concessionária, com garantia e manutenção incluídas pelo fabricante.
*   Swarm na Nuvem (Manual): Comprar todas as peças separadamente e montar o carro você mesmo na sua garagem. Funciona, mas exige mais esforço e conhecimento de mecânica.

---

📄 Página 254

🔍 Resumo dos principais pontos:

*   Deploy de clusters é menos frequente que deploy de apps.
*   **Operações Contínuas: Swarm é mais simples que K8s.**
    *   Swarm tem menos features, mas o suficiente para muitos, com fração da complexidade do K8s.
    *   YAML do Swarm: Extensão do Docker Compose (conciso, lógico).
    *   YAML do K8s: Muito mais complexo e verboso (5 a 10x mais YAML para mesma app).
*   **Conselho:** Começar com Swarm, migrar para K8s *se* precisar de feature que Swarm não tem.
    *   Investimento em Dockerizar apps não é perdido (mesmas imagens).
*   **Fatores para Escolha (Swarm vs. K8s):**
    1.  **Infraestrutura:** Nuvem -> K8s gerenciado é mais simples. Datacenter -> Swarm é mais fácil de operar. Time 100% Windows -> Swarm pode ser mais fácil.
    2.  **Curva de Aprendizado:** Swarm é extensão natural de Docker/Compose. K8s é ecossistema totalmente novo.
    3.  **Conjunto de Features:** K8s é mais rico/configurável (ex: blue/green deploy, autoscaling, RBAC). Swarm tem o essencial.
    4.  **Investimento Futuro:** K8s tem comunidade maior/mais ativa, novas features constantes. Swarm é mais estável/maduro, menos features novas.
*   Provável roadmap futuro leva a K8s, mas Swarm é ótimo começo. Exemplo: Visa usa Swarm em produção.

🧠 Explicação detalhada e didática:

*   Embora configurar um cluster K8s gerenciado na nuvem seja mais fácil, a página argumenta que, no **dia a dia da operação e implantação de aplicações**, o **Docker Swarm é consideravelmente mais simples** que o Kubernetes.
*   **Complexidade:** O Swarm tem um conjunto menor de conceitos e funcionalidades, o que o torna mais fácil de aprender e gerenciar. O Kubernetes tem muito mais opções, o que traz flexibilidade, mas também uma curva de aprendizado e complexidade operacional significativamente maiores.
*   **Definição da Aplicação (YAML):** O Swarm utiliza uma extensão do formato Docker Compose, que muitos já conhecem. O Kubernetes usa seus próprios arquivos de manifesto YAML, que são notavelmente mais verbosos e complexos para descrever a mesma aplicação.
*   **Recomendação do Autor:** Para quem está começando com orquestração, faz sentido iniciar com o Swarm. Se, e somente se, você encontrar uma necessidade que o Swarm não atende, aí sim considere a migração para o Kubernetes. A boa notícia é que o trabalho de criar as imagens Docker é reaproveitado em ambos.
*   A página lista fatores importantes a considerar na decisão: o ambiente de infraestrutura (nuvem vs. datacenter, Windows vs. Linux), a capacidade e disposição da equipe para aprender uma nova tecnologia complexa (K8s), as funcionalidades específicas necessárias, e a direção futura do desenvolvimento de cada plataforma.
*   Conclui que, embora K8s seja o destino provável para muitos a longo prazo, o Swarm é uma plataforma de produção robusta e comprovada (citando o caso da Visa).

💻 Exemplos práticos (para meu ambiente Windows):

*   Os comandos que você aprendeu para `docker service` e `docker stack` (usando arquivos Compose) são a base do gerenciamento no Swarm.
*   Para Kubernetes, você usaria uma ferramenta de linha de comando diferente (`kubectl`) e arquivos YAML com uma estrutura totalmente diferente para definir `Deployments`, `Services`, `Pods`, `Ingresses`, etc.

💡 Analogias ou observações úteis (se houver):

*   Swarm vs. K8s (Complexidade): Swarm é como um smartphone com as funções essenciais bem feitas e fáceis de usar. K8s é como um smartphone topo de linha com centenas de apps e configurações avançadas - poderoso, mas leva tempo para dominar tudo.
*   YAML Swarm vs. K8s: Descrever como fazer um sanduíche. Swarm: "Pão, queijo, presunto, alface. Montar." K8s: "Obtenha um objeto 'Pão' tipo 'Integral', fatia 1. Obtenha objeto 'Queijo' tipo 'Prato', fatia 1. Aplique 'Queijo' sobre 'Pão'. Obtenha 'Presunto'... etc." (Mais detalhado, mais controle, mas mais verboso).

---

📄 Página 255

🔍 Resumo dos principais pontos:

*   **12.6 Lab** (Laboratório)
    *   Objetivo: Ganhar mais experiência com serviços Docker Swarm.
*   **Tarefa:** Rodar o app Random Number (Cap. 8) no cluster Swarm (pode ser o de 1 nó no Docker Desktop).
    *   Precisa de 2 serviços (`numbers-api`, `numbers-web`) e 1 rede overlay para conectá-los.
    *   Usar imagens prontas do Docker Hub:
        *   `diamol/ch08-numbers-api:v3`
        *   `diamol/ch08-numbers-web:v3`
*   Solução no GitHub (poucos comandos).

🧠 Explicação detalhada e didática:

*   O **Lab do Capítulo 12** pede para você aplicar os comandos `docker network create --driver overlay` e `docker service create` que aprendeu neste capítulo para implantar a aplicação Random Number (API + Web) como serviços no seu ambiente Swarm.
*   Os passos são:
    1.  Criar uma rede overlay (ex: `numbers-net`).
    2.  Criar o serviço `numbers-api` usando a imagem `:v3` fornecida, conectando-o à rede `numbers-net`. Você pode definir o número de réplicas (ex: `--replicas 1` ou mais).
    3.  Criar o serviço `numbers-web` usando a imagem `:v3` fornecida, conectando-o à rede `numbers-net` e **publicando uma porta** (ex: `--publish 8088:80`) para poder acessá-lo externamente via rede Ingress.
*   O objetivo é praticar a criação manual de serviços e redes no Swarm.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Modo Swarm:** Verifique com `docker info | Select-String "Swarm"`. Deve dizer `active`. Se não, `docker swarm init`.
2.  **Limpar Serviços Anteriores (Opcional):**
    ```powershell
    docker service rm image-gallery iotd accesslog # Remove serviços do Image Gallery
    docker network rm iotd-net # Remove rede antiga
    ```
3.  **Criar Rede Overlay:**
    ```powershell
    docker network create --driver overlay numbers-net
    ```
4.  **Criar Serviço API:**
    ```powershell
    docker service create --detach --name numbers-api --network numbers-net --replicas 1 diamol/ch08-numbers-api:v3
    ```
5.  **Criar Serviço Web:**
    ```powershell
    docker service create --detach --name numbers-web --network numbers-net --publish 8088:80 --replicas 1 diamol/ch08-numbers-web:v3
    ```
6.  **Verificar:**
    ```powershell
    docker service ls
    # Deve mostrar ambos os serviços com 1/1 réplicas.
    ```
7.  **Testar (Se estiver em modo Linux no Docker Desktop):**
    *   Acesse `http://localhost:8088`. Clique no botão. Deve funcionar (pelas 3 primeiras vezes, lembre-se do healthcheck do v3 que deve reiniciar a API depois).

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch12/lab/README.md

---

📄 Página 256

🔍 Resumo dos principais pontos:

*   **Capítulo 13: Deploying distributed applications as stacks in Docker Swarm** (Implantando aplicações distribuídas como stacks no Docker Swarm)
*   Confissão: Criar serviços manualmente com `docker service create` (como no Cap. 12) não é feito em projetos reais.
*   Abordagem Real: Descrever a aplicação em arquivo **YAML** (formato Docker Compose) e enviar ao manager Swarm.
*   Swarm lê o YAML e cria/atualiza os recursos (serviços, redes, etc.) para atingir o estado desejado. Mesma abordagem declarativa do Compose.
*   **Stack:** Recurso Swarm que agrupa os recursos de uma aplicação definida em um arquivo Compose.
*   **Vantagem:** Usa o mesmo formato de arquivo (`docker-compose.yml`) familiar de dev/teste para deploys em produção no Swarm.
*   Mas há diferenças: algumas configs Compose só fazem sentido em single-host (ex: `build`), outras só em Swarm (`deploy`). Especificação é flexível.
*   Foco Cap. 13: Usar Compose para definir e implantar **stacks** no Swarm.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 13 corrigindo uma impressão do capítulo anterior: embora `docker service create` funcione, a maneira idiomática e prática de implantar aplicações no Docker Swarm é usando arquivos **Docker Compose** e o conceito de **Stack**.
*   **Stack:** Pense em uma "stack" (pilha) como a representação, dentro do Swarm, de uma aplicação completa definida em um arquivo Docker Compose. Quando você "implanta uma stack", você entrega seu arquivo `docker-compose.yml` ao Swarm manager.
*   **Processo:**
    1.  Você escreve um `docker-compose.yml` descrevendo seus serviços, redes, volumes, configs, secrets.
    2.  Você usa o comando `docker stack deploy -c <arquivo.yml> <nome_da_stack>`.
    3.  O Swarm manager lê o arquivo, compara com o estado atual dos recursos pertencentes àquela stack (se ela já existir) e cria, atualiza ou remove serviços, redes, etc., conforme necessário para corresponder à definição no arquivo.
*   **Benefício:** Reutiliza o conhecimento e os arquivos do Docker Compose, tornando a transição de desenvolvimento (Compose) para produção (Swarm stack) mais suave.
*   **Adaptação:** O formato do arquivo Compose usado para stacks Swarm é o mesmo do Compose standalone, mas o Swarm *ignora* certas seções que não fazem sentido em um cluster (como `build`) e *interpreta* seções específicas do Swarm (principalmente a seção `deploy` dentro de um serviço).

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará arquivos `docker-compose.yml` (ou apenas `.yml`) como entrada.
*   O comando principal será `docker stack deploy`.
*   Outros comandos relacionados: `docker stack ls`, `docker stack ps`, `docker stack services`, `docker stack rm`.

💡 Analogias ou observações úteis (se houver):

*   `docker service create` vs. `docker stack deploy`:
    *   `service create`: Como dar instruções verbais separadas ao gerente da loja para contratar um caixa, depois um empacotador, depois um estoquista.
    *   `stack deploy`: Como entregar ao gerente um documento completo (o YAML) descrevendo *toda* a equipe necessária (caixas, empacotadores, estoquistas, suas funções, quantos de cada) e deixar que ele cuide de contratar/organizar todos de uma vez.

---

📄 Página 257

🔍 Resumo dos principais pontos:

*   **13.1 Using Docker Compose for production deployments** (Usando Docker Compose para implantações de produção)
*   Poder real do Swarm vem do Compose: mesmo formato de dev/teste a prod.
*   Exemplo Simples (`v1.yml`): Deploy básico do To-Do app.
*   **Listagem 13.1:** Arquivo Compose (`v1.yml`) para deploy Swarm.
    ```yaml
    version: "3.7"
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
    ```
*   Este arquivo pode ser usado com `docker-compose up` (cria 1 container) OU com `docker stack deploy` (cria 1 serviço com 1 réplica).
*   **Stack:** Agrupa recursos (serviços, redes, etc.) de um deploy Swarm.
*   **Exercício Try-it-now:** Implantar o Compose simples como stack.
    *   Garantir modo Swarm ativo. Navegar para `ch13/exercises`.
    *   Comando deploy: `docker stack deploy -c ./todo-list/v1.yml todo`. (`todo` é o nome da stack).
    *   Verificar stacks: `docker stack ls`.
    *   Verificar serviços criados: `docker service ls`.

🧠 Explicação detalhada e didática:

*   A seção 13.1 demonstra como até mesmo um arquivo `docker-compose.yml` muito simples, sem nenhuma configuração específica do Swarm, pode ser implantado como uma **stack** no Docker Swarm.
*   A Listagem 13.1 mostra um arquivo mínimo para o To-Do app, apenas especificando a imagem e a porta.
*   O comando chave é `docker stack deploy`. Ele recebe:
    *   `-c <arquivo.yml>`: O caminho para o arquivo Compose que define a aplicação.
    *   `<nome_da_stack>`: Um nome que você escolhe para agrupar todos os recursos criados a partir deste arquivo (serviços, redes, etc.).
*   O exercício instrui a implantar a Listagem 13.1 como uma stack chamada `todo`. O Swarm lerá o arquivo e criará um serviço chamado `todo_todo-web` (prefixado pelo nome da stack) com 1 réplica, conectado a uma rede padrão criada para a stack (ou uma rede definida no YAML), e publicará a porta 8080 via rede Ingress.
*   Os comandos `docker stack ls` e `docker service ls` são usados para verificar que a stack e o serviço foram criados.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Modo Swarm:** `docker info | Select-String "Swarm"` (deve ser `active`).
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch13\exercises
    ```
3.  **Implantar Stack:**
    ```powershell
    docker stack deploy -c ./todo-list/v1.yml todo
    ```
    Observe a saída (criando rede, criando serviço).
4.  **Verificar Stacks:**
    ```powershell
    docker stack ls
    # Deve listar a stack 'todo'
    ```
5.  **Verificar Serviços:**
    ```powershell
    docker service ls
    # Deve listar o serviço 'todo_todo-web' com 1/1 réplicas.
    ```

---

📄 Página 258

🔍 Resumo dos principais pontos:

*   Figura 13.1: Mostra saída do `stack deploy`, `stack ls`, `service ls`. Confirma criação da rede padrão e do serviço `todo_todo-web`.
*   Acesso ao app (Linux `localhost:8080`, Windows requer IP externo) funciona como antes.
*   Benefício: Mesmo arquivo Compose simples já dá alta disponibilidade (HA) no Swarm (se multi-nó, Swarm substitui réplica se nó falhar).
*   **Seção `deploy:` no Compose:** Adiciona configurações específicas do Swarm.
    *   Ignorada por `docker-compose up`, usada por `docker stack deploy`.
*   **Listagem 13.2:** Trecho de `v2.yml` com seção `deploy:`.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
        deploy: # Configs específicas do Swarm
          replicas: 2 # Rodar 2 instâncias
          resources: # Limites de recursos
            limits:
              cpus: "0.50" # Max 50% de 1 core CPU por réplica
              memory: 100M # Max 100 MB de RAM por réplica
    ```

🧠 Explicação detalhada e didática:

*   A página confirma que o deploy da stack funcionou e que o acesso ao app é o mesmo de antes (com a ressalva do `localhost` no Windows). Mesmo com esse deploy simples, se o Swarm tivesse múltiplos nós, a aplicação já teria um nível básico de alta disponibilidade.
*   Introduz a seção `deploy:` dentro da definição de um serviço no arquivo Compose. Esta seção é a maneira padrão de especificar configurações que são relevantes *apenas* para o modo Swarm (e Kubernetes, com algumas adaptações de sintaxe em outra ferramenta chamada Kompose).
*   **Importante:** O `docker-compose` (standalone) **ignora** completamente a seção `deploy:`. O `docker stack deploy` (Swarm) **lê e aplica** as configurações dentro de `deploy:`.
*   A Listagem 13.2 mostra exemplos de configurações comuns em `deploy:`:
    *   `replicas: 2`: Instrui o Swarm a manter 2 réplicas (contêineres) rodando para este serviço, em vez do padrão 1.
    *   `resources: limits:`: Define limites máximos de consumo de CPU e memória para cada réplica. Isso é crucial em produção para evitar que um serviço "fuja" e consuma todos os recursos do nó, afetando outros serviços. `cpus: "0.50"` limita a meio core de CPU. `memory: 100M` limita a 100 Megabytes de RAM.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar `v2.yml`:** Na pasta `ch13\exercises\todo-list`, crie um arquivo `v2.yml` com o conteúdo completo (incluindo `version:` e a seção `services:` da Listagem 13.2).
2.  **(Opcional) Testar com Compose:** Rode `docker-compose -f v2.yml config`. Note que a seção `deploy:` *não aparece* na saída final, pois o Compose a ignora.
3.  O próximo exercício usará `docker stack deploy` com este arquivo `v2.yml`.

💡 Analogias ou observações úteis (se houver):

*   Seção `deploy:`: Instruções extras na planta da casa que só o síndico do condomínio (Swarm) entende, não o construtor de uma única casa (Compose). Ex: "Construa duas casas deste modelo" (`replicas: 2`), "Cada casa não pode usar mais que X de eletricidade" (`resources: limits:`).

---

📄 Página 259

🔍 Resumo dos principais pontos:

*   Config `deploy:` são básicas para produção (múltiplas réplicas para HA/carga, limites para proteger cluster).
*   Calcular limites exige conhecer pico de uso do app (métricas do Cap. 9 ajudam).
*   `limits:` de CPU/memória são importantes.
*   **Atualizando Stack:** Mesmo comando `docker stack deploy -c <novo_arquivo.yml> <nome_stack_existente>`.
    *   Swarm compara novo YAML com estado atual e aplica diferenças.
*   **Exercício Try-it-now:** Atualizar stack `todo` usando `v2.yml`. Verificar tarefas.
    *   Comando deploy: `docker stack deploy -c ./todo-list/v2.yml todo`.
    *   Verificar tarefas/réplicas do serviço: `docker service ps todo_todo-web`.
*   Figura 13.2: Saída da atualização e `service ps`.
*   Observação: `service ps` mostra réplica original (`.1`) sendo substituída (`Shutdown`) por duas novas (`.1` e `.2` - *Nota: Swarm reusa o índice 1*).
*   **Por que a réplica original foi substituída, não apenas adicionada uma nova?** Porque adicionar `resources: limits:` muda a definição do contêiner, exigindo recriação.

🧠 Explicação detalhada e didática:

*   A página reforça a importância de `replicas` e `resources: limits:` para produção. Determinar os limites corretos é um processo que envolve observar o consumo real da aplicação sob carga (usando ferramentas como Prometheus/Grafana).
*   Explica que para **atualizar** uma stack existente, usa-se o **mesmo comando `docker stack deploy`**, apenas fornecendo o novo arquivo YAML (ou o mesmo arquivo modificado) e o mesmo nome da stack. O Swarm é inteligente o suficiente para calcular as diferenças ("diff") entre a nova definição e a que está rodando e aplicar apenas as mudanças necessárias (ex: escalar réplicas, atualizar imagem, mudar limites).
*   O exercício aplica o arquivo `v2.yml` (que define `replicas: 2` e `limits`) à stack `todo` existente.
*   A análise da saída do `docker service ps todo_todo-web` (Figura 13.2) revela um detalhe importante: não apenas uma nova réplica foi adicionada para chegar a 2, mas a réplica original também foi substituída. A razão é que a adição da seção `resources: limits:` na definição do serviço alterou a especificação fundamental de como as réplicas devem ser criadas, forçando o Swarm a recriar todas elas com os novos limites aplicados.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Atualizar Stack:** (Estando em `ch13/exercises`)
    ```powershell
    docker stack deploy -c ./todo-list/v2.yml todo
    ```
    Observe a saída (ex: `Updating service todo_todo-web`).
2.  **Verificar Tarefas:** Espere alguns segundos para o update completar.
    ```powershell
    docker service ps todo_todo-web
    ```
    Você deve ver a tarefa original (`.1`) com status `Shutdown` e duas novas tarefas (provavelmente `.1` e `.2` nos nomes) com status `Running`, ambas usando a mesma imagem mas agora com limites aplicados.
3.  **Verificar Serviço:**
    ```powershell
    docker service ls
    # Deve mostrar todo_todo-web com REPLICAS 2/2
    ```

---

📄 Página 260

🔍 Resumo dos principais pontos:

*   Figura 13.2: Confirma substituição de réplicas na atualização com limites.
*   Containers rodam sem limites por padrão; OK para dev/teste, mas ruim para prod (risco de consumo total de recursos).
*   `limits` são aplicados na criação do container; mudá-los causa recriação/update.
*   **Gerenciamento de Stacks:**
    *   Agrupam apps; cluster pode ter muitos apps/stacks.
    *   Gerenciar app como um todo via `docker stack ...` (ls, services, ps, rm).
*   **Exercício Try-it-now:** Explorar comandos `stack`.
    *   Listar serviços da stack `todo`: `docker stack services todo`.
    *   Listar réplicas/tarefas de todos os serviços da stack: `docker stack ps todo`.
    *   Remover a stack inteira: `docker stack rm todo`.

🧠 Explicação detalhada e didática:

*   A página explica por que os limites de recursos causam a recriação das réplicas: os limites são parte fundamental da definição do contêiner e só podem ser aplicados quando ele é criado.
*   Introduz os comandos `docker stack ...` como a forma principal de gerenciar uma aplicação implantada como um todo no Swarm. Eles oferecem uma visão agregada dos recursos pertencentes a uma stack específica:
    *   `docker stack ls`: Lista todas as stacks rodando no Swarm.
    *   `docker stack services <stack_name>`: Lista apenas os serviços que pertencem a essa stack.
    *   `docker stack ps <stack_name>`: Lista todas as tarefas/réplicas de todos os serviços pertencentes a essa stack.
    *   `docker stack rm <stack_name>`: Remove a stack e **todos os recursos associados a ela** que foram criados pelo deploy (serviços, e redes/configs/secrets *não* marcados como `external`). **Use com cuidado!**
*   O exercício demonstra esses comandos para visualizar e depois remover completamente a stack `todo`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Explorar Stack `todo`:**
    ```powershell
    docker stack services todo
    # Deve listar apenas 'todo_todo-web'

    docker stack ps todo
    # Deve listar as 2 réplicas 'Running' e a 1 'Shutdown' de 'todo_todo-web'
    ```
2.  **Remover Stack:**
    ```powershell
    docker stack rm todo
    ```
    Observe a saída (removendo serviço, removendo rede).
3.  **Verificar Remoção:**
    ```powershell
    docker stack ls # Não deve listar mais 'todo'
    docker service ls # Não deve listar mais 'todo_todo-web'
    docker network ls # Rede padrão da stack 'todo' (ex: 'todo_default') deve ter sido removida
    ```

---

📄 Página 261

🔍 Resumo dos principais pontos:

*   Figura 13.3: Mostra saída dos comandos `stack services`, `stack ps`, `stack rm`.
*   Gerenciamento via `stack` funciona mesmo para apps grandes (dezenas de serviços, centenas de réplicas).
*   Definições ficam no cluster DB; não precisa do YAML para `stack rm` ou outros comandos de gerenciamento. DB é replicado entre managers (seguro).
*   Próximo: Configuração de app no Swarm usando arquivos armazenados no cluster.
*   **13.2 Managing app configuration with config objects** (Gerenciando configuração de app com objetos de configuração)
*   Apps precisam carregar config do ambiente (env vars, arquivos).
*   Docker provê ambiente (seta env vars, constrói filesystem).
*   Produção (Swarm): Usa **Docker Config Objects** armazenados no cluster.
*   Diagrama (Figura 13.4): Mostra fluxo de config em diferentes ambientes.
    *   Dev (Compose): Env Vars / Bind Mounts.
    *   Prod (Swarm): Config Objects / Secrets.
    *   Mesma imagem Docker em todos.

🧠 Explicação detalhada e didática:

*   A página conclui a introdução às stacks, reforçando que elas simplificam o gerenciamento de aplicações complexas no Swarm e que as definições persistem no cluster, diferentemente do Compose.
*   A seção 13.2 introduz a maneira específica do Swarm (e K8s tem algo similar) para lidar com **arquivos de configuração**: os **Objetos de Configuração (Config Objects)** ou simplesmente **Configs**.
*   A ideia é que, em vez de montar arquivos de configuração do host via bind mount (o que exigiria que o arquivo existisse em todos os nós do cluster onde uma réplica pudesse rodar) ou embuti-los na imagem (o que é ruim para flexibilidade e segurança), você **carrega o conteúdo do arquivo de configuração para dentro do cluster Swarm como um objeto gerenciado**.
*   Esse "Config Object" é armazenado de forma segura no banco de dados distribuído do Swarm. Ao definir um serviço, você pode especificar que ele precisa de um determinado Config Object. O Swarm então garante que o conteúdo desse objeto seja disponibilizado como um arquivo *dentro* do filesystem de cada réplica daquele serviço, onde quer que ela rode no cluster.
*   A Figura 13.4 ilustra o conceito geral: a *mesma imagem Docker* é usada em Dev e Prod, mas a *fonte* da configuração muda. Em Dev (Compose), podem ser variáveis de ambiente ou bind mounts locais. Em Prod (Swarm), são Config Objects e Secrets gerenciados pelo cluster.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará o comando `docker config create` para criar um Config Object no Swarm a partir de um arquivo local.
*   Você usará a seção `configs:` no arquivo Compose (similar a `secrets:`) para declarar que um serviço precisa usar um Config Object e onde montá-lo como arquivo dentro do contêiner.

💡 Analogias ou observações úteis (se houver):

*   Config Object: É como fazer upload de um documento importante (arquivo de configuração) para um cofre central seguro no condomínio (cluster Swarm). Quando um morador autorizado (réplica de serviço) precisa do documento, o síndico (Swarm) entrega uma cópia diretamente dentro do apartamento dele (monta como arquivo no contêiner).

---

📄 Página 262

🔍 Resumo dos principais pontos:

*   Configs são recurso de primeira classe em orquestradores.
*   Desacopla deploy da app do gerenciamento de config.
    *   Equipe de config pode gerenciar configs em sistema central e carregá-los no cluster Swarm, separadamente do deploy da app.
*   **Configs:** Criados a partir de arquivo existente.
*   **Exercício Try-it-now:** Criar um Config Object a partir de arquivo JSON local.
    *   App To-Do usa JSON para config. Config padrão (na imagem) usa SQLite. Nova config usará Postgres.
    *   Comando: `docker config create todo-list-config ./todo-list/configs/config.json`.
    *   Verificar configs: `docker config ls`.
*   Figura 13.5: Mostra saída da criação e listagem do config. Objeto criado com nome e ID longo.

🧠 Explicação detalhada e didática:

*   A página reforça que os **Configs** (e Secrets) são entidades gerenciadas pelo próprio orquestrador, o que permite separar as responsabilidades: a equipe de desenvolvimento entrega a imagem Docker, e a equipe de operações (ou automação) carrega a configuração apropriada para o ambiente (Teste, UAT, Produção) no cluster como Config Objects. A definição do serviço no deploy apenas *referencia* o Config Object pelo nome.
*   O comando `docker config create <nome_do_config> <caminho_arquivo_local>` é usado para carregar o conteúdo de um arquivo do seu sistema local para dentro do Swarm como um Config Object com o nome especificado.
*   O exercício cria um config chamado `todo-list-config` a partir de um arquivo `config.json` local. Este arquivo contém as configurações para o app To-Do usar o banco de dados PostgreSQL.
*   O comando `docker config ls` lista os Config Objects que existem no cluster Swarm.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:** (Estando em `ch13/exercises`)
    ```powershell
    # O arquivo deve estar em ./todo-list/configs/config.json
    # Verifique o conteúdo (opcional):
    # Get-Content ./todo-list/configs/config.json | ConvertFrom-Json
    ```
2.  **Criar Config Object:**
    ```powershell
    docker config create todo-list-config ./todo-list/configs/config.json
    ```
3.  **Listar Configs:**
    ```powershell
    docker config ls
    # Deve listar 'todo-list-config'
    ```

---

📄 Página 263

🔍 Resumo dos principais pontos:

*   Gerenciamento de Configs: `docker config rm`, `docker config inspect`.
*   `inspect` mostra conteúdo do config.
*   **Importante:** Configs **NÃO são para dados sensíveis**.
    *   Conteúdo *não* é criptografado no Swarm DB nem em trânsito para os nós.
*   **Exercício Try-it-now:** Inspecionar o config object criado.
    *   Comando: `docker config inspect --pretty todo-list-config`. (`--pretty` formata a saída).
*   Figura 13.6: Mostra saída do `inspect`, incluindo metadados e o conteúdo JSON original do arquivo.

🧠 Explicação detalhada e didática:

*   A página introduz os comandos para gerenciar Config Objects existentes: `docker config rm` (remover) e `docker config inspect` (ver detalhes).
*   Uma advertência crucial é dada: **Config Objects não são seguros para dados confidenciais!** O conteúdo deles é armazenado em texto plano (ou no formato original) no banco de dados do Swarm e transmitido sem criptografia adicional para os nós que precisam dele. Qualquer pessoa com acesso ao Swarm manager (ou potencialmente à rede do cluster) pode conseguir ler o conteúdo de um Config Object.
*   O exercício usa `docker config inspect --pretty` para visualizar o conteúdo do `todo-list-config` que foi criado. A flag `--pretty` apenas formata a saída JSON para ser mais legível. O resultado (Figura 13.6) mostra claramente o conteúdo do arquivo `config.json` original, confirmando a falta de criptografia.
*   Para dados sensíveis (senhas, chaves API, certificados), deve-se usar o recurso irmão: **Secrets** (próxima seção).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Inspecionar Config:**
    ```powershell
    docker config inspect --pretty todo-list-config
    ```
    Examine a saída. Na chave `Spec.Data`, você verá o conteúdo do `config.json` (provavelmente codificado em Base64, mas `inspect --pretty` pode decodificá-lo para exibição).

💡 Analogias ou observações úteis (se houver):

*   Config Object vs. Secret:
    *   Config Object: Um memorando importante deixado no quadro de avisos do condomínio. Todos os moradores autorizados (réplicas) podem ler.
    *   Secret: Uma carta registrada entregue em um envelope lacrado e seguro apenas ao morador específico que precisa dela. Só ele consegue ler o conteúdo.

---

📄 Página 264

🔍 Resumo dos principais pontos:

*   Figura 13.6: Confirma que conteúdo do config é visível.
*   Fluxo de trabalho pode ser separado (ops cria configs, dev/pipeline deploya app que usa configs).
*   **Consumindo Configs em Serviços:** Especificados no Compose file.
*   **Listagem 13.3:** Trecho de `v3.yml` mostrando uso do config no serviço `todo-web`.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
        configs: # Seção para usar configs
          - source: todo-list-config # Nome do Config Object no Swarm
            target: /app/config/config.json # Caminho onde arquivo será montado no container
        #... (outras configs do serviço)
    #... (outras seções do YAML)
    configs: # Seção para declarar configs usados na stack
      todo-list-config:
        external: true # Indica que 'todo-list-config' já existe no Swarm
    ```
*   **Explicação:**
    *   Seção `configs:` dentro do serviço `todo-web`: Mapeia o `source` (nome do Config Object no Swarm) para o `target` (caminho do arquivo dentro do contêiner). App lerá `/app/config/config.json`.
    *   Sintaxe curta (omitir `target`) existe, mas não recomendada (path default varia com OS). Melhor ser explícito.
    *   Seção `configs:` no nível raiz do YAML: Declara os Config Objects que a stack usará. `external: true` significa que o `docker stack deploy` *não* tentará criar o `todo-list-config`; ele espera que já exista no Swarm (criado via `docker config create`).

🧠 Explicação detalhada e didática:

*   A página mostra como um serviço (definido em um arquivo Compose para `docker stack deploy`) consome um Config Object que já existe no Swarm.
*   **Listagem 13.3** ilustra as duas partes necessárias:
    1.  **Dentro da definição do serviço (`services: todo-web: configs:`):**
        *   Especifica-se qual Config Object usar (`source: todo-list-config`).
        *   Especifica-se onde o conteúdo desse objeto deve aparecer como um arquivo dentro do contêiner (`target: /app/config/config.json`). A aplicação `todo-web` está programada para procurar um arquivo de configuração nesse local.
    2.  **No nível raiz do arquivo Compose (`configs:`):**
        *   Declara-se o nome do Config Object (`todo-list-config:`).
        *   Usa-se `external: true` para dizer ao Swarm: "Este config object já foi criado externamente (via `docker config create`), não tente criá-lo como parte desta stack". Se `external: true` fosse omitido, o Swarm esperaria encontrar a definição do *conteúdo* do config dentro do próprio arquivo Compose (usando `file:` ou `content:`), o que é menos comum para configs gerenciados separadamente.
*   Essa separação (criar config via `docker config create`, referenciar como `external: true` no deploy da stack) reforça o desacoplamento entre gerenciamento de configuração e implantação da aplicação.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar `v3.yml`:** Na pasta `ch13/exercises/todo-list`, crie `v3.yml` contendo:
    *   `version: '3.7'`
    *   A seção `services:` com `todo-web` (Listagem 13.3) e também um serviço `todo-db` (baseado em `diamol/postgres`, conectado à `app-net`, sem `ports` publicados, talvez com `restart: unless-stopped`).
    *   A seção `networks:` definindo `app-net` (pode ser criada pela stack ou `external: true` se você criar antes).
    *   A seção `configs:` no nível raiz (Listagem 13.3).
2.  O próximo exercício usará este arquivo para atualizar a stack.

---

📄 Página 265

🔍 Resumo dos principais pontos:

*   Fluxo de deploy: criar configs primeiro, depois deployar stack que os usa.
*   Arquivo `v3.yml` também inclui serviço para BD SQL (Postgres) para compartilhar entre réplicas web.
*   **Exercício Try-it-now:** Atualizar stack `todo` usando `v3.yml`.
    *   Comando: `docker stack deploy -c ./todo-list/v3.yml todo`.
    *   Verificar serviços: `docker stack services todo`.
*   Resultado: Swarm cria serviço `todo-db` e atualiza `todo_todo-web` para usar o config object. Réplica `todo_todo-web` é substituída.
*   Figura 13.7: Mostra saída do deploy `v3`, com criação/atualização de serviços.

🧠 Explicação detalhada e didática:

*   A página descreve o fluxo recomendado: primeiro garantir que os Config Objects necessários existam no cluster (via `docker config create`) e depois implantar a stack (`docker stack deploy`) que os referencia como `external: true`.
*   Menciona que o arquivo completo `v3.yml` também define o serviço do banco de dados PostgreSQL (`todo-db`), necessário agora que a configuração do `todo-web` foi alterada (pelo `todo-list-config`) para usar Postgres em vez de SQLite. Ambos os serviços (`todo-web` e `todo-db`) serão conectados à mesma rede overlay para poderem se comunicar.
*   O exercício instrui a aplicar esta nova configuração à stack `todo` existente (que estava vazia após o `rm` da p.260). O Swarm criará a rede (se não externa), o serviço `todo-db`, e o serviço `todo-web` (que agora usa o `todo-list-config`).
*   A Figura 13.7 mostra a saída esperada, indicando que os recursos estão sendo criados/atualizados.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Config Existe:** (Já criado na p.262)
    ```powershell
    docker config ls # Deve listar 'todo-list-config'
    ```
2.  **Verificar/Criar `v3.yml`:** Certifique-se que `ch13\exercises\todo-list\v3.yml` existe e contém as definições completas para `version`, `services` (`todo-web` e `todo-db`), `networks` (definindo `app-net`), e `configs` (declarando `todo-list-config` como externo).
3.  **Implantar/Atualizar Stack:**
    ```powershell
    docker stack deploy -c ./todo-list/v3.yml todo
    ```
4.  **Verificar Serviços:**
    ```powershell
    docker stack services todo
    # Deve listar 'todo_todo-db' (1/1) e 'todo_todo-web' (1/1, ou o nº de réplicas definido em v3.yml)
    ```

---

📄 Página 266

🔍 Resumo dos principais pontos:

*   App agora configurado para usar Postgres (via config object).
*   **Problema:** Acessar app (`http://localhost:8080`) não funciona. Logs do `todo-web` mostram erros de conexão com BD.
*   **Causa:** Config object (`todo-list-config`) definiu *que* usar Postgres, mas *não forneceu* os detalhes de conexão (string de conexão). App falha ao tentar conectar.
*   **Dados Sensíveis:** Não devem ir em Config Objects (não criptografados).
    *   Ex: Strings de conexão (com user/pass), URLs de produção, chaves API.
    *   Usar defesa em profundidade; mesmo se acesso ao cluster for difícil, não expor segredos em configs.
*   Solução Swarm: **Secrets** para dados confidenciais.
*   **13.3 Managing confidential settings with secrets** (Gerenciando configurações confidenciais com secrets)
    *   Secrets: Recurso Swarm, funciona quase igual a Configs.
    *   Criados de arquivo local -> armazenados no cluster DB.
    *   Referenciados no serviço -> montados como arquivo no container.
    *   **Diferença Chave:** Conteúdo só legível em texto plano *dentro* do contêiner.

🧠 Explicação detalhada e didática:

*   A página revela que, apesar do deploy bem-sucedido da stack `v3`, a aplicação ainda não funciona. A investigação dos logs do `todo-web` mostraria que ele está tentando se conectar ao Postgres (conforme instruído pelo `todo-list-config`), mas está falhando porque não sabe *como* se conectar (falta a string de conexão).
*   Isso leva à introdução dos **Secrets** do Docker Swarm. Eles são projetados especificamente para armazenar e distribuir informações **confidenciais** de forma segura, resolvendo a limitação dos Config Objects (que são inseguros para dados sensíveis).
*   O mecanismo dos Secrets é muito similar ao dos Configs:
    1.  Você cria um Secret no Swarm a partir de um arquivo local (`docker secret create`).
    2.  O Swarm armazena o conteúdo do secret de forma segura.
    3.  Você referencia o Secret na definição do serviço no seu arquivo Compose (`services: ... secrets:`).
    4.  Você declara o Secret como `external: true` na seção raiz `secrets:` do Compose.
    5.  O Swarm monta o conteúdo do Secret como um arquivo dentro das réplicas do serviço.
*   A **grande diferença** é a segurança: Secrets são criptografados em repouso (no DB do Swarm) e em trânsito (entre managers e workers). O conteúdo só é descriptografado e disponibilizado em texto plano dentro do ambiente isolado do contêiner que o solicitou (geralmente em um filesystem temporário em memória, `tmpfs`).

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker secret create` (similar a `docker config create`).
*   Você usará a seção `secrets:` no Compose (exatamente como usou para configs, Listagens 10.3 e 10.4).
*   O próximo exercício criará o secret com a string de conexão do Postgres.

---

📄 Página 267

🔍 Resumo dos principais pontos:

*   Secrets são criptografados em repouso (DB Swarm) e em trânsito (manager -> worker).
*   Descriptografados apenas dentro do container (ex: em `tmpfs`).
*   Usar secret para string de conexão do To-Do app.
*   **Exercício Try-it-now:** Criar e inspecionar um secret.
    *   Comando criar: `docker secret create todo-list-secret ./todo-list/secrets/secrets.json`.
    *   Comando inspecionar: `docker secret inspect --pretty todo-list-secret`.
*   Figura 13.8: Saída do `secret inspect`. Mostra apenas metadados (ID, nome, timestamps), **NÃO** o conteúdo do secret. Confirma segurança.

🧠 Explicação detalhada e didática:

*   A página detalha as garantias de segurança dos **Secrets**: criptografia ponta a ponta desde o armazenamento no cluster até a entrega no nó worker, com descriptografia ocorrendo apenas dentro do ambiente de execução do contêiner (geralmente montado em um local seguro como `/run/secrets/<secret_name>` por padrão, ou no `target` especificado).
*   O exercício demonstra a criação de um secret (`docker secret create`) e a tentativa de inspecioná-lo (`docker secret inspect`).
*   O resultado crucial (Figura 13.8) é que `docker secret inspect` **NÃO revela o conteúdo** do secret, ao contrário do `docker config inspect`. Isso comprova que, uma vez criado no Swarm, o conteúdo do secret não pode ser facilmente lido por administradores ou usuários com acesso ao cluster, aumentando a segurança.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Arquivo Local:** Certifique-se que o arquivo `ch13\exercises\todo-list\secrets\secrets.json` existe e contém a string de conexão JSON correta (como na p.137).
2.  **Criar Secret:**
    ```powershell
    docker secret create todo-list-secret ./todo-list/secrets/secrets.json
    ```
3.  **Listar Secrets:**
    ```powershell
    docker secret ls
    # Deve listar 'todo-list-secret'
    ```
4.  **Inspecionar Secret:**
    ```powershell
    docker secret inspect --pretty todo-list-secret
    ```
    Observe a saída. Você verá informações como ID, Version, CreatedAt, UpdatedAt, Spec (com Name e Labels), mas **NÃO** verá a string de conexão JSON em lugar nenhum.

---

📄 Página 268

🔍 Resumo dos principais pontos:

*   Secret armazenado; agora usar no deploy da app (`v4.yml`).
*   Sintaxe Compose para `secrets` é muito similar a `configs`.
*   **Listagem 13.4:** Trechos de `v4.yml` adicionando uso do secret.
    ```yaml
    services:
      todo-web:
        image: diamol/ch06-todo-list
        ports:
          - 8080:80
        configs: # Ainda usa o config object para settings não-sensíveis
          - source: todo-list-config
            target: /app/config/config.json
        secrets: # Adiciona o uso do secret
          - source: todo-list-secret # Nome do Secret no Swarm
            target: /app/config/secrets.json # Onde montar como arquivo
        #...
    secrets: # Declara o secret como externo
      todo-list-secret:
        external: true
    ```
*   Conteúdo do secret (JSON da string de conexão) será carregado em `/app/config/secrets.json`, que o app também lê como fonte de config.
*   App agora terá config completa (tipo de BD do config object, conexão do secret).
*   **Exercício Try-it-now:** Implantar `v4.yml` para atualizar a stack `todo`.
    *   Comando: `docker stack deploy -c ./todo-list/v4.yml todo`.
    *   Verificar tarefas: `docker stack ps todo`.
*   Deploy atualiza só serviço `todo-web` (adicionou `secrets:`), mas CLI pode listar ambos como "updating".
*   Figura 13.9: Saída do deploy `v4`.

🧠 Explicação detalhada e didática:

*   Agora que o secret `todo-list-secret` (contendo a string de conexão) existe no Swarm, podemos atualizar a definição da aplicação para usá-lo.
*   A Listagem 13.4 mostra as modificações no arquivo Compose (`v4.yml`):
    *   Dentro do serviço `todo-web`, uma seção `secrets:` é adicionada (similar à seção `configs:` existente) para mapear o `source: todo-list-secret` para o `target: /app/config/secrets.json`. A aplicação To-Do está configurada para ler ambos os arquivos (`config.json` e `secrets.json`) e mesclar as configurações.
    *   Na seção raiz `secrets:`, o `todo-list-secret` é declarado como `external: true`, indicando que ele já existe no Swarm.
*   Com essa atualização, a réplica do `todo-web` terá:
    *   O arquivo `config.json` (do Config Object) dizendo para usar `Database:Provider=Postgres`.
    *   O arquivo `secrets.json` (do Secret) fornecendo a string de conexão para o `Host=todo-db`.
*   O exercício instrui a aplicar este `v4.yml` à stack `todo`. Isso causará um rolling update apenas no serviço `todo-web` (pois só ele mudou), substituindo a réplica existente por uma nova que terá o secret montado. A saída do `docker stack deploy` pode ser um pouco enganosa e listar todos os serviços como "updating", mesmo que apenas alguns sejam realmente alterados.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar/Criar `v4.yml`:** Certifique-se que `ch13\exercises\todo-list\v4.yml` existe e contém as seções `services: todo-web: secrets:` e a seção raiz `secrets:` conforme a Listagem 13.4 (além do resto da definição da stack v3).
2.  **Implantar Atualização v4:**
    ```powershell
    docker stack deploy -c ./todo-list/v4.yml todo
    ```
    Observe a saída.
3.  **Verificar Tarefas:** Espere alguns segundos.
    ```powershell
    docker stack ps todo
    ```
    Você deve ver que a réplica de `todo-web` foi substituída (a antiga estará `Shutdown`, a nova `Running`), enquanto a réplica de `todo-db` permaneceu a mesma.

---

📄 Página 269

🔍 Resumo dos principais pontos:

*   Figura 13.9: Saída do deploy v4.
*   App agora funciona corretamente (`http://localhost:8080` ou IP externo no Windows).
*   Figura 13.10: Diagrama da arquitetura final rodando no Swarm.
    *   Tráfego externo -> Nó Swarm -> Rede Ingress -> Réplica `todo-web`.
    *   Réplica `todo-web` -> Rede Overlay -> Réplica `todo-db`.
    *   `todo-web` lê config do Config Object e do Secret montados pelo Swarm.
*   Arquitetura funciona igual em Swarms de qualquer tamanho (cluster abstrai hardware).
*   **Importante sobre Updates de Config/Secret:**
    *   Configs e Secrets no Swarm são **imutáveis** (não podem ser atualizados no local).
    *   Para mudar config:
        1.  Criar *novo* config/secret com conteúdo atualizado e nome *diferente*.
        2.  Atualizar arquivo Compose para referenciar o *novo* nome.
        3.  Fazer `docker stack deploy` com Compose atualizado.
    *   Isso *sempre* causa update do serviço (recriação das réplicas).
*   Comparação com K8s: K8s *permite* atualizar ConfigMaps/Secrets existentes, mas apps podem não pegar a mudança sem reinício de qualquer forma. Swarm é consistente (mudança = update do serviço).

🧠 Explicação detalhada e didática:

*   A página confirma que, com o secret fornecendo a string de conexão, a aplicação To-Do agora funciona corretamente, conectando-se ao banco de dados Postgres.
*   A Figura 13.10 resume a arquitetura completa em execução no Swarm, destacando o fluxo de rede (Ingress para acesso externo, Overlay para comunicação interna) e o fornecimento de configuração via Configs e Secrets gerenciados pelo cluster.
*   Um ponto técnico **muito importante** sobre Configs e Secrets no Docker Swarm é que eles são **imutáveis**. Uma vez que você cria um `docker config create meu-cfg ...` ou `docker secret create minha-senha ...`, o conteúdo associado a esses nomes (`meu-cfg`, `minha-senha`) **não pode ser alterado**.
*   Se você precisar atualizar uma configuração ou um segredo, o processo é:
    1.  Crie um **novo** objeto com um nome **diferente** (ex: `meu-cfg-v2`, `minha-senha-nova`).
    2.  Modifique seu arquivo `docker-compose.yml` para que o serviço passe a referenciar o **novo** nome na seção `configs:` ou `secrets:`.
    3.  Execute `docker stack deploy` com o arquivo `.yml` modificado.
*   Isso garante que as atualizações de configuração sejam explícitas e sempre resultem em um rolling update do serviço, garantindo que as novas réplicas recebam a configuração atualizada. A página contrasta isso com o Kubernetes, que permite alterar objetos existentes, mas o que pode levar a inconsistências se as aplicações não recarregarem a configuração dinamicamente.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Simular Update de Config:**
    1.  Crie um novo arquivo de config local, ex: `config_v2.json`, com um valor diferente.
    2.  Crie um novo config object no Swarm: `docker config create todo-list-config-v2 ./config_v2.json`.
    3.  Edite `v4.yml` (ou crie `v4.1.yml`) e mude a referência no serviço `todo-web` e na seção raiz `configs:` de `todo-list-config` para `todo-list-config-v2`.
    4.  Execute `docker stack deploy -c v4.1.yml todo`. O serviço `todo-web` será atualizado para usar a nova configuração.

---

📄 Página 270

🔍 Resumo dos principais pontos:

*   Atualizar serviços é normal e esperado (features, segurança OS/deps). Mínimo: mensalmente.
*   Próximo: Aplicações stateful no Swarm. Containers são substituídos, precisa de Volumes. Volumes funcionam diferente no Swarm.
*   **13.4 Storing data with volumes in the Swarm** (Armazenando dados com volumes no Swarm)
    *   Revisão Cap. 6: Volumes têm ciclo de vida separado, dados persistem. App atualizado anexa mesmo volume.
    *   Conceito igual em orquestradores (Compose usa `volumes:` no serviço).
    *   **Diferença Swarm:** Onde os dados do volume são *fisicamente* armazenados?
        *   Padrão Swarm: **Volume Local** no nó onde a réplica está rodando.
*   **Problema Volume Local:** Se réplica é substituída e agendada em *outro* nó, ela *não* terá acesso aos dados do volume local do nó original.
*   **Solução 1 (Simples, Limitada): Pinning de Serviço (Constraints).**
    *   Forçar réplicas de um serviço a rodarem *sempre* em um nó (ou conjunto de nós) específico que tem os dados.
    *   Funciona para: dados precisam sobreviver a updates, mas app não precisa de >1 réplica e pode tolerar indisponibilidade se *aquele* nó específico falhar.
    *   Implementação: Adicionar **label** ao nó, usar **constraint** de posicionamento (`deploy: placement: constraints:`) no serviço.

🧠 Explicação detalhada e didática:

*   A página faz a transição para o último tópico de recursos do Swarm no capítulo: **Volumes** para aplicações stateful, como o banco de dados `todo-db`.
*   O conceito de usar volumes para persistência é o mesmo do Cap. 6. A complicação no Swarm (e K8s) surge porque as réplicas podem rodar em **nós diferentes** do cluster.
*   Por padrão, quando você define um volume nomeado em uma stack Swarm (como faremos), o Swarm cria um **Volume Local** no disco do nó onde a réplica é agendada.
*   **Problema:** Se a réplica `todo-db.1` roda no `node-A` e escreve dados no volume local `db-data@node-A`, e depois essa réplica falha e o Swarm a substitui por `todo-db.2` no `node-B`, a nova réplica `todo-db.2` **não terá acesso** aos dados que estavam em `db-data@node-A`. Os dados foram perdidos para a aplicação (embora ainda existam fisicamente no disco do `node-A`).
*   A primeira solução apresentada para lidar com isso (para casos de uso específicos) é usar **Constraints de Posicionamento (Placement Constraints)**. Você pode "etiquetar" (`label`) um nó específico (ex: o que tem um disco rápido ou grande) e depois dizer ao Swarm, na definição do serviço `todo-db`, que ele *só pode* rodar réplicas nos nós que possuem aquela etiqueta. Isso garante que a réplica (e suas substitutas) sempre rodem no mesmo nó, podendo assim reutilizar o volume local daquele nó. A desvantagem é que isso cria um ponto único de falha: se aquele nó específico cair, o serviço ficará indisponível.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker node update --label-add <chave>=<valor> <nome_no>` para adicionar uma label a um nó.
*   Você usará a seção `deploy: placement: constraints:` no arquivo Compose para especificar as labels que o nó deve ter para rodar o serviço.
*   O próximo exercício demonstrará isso (embora em um Swarm de 1 nó, o efeito do constraint seja trivial).

---

📄 Página 271

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Adicionar label `storage=raid` ao nó Swarm.
    *   Comando: `docker node update --label-add storage=raid $(docker node ls -q)`. (Aplica ao nó único no setup atual).
*   Saída é só ID do nó.
*   **Listagem 13.5:** Trecho de `v5.yml` com `constraints` e `volumes` para `todo-db`.
    ```yaml
    services:
      todo-db:
        image: diamol/postgres:11.5
        volumes: # Monta volume nomeado no path de dados do Postgres
          - todo-db-data:/var/lib/postgresql/data
        deploy:
          placement:
            constraints: # Só roda em nós com esta label
              - node.labels.storage == raid
        #...
    volumes: # Declara o volume nomeado a ser criado/usado pela stack
      todo-db-data: {} # {} indica usar driver padrão (local)
    ```
*   **Explicação:**
    *   `deploy: placement: constraints:` força réplica a rodar em nó com `storage=raid`.
    *   `volumes:` (no serviço) monta o volume `todo-db-data` no diretório de dados do Postgres.
    *   `volumes:` (raiz) declara `todo-db-data`. `{}` vazio usa o driver de volume padrão do Swarm (geralmente `local`, significando volume no disco do nó onde a réplica roda).
*   **Exercício Try-it-now:** Verificar volumes antes e depois de deployar `v5.yml`.
    *   `docker volume ls -q` (antes).
    *   `docker stack deploy -c ./todo-list/v5.yml todo` (Linux) ou `v5-windows.yml` (Windows).
    *   `docker volume ls -q` (depois).
*   Figura 13.11: Saída mostrando muitos volumes (incluindo anônimos de execuções anteriores) e o novo volume nomeado `todo_todo-db-data` (prefixado pela stack).

🧠 Explicação detalhada e didática:

*   O primeiro exercício adiciona uma label `storage=raid` ao único nó do Swarm local. Em um cluster real, você adicionaria essa label apenas aos nós que tivessem o armazenamento desejado.
*   A Listagem 13.5 mostra como usar essa label na definição do serviço `todo-db` dentro do arquivo Compose (`v5.yml`):
    *   A seção `deploy: placement: constraints:` contém uma lista de condições. `node.labels.storage == raid` instrui o Swarm a agendar réplicas *apenas* em nós que possuam a label `storage` com o valor `raid`.
    *   A seção `volumes:` dentro do serviço monta um volume chamado `todo-db-data` no diretório `/var/lib/postgresql/data` (local padrão de dados do Postgres).
    *   A seção `volumes:` no nível raiz do YAML declara o volume `todo-db-data`. Deixá-lo vazio (`{}`) significa que o Swarm deve usar o driver de volume padrão (que é `local` na maioria das instalações), criando o volume no disco local do nó selecionado pela constraint.
*   O segundo exercício implanta essa stack `v5`. Antes e depois, `docker volume ls` é usado para observar os volumes. O importante é notar que, após o deploy, um novo volume nomeado `todo_todo-db-data` aparecerá (o Swarm prefixa volumes definidos na stack com o nome da stack). Volumes anônimos criados por execuções anteriores (sem `-v` ou volume nomeado) também podem estar presentes.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Adicionar Label ao Nó:**
    ```powershell
    docker node update --label-add storage=raid $(docker node ls -q)
    # Verificar (opcional): docker node inspect $(docker node ls -q) -f '{{.Spec.Labels}}'
    ```
2.  **Verificar Volumes Atuais:**
    ```powershell
    docker volume ls -q
    ```
3.  **Verificar/Criar `v5.yml` ou `v5-windows.yml`:** Certifique-se que o arquivo existe em `ch13/exercises/todo-list` e contém a Listagem 13.5 (mais o serviço `todo-web` de antes, `version`, `networks`, `configs`, `secrets`). O `v5-windows.yml` provavelmente ajusta o path no `volumes:` do serviço `todo-db` para `C:\...` se necessário para a imagem Postgres Windows.
4.  **Implantar Stack v5:** (Use o arquivo apropriado)
    ```powershell
    docker stack deploy -c ./todo-list/v5.yml todo
    # Ou: docker stack deploy -c ./todo-list/v5-windows.yml todo
    ```
5.  **Verificar Volumes Novamente:**
    ```powershell
    docker volume ls
    # Procure pelo volume chamado 'todo_todo-db-data'.
    ```

---

📄 Página 272

🔍 Resumo dos principais pontos:

*   Figura 13.11: Confirma criação do volume nomeado `todo_todo-db-data`.
*   Diferença: Volumes anônimos (`VOLUME` no Dockerfile ou `-v` sem nome) são removidos com a stack (`stack rm`). Volumes nomeados no Compose (`volumes:` raiz) *não* são removidos com a stack, garantindo persistência.
*   Deploy com constraint garante:
    *   Réplica do BD roda no nó com label `storage=raid`.
    *   Nó cria volume local `todo_todo-db-data`.
    *   Se réplica falhar/for atualizada, substituta roda no *mesmo* nó (devido à constraint) e re-anexa o *mesmo* volume local.
*   Dados persistem entre containers, desde que o nó esteja disponível.
*   **Exercício Try-it-now:** Atualizar versão do Postgres para testar persistência.
    *   Arquivo `v6.yml` (ou `v6-windows.yml`) é idêntico ao v5, exceto por usar imagem Postgres mais nova.
    *   1. Deploy v6: `docker stack deploy -c ./todo-list/v6.yml todo` (ou `v6-windows.yml`).
    *   2. Verificar tarefas: `docker stack ps todo`.
    *   3. Verificar volumes: `docker volume ls -q`.

🧠 Explicação detalhada e didática:

*   A página esclarece uma diferença importante no ciclo de vida: volumes **nomeados** definidos na seção `volumes:` raiz do Compose para uma stack **não são removidos** quando a stack é removida (`docker stack rm`). Isso é diferente de volumes *anônimos* ou volumes criados implicitamente pela stack (sem definição na raiz), que *são* removidos com a stack. Usar volumes nomeados declarados na raiz é, portanto, essencial para dados que precisam sobreviver além da própria stack.
*   Reafirma que a combinação da **constraint de posicionamento** (forçando o serviço a rodar em um nó específico) e o **volume nomeado local** nesse nó garante que os dados do banco de dados persistam através de reinícios ou atualizações da réplica do banco de dados, *contanto que aquele nó específico esteja funcional*.
*   O exercício demonstra isso na prática, simulando uma atualização do servidor PostgreSQL. Um novo arquivo Compose (`v6.yml`) é idêntico ao `v5.yml`, exceto pela tag da imagem do Postgres. Ao aplicar `v6.yml` com `docker stack deploy`:
    *   O Swarm detecta a mudança na imagem do serviço `todo-db`.
    *   Ele executa um rolling update para `todo-db`.
    *   A nova réplica é agendada no *mesmo* nó (devido à constraint).
    *   A nova réplica anexa o *mesmo* volume nomeado `todo_todo-db-data` que a réplica antiga usava.
    *   Nenhum volume novo é criado, e os dados são preservados.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **(Opcional) Adicionar Dados:** Acesse `http://localhost:8080` (ou IP externo) e adicione/verifique itens na lista To-Do (para ter dados a preservar).
2.  **Verificar/Criar `v6.yml`:** Certifique-se que `v6.yml` (ou `v6-windows.yml`) existe e apenas atualiza a tag da imagem `diamol/postgres`.
3.  **Implantar Atualização v6:**
    ```powershell
    docker stack deploy -c ./todo-list/v6.yml todo
    # Ou: docker stack deploy -c ./todo-list/v6-windows.yml todo
    ```
4.  **Verificar Tarefas:** Espere update completar.
    ```powershell
    docker stack ps todo
    # Veja réplica antiga de todo-db como Shutdown, nova como Running.
    ```
5.  **Verificar Volumes:**
    ```powershell
    docker volume ls -q
    # O número de volumes não deve ter aumentado. O volume 'todo_todo-db-data' ainda existe.
    ```
6.  **Testar Persistência:** Acesse `http://localhost:8080` novamente. Os itens que você tinha devem continuar lá, mesmo após a atualização do BD.

---

📄 Página 273

🔍 Resumo dos principais pontos:

*   Figura 13.12: Mostra saída do `stack ps` e `volume ls` após update v6, confirmando substituição da réplica e reuso do volume.
*   Exemplo simples; storage fica mais complexo com requisitos diferentes.
*   **Problema do Volume Local (Revisitado):** Dados locais não são replicados. Se nó com constraint falhar, app stateful fica indisponível.
*   Solução para HA real de dados: **Storage Distribuído**.
    *   Docker tem sistema de plugins para **drivers de volume**.
    *   Permite Swarm usar storage externo (cloud - EBS, Azure Disk; datacenter - SAN, Ceph, etc.) como backend para volumes.
    *   Configuração depende da infra; mas consumo no Compose é similar (especifica driver e opções no `volumes:` raiz).

🧠 Explicação detalhada e didática:

*   A página confirma que a atualização do Postgres funcionou e os dados persistiram graças ao volume nomeado e à constraint.
*   No entanto, volta a apontar a **limitação fundamental** da abordagem com volume local + constraint: ela não fornece verdadeira alta disponibilidade (HA) para os dados. Se o único nó permitido pela constraint falhar, o serviço stateful (o banco de dados) não poderá ser iniciado em outro lugar, e a aplicação ficará degradada ou indisponível.
*   A solução para HA de dados em um cluster é usar **armazenamento distribuído** ou **armazenamento compartilhado** que seja acessível por *todos* (ou múltiplos) nós do cluster. O Docker Swarm suporta isso através de **drivers de volume de terceiros**.
*   **Drivers de Volume:** São plugins que ensinam o Docker a interagir com sistemas de armazenamento externos. Existem drivers para:
    *   Storage em bloco da nuvem (AWS EBS, Azure Disk, Google Persistent Disk): O driver pode montar/desmontar o disco no nó correto.
    *   Sistemas de arquivos de rede (NFS, CIFS/SMB).
    *   Sistemas de arquivos distribuídos (GlusterFS, Ceph).
    *   Soluções de Storage Definido por Software (SDS) específicas para contêineres.
*   Ao usar um driver de volume externo, você o especifica na definição do volume no arquivo Compose (`volumes: meu-volume-distribuido: driver: meu-driver-externo`). O Swarm então usa esse driver para provisionar e anexar o volume ao nó apropriado, e o volume pode potencialmente ser re-anexado a um nó diferente se a réplica se mover, mantendo o acesso aos dados. A configuração inicial do driver no cluster depende do provedor do driver e da infraestrutura.

💻 Exemplos práticos (para meu ambiente Windows):

*   Configurar drivers de volume externos está fora do escopo do Docker Desktop básico. Em produção, você seguiria a documentação do driver escolhido (ex: driver CSI para Kubernetes/Swarm com Azure Disk, ou driver NFS).
*   A definição no Compose seria algo como:
    ```yaml
    services:
      todo-db:
        # ...
        volumes:
          - todo-db-shared:/var/lib/postgresql/data
    volumes:
      todo-db-shared:
        driver: azure_file # Exemplo hipotético para Azure Files
        driver_opts: # Opções específicas do driver
          share_name: meu-share-postgres
          storage_account_name: minha-conta
    ```

---

📄 Página 274

🔍 Resumo dos principais pontos:

*   **13.5 Understanding how the cluster manages stacks** (Entendendo como o cluster gerencia stacks)
*   Stack = grupo lógico de recursos gerenciados pelo cluster.
*   Recursos (volumes, secrets, configs, networks, services) têm comportamentos diferentes no gerenciamento pelo Swarm.
*   Diagrama (Figura 13.13): Resume gerenciamento de cada tipo de recurso pela stack.
*   **Resumo do Gerenciamento por Tipo:**
    1.  **Volumes:**
        *   Anônimos/Implícitos: Criados/Removidos com a stack.
        *   Nomeados (declarados na raiz `volumes:`): Criados pela stack (se não `external`), **NÃO** removidos com `stack rm`. Persistem além da stack.
    2.  **Secrets & Configs:**
        *   Criados externamente (`docker secret/config create`).
        *   Declarados como `external: true` na stack.
        *   NÃO são criados/removidos pela stack. Ciclo de vida independente. Imutáveis.
    3.  **Networks:**
        *   Podem ser gerenciadas externamente (criadas antes, `external: true`) OU pela stack (criadas/removidas com `stack deploy/rm`).
        *   Stack sempre garante que serviços estejam em uma rede (cria `_default` se nenhuma for especificada).
    4.  **Services:**
        *   Criados/Atualizados/Removidos por `stack deploy/rm`.
        *   Swarm monitora continuamente, mantém réplicas desejadas, substitui falhas.

🧠 Explicação detalhada e didática:

*   Esta seção final resume como o Docker Swarm gerencia os diferentes tipos de recursos (volumes, secrets, configs, redes, serviços) que compõem uma **stack**. O comportamento não é uniforme para todos os tipos.
*   A Figura 13.13 e a lista subsequente esclarecem o ciclo de vida de cada um em relação à stack:
    *   **Volumes:** O ponto chave é a diferença entre volumes anônimos/implícitos (que morrem com a stack) e volumes **nomeados e declarados na seção `volumes:` raiz** (que **sobrevivem** à remoção da stack). Para persistência real, use volumes nomeados declarados.
    *   **Secrets e Configs:** São tipicamente gerenciados **fora** do ciclo de vida da stack. Você os cria antes (`docker secret/config create`), e a stack apenas os referencia como `external: true`. Eles não são afetados por `stack rm`. Lembre-se que são imutáveis.
    *   **Networks:** Podem ser gerenciadas pela stack (criadas no `deploy`, removidas no `rm`) ou externamente (criadas antes, `external: true`). Uma rede padrão é criada se nenhuma for especificada.
    *   **Services:** São o principal recurso gerenciado pela stack. `stack deploy` os cria ou atualiza, `stack rm` os remove. O Swarm ativamente gerencia suas réplicas enquanto a stack existe.
*   Compreender essas diferenças é importante para evitar surpresas (como perder dados de um volume anônimo ao remover uma stack de teste).

💻 Exemplos práticos (para meu ambiente Windows):

*   Revise os arquivos Compose (`.yml`) usados no capítulo. Identifique:
    *   Volumes nomeados declarados na raiz (ex: `todo-db-data:` na p.271).
    *   Secrets/Configs declarados na raiz com `external: true` (ex: `todo-list-config: external: true` na p.264).
    *   Redes declaradas na raiz (podem ter `external: true` ou não).
*   Experimente remover uma stack (`docker stack rm todo`) e depois verificar quais recursos ainda existem (`docker volume ls`, `docker secret ls`, `docker config ls`, `docker network ls`). Os volumes nomeados e os secrets/configs externos devem permanecer.

---

📄 Página 275

🔍 Resumo dos principais pontos:

*   Stack agrupa componentes, mas **NÃO define dependências** entre serviços.
*   `stack deploy` inicia réplicas o mais rápido possível, sem ordem garantida entre serviços.
*   Não se pode forçar Swarm a iniciar Serviço A completamente antes do Serviço B.
*   Precisa assumir ordem aleatória; usar **health checks** e **dependency checks** nas imagens.
*   Checks permitem "fail fast", cluster repara -> app auto-reparável (self-healing).
*   **13.6 Lab** (Laboratório)
    *   Objetivo: Ganhar experiência com deploy de stack Swarm e arquivos Compose.
*   **Tarefa:** Escrever arquivo Compose (`.yml`) único para deploy **produção** do app Image Gallery (Cap. 9).
    *   **Requisitos:**
        1.  Serviço `access-log` (imagem `diamol/ch09-access-log`), 3 réplicas, interno (sem porta publicada).
        2.  Serviço `iotd` (imagem `diamol/ch09-image-of-the-day`), 5 réplicas, porta 8088 publicada.
        3.  Serviço `image-gallery` (imagem `diamol/ch09-image-gallery`), 2 réplicas, porta 80 publicada.
        4.  Todos os componentes com limites (`deploy: resources: limits:`) sensatos de CPU/memória. (Pode exigir tentativa e erro para achar valores).
        5.  App deve funcionar após `docker stack deploy`.
    *   Sem volumes/configs/secrets complexos neste app.
*   Solução no GitHub.

🧠 Explicação detalhada e didática:

*   Um ponto crucial sobre `docker stack deploy` (e deploy em orquestradores em geral) é reiterado: ele **não garante a ordem de inicialização** entre serviços diferentes. O orquestrador tenta iniciar tudo o mais rápido possível. Portanto, você **não pode** confiar que o serviço `todo-db` estará totalmente pronto quando o `todo-web` iniciar.
*   É por isso que as técnicas do Capítulo 8 (dependency checks na inicialização, health checks para monitoramento contínuo) são **essenciais** em ambientes orquestrados. Eles permitem que os contêineres lidem com a inicialização fora de ordem e sinalizem seu estado real para o orquestrador, habilitando a auto-recuperação.
*   O **Lab do Capítulo 13** é um exercício prático para consolidar o uso de `docker stack deploy` com um arquivo Compose que define múltiplos serviços, réplicas e limites de recursos.
*   A tarefa é criar um único arquivo `.yml` para a aplicação Image Gallery, especificando:
    *   Os três serviços (`access-log`, `iotd`, `image-gallery`) com suas respectivas imagens do Cap. 9.
    *   O número de `replicas:` desejado para cada um dentro da seção `deploy:`.
    *   As `ports:` publicadas para `iotd` (8088) e `image-gallery` (80). `access-log` não tem portas publicadas.
    *   A seção `deploy: resources: limits:` para *cada* serviço, com valores que você estime serem razoáveis (ex: `cpus: "0.25"`, `memory: 128M`). Encontrar valores ideais requereria monitoramento real (Cap. 9).
    *   Garantir que todos os serviços estejam conectados a uma rede overlay (definida na seção `networks:` raiz, ou deixar o Swarm criar uma `_default`).
*   O objetivo final é ter um arquivo `.yml` que, ao ser implantado com `docker stack deploy -c <seu_arquivo.yml> image-gallery-stack`, resulte na aplicação funcionando corretamente.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 13:**
    1.  Crie um arquivo `image-gallery-stack.yml` (ou similar) na pasta `ch13\lab`.
    2.  Adicione `version: '3.7'`.
    3.  Adicione seção `networks:` definindo uma rede overlay, ex: `app-net: driver: overlay`.
    4.  Adicione seção `services:`.
    5.  Para cada serviço (`access-log`, `iotd`, `image-gallery`):
        *   Defina `image:` com a imagem correta (`diamol/ch09-...`).
        *   Adicione `networks: [app-net]`.
        *   Adicione `deploy:`:
            *   Dentro de `deploy:`, adicione `replicas:` com o número pedido (3, 5, 2).
            *   Dentro de `deploy:`, adicione `resources: limits:` com `cpus:` e `memory:` (ex: "0.1", "64M").
        *   Para `iotd` e `image-gallery`, adicione a seção `ports:` com os mapeamentos pedidos (`["8088:80"]`, `["80:80"]`).
    6.  **Implantar:** `docker stack deploy -c image-gallery-stack.yml ig-stack`.
    7.  **Verificar:** `docker stack services ig-stack`, `docker stack ps ig-stack`.
    8.  **Testar:** Acesse `http://localhost` (porta 80). Deve mostrar a aplicação. (Lembre-se da limitação do Windows para `localhost` se estiver nesse modo).

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch13/lab/README.md

---

📄 Página 276

🔍 Resumo dos principais pontos:

*   **Capítulo 14: Automating releases with upgrades and rollbacks** (Automatizando lançamentos com atualizações e reversões)
*   Atualizações de contêineres devem ter **zero downtime**, gerenciadas pelo orquestrador.
*   Pré-requisitos: capacidade extra no cluster, health checks nas imagens.
*   Processo já visto com `docker stack deploy` no Cap. 13.
*   Foco Cap. 14: Configuração detalhada do processo de update/rollback no Swarm.
*   Importância: Entender e configurar rollouts evita problemas em produção. Foco em Swarm, mas conceitos similares em K8s.
*   Objetivo: Deployments frequentes e confiáveis (funcionam ou revertem automaticamente).
*   **14.1 The application upgrade process with Docker** (O processo de atualização de aplicação com Docker)
    *   Imagens Docker parecem simples, mas há múltiplas "cadências" de atualização a considerar.
    *   Diagrama (Figura 14.1): Mostra dependências de uma imagem .NET Core/Linux.
        *   App -> Deps Nuget -> Runtime .NET -> SDK .NET -> Imagem Base OS -> Kernel OS.
    *   Necessário atualizar imagem não só por mudanças no app, mas também por updates em qualquer camada abaixo (segurança OS, patches runtime, etc.).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 14, focado em aprofundar o processo de **atualização (upgrade)** e **reversão (rollback)** de aplicações rodando como stacks/serviços no Docker Swarm.
*   O objetivo ideal é ter atualizações com **zero downtime**, onde o orquestrador substitui as réplicas antigas pelas novas de forma gradual e segura, usando os health checks para validar o sucesso.
*   Embora `docker stack deploy` já faça isso com configurações padrão, este capítulo mostrará como **customizar** o processo de rollout (quantas réplicas atualizar por vez, quanto tempo esperar, o que fazer em caso de falha, etc.) para torná-lo mais rápido e/ou mais seguro, dependendo das necessidades da aplicação. Entender essas configurações é crucial para evitar problemas em produção.
*   A seção 14.1 começa contextualizando *por que* as atualizações são frequentes e necessárias. Não é apenas quando o código da sua aplicação muda. Uma imagem Docker é construída sobre várias camadas de dependências (Figura 14.1): o código da aplicação depende de bibliotecas, que rodam sobre um runtime (ex: .NET, Java), que por sua vez roda sobre uma imagem base de sistema operacional. Qualquer uma dessas camadas pode receber atualizações (especialmente de segurança). Portanto, é uma boa prática reconstruir e reimplantar suas imagens regularmente (ex: mensalmente, ou até diariamente via CI) para incorporar essas atualizações das camadas base, mesmo que seu próprio código não tenha mudado.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará a seção `deploy: update_config:` e `deploy: rollback_config:` dentro das definições de serviço no seu arquivo Compose para controlar o comportamento dos updates e rollbacks no Swarm.

💡 Analogias ou observações úteis (se houver):

*   Cadências de Update: Pense no seu celular. Você atualiza seus apps (sua aplicação). Mas, de vez em quando, o sistema operacional (Android/iOS - imagem base OS/runtime) também precisa ser atualizado. Ignorar atualizações do SO pode deixar seu celular vulnerável. O mesmo vale para imagens Docker.

---

📄 Página 277

🔍 Resumo dos principais pontos:

*   Ideal: Pipeline CI/CD rodando diariamente para pegar updates das bases.
*   Releases frequentes e automatizados aumentam confiança e agilidade (vs. releases manuais raras e arriscadas).
*   Confiança vem de releases bem-sucedidos, que dependem de **health checks** (sem eles, não há auto-recuperação ou updates/rollbacks seguros).
*   Exemplo: App Random Number (Cap. 8) com overrides Compose (Cap. 10).
    *   `docker-compose.yml` (core), `prod.yml` (override prod), `vX.yml` (overrides de versão).
*   **Problema:** `docker stack deploy` *não* suporta múltiplos arquivos `-f` como `docker-compose`.
*   **Solução:** Usar `docker-compose config` para *primeiro* mesclar os arquivos em um único YAML final, e *depois* usar `docker stack deploy -c` com esse arquivo mesclado.
*   **Exercício Try-it-now:** Deployar versão inicial (build 1) do Random Number.
    *   Garantir modo Swarm ativo.
    *   1. Mesclar arquivos core + prod: `docker-compose -f ./numbers/core.yml -f ./numbers/prod.yml config > stack.yml`.
    *   2. Implantar stack com arquivo mesclado: `docker stack deploy -c stack.yml numbers`.
    *   3. Verificar serviços: `docker stack services numbers`.

🧠 Explicação detalhada e didática:

*   A página argumenta a favor de **lançamentos frequentes e automatizados**, habilitados por um pipeline CI/CD robusto e pela confiança gerada por health checks que permitem rollouts/rollbacks seguros. O objetivo é tornar o release um evento "chato" e rotineiro, não um trauma.
*   Introduz o cenário prático do capítulo: usar a aplicação Random Number e gerenciá-la com uma estrutura de arquivos Compose com overrides (um core, um para produção, e outros para versões específicas).
*   Aponta uma **limitação importante do `docker stack deploy`**: ele **não** aceita múltiplos arquivos `-f` para mesclagem como o `docker-compose` faz. Você só pode fornecer *um* arquivo `-c`.
*   A solução/workaround é usar o `docker-compose config` (que *aceita* múltiplos `-f`) para pré-processar e mesclar todos os arquivos necessários em um único arquivo YAML final (redirecionando a saída para `stack.yml`, por exemplo), e então usar `docker stack deploy -c stack.yml ...`.
*   O exercício demonstra esse fluxo de duas etapas: primeiro mesclar o `core.yml` e `prod.yml` em `stack.yml`, e depois implantar `stack.yml` no Swarm.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Swarm:** `docker info | Select-String "Swarm"` (active).
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch14\exercises
    ```
3.  **Mesclar Arquivos:** (Verifique se os arquivos se chamam `docker-compose.yml` e `prod.yml` na pasta `numbers`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml config > stack.yml
    # Verifique o conteúdo de stack.yml (opcional): Get-Content stack.yml
    ```
4.  **Implantar Stack:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
5.  **Verificar Serviços:**
    ```powershell
    docker stack services numbers
    # Deve mostrar numbers_numbers-api e numbers_numbers-web
    ```

---

📄 Página 278

🔍 Resumo dos principais pontos:

*   Figura 14.2: Mostra saída da mesclagem e deploy. Stack `numbers` criada com serviços `api` (replicated) e `web` (global).
*   **Modo Global vs. Replicated:**
    *   `prod.yml` configura `numbers-web` como `deploy: mode: global`.
    *   Serviço Global: Roda **uma** réplica em **cada** nó do Swarm. Útil para agentes de monitoramento, ou frontends leves onde 1 por nó é suficiente.
    *   Serviço Replicated (padrão): Roda número `replicas:` desejado, distribuído entre nós disponíveis.
*   `prod.yml` também configura `numbers-web` com `ports: mode: host`.
*   **Modo Host vs. Ingress:**
    *   `mode: host` (em `ports`): bypassa rede Ingress. Container publica porta *diretamente* no IP do nó onde está rodando.
    *   Só pode ter *um* container por nó usando a mesma porta host nesse modo. Funciona bem com `mode: global` (garante 1 réplica por nó).
    *   Pode ter melhor performance (sem hop extra do Ingress), mas menos flexibilidade (precisa saber IP do nó específico, ou ter LB externo apontando para todos os nós).
*   Deploy atual usa imagens v1 (sem health checks). App API tem bug (falha após 3 chamadas).
*   Pode-se acessar `http://localhost`. Múltiplas chamadas usam LB do Swarm entre as 6 réplicas da API. Eventualmente, todas quebrarão e app para de funcionar (sem auto-reparo).

🧠 Explicação detalhada e didática:

*   A página analisa o resultado do deploy inicial, notando que o serviço `numbers-web` foi implantado de forma diferente dos exemplos anteriores, usando `mode: global` e `ports: mode: host`.
*   **Modo de Serviço (`deploy: mode:`):**
    *   `replicated` (Padrão): Você especifica quantas réplicas quer (`replicas: N`), e o Swarm as distribui entre os nós disponíveis. É o modo mais comum.
    *   `global`: O Swarm automaticamente roda exatamente uma réplica em *cada* nó ativo do cluster. Se um nó é adicionado, ele ganha uma réplica. Se um nó sai, sua réplica é removida. Útil para serviços que precisam estar presentes em todos os nós (ex: agentes de log ou monitoramento).
*   **Modo de Publicação de Porta (`ports: mode:`):**
    *   `ingress` (Padrão): Usa a rede Ingress do Swarm. Todos os nós escutam na porta publicada, e o Swarm roteia para as réplicas. Permite múltiplas réplicas ouvindo na mesma porta externa.
    *   `host`: O contêiner se liga diretamente à porta no IP do nó onde ele está rodando. A rede Ingress *não* é usada. Só pode haver um contêiner por nó usando essa porta específica. Frequentemente usado em conjunto com `deploy: mode: global` para garantir que não haja conflitos de porta. Pode oferecer performance ligeiramente melhor em alguns casos, mas perde o balanceamento de carga e a abstração do Ingress (você precisa saber o IP do nó para acessá-lo, ou usar um balanceador externo).
*   O deploy atual (v1 sem health checks) é frágil. Embora o Swarm balanceie a carga inicial entre as 6 réplicas da API, o bug eventualmente travará todas elas, e como não há health check, o Swarm não fará nada para corrigi-las.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Detalhes do Serviço Web:**
    ```powershell
    docker service inspect --pretty numbers_numbers-web
    # Procure por "Mode: global" e "PublishMode: host"
    ```
2.  **Testar App (Modo Linux):**
    *   Acesse `http://localhost` (porta 80, modo host).
    *   Clique "Get Random Number" múltiplas vezes (mais de 18 vezes para garantir quebrar todas as 6 réplicas da API).
    *   Eventualmente, o botão deve parar de retornar números e mostrar erro.
    *   Verifique `docker service ps numbers_numbers-api`. Todas as 6 réplicas devem estar `Running`, mesmo que a aplicação interna tenha falhado.

---

📄 Página 279

🔍 Resumo dos principais pontos:

*   Figura 14.3: Mostra a saída do `stack ps` após o deploy inicial v1 (API x6, Web x1 - global).
*   Falta de health checks impede auto-reparo e updates seguros.
*   **Deploy v2:** Usa imagens v2 (com health checks).
    *   `v2.yml`: Override que muda `image:` para tag `:v2`.
    *   `prod-healthcheck.yml`: Override que adiciona seção `healthcheck:` com parâmetros (`interval`, `timeout`, etc.) aos serviços.
*   Agora, com health checks, Swarm pode detectar API quebrada e substituí-la.
*   **Exercício Try-it-now:** Mesclar arquivos para v2+healthcheck e atualizar stack.
    *   1. Mesclar: `docker-compose -f ...core -f ...prod -f ...prod-healthcheck -f ...v2 config > stack.yml`. Adiciona `--log-level ERROR` para suprimir warnings de Compose sobre campos não usados (como `depends_on`).
    *   2. Atualizar: `docker stack deploy -c stack.yml numbers`.
    *   3. Verificar tarefas: `docker stack ps numbers`.

🧠 Explicação detalhada e didática:

*   A página enfatiza que o deploy `v1` é inseguro devido à falta de health checks. A solução é implantar a `v2` das imagens, que os possuem.
*   O processo de atualização para `v2` envolve mesclar *mais* arquivos override usando `docker-compose config`:
    *   `docker-compose.yml`: Base.
    *   `prod.yml`: Configs de produção (réplicas API, modo global/host web).
    *   `prod-healthcheck.yml`: Adiciona/configura os parâmetros de `healthcheck` para os serviços (intervalo, retries, etc.).
    *   `v2.yml`: Muda a `image:` para usar a tag `:v2`.
*   A flag `--log-level ERROR` é adicionada ao `docker-compose config` para evitar mensagens de aviso sobre chaves YAML (como `depends_on`) que podem existir nos arquivos mas são ignoradas pelo `stack deploy`.
*   O `stack.yml` resultante é então usado para atualizar a stack `numbers`. A expectativa é que o Swarm execute um rolling update para ambos os serviços, substituindo as réplicas `v1` por `v2`.
*   Com a `v2` rodando (e os health checks definidos em `prod-healthcheck.yml` ativos), o Swarm agora *poderá* detectar quando as réplicas da API falharem e as substituirá automaticamente.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Mesclar Arquivos para v2:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/v2.yml --log-level ERROR config > stack.yml
    ```
2.  **Atualizar Stack para v2:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
3.  **Verificar Tarefas:** Espere o update completar.
    ```powershell
    docker stack ps numbers
    ```
    Você verá as réplicas v1 como `Shutdown` e as novas réplicas v2 como `Running`.

---

📄 Página 280

🔍 Resumo dos principais pontos:

*   Update v2 substitui réplicas v1.
*   Análise do Rollout Padrão Swarm:
    *   Padrão: Para réplicas (`replicated`), para antigas (`stop`) *antes* de iniciar novas (`start`).
    *   Justificativa `stop-first`: Garante que nº total de réplicas nunca exceda o desejado. Necessário para `mode: host` (evita conflito de porta).
    *   Desvantagem `stop-first`: Reduz capacidade temporariamente durante update.
*   Figura 14.4: Mostra saída do `stack ps` durante o update v2. Destaca que réplicas v1 são paradas (`Shutdown`) antes das v2 iniciarem (`Starting`/`Running`).
*   **Configuração Padrão de Update Swarm:**
    *   Cautelosa: 1 réplica por vez (`parallelism: 1`).
    *   Verifica se container inicia corretamente.
    *   *Não* espera health check por padrão (`monitor: 0s`).
    *   Se falha (container não inicia), pausa rollout (`failure_action: pause`).
*   Problemas com padrão: `stop-first` arriscado (e se nova não funcionar?). Pausar em falha pode deixar sistema inconsistente.
*   Melhor: Configurar rollout com opções mais sensatas.
*   **14.2 Configuring production rollouts with Compose** (Configurando rollouts de produção com Compose)
    *   App v2 agora é auto-reparável (health checks -> Swarm substitui API falha).

🧠 Explicação detalhada e didática:

*   A página analisa o comportamento do rolling update que acabou de ocorrer para a v2. Destaca a estratégia padrão do Swarm: **`stop-first`**. Para cada réplica a ser atualizada, o Swarm primeiro para a réplica antiga e depois inicia a nova. Embora isso garanta que o número de réplicas nunca exceda o configurado e seja necessário para o modo `host`, reduz a capacidade da aplicação durante a atualização.
*   Além disso, a configuração padrão completa do update é descrita como "cautelosa", mas talvez não ideal:
    *   `parallelism: 1`: Atualiza apenas uma réplica de cada vez. Lento para serviços com muitas réplicas.
    *   `monitor: 0s`: Após iniciar a nova réplica, o Swarm *não* espera para ver se ela fica saudável (via health check) antes de prosseguir para a próxima. Ele só verifica se o contêiner iniciou.
    *   `failure_action: pause`: Se uma nova réplica falhar ao iniciar, o Swarm simplesmente *para* o processo de atualização, deixando o serviço em um estado misto (algumas réplicas novas, outras antigas).
*   A página critica essas padrões, especialmente `stop-first` e `pause`, sugerindo que há configurações melhores para a maioria dos cenários.
*   A seção 14.2 introduz como customizar essas configurações usando a seção `deploy: update_config:` no arquivo Compose.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Testar Auto-Reparo (v2):**
    *   Com a stack `v2` rodando, acesse `http://localhost` e cause o bug na API (muitos cliques).
    *   Aguarde (depende do `interval` e `retries` do healthcheck definido em `prod-healthcheck.yml`).
    *   Use `docker service ps numbers_numbers-api`. Você deverá ver réplicas antigas como `Failed` ou `Shutdown` e novas réplicas `Running` que o Swarm iniciou automaticamente.
    *   Tente acessar o app novamente; ele deve voltar a funcionar.
*   A próxima listagem mostrará como definir `update_config`.

---

📄 Página 281

🔍 Resumo dos principais pontos:

*   Seção `deploy: update_config:` no Compose permite customizar rollout.
*   **Listagem 14.2:** Trecho de `prod-update-config.yml` com `update_config` customizada para `numbers-api`.
    ```yaml
    numbers-api:
      deploy:
        update_config:
          parallelism: 3 # Atualizar 3 réplicas em paralelo
          monitor: 60s # Esperar 60s após update de lote, monitorando health check
          failure_action: rollback # Se falha no monitoramento, reverter AUTOMATICAMENTE
          order: start-first # Iniciar nova réplica ANTES de parar a antiga
    ```
*   **Explicação das Propriedades `update_config`:**
    *   `parallelism`: Nº réplicas a atualizar simultaneamente (default 1). Aumenta velocidade.
    *   `monitor`: Tempo (ex: `60s`) para Swarm esperar *após* um lote de réplicas ser atualizado, verificando se elas ficam `healthy` (se houver health check). Default 0s (sem espera/verificação de saúde). Aumenta segurança.
    *   `failure_action`: O que fazer se update falhar (container não inicia ou falha no `monitor`). `pause` (default) ou `rollback` (reverte para versão anterior). `rollback` é mais seguro.
    *   `order`: Ordem de substituição. `stop-first` (default) ou `start-first`. `start-first` inicia a nova réplica, espera ficar `healthy` (se `monitor` > 0), *depois* para a antiga. Mais seguro (sem redução de capacidade), mas requer capacidade extra temporária no cluster (para rodar N + `parallelism` réplicas).

🧠 Explicação detalhada e didática:

*   Esta página apresenta a seção `deploy: update_config:` do Compose, que permite ajustar finamente como o Swarm realiza os rolling updates. A Listagem 14.2 mostra uma configuração customizada considerada uma boa prática geral:
    *   `parallelism: 3`: Acelera o rollout atualizando 3 réplicas de cada vez (em vez de 1). O valor ideal depende do número total de réplicas (ex: 20-30% do total).
    *   `monitor: 60s`: Após atualizar um lote de 3 réplicas, o Swarm espera 60 segundos. Durante esse tempo, ele verifica os health checks das novas réplicas. Somente se todas ficarem `healthy` dentro desses 60s, ele prossegue para o próximo lote de 3. Isso garante que a nova versão está funcionando antes de continuar.
    *   `failure_action: rollback`: Se *qualquer* réplica nova falhar ao iniciar ou não ficar `healthy` dentro do período de `monitor`, o Swarm **automaticamente reverte** toda a atualização, voltando para a versão anterior do serviço. Isso é muito mais seguro do que o `pause` padrão.
    *   `order: start-first`: O Swarm inicia a(s) nova(s) réplica(s), espera(m) ficar(em) `healthy` (se `monitor` > 0), e só então para a(s) réplica(s) antiga(s) correspondente(s). Isso evita a queda temporária de capacidade do `stop-first`, mas exige que o cluster tenha recursos livres para acomodar brevemente mais réplicas do que o normal (`replicas` + `parallelism`).

💻 Exemplos práticos (para meu ambiente Windows):

*   Você precisará de um arquivo `prod-update-config.yml` contendo a Listagem 14.2 (e talvez configs similares para `numbers-web`).
*   No próximo deploy (para v3), você incluirá este arquivo na mesclagem com `docker-compose config`, e o rollout subsequente usará essas novas configurações.

💡 Analogias ou observações úteis (se houver):

*   `update_config`: Ajustar a velocidade e segurança da troca de turno dos funcionários (réplicas).
    *   `parallelism`: Quantos funcionários trocar de uma vez.
    *   `monitor`: Quanto tempo observar o novo funcionário para ver se ele sabe fazer o trabalho (health check) antes de mandar o próximo antigo embora.
    *   `failure_action: rollback`: Se o novo funcionário for um desastre, automaticamente chamar o antigo de volta.
    *   `order: start-first`: Trazer o novo funcionário, treiná-lo e vê-lo trabalhando bem *antes* de liberar o antigo para ir para casa.

---

📄 Página 282

🔍 Resumo dos principais pontos:

*   Config `update_config` é boa prática geral, mas ajustar para cada app.
*   **Importante:** Config `update_config` é aplicada *antes* do rollout. Se deploy inclui mudança no `update_config` *e* na `image`, o rollout já usa a *nova* config de update.
*   **Exercício Try-it-now:** Deployar v3 da imagem, incluindo `prod-update-config.yml`.
    *   Rollout usará a nova config (parallelism 3, monitor 60s, rollback, start-first).
    *   1. Mesclar arquivos (core, prod, healthcheck, **update-config**, v3): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Verificar tarefas: `docker stack ps numbers`.
*   Saída do `stack ps` fica grande com muitos updates (mostra histórico).
*   Observar que 3 réplicas API são atualizadas/monitoradas antes das próximas 3.

🧠 Explicação detalhada e didática:

*   A página reitera que a configuração de `update_config` deve ser ajustada para cada aplicação.
*   Explica uma sutileza importante: quando você faz um `docker stack deploy` que altera tanto a configuração de `update_config` quanto a imagem (ou outra propriedade) do serviço, o Swarm primeiro aplica a **nova** `update_config` e depois executa o rolling update usando essa **nova** configuração.
*   O exercício instrui a implantar a `v3` da imagem, mas desta vez incluindo o arquivo `prod-update-config.yml` na mesclagem. Isso significa que a substituição das réplicas `v2` pelas `v3` ocorrerá seguindo as regras definidas na Listagem 14.2 (3 em paralelo, monitorando por 60s, revertendo em falha, start-first).
*   Ao observar a saída de `docker stack ps numbers` durante ou após a atualização, você verá evidências desse comportamento (ex: 3 réplicas iniciando/rodando a v3 enquanto 3 réplicas v2 ainda estão rodando ou sendo desligadas, depois as próximas 3 sendo atualizadas). A lista de tarefas (`ps`) acumula o histórico, mostrando réplicas de todas as versões anteriores com status `Shutdown`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Mesclar Arquivos para v3:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/v3.yml --log-level ERROR config > stack.yml
    ```
2.  **Atualizar Stack para v3:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
3.  **Observar Tarefas (durante e após):** Execute repetidamente:
    ```powershell
    docker stack ps numbers
    # Ou focado na API:
    # docker service ps numbers_numbers-api
    ```
    Tente observar o padrão de 3 réplicas sendo atualizadas por vez. O estado pode passar por `Preparing`, `Starting`, `Running (health: starting)`, `Running` (saudável). As antigas passarão para `Shutdown`.

---

📄 Página 283

🔍 Resumo dos principais pontos:

*   Figura 14.4: Mostra saída (truncada) do `stack ps` após deploy v3, indicando 3 réplicas v2 Shutdown e 3 v3 Running.
*   Forma mais limpa de ver status: `docker service inspect --pretty <stack>_<service>`.
    *   Mostra spec do serviço, config de update, status do último update.
*   **Exercício Try-it-now:** Inspecionar serviço API após update v3.
    *   Comando: `docker service inspect --pretty numbers_numbers-api`.

🧠 Explicação detalhada e didática:

*   A página reconhece que a saída de `docker stack ps` (ou `docker service ps`) pode ficar muito poluída com o histórico de réplicas de atualizações anteriores.
*   Apresenta `docker service inspect --pretty <nome_servico>` como uma maneira melhor de obter um resumo do estado atual do serviço e o resultado da última operação de atualização. O nome do serviço no Swarm geralmente segue o padrão `<nome_stack>_<nome_servico_no_compose>`.
*   O exercício usa esse comando para inspecionar o serviço `numbers_numbers-api`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Inspecionar Serviço API:**
    ```powershell
    docker service inspect --pretty numbers_numbers-api
    ```
2.  **Analisar Saída:** Procure por seções como:
    *   `Service:` (Nome, ID, Labels)
    *   `UpdateStatus:` (State: completed, StartedAt, CompletedAt, Message: update completed)
    *   `Spec:` (Contém a definição completa, incluindo `TaskTemplate.ContainerSpec.Image` mostrando `:v3`, `UpdateConfig` com seus valores customizados, `RollbackConfig`, `Resources.Limits`, etc.)

---

📄 Página 284

🔍 Resumo dos principais pontos:

*   Figura 14.5: Mostra saída (truncada) do `service inspect --pretty`, destacando `UpdateStatus: completed` e trechos da especificação (imagem v3, update config).
*   **Importante:** Config de update/rollback precisa ser incluída em *todos* os deploys subsequentes.
    *   Se deploy futuro *não* incluir o override `prod-update-config.yml`, Swarm reverterá `update_config` para os defaults *antes* de fazer o rollout da nova imagem.
*   Configurações de Rollback (`rollback_config`):
    *   Conjunto idêntico de opções (`parallelism`, `monitor`, `failure_action`, `order`).
    *   Permite definir como a *reversão* deve ocorrer (ex: rápida vs. cautelosa).
*   Importante testar configs de update/rollback com app real em escala para ganhar confiança.
*   Objetivo: Rollouts rápidos mas seguros, com rollback automático rápido se necessário.
*   **14.3 Configuring service rollbacks** (Configurando reversões de serviço)
    *   Rollback manual (`docker service update --rollback`) não deve ser necessário normalmente.
    *   Ideal: Rollback automático acionado por falha durante `monitor` do update.

🧠 Explicação detalhada e didática:

*   A página analisa a saída do `service inspect` (Figura 14.5), que fornece um bom resumo do estado atual e da última atualização.
*   Faz um alerta crucial: as configurações customizadas de `update_config` (e `rollback_config`) **não são "pegajosas"** por si só. Elas são parte da *definição do serviço*. Se você fizer um `docker stack deploy` futuro que *não* inclua o arquivo `.yml` que define essas configurações customizadas, o Swarm atualizará a definição do serviço para usar os **defaults** novamente, *antes* de executar o rollout da nova imagem (se houver). Portanto, os arquivos que definem `update_config` e `rollback_config` devem ser incluídos em *todas* as mesclagens de `docker-compose config` para seus deploys de produção.
*   Introduz a seção `deploy: rollback_config:`, que permite configurar como um rollback (seja automático ou manual) deve se comportar, usando as mesmas opções (`parallelism`, `monitor`, `failure_action`, `order`) que `update_config`. Você pode querer um rollback mais rápido e agressivo do que um update normal.
*   A seção 14.3 foca em como configurar e testar o comportamento de rollback, especialmente o automático.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você adicionará uma seção `rollback_config:` ao serviço `numbers-api` em um novo arquivo override (`prod-rollback-config.yml`) e o incluirá nos próximos deploys.
*   Exemplo de `rollback_config:` (será visto na próxima página):
    ```yaml
    deploy:
      rollback_config:
        parallelism: 2 # Exemplo: reverter 2 por vez
        order: stop-first # Exemplo: parar novas falhas antes de iniciar antigas
        # ... outras opções
    ```

---

📄 Página 285

🔍 Resumo dos principais pontos:

*   Rollback automático acontece se novas réplicas falham durante `monitor` do update.
*   Se bem configurado, pode nem perceber que rollback ocorreu.
*   Simular falha: Deployar v5 do app, que requer config não fornecida -> API falha imediatamente.
*   **Exercício Try-it-now:** Deployar v5 "ruim" (`v5-bad.yml`) e observar rollback automático.
    *   `v5-bad.yml`: Usa imagem `:v5`, mas não fornece config necessária -> réplicas v5 falharão health check.
    *   1. Mesclar arquivos (core, prod, healthcheck, update-config, **v5-bad**): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Esperar ~1 min e inspecionar serviço: `docker service inspect --pretty numbers_numbers-api`.
*   Resultado esperado: Update inicia, réplicas v5 falham health check durante `monitor: 60s`, `failure_action: rollback` é acionado, Swarm reverte para v3 automaticamente. `inspect` mostrará `UpdateStatus: rollback_completed` e imagem de volta para `:v3`.
*   Figura 14.6: Mostra `inspect` após rollback, com estado `rollback_completed`.

🧠 Explicação detalhada e didática:

*   Esta página descreve o cenário ideal de **rollback automático**. Graças à combinação de:
    *   `monitor: <tempo>` (que dá tempo para health checks rodarem nas novas réplicas)
    *   `failure_action: rollback` (que instrui o Swarm a reverter se falhas ocorrerem durante o monitoramento)
    ... um deploy de uma versão quebrada da aplicação pode ser automaticamente desfeito pelo Swarm, retornando ao estado funcional anterior sem intervenção manual significativa.
*   O exercício simula exatamente isso:
    1.  Prepara-se um `stack.yml` que inclui uma atualização para a imagem `:v5` (definida em `v5-bad.yml`), mas *sem* a configuração que essa versão `v5` requer para funcionar. As configurações de `update_config` (parallelism 3, monitor 60s, failure_action rollback, order start-first) ainda estão ativas (incluídas na mesclagem).
    2.  Executa-se `docker stack deploy`.
    3.  O Swarm inicia o rolling update para `v5`. As novas réplicas `v5` iniciam, mas seus health checks falham quase imediatamente (porque a config necessária está faltando).
    4.  Como as falhas ocorrem dentro do período de `monitor: 60s`, a condição para `failure_action: rollback` é satisfeita.
    5.  O Swarm **automaticamente** inicia um processo de rollback, substituindo as réplicas `v5` falhas por novas réplicas `v3` (a versão anterior).
    6.  Após algum tempo, o comando `docker service inspect` mostrará que o estado da última atualização é `rollback_completed` e que a imagem em uso é a `:v3`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Mesclar Arquivos para v5 Ruim:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/v5-bad.yml --log-level ERROR config > stack.yml
    ```
2.  **Implantar v5 Ruim:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
3.  **Observar e Inspecionar:** Aguarde cerca de 1 a 2 minutos (para o `monitor` expirar e o rollback ocorrer).
    ```powershell
    docker service inspect --pretty numbers_numbers-api
    ```
    Verifique o `UpdateStatus.State` (deve ser `rollback_completed` ou `rollback_paused` se algo der errado no rollback) e a `Spec.TaskTemplate.ContainerSpec.Image` (deve ter voltado para `:v3`). Você também pode usar `docker service ps numbers_numbers-api` para ver o histórico de réplicas v5 falhando e sendo substituídas por v3.

---

📄 Página 286

🔍 Resumo dos principais pontos:

*   Figura 14.6: Confirma `UpdateStatus: rollback_completed`.
*   Rollback automático mantém app rodando, mesmo com deploy falho.
*   Comparação `start-first` vs `stop-first` durante falha/rollback:
    *   `start-first`: Mantém capacidade total (ou quase) durante a tentativa de update e rollback, pois réplicas antigas só são paradas após novas (v5 falhas, ou v3 no rollback) estarem "prontas" (ou falharem).
    *   `stop-first` (padrão): Reduz capacidade temporariamente, pois réplicas antigas (v3) são paradas *antes* das novas (v5) iniciarem e falharem. App fica com menos réplicas durante a falha e o rollback.
*   Pode-se customizar `rollback_config` (similar a `update_config`).
*   **Listagem 14.3:** Config `rollback_config` agressiva (em `prod-rollback-config.yml`).
    ```yaml
    numbers-api:
      deploy:
        rollback_config:
          parallelism: 6 # Reverter todas as 6 réplicas de uma vez
          monitor: 0s # Não esperar/monitorar saúde das réplicas revertidas (assume v3 é boa)
          failure_action: continue # Se rollback falhar, continuar mesmo assim (não pausar)
          order: start-first # Iniciar v3 antes de parar v5 falha
    ```
*   Objetivo: Reverter o mais rápido possível, assumindo que versão anterior era estável.

🧠 Explicação detalhada e didática:

*   A página analisa o rollback automático bem-sucedido e discute o impacto da estratégia `order: start-first` (definida no `update_config`) versus a padrão `stop-first` nesse cenário de falha: `start-first` é geralmente melhor para manter a capacidade da aplicação durante a turbulência do update falho e do rollback.
*   Introduz a seção `deploy: rollback_config:` que permite definir *como* o processo de rollback deve ocorrer, usando as mesmas chaves de `update_config`.
*   A Listagem 14.3 mostra uma configuração de rollback **agressiva**:
    *   `parallelism: 6`: Reverte todas as 6 réplicas da API de uma só vez.
    *   `monitor: 0s`: Não perde tempo monitorando as réplicas v3 que estão voltando (assume que elas funcionarão).
    *   `failure_action: continue`: Mesmo que uma réplica v3 falhe ao iniciar durante o rollback, o processo continua para as outras (não pausa).
    *   `order: start-first`: Inicia as réplicas v3 antes de remover as v5 falhas.
*   A lógica por trás dessa configuração agressiva é: "A versão anterior (v3) era boa. Se a v5 falhou, quero voltar para a v3 o mais rápido possível, sem muita cautela."

💻 Exemplos práticos (para meu ambiente Windows):

*   Você precisará de um arquivo `prod-rollback-config.yml` com a Listagem 14.3.
*   No próximo deploy (tentando v5 ruim novamente), você incluirá este arquivo na mesclagem, e o rollback resultante deverá ser mais rápido.

---

📄 Página 287

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Tentar deploy v5 ruim novamente, agora com `prod-rollback-config.yml` incluído.
    *   Rollout ainda falha, mas rollback deve ser mais rápido.
    *   1. Mesclar arquivos (core, prod, healthcheck, update-config, **rollback-config**, v5-bad): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Inspecionar serviço: `docker service inspect --pretty numbers_numbers-api`.
*   Figura 14.7: Mostra `inspect` capturado durante o início do rollback (`rollback_started`).
*   **Comportamento Estranho:** Após rollback completar, `inspect` mostrará que `rollback_config` voltou aos *defaults*!
    *   Causa: Rollback reverte *toda* a especificação do serviço para a anterior (v3), e a spec v3 *não* incluía a `rollback_config` customizada.
    *   O rollback *usou* a config agressiva, mas depois a própria config de rollback foi revertida.
*   Lição: Precisa incluir overrides `update_config` e `rollback_config` em *todos* os deploys futuros para mantê-los ativos.
*   Perigo de múltiplos overrides: Fácil esquecer um. Melhor agrupar configs de produção em um único override.
*   **Deploy Final (v5 corrigido):**
    *   `v5.yml`: Override que adiciona config faltante para v5 funcionar.
    *   `prod-full.yml`: Novo override que combina todas as configs de produção (prod, healthcheck, update, rollback).
*   **Exercício Try-it-now:** Deployar v5 funcionando, usando estrutura de overrides mais limpa.
    *   1. Mesclar arquivos (core, **prod-full**, v5): `docker-compose ... config > stack.yml`.
    *   2. Atualizar stack: `docker stack deploy -c stack.yml numbers`.
    *   3. Esperar e inspecionar: `docker service inspect --pretty numbers_numbers-api`.

🧠 Explicação detalhada e didática:

*   O exercício aplica a configuração de rollback agressiva e tenta novamente o deploy da v5 quebrada. O esperado é que o rollback ocorra mais rapidamente (substituindo todas as 6 réplicas de uma vez, sem monitoramento).
*   A página revela um comportamento contraintuitivo e importante: quando um rollback acontece, ele reverte **toda a especificação do serviço** para o estado anterior. Se a configuração de `rollback_config` customizada só foi introduzida na versão que *falhou* (v5), então após o rollback para a versão anterior (v3, que não tinha a `rollback_config` customizada), a definição do serviço voltará a ter a `rollback_config` *padrão*. O rollback em si usou a configuração agressiva, mas essa configuração não persistiu após a reversão.
*   **Moral da história:** Para que configurações como `update_config` e `rollback_config` sejam permanentes, elas precisam estar definidas no arquivo override que representa o estado *estável* do seu ambiente (ex: um `prod.yml` ou `prod-full.yml` que é sempre incluído).
*   O exercício final demonstra a abordagem correta:
    1.  Cria-se um `prod-full.yml` que consolida todas as configurações desejadas para produção (réplicas, limites, healthcheck, update, rollback).
    2.  Cria-se um `v5.yml` que contém a `image: :v5` *e* a correção de configuração que faltava no `v5-bad.yml`.
    3.  O deploy agora mescla apenas `core.yml`, `prod-full.yml`, e `v5.yml`.
*   Este deploy da v5 deve ser bem-sucedido, pois a configuração está correta, e as configurações de update/rollback de `prod-full.yml` serão aplicadas e persistirão.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Mesclar com Rollback Config:** (Estando em `ch14/exercises`)
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod.yml -f ./numbers/prod-healthcheck.yml -f ./numbers/prod-update-config.yml -f ./numbers/prod-rollback-config.yml -f ./numbers/v5-bad.yml --log-level ERROR config > stack.yml
    docker stack deploy -c stack.yml numbers
    # Observe o rollback (deve ser mais rápido)
    Start-Sleep -Seconds 30 # Tempo menor para verificar
    docker service inspect --pretty numbers_numbers-api # Verificar se voltou para v3 e se rollback_config sumiu/voltou ao default
    ```
2.  **Mesclar para v5 Corrigido:**
    ```powershell
    docker-compose -f ./numbers/docker-compose.yml -f ./numbers/prod-full.yml -f ./numbers/v5.yml --log-level ERROR config > stack.yml
    ```
3.  **Implantar v5 Corrigido:**
    ```powershell
    docker stack deploy -c stack.yml numbers
    ```
4.  **Verificar Sucesso:** Espere ~1 minuto (para passar o `monitor` do update).
    ```powershell
    docker service inspect --pretty numbers_numbers-api
    # Deve mostrar UpdateStatus.State: completed e Image: :v5
    # Verifique também se UpdateConfig e RollbackConfig estão com os valores de prod-full.yml
    ```

---

📄 Página 288

🔍 Resumo dos principais pontos:

*   Figura 14.8: Mostra `inspect` após deploy v5 bem-sucedido, confirmando estado `completed` e imagem `:v5`.
*   Agora v5 está rodando com health checks. Se API quebrar, Swarm substitui réplicas.
*   **Ponto Final sobre Rollbacks:** Falhas de health check *durante* o `monitor` do update disparam rollback. Falhas de health check *após* o update completar (app rodando normalmente) disparam apenas substituição de réplica, *não* rollback para versão anterior.
*   Diagrama (Figura 14.9): Fluxograma do processo de update.
    *   Update iniciado -> Atualiza lote de réplicas -> Monitora saúde -> [Falhou? -> Rollback] / [OK? -> Próximo lote ou Fim].

🧠 Explicação detalhada e didática:

*   A página confirma o sucesso do deploy da v5 corrigida.
*   Esclarece um ponto importante sobre quando os rollbacks automáticos ocorrem:
    *   O `failure_action: rollback` definido no `update_config` só é acionado se uma réplica falhar ao iniciar ou falhar no health check **durante o período de `monitor`** especificado, logo após ela ter sido atualizada.
    *   Se a atualização for concluída com sucesso (todas as réplicas passaram pelo `monitor` OK) e, *depois*, uma réplica falhar no seu health check normal, o Swarm simplesmente a **substituirá** por uma nova réplica da *mesma versão atual (v5)*. Ele **não** reverterá para a versão anterior (v3) nesse caso. O rollback automático é uma proteção contra deploys *quebrados*, não contra falhas operacionais posteriores.
*   A Figura 14.9 resume visualmente o fluxo de decisão durante um rolling update configurado com monitoramento e rollback.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Testar Auto-Reparo v5:**
    *   Com a stack `v5` rodando, acesse `http://localhost` e cause o bug na API (muitos cliques).
    *   Aguarde (depende do `health-interval` e `retries`).
    *   Use `docker service ps numbers_numbers-api`. Você verá réplicas v5 sendo substituídas por *novas* réplicas v5. O serviço permanecerá na versão 5.

💡 Analogias ou observações úteis (se houver):

*   Rollback vs. Substituição:
    *   Rollback (automático): O controle de qualidade detecta um defeito grave *na linha de produção* do novo modelo de carro (update falha no `monitor`). A fábrica inteira para de produzir o novo modelo e volta a produzir o antigo.
    *   Substituição (pós-update): Um carro do novo modelo já entregue ao cliente apresenta um problema (falha no health check normal). A concessionária apenas troca aquele carro específico por outro *do mesmo modelo novo*. A produção do novo modelo continua.

---

📄 Página 289

🔍 Resumo dos principais pontos:

*   Figura 14.9: Fluxograma do update.
*   Conclusão sobre updates/rollbacks: Configurar e testar cenários de falha dá confiança para releases frequentes.
*   Próximo tópico: Como uptime é impactado por downtime de **nós** do cluster.
*   **14.4 Managing downtime for your cluster** (Gerenciando tempo de inatividade para seu cluster)
    *   Orquestrador agrupa máquinas, mas máquinas falham (disco, rede, energia).
    *   Cluster mantém apps rodando na maioria das falhas, mas algumas exigem intervenção.
    *   Gerenciar **downtime planejado** (updates OS, etc.) facilita trabalho do Swarm.
*   **Necessidade de Cluster Multi-Nó para Exercícios:**
    *   Conceitos de gerenciamento de nós só fazem sentido com >1 nó.
    *   Opções: VMs locais, ou playground online **Play with Docker (PWD)**.
    *   PWD recomendado: `https://labs.play-with-docker.com`. Gratuito, temporário.
*   **Exercício Try-it-now:** Configurar Swarm multi-nó no PWD.
    *   1. Acessar PWD, login Docker Hub, clicar "Add New Instance" 5 vezes (cria 5 VMs Docker temporárias).
    *   2. No terminal do `node1`: `ip=$(hostname -i)`, `docker swarm init --advertise-addr $ip`.
    *   3. Obter tokens manager/worker: `docker swarm join-token manager`, `docker swarm join-token worker`.
    *   4. Nos terminais de `node2` e `node3`: Colar comando `join` de *manager*.
    *   5. Nos terminais de `node4` e `node5`: Colar comando `join` de *worker*.
    *   6. De volta ao `node1`, verificar nós: `docker node ls`.

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre configuração de updates/rollbacks e introduz o próximo desafio em clusters: lidar com a falha ou manutenção dos próprios **servidores (nós)** que compõem o cluster.
*   Embora o Swarm seja resiliente, há cenários de falha de nó que podem exigir intervenção manual, e para manutenções *planejadas* (como aplicar patches no SO de um nó), existem procedimentos para minimizar o impacto na aplicação.
*   Para praticar esses conceitos, é necessário um **cluster com múltiplos nós**. Como configurar isso localmente pode ser complexo (requer VMs), a página recomenda fortemente o uso do **Play with Docker (PWD)**. PWD é um ambiente online gratuito que fornece acesso rápido a múltiplas instâncias de Docker (rodando em contêineres) que podem ser conectadas para formar um cluster Swarm temporário.
*   O exercício guia passo a passo na criação de um Swarm de 5 nós no PWD: 3 managers e 2 workers. Isso fornecerá um ambiente realista (com redundância de managers) para os próximos exercícios de gerenciamento de nós.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação Principal:** Seguir as instruções para configurar o Swarm no Play with Docker.
    1.  Acesse `https://labs.play-with-docker.com` e faça login.
    2.  Clique "+ ADD NEW INSTANCE" 5 vezes. Você terá `node1` a `node5`.
    3.  Selecione `node1` no painel esquerdo. No terminal dele, execute:
        ```bash
        ip=$(hostname -i)
        docker swarm init --advertise-addr $ip
        ```
    4.  Copie o comando `docker swarm join ...` para *managers* exibido.
    5.  Copie o comando `docker swarm join ...` para *workers* exibido (use `docker swarm join-token worker` se precisar ver de novo).
    6.  Selecione `node2`, cole e execute o comando join de *manager*.
    7.  Selecione `node3`, cole e execute o comando join de *manager*.
    8.  Selecione `node4`, cole e execute o comando join de *worker*.
    9.  Selecione `node5`, cole e execute o comando join de *worker*.
    10. Selecione `node1` novamente e execute:
        ```bash
        docker node ls
        ```
        Você deve ver 5 nós listados, 3 com `MANAGER STATUS` (um como `Leader`) e 2 sem (workers). Todos devem estar `Ready` e `Active`.

---

📄 Página 290

🔍 Resumo dos principais pontos:

*   Figura 14.10: Mostra a saída do `docker node ls` no PWD com 5 nós (3 managers, 2 workers).
*   **Cenário 1: Downtime Planejado (Manutenção de Nó).**
    *   Objetivo: Remover containers do nó, impedir novos agendamentos, fazer manutenção (ex: update OS, reboot), depois retornar nó ao serviço.
    *   Solução Swarm: **Drain Mode**.
*   **Exercício Try-it-now:** Colocar nós em modo Drain.
    *   No `node1` (manager):
        *   Colocar worker `node5` em drain: `docker node update --availability drain node5`.
        *   Colocar manager `node3` em drain: `docker node update --availability drain node3`.
    *   Verificar: `docker node ls`.
*   **Modo Drain:**
    *   Para Workers e Managers: Todas as réplicas rodando no nó são paradas (Swarm tenta reagendá-las em outros nós ativos). Novas réplicas *não* são agendadas neste nó.
    *   Para Managers: Nó continua participando do quorum/gerenciamento (replica DB, responde API), *a menos* que seja o Leader (nesse caso, outro manager vira Leader).

🧠 Explicação detalhada e didática:

*   Com o cluster multi-nó PWD pronto, a página aborda o gerenciamento de **downtime planejado**. Se você precisa tirar um servidor do cluster temporariamente para manutenção, o procedimento correto é colocá-lo em modo **Drain**.
*   O comando `docker node update --availability drain <nome_no>` instrui o Swarm a:
    1.  **Parar** todas as réplicas de serviço que estão rodando atualmente naquele nó.
    2.  **Reagendar** essas réplicas em outros nós *ativos* do cluster (se houver capacidade e as constraints permitirem).
    3.  **Marcar** o nó como `Drain` para que o agendador não envie *nenhuma nova réplica* para ele.
*   Isso permite que você desligue, atualize, reinicie o nó com segurança, sem impactar (idealmente) a disponibilidade dos serviços (pois as réplicas foram movidas).
*   Importante: Mesmo em modo `Drain`, um nó Manager continua participando do consenso do cluster (a menos que seja o único manager restante ou o líder precise ser substituído).
*   Após a manutenção, você retorna o nó ao serviço com `docker node update --availability active <nome_no>`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **No terminal do `node1` no PWD:**
    ```bash
    # Colocar node5 em drain
    docker node update --availability drain node5

    # Colocar node3 em drain
    docker node update --availability drain node3

    # Verificar status dos nós
    docker node ls
    ```
    Observe que `node3` e `node5` agora mostram `Drain` na coluna `AVAILABILITY`. Se houvesse serviços rodando, `docker service ps <servico>` mostraria as réplicas sendo movidas desses nós.

---

📄 Página 291

🔍 Resumo dos principais pontos:

*   Figura 14.11: Mostra saída do `docker node ls` com `node3` e `node5` em `Drain`.
*   **Managers e Leader:**
    *   Precisa de múltiplos managers para HA.
    *   Modelo Ativo-Passivo: Só 1 manager é o **Leader** (controla cluster). Outros replicam DB, podem responder API, podem virar leader se atual falhar.
    *   **Eleição de Leader:** Requer **maioria** (quorum) dos managers. Por isso, número **ímpar** de managers (3 ou 5) é recomendado.
*   **Cenário 2: Perda de Manager / Quorum Par.**
    *   Se perde manager e fica com nº par (ex: 2 de 3), cluster ainda funciona, mas não tolera *mais nenhuma* falha de manager (perderia quorum).
    *   Solução: Promover um Worker a Manager (`docker node promote <worker>`).
*   **Exercício Try-it-now:** Simular perda de leader e promover worker.
    *   1. No `node1` (Leader atual): Forçar saída do Swarm: `docker swarm leave --force`.
    *   2. No `node2` (outro manager):
        *   Retornar `node5` (worker) para ativo: `docker node update --availability active node5`.
        *   Promover `node5` a manager: `docker node promote node5`.
        *   Verificar nós: `docker node ls`.

🧠 Explicação detalhada e didática:

*   A página explica o papel dos **Managers** e do **Leader** no Swarm. Para que o cluster Swarm seja altamente disponível (possa sobreviver à falha de um manager), você precisa de múltiplos managers (recomendado 3 ou 5). Apenas um deles é o Leader ativo a qualquer momento, mas os outros estão prontos para assumir se o Leader falhar, através de um processo de eleição que exige um quorum (mais da metade) dos managers. É por isso que um número ímpar é crucial (com 3, precisa de 2 para quorum; com 5, precisa de 3).
*   Aborda o cenário onde se perde um manager e o número restante fica par. Para restaurar a tolerância a falhas, deve-se **promover** um nó Worker existente para se tornar um Manager usando `docker node promote`.
*   O exercício simula a perda do Leader (`node1` sai do Swarm) e depois, a partir de outro manager (`node2`), promove um worker (`node5`, que é reativado primeiro) para restaurar o número ímpar de managers (agora `node2`, `node3` em drain, `node5` promovido).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **No terminal do `node1` no PWD:**
    ```bash
    docker swarm leave --force
    # Este nó não faz mais parte do Swarm.
    ```
2.  **No terminal do `node2` no PWD:**
    ```bash
    # Reativar node5
    docker node update --availability active node5

    # Promover node5
    docker node promote node5

    # Verificar nós
    docker node ls
    ```
    Você verá que `node1` está `Down` e `Unreachable`. `node2` provavelmente se tornou o novo `Leader`. `node3` ainda está `Drain`. `node4` é worker. `node5` agora tem `Reachable` como `MANAGER STATUS`. O Swarm tem 3 managers alcançáveis (`node2`, `node3`, `node5`), mantendo o quorum.

---

📄 Página 292

🔍 Resumo dos principais pontos:

*   Figura 14.12: Mostra `node ls` após `leave` e `promote`, com `node1` Down, `node2` Leader, `node5` Manager Reachable.
*   Swarm agora tem 3 managers novamente (alta disponibilidade restaurada).
*   Se `node1` voltasse, poderia rebaixar (`demote`) outro manager para worker.
*   **Outros Cenários de Falha/Gerenciamento:**
    1.  **Todos Managers Offline:** Workers continuam rodando apps existentes, mas Swarm não gerencia mais (sem scheduling, sem substituir falhas). Precisa trazer managers online.
    2.  **Perda de Quorum (Só 1 Manager Sobrevive, não era Leader):** Cluster perde controle. Precisa forçar o manager restante a ser leader: `docker swarm init --force-new-cluster` (preserva dados/tarefas). Depois adicionar mais managers.
    3.  **Rebalancear Réplicas:** Adicionar nós não move réplicas automaticamente. Para redistribuir: `docker service update --force <servico>` (sem mudar nada mais).
*   **14.5 Understanding high availability in Swarm clusters** (Entendendo alta disponibilidade em clusters Swarm)
    *   Múltiplas camadas de HA no Swarm (health checks -> substituição; múltiplos workers -> reagendamento; múltiplos managers -> redundância de controle).
    *   Última camada: **Falha de Datacenter/Região**.

🧠 Explicação detalhada e didática:

*   A página confirma o resultado do exercício anterior (Figura 14.12), mostrando o Swarm se recuperando da perda do líder e mantendo a alta disponibilidade do plano de controle ao promover um worker.
*   Apresenta brevemente três outros cenários de gerenciamento e recuperação:
    1.  **Perda Total dos Managers:** As aplicações continuam rodando nos workers, mas o "cérebro" do cluster se foi. Nenhuma nova decisão (scheduling, substituição) pode ser tomada até que os managers voltem.
    2.  **Perda de Quorum:** Se apenas um manager não-líder sobreviver, ele não pode eleger a si mesmo. O comando `docker swarm init --force-new-cluster` é um procedimento de recuperação de desastre para forçar aquele nó a se tornar o líder de um "novo" cluster que reutiliza o estado antigo.
    3.  **Rebalanceamento:** O Swarm não move réplicas automaticamente só porque um novo nó entrou no cluster. Se você quiser forçar uma redistribuição mais equilibrada, precisa usar `docker service update --force` no serviço.
*   A seção 14.5 introduz a consideração final sobre alta disponibilidade: o que acontece se todo o datacenter ou região onde seu cluster está hospedado ficar indisponível?

💻 Exemplos práticos (para meu ambiente Windows):

*   **Rebalanceamento (Exemplo conceitual):**
    ```powershell
    # Se você tivesse um serviço 'my-app' rodando e adicionasse nós
    docker service update --force my-app
    # O Swarm tentaria parar e iniciar réplicas para distribuí-las melhor.
    ```
*   Os cenários de perda total de managers ou quorum são situações de recuperação de desastre que você espera nunca precisar usar, mas é bom saber que existem.

---

📄 Página 293

🔍 Resumo dos principais pontos:

*   **HA Inter-Regiões:** Tentar estender um *único* cluster Swarm por múltiplos datacenters/regiões é **problemático**.
    *   **Latência de Rede:** Nós Swarm (especialmente managers) precisam de comunicação de baixa latência. Latência alta entre regiões pode fazer managers acharem que outros nós/managers falharam, causando agendamento/eleições incorretas.
    *   **Risco de Split-Brain:** Managers em regiões diferentes podem perder comunicação e ambos acharem que são o Leader, levando a um estado inconsistente do cluster.
*   **Solução Segura para HA Regional:** **Múltiplos Clusters Independentes.**
    *   Um cluster Swarm por região/datacenter.
    *   Gerenciamento de tráfego entre clusters feito por fora (ex: DNS Global Load Balancing, Traffic Manager).
*   Desvantagens: Maior sobrecarga de gerenciamento (múltiplos clusters), risco de drift entre clusters/apps. Mas são gerenciáveis (automação, GitOps).
*   Diagrama (Figura 14.13): Mostra configuração com múltiplos clusters em regiões diferentes, com balanceador de carga global direcionando tráfego.

🧠 Explicação detalhada e didática:

*   A página aborda a questão da alta disponibilidade em caso de falha de um datacenter inteiro ou região geográfica.
*   **Alerta:** Embora tecnicamente possível, esticar um *único* cluster Docker Swarm através de múltiplas regiões com alta latência de rede entre elas é **altamente desaconselhado**. A comunicação constante necessária entre os nós managers para manter o consenso do cluster é muito sensível à latência. Uma lentidão temporária na rede pode levar o Swarm a conclusões erradas sobre a saúde dos nós remotos, resultando em re-agendamentos desnecessários ou, pior, em um cenário de "split-brain" onde diferentes conjuntos de managers acham que são os líderes, corrompendo o estado do cluster.
*   A **solução recomendada e segura** para redundância regional é ter **clusters Swarm completamente independentes** em cada região. A lógica para direcionar o tráfego para o cluster apropriado (ou para fazer failover entre eles) deve ser implementada *acima* dos clusters, usando balanceadores de carga globais, DNS inteligente (como Azure Traffic Manager, AWS Route 53 com health checks), etc.
*   Isso adiciona complexidade no gerenciamento (você tem múltiplos clusters para cuidar), mas é uma arquitetura muito mais robusta e previsível do que um único cluster esticado geograficamente. A Figura 14.13 ilustra essa abordagem multi-cluster.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando direto. É uma consideração de arquitetura de alta disponibilidade para cenários de produção distribuídos globalmente.

💡 Analogias ou observações úteis (se houver):

*   Cluster Swarm Esticado vs. Múltiplos Clusters:
    *   Esticado: Tentar gerenciar filiais de uma empresa em países diferentes com uma única linha telefônica direta e lenta entre os gerentes. A comunicação falha, decisões erradas são tomadas.
    *   Múltiplos: Cada filial tem sua própria gerência local (cluster independente). A matriz (balanceador global) decide para qual filial enviar os clientes, e cada filial opera autonomamente.

---

📄 Página 294

🔍 Resumo dos principais pontos:

*   **14.6 Lab** (Laboratório)
    *   Objetivo: Ganhar experiência com configuração de rollout/rollback em stack Swarm.
    *   App: Image Gallery (Cap. 9).
*   **Tarefa:**
    1.  Escrever arquivo Compose único (`.yml`) para deploy produção do Image Gallery.
    2.  **Serviço API (`iotd`, imagem `diamol/ch04-image-of-the-day` - *Nota: usa imagem antiga sem healthcheck!*):**
        *   4 réplicas.
        *   **Adicionar `healthcheck`** na definição do *serviço* no Compose (imagem não tem).
        *   Configurar `update_config` (rápido mas seguro).
        *   Configurar `rollback_config` (rápido).
    3.  Serviço Web (`image-gallery`, imagem `diamol/ch04-image-gallery`): Outras configs (porta, réplicas - não especificado no lab, pode usar defaults ou definir).
    4.  Serviço Log (`access-log`, imagem `diamol/ch04-access-log`): Outras configs (réplicas - não especificado).
    5.  Após deploy inicial, preparar *outro* stack file (`.yml`) que atualiza *todas* as imagens para as versões do Cap. 9 (com métricas/healthchecks embutidos): `diamol/ch09-...`.
    6.  Implantar o update. Verificar se API (`iotd`) atualiza conforme política definida e *não* faz rollback (pois health check adicionado no serviço deve funcionar com imagem antiga, e nova imagem também é saudável).

🧠 Explicação detalhada e didática:

*   O **Lab do Capítulo 14** foca em aplicar as configurações de `deploy: update_config:` e `deploy: rollback_config:` aprendidas.
*   O cenário usa a aplicação Image Gallery, mas com uma pegadinha: o deploy inicial usará as imagens do **Capítulo 4**, onde a API (`iotd`) **não tinha** um `HEALTHCHECK` embutido.
*   **Parte 1: Deploy Inicial:**
    *   Criar um `docker-compose.yml` (ou `stack.yml`) para a stack.
    *   Definir os 3 serviços (`access-log`, `iotd`, `image-gallery`) usando imagens `diamol/ch04-...`.
    *   Para `iotd`:
        *   `replicas: 4`.
        *   Adicionar seção `healthcheck:` com um `test:` que verifique a saúde da API (ex: `["CMD", "curl", "-f", "http://localhost:8080/"]` - assumindo porta interna 8080).
        *   Adicionar seção `deploy: update_config:` com settings "rápidos mas seguros" (ex: `parallelism: 2`, `monitor: 30s`, `failure_action: rollback`, `order: start-first`).
        *   Adicionar seção `deploy: rollback_config:` com settings "rápidos" (ex: `parallelism: 4`, `monitor: 0s`, `failure_action: continue`, `order: start-first`).
    *   Definir outras propriedades (portas, redes, limites de recursos - opcional mas bom praticar).
    *   Implantar com `docker stack deploy`.
*   **Parte 2: Deploy de Update:**
    *   Criar um *segundo* arquivo YAML (ou modificar o primeiro) onde as `image:` dos 3 serviços são alteradas para as versões do Cap. 9 (`diamol/ch09-...`). Manter as configs de deploy.
    *   Implantar a atualização com `docker stack deploy` usando este novo arquivo.
    *   **Observar:** O rollout do `iotd` deve seguir a política definida (ex: 2 em paralelo, monitorando por 30s). Como ambas as imagens (v4 e v9) devem ser saudáveis (com o health check definido no serviço), o update deve completar com sucesso, *sem* rollback.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 14:**
    1.  Crie `ig-stack-v1.yml` na pasta `ch14\lab`.
    2.  Defina `version`, `networks`, `services`.
    3.  No serviço `iotd`: `image: diamol/ch04...`, `replicas: 4`, adicione `healthcheck: test: ...`, `deploy: update_config: ...`, `deploy: rollback_config: ...`.
    4.  Defina `image-gallery` e `access-log` (com imagens `ch04`, réplicas, portas, limites).
    5.  `docker stack deploy -c ig-stack-v1.yml ig-lab`. Verifique.
    6.  Crie `ig-stack-v2.yml` copiando `v1` e mudando *apenas* as `image:` para `diamol/ch09...`.
    7.  `docker stack deploy -c ig-stack-v2.yml ig-lab`.
    8.  Observe o update com `docker service ps ig-lab_iotd`. Verifique se segue o `parallelism` e `monitor`, e se completa com sucesso.

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch14/lab/README.md

---

📄 Página 295

🔍 Resumo dos principais pontos:

*   Encerramento do Lab 14.
*   **Capítulo 15: Configuring Docker for secure remote access and CI/CD** (Configurando Docker para acesso remoto seguro e CI/CD)
*   Revisão: CLI `docker` não faz o trabalho, só envia instruções para API do Docker Engine.
*   **Benefícios da Separação CLI <-> API:**
    1.  Outras ferramentas (GUIs, scripts, etc.) podem usar a API.
    2.  CLI local pode ser configurada para falar com Engine *remoto*.
*   Permite gerenciar cluster de dezenas de nós da sua máquina local, usando mesmos comandos `docker`.
*   **Acesso Remoto:**
    *   Usado para administrar ambientes (teste, prod).
    *   Usado para depurar problemas.
    *   Habilita parte de **Implantação Contínua (CD)** do pipeline CI/CD.
*   **Fluxo CI/CD com Acesso Remoto:**
    *   CI (local ou build server): Builda imagem, testa, push para registro.
    *   CD (mesmo pipeline/job): Conecta-se ao Engine Docker *remoto* (ex: UAT, Prod) e implanta a nova versão da imagem (ex: via `docker stack deploy`).
*   Foco Cap. 15: Como expor API Docker remotamente de forma *segura* e como conectar a ela (localmente e de pipelines).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 15, que trata do **acesso remoto seguro** à API do Docker Engine.
*   Relembra a arquitetura cliente-servidor do Docker: a CLI `docker` é um cliente que fala com a API exposta pelo Docker Engine (servidor).
*   Essa separação permite que a CLI local se conecte a um Docker Engine rodando em outra máquina (se a API estiver exposta e acessível pela rede).
*   Esse acesso remoto é fundamental para:
    *   **Gerenciamento:** Administrar servidores Docker que não estão na sua máquina local (servidores de teste, produção).
    *   **Implantação Contínua (CD):** Permitir que o pipeline de CI/CD, após construir e testar uma imagem, se conecte automaticamente a um ambiente de destino (UAT, produção) e execute o comando de implantação (como `docker stack deploy`).
*   O capítulo abordará como configurar o Docker Engine para permitir esse acesso remoto de forma **segura** (evitando os riscos do acesso HTTP inseguro visto brevemente antes) e como configurar a CLI (ou o pipeline) para se conectar a esses Engines remotos seguros.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você configurará o Docker Engine (provavelmente no Play With Docker, pois o Docker Desktop tem limitações aqui) para usar TLS ou SSH para expor sua API.
*   Você usará flags na CLI `docker` (como `--host`, `--tlsverify`, `--tlscacert`, etc.) ou Docker Contexts para se conectar a esse Engine remoto a partir da sua máquina local ou de um contêiner Jenkins.

---

📄 Página 296

🔍 Resumo dos principais pontos:

*   Continuação da introdução ao Cap. 15.
*   **15.1 Endpoint options for the Docker API** (Opções de endpoint para a API Docker)
*   Instalação padrão Docker: Engine ouve em canal local (socket Linux, named pipe Windows), CLI usa mesmo canal. Não permite acesso remoto.
*   Para acesso remoto, precisa configurar Engine explicitamente.
*   Opção 1 (Ruim): **HTTP simples, inseguro**.
    *   Habilita API em endpoint HTTP (ex: `tcp://0.0.0.0:2375`).
    *   **Perigosíssimo:** Qualquer um na rede pode conectar e controlar Docker *sem autenticação*.
    *   Risco de ataque (ex: site malicioso acessando `localhost:2375` e rodando container de mineração).
    *   **Não usar em produção, nem mesmo em dev.**
*   Exercício será feito para aprendizado, mas instrui a desabilitar depois.

🧠 Explicação detalhada e didática:

*   A seção 15.1 começa a discutir as diferentes maneiras pelas quais a API do Docker Engine pode ser exposta para comunicação.
*   O padrão é usar um canal de comunicação local e privado (socket Unix no Linux/Mac, Named Pipe no Windows), que é seguro por natureza, pois só pode ser acessado por processos rodando na mesma máquina e com as permissões corretas.
*   Para habilitar o acesso remoto, a primeira opção (e a mais **insegura e desaconselhada**) é configurar o Engine para ouvir em um endereço TCP usando HTTP simples, geralmente na porta 2375.
*   A página enfatiza veementemente os **riscos** dessa abordagem:
    *   **Sem Autenticação:** Qualquer pessoa que consiga alcançar a porta 2375 do seu Docker Engine pela rede pode enviar comandos e controlar totalmente o Docker.
    *   **Sem Criptografia:** Toda a comunicação (comandos, respostas, segredos potenciais passados como variáveis) viaja em texto plano pela rede.
    *   **Vetor de Ataque:** Mesmo em uma máquina local, um malware ou um site malicioso poderia tentar se conectar a `localhost:2375` e executar ações danosas.
*   Embora a configuração seja mostrada a seguir para fins didáticos, a instrução clara é **nunca usar HTTP inseguro** em um ambiente real.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício mostrará como habilitar essa opção insegura (temporariamente) no Docker Desktop ou via `daemon.json`. **Lembre-se de desfazer essa configuração logo em seguida.**

---

📄 Página 297

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Habilitar acesso HTTP inseguro (Temporário!).
    *   **Docker Desktop:** Settings > General (ou Daemon/Docker Engine) > Marcar "Expose daemon on tcp://localhost:2375 without TLS". Docker reinicia. (Figura 15.1).
    *   **Docker Engine (Linux/Server):** Editar `daemon.json`. Adicionar `"tcp://0.0.0.0:2375"` à lista `hosts`. Garantir que canal local (`npipe://` ou `fd://`) também esteja lá. (Listagem 15.1). Reiniciar serviço Docker.
*   **Listagem 15.1:** Exemplo `daemon.json` com `hosts` para TCP e canal local.
    ```json
    {
      "hosts": [
        "tcp://0.0.0.0:2375", // HTTP inseguro
        "npipe://" // Canal local Windows
        // ou "fd://" para Linux
      ],
      "insecure-registries": [ // Exemplo de outra config
        "registry.local:5000"
      ]
    }
    ```

🧠 Explicação detalhada e didática:

*   Este exercício mostra **como** habilitar o acesso remoto via HTTP inseguro, **apenas para fins de demonstração**.
*   **No Docker Desktop:** É uma simples caixa de seleção nas configurações gerais (o local exato pode variar ligeiramente entre versões). Marcar essa caixa e aplicar fará o Docker Desktop reconfigurar e reiniciar seu Engine interno para ouvir na porta TCP 2375 sem segurança TLS.
*   **Em Servidores (Linux ou Windows Server):** Requer editar o arquivo `daemon.json` manualmente. A chave `hosts` aceita uma lista de endpoints onde o Engine deve escutar. É crucial adicionar `"tcp://0.0.0.0:2375"` para habilitar o acesso TCP em todas as interfaces, mas também **manter** a entrada original do canal local (`"npipe://"` para Windows, `"unix:///var/run/docker.sock"` ou `"fd://"` para Linux via systemd) para que a CLI local continue funcionando da maneira padrão. Após salvar o `daemon.json`, o serviço Docker precisa ser reiniciado.
*   **A Listagem 15.1** mostra um exemplo de como o `daemon.json` ficaria.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Habilitar no Docker Desktop:**
    *   Clique direito no ícone Docker > Settings.
    *   Vá para General.
    *   Marque a caixa "Expose daemon on tcp://localhost:2375 without TLS".
    *   Clique "Apply & Restart". Aguarde reiniciar.
2.  **(Alternativa - NÃO FAÇA NO DOCKER DESKTOP, apenas em servidor) Editar `daemon.json`:**
    *   Encontre o arquivo (`C:\ProgramData\docker\config\daemon.json`).
    *   Edite como administrador, adicionando `tcp://0.0.0.0:2375` à lista `hosts` (crie `hosts` se não existir), mantendo `npipe://`.
    *   Salve e reinicie o serviço: `Restart-Service docker`.

*   **Lembrete:** Esta configuração é insegura. Prepare-se para desfazê-la após os próximos testes.

---

📄 Página 298

🔍 Resumo dos principais pontos:

*   Figura 15.1: Mostra opção no Docker Desktop Settings para expor daemon sem TLS.
*   Verificar acesso remoto inseguro:
    *   Via CLI Docker com flag `--host`.
    *   Via API REST com `curl`.
*   **Exercício Try-it-now:** Testar acesso via TCP/HTTP.
    *   Comando CLI: `docker --host tcp://localhost:2375 container ls`.
    *   Comando API: `curl http://localhost:2375/containers/json`.
*   Figura 15.2: Mostra saída bem-sucedida de ambos os comandos, confirmando acesso inseguro.
*   CLI `docker` e `docker-compose` suportam parâmetro `--host` (ou `-H`) para especificar Engine remoto.
*   Sem segurança: só precisa do endereço; sem autenticação/criptografia.

🧠 Explicação detalhada e didática:

*   Após habilitar o acesso HTTP inseguro na porta 2375, esta página mostra como verificar se funcionou e como usar a flag `--host` (ou `-H`) para direcionar os comandos `docker` (e `docker-compose`) para esse endpoint TCP em vez do canal local padrão.
*   O primeiro comando do exercício, `docker --host tcp://localhost:2375 container ls`, instrui a CLI `docker` a se conectar ao Engine via TCP na porta 2375 da própria máquina (`localhost`) e executar o comando `container ls`.
*   O segundo comando, `curl http://localhost:2375/containers/json`, acessa diretamente a API REST do Docker usando `curl`. O endpoint `/containers/json` é o equivalente API ao comando `docker container ls`.
*   Se ambos os comandos funcionarem e retornarem a lista de contêineres (ou uma lista vazia, se não houver nenhum rodando), isso confirma que a API está exposta via HTTP na porta 2375 e acessível sem qualquer segurança.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Testar Acesso via CLI:** (Após habilitar na página anterior)
    ```powershell
    docker --host tcp://localhost:2375 container ls
    # Deve listar seus containers (ou nada, se não houver). Funciona!
    ```
2.  **Testar Acesso via API (curl ou PowerShell):**
    ```powershell
    # Usando curl (se instalado):
    # curl http://localhost:2375/containers/json

    # Usando PowerShell:
    Invoke-WebRequest http://localhost:2375/containers/json | ConvertFrom-Json
    # Deve retornar um JSON com a lista de containers. Funciona!
    ```

💡 Analogias ou observações úteis (se houver):

*   Flag `--host`: É como colocar o endereço de entrega específico (`tcp://localhost:2375`) no envelope do seu comando `docker`, em vez de usar o endereço padrão (canal local).

---

📄 Página 299

🔍 Resumo dos principais pontos:

*   Figura 15.2: Confirma acesso bem-sucedido via CLI e API.
*   Reforça o perigo: Ops team ficaria horrorizado com acesso inseguro. Qualquer um pode fazer qualquer coisa.
*   Perigo extra: Risco de escalonamento de privilégios.
    *   Containers Linux rodando como `root` podem ter acesso admin ao host.
    *   Containers Windows: Menos risco de acesso total ao host, mas ainda perigoso.
*   **Contexto de Execução Remota:**
    *   Comandos `--host <remoto>` rodam no *contexto da máquina remota*.
    *   Ex: `docker --host <remoto> run -v /data:/app/data ...` monta `/data` do host *remoto*, não local.
    *   Pode causar confusão (erro "diretório não existe") ou problemas de segurança (acesso a arquivos inesperados no servidor remoto).
    *   Permite "navegar" no filesystem do host remoto se tiver acesso Docker.

🧠 Explicação detalhada e didática:

*   A página volta a martelar nos **riscos graves** do acesso HTTP inseguro. Além da falta de autenticação e criptografia, há o risco de **escalonamento de privilégios**. Se um invasor conseguir executar um contêiner como `root` (administrador do Linux) através de uma API Docker insegura, ele pode conseguir acesso administrativo ao próprio servidor host onde o Docker está rodando, especialmente se puder usar bind mounts. O risco é um pouco menor com contêineres Windows, mas ainda existe.
*   Outro ponto importante é o **contexto de execução**. Quando você usa `--host` para executar um comando em um Docker Engine remoto, qualquer referência a recursos locais no comando (como um caminho de host em um bind mount `-v /meu/local:/app/data`) será interpretada no **contexto da máquina remota**, não da sua máquina local. Isso pode ser confuso (o comando falha porque `/meu/local` não existe no servidor remoto) ou perigoso (o comando monta um diretório sensível do servidor remoto que você não pretendia acessar).

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício demonstrará como um usuário com acesso à API insegura pode explorar o filesystem do host.

💡 Analogias ou observações úteis (se houver):

*   Acesso Inseguro + Bind Mount: É como deixar a porta da sua casa destrancada (API insegura) e também deixar a chave da sua garagem (acesso ao filesystem via `-v`) pendurada na porta. Alguém pode entrar e não só bagunçar a casa (Docker), mas também levar seu carro (acessar/modificar arquivos do host).

---

📄 Página 300

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Demonstrar risco de acesso ao filesystem do host via API insegura.
    *   Rodar contêiner (Linux ou Windows) conectando via TCP (`--host`), usando bind mount para mapear a raiz do disco do host (`/` ou `C:\`) para dentro do contêiner (`/host-drive`).
    *   Comando Linux: `docker --host ... run -it -v /:/host-drive diamol/base`.
    *   Comando Windows: `docker --host ... run -it -v C:\:C:\host-drive diamol/base`.
    *   Dentro do contêiner, listar conteúdo de `/host-drive`.
*   Figura 15.3: Mostra acesso aos arquivos do host de dentro do contêiner.

🧠 Explicação detalhada e didática:

*   Este exercício demonstra de forma prática e alarmante o risco do acesso inseguro combinado com bind mounts.
*   O comando `docker run -v /:/host-drive` (Linux) ou `-v C:\:C:\host-drive` (Windows) instrui o Docker Engine (acessado remotamente via `--host tcp://localhost:2375`) a montar o diretório raiz (`/` ou `C:\`) do próprio **servidor host** dentro do contêiner no caminho `/host-drive` (ou `C:\host-drive`).
*   Como o contêiner `diamol/base` provavelmente roda com privilégios elevados (root/Administrator), o processo dentro do contêiner agora tem acesso de leitura e escrita a **todo o sistema de arquivos do host**.
*   Ao listar o conteúdo de `/host-drive` (ou `C:\host-drive`) de dentro do contêiner, você verá os diretórios e arquivos reais da sua máquina host, comprovando a vulnerabilidade.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação Principal:**
    ```powershell
    # Rodar container Windows montando C:\ do host
    docker --host tcp://localhost:2375 container run -it -v C:\:C:\host-drive diamol/base cmd
    # (Se estiver usando modo Linux no Desktop, adapte para Linux):
    # docker --host tcp://localhost:2375 container run -it -v /:/host-drive diamol/base sh
    ```
*   **Dentro do Contêiner (prompt `C:\>` ou `/ #`):**
    ```
    # Se Windows:
    dir C:\host-drive
    # Se Linux:
    ls /host-drive
    ```
    Você verá listados os diretórios raiz do seu disco C: (ou / no Linux da VM do Docker Desktop).
*   **Sair do Contêiner:** `exit`.

**=> IMPORTANTE: Desfaça a configuração insegura AGORA!**
   *   No Docker Desktop: Settings > General > Desmarque "Expose daemon..." > Apply & Restart.
   *   Se editou `daemon.json`: Remova a linha `"tcp://0.0.0.0:2375"` de `hosts`, salve, reinicie o serviço Docker.

---

📄 Página 301

🔍 Resumo dos principais pontos:

*   Figura 15.3: Confirma acesso ao host filesystem.
*   Exemplo conecta ao localhost, mas risco é real para servidores remotos com API insegura (ex: sistema de folha de pagamento).
*   Reforço: **Nunca habilitar acesso inseguro** (exceto para aprender e desabilitar logo).
*   Instrução para desabilitar acesso HTTP inseguro antes de prosseguir.
*   **15.2 Configuring Docker for secure remote access** (Configurando Docker para acesso remoto seguro)
    *   Duas opções seguras:
        1.  **Mutual TLS (mTLS):** Usa certificados digitais (HTTPS). Servidor tem certificado, cliente *também* tem certificado. Ambos emitidos por uma CA confiada por ambos. Criptografa tráfego, autentica ambos os lados.
        2.  **SSH (Secure Shell):** Usa protocolo SSH padrão (usado para conectar a servidores Linux/Unix, também disponível no Windows). Autenticação via user/pass ou chaves SSH. Criptografa tráfego, autentica cliente.
*   Diagrama (Figura 15.4): Mostra os 3 canais (Local seguro, TCP inseguro, TCP seguro com TLS, SSH seguro).

🧠 Explicação detalhada e didática:

*   A página conclui a demonstração do risco do acesso inseguro e instrui explicitamente o leitor a **desfazer essa configuração imediatamente**.
*   A seção 15.2 introduz as duas alternativas **seguras** para acesso remoto à API Docker:
    1.  **Mutual TLS (mTLS):** Funciona de forma similar ao HTTPS que você usa em sites seguros, mas com uma camada extra de segurança. Não só o servidor Docker prova sua identidade com um certificado, mas o *cliente* (sua CLI `docker`) também precisa apresentar seu próprio certificado válido para provar sua identidade ao servidor. Ambos os certificados devem ser emitidos ou confiados por uma Autoridade Certificadora (CA) comum. Isso garante **criptografia** e **autenticação mútua**. É a forma mais tradicional e robusta para APIs.
    2.  **SSH:** Utiliza o protocolo Secure Shell, muito comum para acesso a servidores remotos. A CLI `docker` pode usar uma conexão SSH existente (ou estabelecer uma nova) para tunelar os comandos da API Docker de forma segura até o Engine remoto. A autenticação é feita pelos mecanismos normais do SSH (senha ou par de chaves pública/privada). Garante **criptografia** e **autenticação do cliente**.
*   A Figura 15.4 compara visualmente os diferentes canais de comunicação.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação Principal:** Certifique-se de que desabilitou o acesso inseguro (desmarcou a caixa no Docker Desktop ou removeu a linha do `daemon.json` e reiniciou).
*   **Preparação para TLS/SSH:** Os próximos exercícios usarão o Play With Docker (PWD) para simular um Engine remoto, pois configurar TLS/SSH no Docker Desktop não é direto. Se você tivesse um servidor Linux/Windows remoto com Docker, os passos seriam:
    *   **TLS:** Gerar certificados CA, servidor e cliente (usando `openssl` ou ferramentas da empresa). Configurar `daemon.json` no servidor com paths dos certs do servidor/CA e flags `tlsverify`. Usar flags `--tlsverify`, `--tlscacert`, `--tlscert`, `--tlskey` na CLI cliente.
    *   **SSH:** Garantir que servidor SSH esteja rodando no host Docker. Garantir que cliente SSH esteja funcionando na máquina cliente. Configurar autenticação SSH (chaves são recomendadas). Usar `-H ssh://user@host` na CLI cliente.

---

📄 Página 302

🔍 Resumo dos principais pontos:

*   **Controle de Acesso:** mTLS (gerenciar certs) vs SSH (gerenciar contas de usuário no servidor).
*   **Limitação Docker Desktop:** Não se pode configurar TLS/SSH diretamente na VM interna. Exercícios usarão **Play with Docker (PWD)**. (Ou use VM/Raspberry Pi remoto se tiver).
*   **Configurando Acesso TLS (mTLS) no PWD:**
    *   Requer gerar certificados/chaves (CA, Server, Client).
    *   Autor já gerou certs compatíveis com PWD.
*   **Exercício Try-it-now:** Configurar nó PWD para usar TLS.
    *   1. Login PWD, criar 1 nó.
    *   2. Criar pasta para certs: `mkdir -p /diamol-certs`.
    *   3. Rodar container utilitário (`diamol/pwd-tls:server`) que monta pastas `/diamol-certs` e `/etc/docker` e copia certs do servidor + `daemon.json` configurado para TLS para dentro do nó PWD.
        *   `docker run -v /diamol-certs:/certs -v /etc/docker:/docker diamol/pwd-tls:server`
    *   4. Reiniciar Docker Engine no PWD para carregar nova config: `pkill dockerd`, `dockerd &>/docker.log &`.

🧠 Explicação detalhada e didática:

*   A página discute brevemente as implicações de gerenciamento de cada abordagem segura (certificados vs. contas de usuário) e reitera que o **Docker Desktop não permite configurar facilmente TLS ou SSH** para seu Engine interno. Por isso, os exercícios práticos usarão o **Play with Docker (PWD)**.
*   O foco se volta para configurar o **mTLS** em um nó do PWD. O processo envolve:
    1.  Obter os arquivos de certificado e chave necessários (CA, certificado do servidor, chave privada do servidor). O autor fornece esses arquivos pré-gerados dentro de uma imagem utilitária (`diamol/pwd-tls:server`) para simplificar.
    2.  Configurar o `daemon.json` do Docker Engine no nó PWD para usar esses certificados e habilitar o TLS na porta TCP 2376 (porta padrão para Docker sobre TLS).
    3.  Reiniciar o Docker Engine no nó PWD.
*   O exercício automatiza os passos 1 e 2 usando o contêiner utilitário. Ele monta o diretório de certificados do host (`/diamol-certs`, que será criado) e o diretório de configuração do Docker do host (`/etc/docker`) dentro do contêiner. O contêiner então copia os arquivos de certificado (CA, server-cert, server-key) para `/diamol-certs` no host e copia um `daemon.json` pré-configurado para TLS para `/etc/docker` no host.
*   O passo final reinicia o `dockerd` (o processo do Docker Engine) no nó PWD para que ele leia a nova configuração e comece a escutar na porta 2376 usando TLS.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação Principal (no PWD):**
    1.  Acesse `https://labs.play-with-docker.com`. Clique "Start", depois "+ ADD NEW INSTANCE".
    2.  No terminal do `node1` que aparece no PWD, execute os comandos do exercício:
        ```bash
        mkdir -p /diamol-certs
        docker run -v /diamol-certs:/certs -v /etc/docker:/docker diamol/pwd-tls:server
        pkill dockerd
        # Espere alguns segundos
        dockerd &>/docker.log &
        # Espere mais alguns segundos para o Docker iniciar
        docker ps # Verifique se o Docker está respondendo
        ```
    *   Agora, o Docker Engine neste nó PWD está (supostamente) escutando na porta 2376 e exigindo autenticação mTLS.

---

📄 Página 303

🔍 Resumo dos principais pontos:

*   Figura 15.5: Mostra a saída dos comandos no PWD configurando TLS.
*   **Expondo Porta PWD:** Precisa expor a porta 2376 do nó PWD para acesso externo (da sua máquina local).
    *   Clicar botão "OPEN PORT" no PWD, digitar 2376.
    *   Nova aba abrirá com erro (ignorar). **Copiar URL da nova aba** (ex: `ip<...>.direct.labs.play-with-docker.com`). Este é o domínio público para acessar a porta 2376 do seu nó PWD.
*   Figura 15.6: Mostra botão "OPEN PORT" no PWD.
*   Nó PWD agora gerenciável remotamente via TLS.
*   **Relação Certificados TLS (Figura 15.7):**
    *   CA (Autoridade Certificadora): Emite/assina certs servidor e cliente. Raiz de confiança.
    *   Cert Servidor: Identifica o servidor Docker. Instalado no Engine.
    *   Cert Cliente: Identifica o usuário/CLI. Usado pelo cliente `docker`.
    *   Chave Privada: Arquivo secreto correspondente a cada certificado (servidor/cliente). Usado para provar posse do certificado e para criptografia.

🧠 Explicação detalhada e didática:

*   Após configurar o Docker Engine no PWD para usar TLS na porta 2376, é preciso tornar essa porta **acessível pela internet**, para que sua máquina local possa se conectar a ela. O Play with Docker tem um recurso para isso: o botão "OPEN PORT".
*   Ao clicar e digitar 2376, o PWD cria um mapeamento e fornece um **URL público único** (que inclui um hostname dinâmico e a porta externa, que pode ou não ser 2376 - mas o PWD faz o mapeamento interno). É **crucial copiar este URL completo** da nova aba que se abre, pois ele será usado como o endereço do host remoto nos próximos comandos `docker`.
*   A Figura 15.7 revisita a estrutura básica do mTLS:
    *   Você tem uma CA (raiz de confiança).
    *   A CA assina um certificado para o servidor (Docker Engine) e um para o cliente (CLI Docker).
    *   Cada certificado tem uma chave privada correspondente.
    *   Para a conexão funcionar:
        *   O cliente deve confiar na CA que emitiu o certificado do servidor.
        *   O servidor deve confiar na CA que emitiu o certificado do cliente.
        *   O cliente deve apresentar seu certificado e provar que possui a chave privada correspondente.
        *   O servidor deve apresentar seu certificado e provar que possui a chave privada correspondente.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Expor Porta no PWD:** No PWD, com o `node1` selecionado, clique no botão "OPEN PORT" acima do terminal. Digite `2376` e clique OK.
2.  **Copiar URL:** Uma nova aba/link aparecerá. **Copie o URL completo** dessa aba (algo como `https://ip-<...>.direct.labs.play-with-docker.com`). Você precisará do **hostname** (a parte entre `https://` e o possível `/`) e talvez da porta se for diferente de 443 (embora PWD geralmente mapeie para 443 ou 80 externamente). Para os comandos `docker`, usaremos o hostname e a porta 2376 (ou a porta externa mapeada pelo PWD). *O livro parece assumir que o hostname PWD resolverá diretamente para a porta 2376, o que pode ou não ser verdade - talvez precise ajustar a porta no comando `--host` depois*. Vamos seguir o livro por enquanto e usar a porta 2376.

---

📄 Página 304

🔍 Resumo dos principais pontos:

*   Figura 15.6: Confirma como abrir a porta no PWD.
*   **Listagem 15.2:** Configuração `daemon.json` aplicada no PWD para TLS.
    ```json
    {
      "hosts": ["unix:///var/run/docker.sock", "tcp://0.0.0.0:2376"], // Ouve local e TCP/TLS
      "tls": true, // TLS desabilitado (?? - Errata: deveria ser true) - CORREÇÃO: Livro impresso pode estar errado, mas contexto exige true.
      "tlscacert": "/diamol-certs/ca.pem", // Path para cert CA
      "tlskey": "/diamol-certs/server-key.pem", // Path para chave servidor
      "tlscert": "/diamol-certs/server-cert.pem" // Path para cert servidor
      // Faltando: "tlsverify": true (Para EXIGIR cert cliente)
    }
    ```
    *(**Nota Importante:** A Listagem 15.2 no PDF tem `"tls": true`, o que está correto. Adicionei `"tlsverify": true` que também é crucial para mTLS)*.
*   Com TLS ativo (e `tlsverify`), não se pode mais usar API/CLI sem fornecer certs cliente válidos (CA, cert cliente, chave cliente).
*   API rejeita conexões sem TLS ou com certs inválidos/não confiáveis.
*   Obter certs cliente: Usar imagem utilitária similar (`diamol/pwd-tls:client`?) para extrair certs cliente para máquina local.

🧠 Explicação detalhada e didática:

*   A Listagem 15.2 mostra a configuração `daemon.json` que (supostamente) foi aplicada ao nó PWD pelo contêiner utilitário. Ela instrui o Docker Engine a:
    *   Ouvir no socket local (`unix://...`) E na porta TCP 2376 (`tcp://...`).
    *   Usar TLS (`"tls": true`).
    *   Usar os arquivos de certificado da CA, certificado do servidor e chave privada do servidor localizados em `/diamol-certs`.
    *   **Crucial (e talvez faltando na listagem original, mas implícito pelo mTLS):** Deveria haver também `"tlsverify": true` para *exigir* que os clientes apresentem um certificado válido assinado pela mesma CA. Sem isso, seria apenas TLS do servidor, não mTLS.
*   A consequência dessa configuração é que qualquer tentativa de conexão à porta 2376 que não use TLS ou não apresente um certificado de cliente válido e confiável (assinado pela `ca.pem`) será **rejeitada** pelo Docker Engine.
*   Para que *sua máquina local* possa se conectar, ela precisa dos arquivos correspondentes do lado do cliente: `ca.pem` (para confiar no servidor), `client-cert.pem` (seu certificado) e `client-key.pem` (sua chave privada). O próximo passo é obter esses arquivos.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você não edita o `daemon.json` diretamente no PWD (o container utilitário fez isso).
*   O próximo exercício mostrará como obter os certificados do cliente e tentar a conexão.

---

📄 Página 305

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Tentar conectar ao PWD (seguro) sem e com certs cliente.
    *   1. Obter/Salvar domínio PWD em variável (`$pwdDomain` ou `pwdDomain`). (Ex: `ip<...>.direct.labs.play-with-docker.com`). **Importante:** Usar apenas o hostname, sem `https://`.
    *   2. Tentar `curl` HTTP (sem TLS): `curl "http://$pwdDomain:2376/containers/json"`. Deve falhar/timeout.
    *   3. Tentar `docker` CLI (sem TLS flags): `docker --host "tcp://$pwdDomain:2376" container ls`. Deve falhar (erro TLS).
    *   4. Extrair certs cliente para pasta local (`/tmp/pwd-certs`):
        *   `mkdir -p /tmp/pwd-certs`
        *   `cd ./ch15/exercises`
        *   `tar -xvf pwd-client-certs.tar -C /tmp/pwd-certs` (Extrai `ca.pem`, `client-cert.pem`, `client-key.pem` de um arquivo .tar).
    *   5. Conectar com CLI `docker` usando flags TLS:
        ```bash
        docker --host "tcp://$pwdDomain:2376" --tlsverify \
          --tlscacert /tmp/pwd-certs/ca.pem \
          --tlscert /tmp/pwd-certs/client-cert.pem \
          --tlskey /tmp/pwd-certs/client-key.pem \
          container ls
        ```
        Deve funcionar e listar containers *no PWD*.
    *   6. Rodar um container no PWD remotamente:
        ```bash
        docker --host "tcp://$pwdDomain:2376" --tlsverify \
          --tlscacert ... --tlscert ... --tlskey ... \
          container run -d -P diamol/apache
        ```
        Deve funcionar.

🧠 Explicação detalhada e didática:

*   Este exercício demonstra a necessidade dos certificados de cliente para acessar um Docker Engine protegido por mTLS.
*   Primeiro, ele salva o hostname público do PWD (obtido na p.303 ao abrir a porta 2376) em uma variável.
*   As tentativas de conexão usando `curl` via HTTP simples e `docker` via TCP simples (sem flags TLS) devem falhar, pois o Engine no PWD agora espera conexões TLS com certificados de cliente válidos.
*   O passo crucial é extrair os arquivos de certificado do cliente (`ca.pem`, `client-cert.pem`, `client-key.pem`) de um arquivo `pwd-client-certs.tar` (fornecido nos materiais do livro) para uma pasta local (ex: `/tmp/pwd-certs` no Linux/Mac, ou um caminho similar no Windows).
*   Finalmente, o comando `docker` é executado novamente, mas desta vez com:
    *   `--host "tcp://$pwdDomain:2376"`: Aponta para o Engine remoto na porta TLS.
    *   `--tlsverify`: Habilita verificação TLS (mútua).
    *   `--tlscacert <path>`: Caminho para o certificado da CA (para verificar o servidor).
    *   `--tlscert <path>`: Caminho para o certificado do cliente.
    *   `--tlskey <path>`: Caminho para a chave privada do cliente.
*   Com essas flags, a conexão TLS mútua deve ser estabelecida com sucesso, e o comando `docker container ls` (e subsequentemente `docker container run`) será executado no Engine remoto do PWD.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Definir Domínio PWD (PowerShell):** (Substitua pelo seu hostname PWD real)
    ```powershell
    $pwdDomain = "ip<...>.direct.labs.play-with-docker.com"
    ```
2.  **Tentar Conexões Inseguras (Devem Falhar):**
    ```powershell
    # Pode dar timeout ou erro de conexão
    Invoke-WebRequest http://$pwdDomain`:2376/containers/json
    # Deve dar erro de TLS handshake ou similar
    docker --host tcp://$pwdDomain`:2376 container ls
    ```
3.  **Extrair Certificados Cliente:**
    ```powershell
    # Criar pasta (ex: no seu diretório de usuário ou C:\DockerLabs)
    $certDir = "$HOME\pwd-certs" # Ou "C:\DockerLabs\pwd-certs"
    mkdir $certDir -ErrorAction SilentlyContinue
    # Navegar para onde está o .tar
    cd C:\DockerLabs\diamol\ch15\exercises
    # Extrair (requer 'tar' no path - Git for Windows inclui; ou use 7-Zip)
    tar -xvf .\pwd-client-certs.tar -C $certDir
    # Verificar se os arquivos .pem estão em $certDir
    Get-ChildItem $certDir
    ```
4.  **Conectar com TLS (PowerShell - comando longo):**
    ```powershell
    docker --host "tcp://$pwdDomain`:2376" --tlsverify `
      --tlscacert "$certDir\ca.pem" `
      --tlscert "$certDir\client-cert.pem" `
      --tlskey "$certDir\client-key.pem" `
      container ls
    # Deve listar containers rodando no PWD (provavelmente nenhum ainda).
    ```
5.  **Rodar Container Remoto:**
    ```powershell
    docker --host "tcp://$pwdDomain`:2376" --tlsverify `
      --tlscacert "$certDir\ca.pem" --tlscert "$certDir\client-cert.pem" --tlskey "$certDir\client-key.pem" `
      container run -d -P diamol/apache
    # Verifique se iniciou: repita o comando 'container ls' do passo 4.
    ```

---

📄 Página 306

🔍 Resumo dos principais pontos:

*   Figura 15.8: Mostra as falhas de conexão sem TLS e o sucesso com as flags TLS.
*   Passar flags TLS em cada comando é inconveniente. Pode-se usar variáveis de ambiente (`DOCKER_HOST`, `DOCKER_TLS_VERIFY`, `DOCKER_CERT_PATH`).
*   **Opção 2: Acesso Remoto Seguro via SSH.**
    *   Vantagem: CLI `docker` usa cliente SSH padrão. **Nenhuma mudança** na config do Docker Engine (`daemon.json`) é necessária.
    *   Desvantagem/Gerenciamento: Autenticação é via servidor SSH. Precisa criar contas de usuário no host Docker para quem precisa de acesso. Gerenciar usuários/chaves SSH.
*   Exercício usará PWD novamente para SSH.

🧠 Explicação detalhada e didática:

*   A página confirma o sucesso da conexão TLS e menciona que, em vez de repetir todas as flags `--tls...` a cada comando, você pode definir variáveis de ambiente como `DOCKER_HOST`, `DOCKER_TLS_VERIFY=1`, e `DOCKER_CERT_PATH` (apontando para o diretório com os arquivos `ca.pem`, `cert.pem`, `key.pem`) para que a CLI `docker` as use automaticamente.
*   Introduz a segunda opção segura: **tunelamento via SSH**.
*   **Como funciona:** A CLI `docker`, ao receber um host no formato `ssh://user@hostname`, usa o cliente SSH local para estabelecer uma conexão segura com o servidor `hostname` como o `user` especificado. Uma vez conectado via SSH, a CLI tunela os comandos da API Docker através dessa conexão segura para o Docker Engine rodando no servidor remoto (que ainda está ouvindo no seu socket/pipe local padrão).
*   **Vantagens:**
    *   **Nenhuma configuração extra no Docker Engine remoto** é necessária (ele continua ouvindo apenas localmente).
    *   Usa o mecanismo de autenticação robusto e familiar do SSH (senhas ou chaves).
*   **Desvantagens:**
    *   Requer que um **servidor SSH** esteja rodando no host Docker remoto.
    *   Requer que **contas de usuário** sejam criadas nesse host para cada pessoa/sistema que precise de acesso Docker via SSH. O gerenciamento de acesso ao Docker se torna o gerenciamento de acesso SSH ao servidor.
    *   Cliente SSH precisa estar disponível na máquina que executa `docker`.
*   A configuração para o próximo exercício usará novamente o PWD, que já vem com SSH pré-configurado entre os nós.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Usar Variáveis de Ambiente para TLS (PowerShell):**
    ```powershell
    $env:DOCKER_HOST = "tcp://$pwdDomain`:2376"
    $env:DOCKER_TLS_VERIFY = "1"
    $env:DOCKER_CERT_PATH = "$HOME\pwd-certs" # Ou onde extraiu
    # Agora pode usar 'docker container ls', 'docker run...' sem flags extras
    docker container ls
    # Limpar variáveis depois (ou fechar/abrir PowerShell):
    # Remove-Variable DOCKER_HOST, DOCKER_TLS_VERIFY, DOCKER_CERT_PATH
    ```
*   **Conexão SSH no Windows:** O Windows 10/11 inclui um cliente OpenSSH (`ssh.exe`). Você pode usá-lo no PowerShell/CMD. A flag para o Docker seria `-H ssh://user@host`.

---

📄 Página 307

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Usar SSH para gerenciar Docker remoto no PWD.
    *   1. No PWD, criar 2º nó (`node2`). Anotar IP do `node1`.
    *   2. No `node2`, salvar IP do `node1` em variável (`node1ip`).
    *   3. Testar conexão SSH direta: `ssh root@$node1ip`. Deve conectar (sem senha no PWD). Sair (`exit`).
    *   4. Listar containers *locais* no `node2`: `docker container ls`.
    *   5. Listar containers *remotos* no `node1` usando SSH: `docker -H ssh://root@$node1ip container ls`.

🧠 Explicação detalhada e didática:

*   Este exercício demonstra o uso da flag `-H` (ou `--host`) com o protocolo `ssh://` para direcionar comandos `docker` para um Engine remoto através de uma conexão SSH.
*   O ambiente PWD é ideal para isso, pois as instâncias já vêm com SSH configurado para permitir conexão entre elas como usuário `root` usando chaves pré-compartilhadas (sem necessidade de senha).
*   **Passos:**
    1.  Cria-se um segundo nó no PWD (`node2`).
    2.  Obtém-se o IP do primeiro nó (`node1`).
    3.  No terminal do `node2`, testa-se a conexão SSH direta para `node1` para confirmar que funciona.
    4.  Executa-se `docker container ls` sem `-H`. Isso mostra os contêineres rodando *localmente* no `node2` (provavelmente nenhum).
    5.  Executa-se `docker -H ssh://root@$node1ip container ls`. Isso instrui a CLI `docker` no `node2` a:
        *   Conectar-se via SSH ao `node1` como usuário `root`.
        *   Uma vez conectado, enviar o comando `container ls` para o Docker Engine que está rodando *localmente* no `node1` (via socket/pipe).
        *   Retornar o resultado para o terminal do `node2`.
*   O resultado esperado é que o último comando liste os contêineres que estão rodando no `node1` (como o `diamol/apache` iniciado no exercício TLS).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Executar no PWD:**
    1.  Se só tem 1 nó no PWD, clique "+ ADD NEW INSTANCE" para criar `node2`.
    2.  Selecione `node1`. Anote seu IP (ex: `192.168.0.18`).
    3.  Selecione `node2`. Execute no terminal dele:
        ```bash
        node1ip=<IP_DO_NODE1> # Substitua pelo IP real
        echo $node1ip # Verifique
        ssh root@$node1ip # Teste SSH
        # Se conectar (prompt muda para node1), digite 'exit' para voltar a node2
        exit
        docker container ls # Deve estar vazio (containers locais de node2)
        docker -H ssh://root@$node1ip container ls # Deve listar containers de node1
        ```

---

📄 Página 308

🔍 Resumo dos principais pontos:

*   Figura 15.9: Mostra saída do exercício SSH no PWD, confirmando conexão e listagem remota.
*   PWD simplifica SSH (pré-configurado). Ambiente real requer setup de usuários/chaves.
*   **Opinião Ops sobre Docker via SSH:** Mista.
    *   Pró: Mais fácil que gerenciar certs TLS, familiar para admins Linux.
    *   Contra: Requer dar acesso SSH ao servidor para quem precisa de acesso Docker (pode ser privilégio demais). Menos familiar/padrão para admins Windows (vs. TLS talvez).
*   **Resumo Segurança:** TLS e SSH oferecem **Criptografia** e **Autenticação**. *NÃO* oferecem **Autorização** (controle fino de permissões) nem **Auditoria** (log de quem fez o quê).
*   Modelo Docker padrão: "Tudo ou Nada" (conectou? pode fazer tudo).
*   Importante considerar quem precisa de acesso a quais ambientes.
*   Risco: Fácil errar e executar comando destrutivo no ambiente errado (ex: prod) se contexto CLI estiver apontando para lá.
*   **15.3 Using Docker Contexts to work with remote engines** (Usando Contextos Docker para trabalhar com engines remotos)
    *   Passar `--host` e flags TLS/SSH a cada comando é inconveniente.
    *   Solução Docker: **Contexts**.

🧠 Explicação detalhada e didática:

*   A página discute as implicações operacionais de usar SSH para acesso remoto ao Docker. É mais simples que TLS em termos de setup inicial se o SSH já estiver estabelecido, mas mistura o gerenciamento de acesso ao servidor com o gerenciamento de acesso ao Docker, o que pode não ser ideal em termos de separação de privilégios.
*   Reafirma uma limitação importante de segurança, tanto para TLS quanto para SSH no Docker CE/EE básico: eles controlam *quem* pode conectar (autenticação) e protegem a comunicação (criptografia), mas uma vez conectado, o usuário tem **acesso total** à API Docker. Não há um sistema de permissões granular (autorização) embutido para dizer "usuário A só pode listar contêineres, usuário B pode criar/remover". (Soluções como Docker UCP ou K8s RBAC oferecem isso). Também não há um log de auditoria detalhado nativo.
*   Isso torna crucial ter cuidado ao conceder acesso remoto e também para o próprio usuário ao trabalhar com múltiplos ambientes, para não executar comandos no lugar errado.
*   A seção 15.3 introduz os **Docker Contexts** como uma maneira de simplificar o trabalho com múltiplos Docker Engines (locais ou remotos, via TLS ou SSH).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Comando para criar contexto:** `docker context create <nome_contexto> --docker "<opções_de_conexão>"`.
*   As `<opções_de_conexão>` serão `host=tcp://...` mais `ca=...`, `cert=...`, `key=...` para TLS, ou `host=ssh://...` para SSH.

💡 Analogias ou observações úteis (se houver):

*   Docker Contexts: É como salvar diferentes servidores remotos nos seus "Favoritos" ou "Conexões Salvas" em um cliente SSH ou RDP. Em vez de digitar o endereço e as credenciais toda vez, você só seleciona o nome salvo (contexto) e a CLI `docker` sabe como se conectar.

---

📄 Página 309

🔍 Resumo dos principais pontos:

*   **Docker Context:** Armazena detalhes de conexão (host, TLS, SSH) localmente, associados a um nome.
*   Permite criar múltiplos contextos e alternar entre eles.
*   **Exercício Try-it-now:** Criar contexto TLS para PWD.
    *   Comando: `docker context create pwd-tls --docker "host=tcp://$pwdDomain:2376,ca=/tmp/pwd-certs/ca.pem,cert=/tmp/pwd-certs/client-cert.pem,key=/tmp/pwd-certs/client-key.pem"`.
    *   (Exemplo SSH também mostrado: `... --docker "host=ssh://user@server"`).
    *   Listar contextos: `docker context ls`.
*   Figura 15.10: Mostra saída do `context create` e `context ls`.
*   `context ls` mostra contexto `default` (engine local) e o novo `pwd-tls`. Asterisco `*` indica contexto ativo. Mostra endpoint Docker e K8s (se aplicável).

🧠 Explicação detalhada e didática:

*   A página explica como criar um Docker Context usando o comando `docker context create`.
*   Os parâmetros chave são:
    *   `<nome_contexto>`: Um nome que você escolhe para identificar essa conexão (ex: `pwd-tls`, `servidor-teste-ssh`, `producao-cluster-a`).
    *   `--docker "<string_de_conexão>"`: Uma string contendo as opções de conexão para o Docker Engine, separadas por vírgula.
        *   Para **TLS**, inclui `host=tcp://...`, `ca=...`, `cert=...`, `key=...`.
        *   Para **SSH**, inclui `host=ssh://...`.
        *   Para o engine **local** padrão, geralmente é `host=npipe://...` (Windows) ou `host=unix://...` (Linux/Mac).
*   O exercício cria um contexto chamado `pwd-tls` usando os detalhes de conexão TLS para o PWD (domínio e caminhos dos certificados locais).
*   O comando `docker context ls` lista todos os contextos definidos localmente, indicando qual está ativo no momento com um asterisco (`*`).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Contexto TLS para PWD:** (Certifique-se que `$pwdDomain` e `$certDir` estão definidos corretamente na sua sessão PowerShell. `$certDir` deve ser o path *absoluto* para a pasta com os .pem).
    ```powershell
    # Use ` para continuar comando em múltiplas linhas no PowerShell
    docker context create pwd-tls --docker "host=tcp://$pwdDomain`:2376,`
    ca=$($certDir -replace '\\', '/')/ca.pem,`
    cert=$($certDir -replace '\\', '/')/client-cert.pem,`
    key=$($certDir -replace '\\', '/')/client-key.pem"
    # Nota: Pode ser necessário usar / em vez de \ nos paths dos certs
    # Ou use paths absolutos diretamente na string
    ```
2.  **Listar Contextos:**
    ```powershell
    docker context ls
    ```
    Você deve ver o contexto `default` e o novo `pwd-tls`. O `default` ainda estará ativo (`*`).

---

📄 Página 310

🔍 Resumo dos principais pontos:

*   Figura 15.10: Confirma criação do contexto `pwd-tls`.
*   Contextos guardam info para TLS ou SSH.
*   Contextos podem apontar para máquinas locais ou remotas.
*   **Como Usar/Trocar Contextos:**
    1.  **Temporário (por sessão):** Variável de ambiente `DOCKER_CONTEXT`. Preferível, pois não afeta outras sessões/scripts.
    2.  **Permanente (global):** Comando `docker context use <nome_contexto>`. Afeta todas as sessões até ser trocado novamente. **Não recomendado** para trocas frequentes.
*   **Exercício Try-it-now:** Demonstrar troca de contexto.
    *   1. Setar `DOCKER_CONTEXT` para `pwd-tls` (comando `env:` Windows, `export` Linux).
    *   2. Listar contextos (`docker context ls` mostra `*` em `pwd-tls` *nesta sessão*).
    *   3. Listar containers (`docker container ls` agora lista os do PWD).
    *   4. Trocar permanentemente para `default`: `docker context use default`.
    *   5. Listar containers (`docker container ls` agora lista os locais).
*   Figura 15.11: Mostra saída do exercício. **Alerta:** Figura mostra que mesmo após `context use default`, `context ls` ainda mostra `*` em `pwd-tls` (provavelmente erro na figura ou comportamento confuso). Variável `DOCKER_CONTEXT` tem precedência.

🧠 Explicação detalhada e didática:

*   Com os contextos criados, a página explica como **ativá-los** para direcionar seus comandos `docker`. Existem duas maneiras:
    1.  **Variável de Ambiente `DOCKER_CONTEXT`:** Definir esta variável (ex: `DOCKER_CONTEXT=pwd-tls`) faz com que a CLI `docker`, *naquela sessão de terminal específica*, use as configurações do contexto nomeado. Isso **não** afeta outras janelas de terminal ou o padrão global. É a forma **recomendada** para trabalho interativo, pois é explícita e temporária.
    2.  **Comando `docker context use <nome>`:** Este comando altera o contexto ativo **globalmente** para a CLI Docker naquela máquina. Qualquer novo terminal que você abrir usará esse contexto por padrão. Isso pode ser perigoso se você esquecer qual contexto está ativo e executar comandos destrutivos no ambiente errado. Use com cautela.
*   O exercício demonstra ambas as formas. Ele primeiro ativa `pwd-tls` via variável de ambiente, lista os contêineres remotos, depois tenta voltar ao padrão com `docker context use default` e lista os contêineres locais.
*   **Importante:** A página (e Figura 15.11) aponta um comportamento potencialmente confuso ou um erro na ilustração: a variável `DOCKER_CONTEXT` **tem precedência** sobre o contexto definido por `docker context use`. Se `DOCKER_CONTEXT` estiver definida, `docker context ls` mostrará o asterisco (`*`) nela, e os comandos `docker` usarão esse contexto, mesmo que `docker context use` tenha sido chamado para definir outro como padrão global. Para realmente voltar ao padrão global, você precisaria *desdefinir* a variável `DOCKER_CONTEXT`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Ativar Contexto PWD (Temporário):**
    ```powershell
    $env:DOCKER_CONTEXT = "pwd-tls"
    docker context ls # Deve mostrar * em pwd-tls
    docker container ls # Deve listar containers do PWD (ex: diamol/apache)
    ```
2.  **Tentar Voltar ao Padrão (Global):**
    ```powershell
    docker context use default
    docker context ls # * AINDA estará em pwd-tls devido à variável de ambiente!
    docker container ls # AINDA lista containers do PWD!
    ```
3.  **Realmente Voltar ao Padrão (Desdefinindo Variável):**
    ```powershell
    Remove-Variable DOCKER_CONTEXT # Ou feche e reabra o PowerShell
    docker context ls # Agora * deve estar em default
    docker container ls # Agora lista containers locais
    ```

---

📄 Página 311

🔍 Resumo dos principais pontos:

*   Figura 15.11: Mostra a confusão entre `context use` e `DOCKER_CONTEXT`.
*   `docker context use` muda padrão global; `DOCKER_CONTEXT` sobrescreve temporariamente e tem precedência.
*   Recomendação: Usar sempre `DOCKER_CONTEXT` para trocas, deixar `default` como global. Evita erros.
*   Acesso remoto não deve ser frequente em produção; ideal é automação via CI/CD.
*   **15.4 Adding continuous deployment to your CI pipeline** (Adicionando implantação contínua ao seu pipeline CI)
    *   Com acesso remoto seguro configurado, pode-se implementar pipeline CI/CD completo.
    *   CI (Cap. 11): Build -> Test -> Push para Registro.
    *   CD: Deploy para UAT -> (Testes UAT/Aprovação) -> Deploy para Prod.
    *   CD usa acesso remoto para conectar aos engines UAT/Prod.
    *   Pipeline usa `docker`/`docker-compose` com `--host` (ou Contexto) e credenciais seguras.
    *   **Credenciais:** NÃO devem ir no source control. Usar gerenciador de secrets do servidor CI (Jenkins, GitLab Secrets, GitHub Secrets, etc.).

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre contextos, recomendando o uso da variável de ambiente `DOCKER_CONTEXT` para trocas temporárias e seguras entre engines.
*   A seção 15.4 conecta o acesso remoto seguro ao objetivo final: criar um pipeline de **Implantação Contínua (CD)** que estende o pipeline de Integração Contínua (CI).
*   **Fluxo CI/CD:**
    1.  **CI:** Código -> Build -> Teste Unitário -> Imagem Docker -> Push para Registro.
    2.  **CD - Estágio UAT:** O pipeline CI/CD se conecta ao Docker Engine do ambiente de **UAT (User Acceptance Testing)** usando acesso remoto seguro (TLS ou SSH). Executa `docker stack deploy` (ou `docker-compose up`) para implantar a imagem recém-construída do registro. (Opcional: rodam-se testes de aceitação automatizados).
    3.  **CD - Aprovação (Opcional):** O pipeline pode pausar, aguardando aprovação manual (de QA, PO, etc.) antes de prosseguir para produção.
    4.  **CD - Estágio Produção:** Se aprovado, o pipeline se conecta ao Docker Engine (ou cluster manager) do ambiente de **Produção** usando acesso remoto seguro. Executa `docker stack deploy` para implantar a imagem validada.
*   A chave para os estágios CD é a capacidade do pipeline de se conectar remotamente e de forma segura aos ambientes de destino. As **credenciais** (certificados TLS cliente, chaves SSH, senhas) necessárias para essas conexões remotas *nunca* devem ser armazenadas no código fonte. Devem ser gerenciadas pelo sistema de CI/CD (ex: Jenkins Credentials, GitHub Secrets) e injetadas no pipeline de forma segura apenas quando necessário.

💻 Exemplos práticos (para meu ambiente Windows):

*   O pipeline Jenkins que será usado a seguir incluirá estágios 'Deploy UAT' e 'Deploy Prod' que usam as credenciais TLS (armazenadas no Jenkins) para se conectar ao PWD (simulando UAT e Prod) e executar `docker-compose`.

---

📄 Página 312

🔍 Resumo dos principais pontos:

*   **Infraestrutura CI/CD Local (com CD para PWD):**
    *   Similar ao Cap. 11 (Gogs, Registry, Jenkins em containers).
    *   **Novidade:** Scripts de inicialização do Jenkins agora criam credenciais Jenkins a partir dos arquivos de certificado PWD cliente (que devem estar na máquina local onde Compose roda).
    *   CD stages do pipeline usarão essas credenciais para deployar no PWD.
*   **Exercício Try-it-now:** Iniciar infraestrutura CI/CD local.
    *   Navegar para `ch15/exercises/infrastructure`.
    *   Rodar `docker-compose ... up -d` (com override windows/linux).
    *   Acessar Jenkins Credentials (`http://localhost:8080/credentials`). Login `diamol`/`diamol`.
    *   Verificar se credenciais `docker-ca.pem`, `docker-cert.pem`, `docker-key.pem` (tipo "Secret file") foram criadas.
*   Figura 15.12: Mostra as credenciais baseadas em arquivo no Jenkins UI.

🧠 Explicação detalhada e didática:

*   Para demonstrar o pipeline CD, uma nova infraestrutura local (Gogs, Registry, Jenkins) é iniciada, muito similar à do Capítulo 11.
*   A principal diferença está no contêiner Jenkins (`diamol/jenkins` provavelmente atualizado). Agora, seus scripts de inicialização não apenas configuram o job, mas também procuram pelos arquivos de certificado do cliente PWD (que você extraiu para `/tmp/pwd-certs` ou `$HOME\pwd-certs` anteriormente) na máquina host (onde o `docker-compose up` é executado) e os carregam como **Credenciais** do tipo "Secret file" dentro do próprio Jenkins.
*   Isso permite que o pipeline Jenkins, ao executar os estágios de CD, acesse esses certificados de forma segura (através do mecanismo `withCredentials` do Jenkins) para autenticar as conexões TLS com o Docker Engine remoto no PWD.
*   O exercício instrui a iniciar essa nova infraestrutura e verificar, na interface do Jenkins, se as credenciais dos certificados foram carregadas corretamente.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Certificados Locais:** Verifique se os arquivos `ca.pem`, `client-cert.pem`, `client-key.pem` ainda existem na pasta que você usou no exercício da p.305 (ex: `$HOME\pwd-certs`). O Jenkins Compose file provavelmente montará essa pasta.
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch15\exercises\infrastructure
    ```
3.  **Parar Infra Anterior (se rodando):**
    ```powershell
    # docker-compose -f ... down # Use os arquivos do cap 11 se necessário
    ```
4.  **Iniciar Nova Infra:**
    ```powershell
    docker-compose -f ./docker-compose.yml -f ./docker-compose-windows.yml up -d
    ```
5.  **Verificar Credenciais Jenkins:**
    *   Aguarde Jenkins iniciar completamente (pode levar um minuto).
    *   Acesse `http://localhost:8080`.
    *   Login `diamol` / `diamol`.
    *   No menu esquerdo, vá para "Manage Jenkins" -> "Manage Credentials".
    *   Em "Stores scoped to Jenkins", clique em "Jenkins".
    *   Em "Global credentials", você deve ver três entradas: `docker-ca.pem`, `docker-cert.pem`, `docker-key.pem`, tipo "Secret file". (Como na Figura 15.12).

---

📄 Página 313

🔍 Resumo dos principais pontos:

*   Figura 15.12: Confirma credenciais no Jenkins.
*   Infra rodando, mas Gogs precisa de setup manual novamente (Install, Register user `diamol`, Create repo `diamol`). Ref Cap 11, Figuras 11.3-11.5.
*   Pipeline a ser rodado: Builda nova versão do app `timecheck` (Cap 12).
*   Precisa editar `Jenkinsfile` para adicionar **seu** domínio PWD.
*   Deploy CD usará PWD como ambiente UAT e Prod (simulação).
*   **Exercício Try-it-now:** Configurar Gogs, editar Jenkinsfile, push para Gogs, rodar build Jenkins.
    *   1. Setup Gogs (`http://localhost:3000`).
    *   2. Editar `Jenkinsfile` (ou `Jenkinsfile.windows`) em `ch15/exercises`.
        *   Achar seção `environment { ... }`.
        *   Substituir `pwd-domain` em `UAT_ENGINE` e `PROD_ENGINE` pelo seu domínio PWD real (copiado na p.303), **incluindo porta `:80`**. (PWD mapeia 80 externamente para 2376 interno).
    *   3. Commit e Push para Gogs local: `git remote add ch15 ...`, `git commit -a ...`, `git push ch15 ...`.
    *   4. Iniciar build no Jenkins (`http://localhost:8080/job/diamol/`, clicar "Build Now").

🧠 Explicação detalhada e didática:

*   Com a infraestrutura rodando e as credenciais carregadas no Jenkins, os últimos passos de setup são:
    1.  **Configurar o Gogs:** Repetir o processo manual do Cap. 11 (instalar, registrar usuário `diamol`, criar repo `diamol`).
    2.  **Configurar o Pipeline:** O pipeline Jenkins (definido no `Jenkinsfile`) precisa saber o endereço dos Docker Engines remotos para os estágios de CD. Esses endereços são definidos como variáveis de ambiente (`UAT_ENGINE`, `PROD_ENGINE`) dentro do próprio `Jenkinsfile`. Você precisa **editar** o `Jenkinsfile` (na sua cópia local do código) e substituir o placeholder `pwd-domain` pelo **hostname público real do seu PWD** (obtido na p.303), seguido por `:80` (pois o PWD expõe a porta 2376 mapeada para a porta 80 externamente).
    3.  **Enviar Código Atualizado:** Fazer `commit` da mudança no `Jenkinsfile` e `push` para o Gogs local.
    4.  **Iniciar Build:** Disparar o build manualmente no Jenkins.
*   O pipeline então executará: CI (Build, Test, Push to local Registry) -> CD (Deploy to UAT via TLS para PWD) -> Pausa para Aprovação -> CD (Deploy to Prod via TLS para PWD).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Setup Gogs:** Repita os passos das p.212-214 (Install, Register `diamol`, Create repo `diamol`).
2.  **Editar Jenkinsfile:**
    *   Abra `C:\DockerLabs\diamol\ch15\exercises\Jenkinsfile.windows` (ou `Jenkinsfile` se usar Linux no PWD) no VS Code.
    *   Localize a seção `environment { ... }`.
    *   Substitua `<your-pwd-domain-from-the-address-bar>` pelo seu hostname PWD real (sem `https://`). Mantenha `:80` no final. Exemplo:
        ```groovy
        UAT_ENGINE = "ip172-18-0-59-bngh3ebjagq000ddjbv0.direct.labs.play-with-docker.com:80"
        PROD_ENGINE = "ip172-18-0-59-bngh3ebjagq000ddjbv0.direct.labs.play-with-docker.com:80"
        ```
    *   Salve o arquivo.
3.  **Commit & Push:**
    ```powershell
    # Estando na raiz C:\DockerLabs\diamol
    git remote remove ch15 # Remover se existir de antes
    git remote add ch15 http://localhost:3000/diamol/diamol.git
    git add .\ch15\exercises\Jenkinsfile.windows # Ou Jenkinsfile
    git commit -m "Update PWD domain for CD"
    git push ch15 # Use creds Gogs (diamol/sua_senha_gogs)
    ```
4.  **Iniciar Build Jenkins:** Vá para `http://localhost:8080/job/diamol/` e clique "Build Now".

---

📄 Página 314

🔍 Resumo dos principais pontos:

*   Figura 15.13: Mostra pipeline Jenkins pausado no estágio "Await Approval" após deploy UAT.
*   Pipeline: CI (Get Code, Build, Test, Push Registry) -> CD UAT -> Pause -> CD Prod.
*   Pausa para aprovação manual é opcional; bom para começar com CD.
*   **Exercício Try-it-now:** Verificar deploy UAT no PWD e aprovar no Jenkins.
    *   1. No PWD (`node1`), verificar container UAT: `docker container ls`, `docker container logs timecheck-uat_timecheck_1`.
    *   2. No Jenkins UI, clicar na caixa azul "Await Approval".
    *   3. Na janela popup, clicar "Do It!" (ou "Proceed"). Pipeline continua para deploy Prod.

🧠 Explicação detalhada e didática:

*   A página descreve o estado do pipeline após a conclusão bem-sucedida dos estágios de CI e do primeiro estágio de CD (Deploy UAT). O pipeline agora está **pausado**, esperando uma intervenção manual no estágio "Await Approval" (Figura 15.13). Essa pausa é uma feature comum em pipelines CD para permitir uma verificação final ou aprovação formal antes de implantar em produção.
*   O exercício instrui a fazer essa verificação e aprovação:
    1.  Conectar-se ao ambiente de UAT (o nó PWD) e usar comandos `docker` para confirmar que a aplicação (`timecheck-uat`) foi implantada corretamente e está funcionando.
    2.  Voltar à interface do Jenkins e clicar na etapa pausada para dar a aprovação, permitindo que o pipeline prossiga para o último estágio: Deploy em Produção.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Deploy UAT no PWD:**
    *   Vá para a aba do navegador com seu PWD (`node1`).
    *   Execute no terminal do PWD:
        ```bash
        docker container ls
        # Procure por um container com nome iniciando 'timecheck-uat_timecheck_...'
        docker container logs <nome_container_uat> # Verifique se está logando
        ```
2.  **Aprovar no Jenkins:**
    *   Vá para a aba do navegador com o Jenkins (`http://localhost:8080/job/diamol/`).
    *   Clique no estágio "Await Approval" que está piscando ou destacado.
    *   Clique no botão "Proceed" (ou similar) na janela que aparece.
*   O pipeline agora continuará e executará o estágio "Prod".

---

📄 Página 315

🔍 Resumo dos principais pontos:

*   Figura 15.14: Mostra verificação no PWD e aprovação no Jenkins.
*   Estágios CD são simples: chamam script que roda `docker-compose` (ou `docker stack deploy`).
*   Script recebe creds (certs TLS) e host remoto via variáveis de ambiente.
*   Separar lógica Docker/Compose (nos scripts) da orquestração do pipeline (Jenkinsfile) -> portabilidade.
*   **Listagem 15.3:** Trecho do `Jenkinsfile` e do script `04-uat.bat` para deploy UAT.
    *   **Jenkinsfile (Estágio 'UAT'):**
        *   `withCredentials([...])`: Bloco Jenkins que torna credenciais (certs) disponíveis como variáveis de ambiente (`$ca`, `$cert`, `$key`) *dentro* do bloco.
        *   `dir(...)`: Muda diretório.
        *   `sh '...'`: Executa script `04-uat.bat`.
    *   **Script `04-uat.bat`:**
        *   Usa `docker-compose`.
        *   Passa host remoto (`--host tcp://$UAT_ENGINE`), flags TLS (`--tlsverify`, `--tlscacert $ca`, etc.) usando variáveis providas pelo `withCredentials`.
        *   Passa nome do projeto (`-p timecheck-uat`) e arquivos Compose (`-f ...`).
        *   Executa `up -d`.

🧠 Explicação detalhada e didática:

*   A página explica a implementação dos estágios de CD no pipeline Jenkins. A lógica segue o mesmo padrão dos estágios CI: o `Jenkinsfile` orquestra a execução de scripts externos.
*   A **Listagem 15.3** mostra como isso funciona para o deploy em UAT:
    *   **`Jenkinsfile`:** Usa a funcionalidade `withCredentials` do Jenkins. Isso busca as credenciais dos certificados (que foram carregadas de arquivos locais na inicialização do Jenkins) e as expõe temporariamente como variáveis de ambiente (`$ca`, `$cert`, `$key`) apenas para os comandos dentro do bloco `withCredentials`. Em seguida, ele executa o script `04-uat.bat`.
    *   **`04-uat.bat`:** Este script contém o comando `docker-compose` real. Crucialmente, ele usa:
        *   A variável `$UAT_ENGINE` (definida na seção `environment` do Jenkinsfile, apontando para o PWD) na flag `--host`.
        *   As variáveis `$ca`, `$cert`, `$key` (vindas do `withCredentials`) nas flags `--tlscacert`, `--tlscert`, `--tlskey`.
        *   Flags `-p` e `-f` para definir o nome do projeto e os arquivos Compose a serem usados para o ambiente UAT.
        *   O comando `up -d` para iniciar/atualizar a aplicação.
*   Essa abordagem mantém os scripts (`.bat`) genéricos, recebendo os detalhes de conexão e credenciais do ambiente do pipeline, o que facilita a adaptação para outras ferramentas de CI/CD.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Explorar Arquivos:**
    *   Abra `ch15/exercises/Jenkinsfile.windows` (ou `Jenkinsfile`). Localize o `stage('UAT')` e o bloco `withCredentials`.
    *   Abra `ch15/exercises/ci/04-uat.bat`. Veja o comando `docker-compose` completo. Note como ele usa as variáveis de ambiente (`%UAT_ENGINE%`, `%ca%`, etc. na sintaxe Batch/Windows, ou `$VAR` na sintaxe sh/Linux).

---

📄 Página 316

🔍 Resumo dos principais pontos:

*   Estágio Prod é quase idêntico ao UAT, só muda arquivos Compose.
*   Após pipeline completar, ambos deploys (UAT e Prod) estarão rodando no PWD.
*   **Exercício Try-it-now:** Verificar deploys no PWD após pipeline concluir.
    *   Comandos PWD: `docker container ls`, `docker container logs timecheck-prod_timecheck_1`.
*   Figura 15.15: Mostra `ls` e `logs` no PWD, confirmando containers `uat` e `prod` rodando.
*   Sucesso: Pipeline CI/CD completo, deployando para "remoto" (PWD) a partir de infra CI local (containers).

🧠 Explicação detalhada e didática:

*   A página confirma que o estágio de deploy em Produção no pipeline segue exatamente o mesmo padrão do UAT, apenas usando um conjunto diferente de arquivos Compose (provavelmente `docker-compose-prod.yml`) e um nome de projeto diferente (`timecheck-prod`).
*   O exercício final instrui a verificar o resultado no PWD após a conclusão de todo o pipeline Jenkins. Você deve encontrar contêineres rodando para *ambos* os ambientes, UAT e Produção (embora, neste exemplo simulado, eles estejam rodando no mesmo Docker Engine do PWD).
*   Isso conclui a demonstração de um pipeline CI/CD funcional que utiliza acesso remoto seguro para implantação.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Pipeline Jenkins:** Volte para `http://localhost:8080/job/diamol/`. O último build deve estar verde (Success) e todos os estágios (incluindo UAT, Await Approval, Prod) devem ter sido concluídos.
2.  **Verificar Deploys no PWD:**
    *   Vá para a aba do PWD (`node1`).
    *   Execute no terminal:
        ```bash
        docker container ls
        # Deve listar containers com nomes como 'timecheck-uat_...' e 'timecheck-prod_...'
        docker container logs timecheck-prod_timecheck_1 # Verifique logs da instância 'prod'
        docker container logs timecheck-uat_timecheck_1 # Verifique logs da instância 'uat'
        ```

---

📄 Página 317

🔍 Resumo dos principais pontos:

*   Figura 15.15: Confirma containers UAT e Prod rodando no PWD.
*   Pipeline CI/CD poderoso, requer apenas Docker server para infra e para targets.
*   **Alerta Final:** Mesmo com acesso seguro (TLS/SSH), modelo de acesso Docker ainda é "Tudo ou Nada".
*   **15.5 Understanding the access model for Docker** (Entendendo o modelo de acesso para Docker)
    *   Não precisa de seção inteira, é simples, mas importante.
    *   Segurança (TLS/SSH) cobre **Criptografia** e **Autenticação**.
    *   **NÃO** cobre **Autorização** (permissões granulares) nem **Auditoria** (quem fez o quê).
    *   Modelo Docker CE/EE básico: Conectou? Pode fazer tudo.

🧠 Explicação detalhada e didática:

*   A página celebra o sucesso do pipeline CI/CD, mas imediatamente introduz uma ressalva de segurança importante na seção 15.5: o **modelo de acesso "tudo ou nada"** do Docker Engine padrão (Community Edition ou mesmo Enterprise Edition básico sem UCP).
*   Mesmo usando conexões seguras com TLS ou SSH, que garantem que apenas usuários autenticados possam se conectar e que a comunicação seja criptografada, uma vez que um usuário está conectado à API Docker, ele tem **permissões totais** para realizar qualquer operação Docker (criar, parar, remover contêineres, volumes, redes, secrets, configs, imagens, etc.).
*   Não existe um mecanismo nativo no Docker Engine básico para dizer "Usuário Alice só pode listar contêineres no ambiente de teste" ou "Pipeline CI só pode fazer deploy, não pode remover volumes".
*   Isso significa que a segurança do acesso remoto depende fortemente de *limitar* quem tem as credenciais (certs TLS, chaves SSH, senhas) para se conectar aos Docker Engines, especialmente os de produção.

💻 Exemplos práticos (para meu ambiente Windows):

*   Com as credenciais TLS do PWD que você usou, você poderia executar `docker --host ... network rm <alguma_rede>` ou `docker --host ... volume prune` no PWD, mesmo que sua intenção fosse apenas fazer deploy. Não há restrição de permissão na API.

💡 Analogias ou observações úteis (se houver):

*   Acesso Docker Básico: É como ter uma chave mestra super segura (TLS/SSH) para entrar no prédio. A chave garante que só pessoas autorizadas entram (autenticação) e que ninguém ouve suas conversas lá dentro (criptografia). Mas, uma vez lá dentro, a chave abre *todas* as portas (sem autorização granular).

---

📄 Página 318

🔍 Resumo dos principais pontos:

*   Modelo "Tudo ou Nada" pode ser assustador dependendo do contexto (segurança, infra).
*   Defesa em profundidade ajuda (rede manager separada, acesso IP restrito, rotação de CA), mas risco interno existe.
*   **Alternativas com Autorização/Auditoria:**
    *   **Kubernetes RBAC (Role-Based Access Control):** K8s tem modelo de permissão granular.
    *   **Docker Enterprise (UCP):** Versão comercial do Docker inclui RBAC.
    *   **GitOps:** Abordagem diferente (pull-based), cluster busca config/deploy do Git, sem acesso externo direto ao cluster. (Figura 15.16).
*   Diagrama (Figura 15.16): Ilustra GitOps. Pipeline CI/CD faz push para Git (imagem + config/manifesto). Agente no cluster detecta mudança no Git e aplica o deploy *de dentro* do cluster.

🧠 Explicação detalhada e didática:

*   A página discute as implicações do modelo de acesso "tudo ou nada" e menciona que, embora medidas de segurança de infraestrutura (defesa em profundidade) ajudem, elas não eliminam completamente o risco, especialmente o risco interno.
*   Apresenta alternativas que *oferecem* controle de acesso mais granular (Autorização) e/ou auditoria:
    1.  **Kubernetes RBAC:** O Kubernetes possui um sistema sofisticado de Role-Based Access Control que permite definir papéis (roles) com permissões específicas (ex: só pode listar pods, pode criar deployments) e associar esses papéis a usuários ou contas de serviço.
    2.  **Docker Enterprise (com UCP):** A plataforma comercial da Docker (agora Mirantis) inclui o Universal Control Plane (UCP), que adiciona RBAC e outras funcionalidades de gerenciamento sobre o Swarm ou K8s.
    3.  **GitOps:** Uma abordagem arquitetural e operacional fundamentalmente diferente. Em vez do pipeline de CI/CD "empurrar" (push) o deploy para o cluster, o fluxo é invertido:
        *   O pipeline faz push dos artefatos (imagem Docker) e da *definição do estado desejado* (manifestos K8s, ou arquivos Compose Swarm) para um repositório **Git**.
        *   Um **agente** rodando *dentro* do cluster monitora esse repositório Git.
        *   Quando o agente detecta uma mudança no Git (ex: nova versão do manifesto), ele "puxa" (pull) essa definição e a aplica ao cluster *localmente*.
        *   **Benefício:** O pipeline de CI/CD não precisa mais de credenciais para acessar o cluster diretamente. O Git se torna a única fonte da verdade, e o acesso ao cluster é muito mais restrito. A Figura 15.16 ilustra esse fluxo pull-based.

💻 Exemplos práticos (para meu ambiente Windows):

*   **GitOps:** Ferramentas populares para implementar GitOps com Kubernetes incluem Argo CD e Flux. Elas rodam no cluster K8s e sincronizam com repositórios Git. Para Swarm, as opções são menos padronizadas, mas o conceito pode ser adaptado com scripts customizados ou ferramentas emergentes.

---

📄 Página 319

🔍 Resumo dos principais pontos:

*   GitOps é abordagem interessante (repetível, versionado, auditável, single source of truth no Git).
*   Requer tempo para implementar, mas pode evoluir gradualmente a partir de pipelines CI/CD simples.
*   **15.6 Lab** (Laboratório)
    *   **Contexto:** Deploy no exercício 15.4 "trapaceou", usou imagem do Docker Hub em vez do registro local, pois PWD não acessa registro local.
    *   **Objetivo:** Corrigir isso. Fazer pipeline CI/CD completo que builda, testa, push para **Docker Hub**, e depois deploya (UAT/Prod no PWD) usando a imagem do **Docker Hub**.
*   **Tarefa:**
    1.  Modificar pipeline (Estágio Push? Adicionar novo estágio?) para **taggear** imagem com sua conta Docker Hub (ex: `<seu_id>/ch15-timecheck:3.0`) além da tag do registro local.
    2.  Modificar pipeline para **fazer push** para Docker Hub (requer login seguro no Hub).
    3.  Modificar arquivos Compose de deploy (UAT/Prod) para usar a imagem do **Docker Hub** (com sua ID) em vez da imagem `diamol/...` usada no exercício.
*   **Dicas:**
    *   Usar credencial Jenkins tipo "Username/password" para login Docker Hub. Disponibilizar via `withCredentials`.
    *   PWD pode parar de ouvir na porta exposta; talvez precise reiniciar sessão PWD (novo domínio -> editar Jenkinsfile de novo).

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre GitOps e introduz o **Lab do Capítulo 15**.
*   O lab corrige uma "trapaça" do exercício anterior: o deploy no PWD usou uma imagem `diamol/...` pré-publicada no Docker Hub, porque o PWD (sendo um ambiente remoto) não conseguiria acessar o contêiner do registro Docker rodando na *sua máquina local*.
*   A tarefa agora é criar um fluxo mais realista:
    1.  O pipeline CI ainda builda a imagem.
    2.  O estágio Push agora deve:
        *   Fazer push para o registro local (`registry.local:5000/...`) como antes (para testes locais talvez).
        *   **E também** fazer login no **Docker Hub** (de forma segura usando credenciais armazenadas no Jenkins).
        *   **Re-taggear** a imagem construída com o nome correto para o seu repositório no Docker Hub (ex: `alexandre/ch15-timecheck:3.0`).
        *   **Fazer push** dessa imagem para o Docker Hub.
    3.  Os estágios de deploy CD (UAT e Prod) devem ser modificados para usar a imagem que acabou de ser enviada para o **Docker Hub** (ex: `alexandre/ch15-timecheck:3.0`) em vez da imagem `diamol/...`. O PWD *consegue* acessar o Docker Hub.
*   As dicas mencionam como lidar com as credenciais do Docker Hub no Jenkins (usando tipo Username/password e `withCredentials`) e alertam sobre a natureza efêmera das sessões PWD.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 15:**
    1.  **Credenciais Docker Hub no Jenkins:**
        *   Acesse Jenkins (`http://localhost:8080`) > Manage Jenkins > Manage Credentials > Jenkins > Global credentials > Add Credentials.
        *   Kind: "Username with password".
        *   Scope: Global.
        *   Username: Seu Docker ID.
        *   Password: Sua senha Docker Hub (ou melhor: um Access Token gerado no Hub).
        *   ID: `dockerhub-creds` (ou outro nome fácil de referenciar).
    2.  **Modificar `Jenkinsfile`:**
        *   No estágio `Push`: Adicionar bloco `withCredentials([usernamePassword(credentialsId: 'dockerhub-creds', usernameVariable: 'DOCKER_USER', passwordVariable: 'DOCKER_PASS')]) { ... }`.
        *   Dentro do bloco, adicionar comandos no script `03-push.bat` (ou similar):
            *   `docker login -u $DOCKER_USER -p $DOCKER_PASS` (ou usar `echo "$DOCKER_PASS" | docker login ... --password-stdin`).
            *   `docker image tag registry.local:5000/diamol/ch15-timecheck:...? <seu_id>/ch15-timecheck:3.0` (adaptar tag local).
            *   `docker image push <seu_id>/ch15-timecheck:3.0`.
    3.  **Modificar Arquivos Compose de Deploy (UAT/Prod):** Nos arquivos `.yml` usados pelos estágios UAT e Prod (ex: `docker-compose-uat.yml`), altere a linha `image:` do serviço `timecheck` para usar a imagem do seu Docker Hub: `image: <seu_id>/ch15-timecheck:3.0`.
    4.  **Commit & Push para Gogs.**
    5.  **Rodar Pipeline Jenkins.** Verificar se push para Hub ocorre e se deploys UAT/Prod no PWD usam a imagem correta (pode verificar com `docker service inspect <stack>_<service>` no PWD).

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch15/lab/README.md

---

📄 Página 320

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 15 no GitHub.

🧠 Explicação detalhada e didática:

*   Fornece o link para a solução completa do Lab 15 no repositório do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 15, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch15/lab/README.md

---

📄 Página 321

🔍 Resumo dos principais pontos:

*   **Capítulo 16: Building Docker images that run anywhere: Linux, Windows, Intel, and Arm** (Construindo imagens Docker que rodam em qualquer lugar...)
*   Revisão: Exercícios do livro funcionam em Mac, Windows, Linux, R-Pi.
*   Motivo: Imagens do livro (`diamol/...`) são **multi-arquitetura (multi-arch)**.
*   **Imagem Multi-Arch:**
    *   Construída/Enviada para registro com **múltiplas variantes**.
    *   Cada variante para um OS ou CPU diferente.
    *   Todas usam o **mesmo nome/tag**.
*   **Como funciona para o usuário:**
    *   `docker pull` ou `docker run <imagem_multi_arch>`: Docker Engine detecta OS/CPU local e automaticamente baixa a variante *compatível* da imagem.
    *   Ex: Rodar `diamol/base` no Windows/Intel baixa variante Windows/amd64; rodar no R-Pi/Arm baixa Linux/arm. App funciona igual.
*   Fácil para usuário, exige esforço do publicador da imagem.
*   Foco Cap. 16: Como produzir builds multi-arch. Aviso para não pular, mesmo se não usar Win/Arm agora.
*   **16.1 Why multi-architecture images are important** (Por que imagens multi-arquitetura são importantes)
    *   Exemplo: AWS oferece VMs Intel/AMD e **Arm** (A1 instances). Arm é quase **50% mais barato**.
    *   Outras nuvens devem seguir. Grande economia potencial.
    *   Desafio: Apps construídos para Intel (`x64`/`amd64`) **não rodam** em Arm.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 16, focado em um conceito avançado mas cada vez mais importante: **imagens multi-arquitetura**.
*   Explica que a razão pela qual os exemplos do livro funcionam em diversas plataformas é que as imagens `diamol/...` fornecidas são, na verdade, "manifestos" que apontam para diferentes versões da imagem, cada uma construída para uma combinação específica de sistema operacional (Linux, Windows) e arquitetura de CPU (Intel/AMD x64, Arm32, Arm64).
*   Quando você executa `docker pull` ou `docker run`, o seu Docker Engine local informa ao registro qual é o seu OS e CPU, e o registro (se a imagem for multi-arch) envia de volta a variante correta. Para o usuário, parece que é uma única imagem que "magicamente" funciona em todo lugar.
*   A seção 16.1 justifica a importância disso, principalmente com o crescimento do uso de processadores **Arm** (comuns em celulares, IoT, Raspberry Pi, e agora também em servidores na nuvem como os da AWS) por serem mais eficientes energeticamente e, consequentemente, mais baratos. O problema é a incompatibilidade: software compilado para Intel/AMD não roda em Arm e vice-versa.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você já usou imagens multi-arch sem saber (as `diamol/...`).
*   O Docker Hub mostra se uma imagem é multi-arch. Vá para `https://hub.docker.com/_/python`, clique em uma tag (ex: `3.10`), e veja a seção "Supported Architectures" listando `linux/amd64`, `linux/arm/v7`, `linux/arm64`, etc.

💡 Analogias ou observações úteis (se houver):

*   Imagem Multi-Arch: É como pedir um manual de instruções para "Python 3.10". A editora (registro Docker) te pergunta "Qual idioma e para qual tipo de dispositivo você quer?" (seu OS/CPU). Você responde "Português para Tablet" (Linux/Arm), e ela te envia a versão correta do manual. Se seu amigo pedir o mesmo manual ("Python 3.10") mas disser "Inglês para Computador" (Windows/Intel), ele recebe uma versão diferente, mas o conteúdo ensina a mesma coisa.

---

📄 Página 322

🔍 Resumo dos principais pontos:

*   IoT também usa Arm (eficiência energética).
*   Desafio Arm: Precisa de plataforma app que rode em Arm + construir app *em* máquina Arm (ou via emulação).
*   Docker resolve isso:
    *   **Docker Desktop (Win/Mac):** Suporta **emulação** para construir e rodar imagens Arm em máquinas Intel.
*   **Exercício Try-it-now:** Usar emulação no Docker Desktop para build Arm. (Não funciona em Docker Engine puro ou PWD).
    *   1. Habilitar **Experimental Features** nas Settings do Docker Desktop (Figura 16.1).
*   Figura 16.1: Mostra opção "Enable experimental features" nas Settings (pode estar em General ou Daemon/Features in development).

🧠 Explicação detalhada e didática:

*   A página continua a motivação para multi-arch, mencionando IoT e Edge computing onde Arm é dominante.
*   Destaca a solução Docker para o desafio de construir para Arm sem ter hardware Arm: **emulação** disponível no **Docker Desktop** (Windows e Mac). O Docker Desktop pode usar tecnologias como QEMU por baixo dos panos para simular um ambiente Arm e permitir que você construa (`docker build`) e até rode (`docker run`) imagens Arm, mesmo estando em uma máquina Intel/AMD.
*   O exercício instrui a habilitar as "Experimental Features" no Docker Desktop, pois a funcionalidade de build multi-plataforma (`docker build --platform ...`) e a emulação associada ainda podem estar sob essa flag (embora em versões recentes, partes dela, como `buildx`, possam estar habilitadas por padrão).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Habilitar Experimental:**
    *   Clique direito no ícone Docker > Settings.
    *   Procure pela seção "General" ou "Features in development" ou "Docker Engine".
    *   Encontre e marque a caixa "Enable experimental features".
    *   Clique "Apply & Restart". Aguarde reiniciar.
2.  **(Verificar se buildx está disponível - comando pode já funcionar mesmo sem experimental em versões recentes):**
    ```powershell
    docker buildx version
    ```
    Se retornar uma versão, você está pronto para builds multiplataforma.

---

📄 Página 323

🔍 Resumo dos principais pontos:

*   **Continuação Ex. Build Arm:** (Após habilitar experimental)
    *   2. Navegar para `ch16/exercises`.
    *   3. Construir imagem para Arm 64-bit usando flag `--platform`:
        *   `docker build -t diamol/ch16-whoami:linux-arm64 --platform linux/arm64 ./whoami`.
    *   4. Verificar arquitetura da imagem criada:
        *   `docker image inspect diamol/ch16-whoami:linux-arm64 -f '{{.Os}}/{{.Architecture}}'`. Deve mostrar `linux/arm64`.
    *   5. Verificar arquitetura nativa do seu Docker Engine:
        *   `docker info -f '{{.OSType}}/{{.Architecture}}'`. Deve mostrar `linux/amd64` (se modo Linux) ou `windows/amd64`.
*   Resultado: Imagem Arm construída em máquina Intel via emulação.
*   Imagem pode ser enviada (`push`) e rodada em máquina Arm real (R-Pi, AWS A1).
*   Figura 16.2: Mostra saída dos comandos, confirmando build cross-platform.

🧠 Explicação detalhada e didática:

*   Com as features experimentais (ou `buildx`) ativas, o exercício demonstra o build **cross-platform**.
*   O comando `docker build` agora usa a flag `--platform linux/arm64`. Isso instrui o Docker a construir a imagem como se estivesse em uma máquina Linux com CPU Arm de 64 bits. O Docker Desktop usará emulação (QEMU) para executar os passos do Dockerfile que precisam rodar no ambiente de destino (ex: comandos `RUN`).
*   O `Dockerfile` na pasta `./whoami` provavelmente usa imagens base (.NET Core SDK/runtime) que *são* multi-arch (incluem variantes `linux/arm64`). O build compila o app .NET Core *para* Arm64.
*   Os comandos `docker image inspect` e `docker info` são usados para confirmar que a imagem resultante foi de fato criada para `linux/arm64`, enquanto a máquina local que fez o build é `linux/amd64` (ou `windows/amd64`).
*   **Importante:** Builds com emulação são significativamente **mais lentos** do que builds nativos. São ótimos para desenvolvimento e CI, mas para produção em larga escala, ter nós de build Arm reais é preferível.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch16\exercises
    ```
2.  **Build Cross-Platform:**
    ```powershell
    docker build -t diamol/ch16-whoami:linux-arm64 --platform linux/arm64 ./whoami
    # Este build será BEM mais lento que um build nativo. Aguarde.
    ```
3.  **Verificar Arquitetura da Imagem:**
    ```powershell
    docker image inspect diamol/ch16-whoami:linux-arm64 -f '{{.Os}}/{{.Architecture}}'
    # Deve retornar linux/arm64
    ```
4.  **Verificar Arquitetura Local:**
    ```powershell
    docker info -f '{{.OSType}}/{{.Architecture}}'
    # Deve retornar linux/amd64 (ou windows/amd64 se modo Windows)
    ```

---

📄 Página 324

🔍 Resumo dos principais pontos:

*   Docker sabe OS/Arch do host e usa para match ao baixar (`pull`) imagens.
*   Otimização de `pull` (descompressão, etc.) só funciona se imagem for compatível.
*   Se não houver variante compatível no registro, `pull` falha com erro.
*   **Exercício Try-it-now:** Tentar baixar imagem Windows em host Linux (ou vice-versa).
    *   Comando (em host Linux/Mac/Win-LinuxMode): `docker image pull mcr.microsoft.com/windows/nanoserver:1809`.
*   Figura 16.3: Mostra erro "no matching manifest for linux/amd64 in the manifest list entries".
*   **Manifest List:** Metadado no registro que lista as variantes (OS/Arch) disponíveis para uma tag de imagem.
*   Erro significa: registro foi consultado, manifest list da imagem foi encontrada, mas nenhuma entrada nela correspondia ao OS/Arch do Docker Engine que fez o pedido.
*   Imagem `nanoserver` não é multi-arch (só Windows).
*   Princípio: Arch da imagem deve bater com Arch do Engine.
    *   Nuanças: Linux pode baixar arch CPU diferente, mas falha no `run` ("exec format error"). Windows com LCOW (experimental) pode rodar Linux, mas complexo/instável.
    *   Melhor prática: Usar arquitetura correspondente. Imagens multi-arch facilitam isso.

🧠 Explicação detalhada e didática:

*   A página explica por que você não pode simplesmente rodar qualquer imagem em qualquer Docker Engine: há uma verificação de compatibilidade de **Sistema Operacional (OS)** e **Arquitetura de CPU (Arch)**.
*   Quando você faz `docker pull` ou `docker run`, seu Docker Engine informa ao registro: "Eu sou um Linux/amd64 (ou Windows/amd64), me dê a imagem X".
*   O registro verifica a **Manifest List** da imagem X. Esse manifesto é um pequeno índice que lista todas as variantes da imagem X disponíveis e para qual combinação de OS/Arch cada variante se destina.
*   Se o registro encontrar uma entrada no manifesto que corresponde ao OS/Arch do seu Engine, ele envia essa variante específica.
*   Se **nenhuma** entrada corresponder, o registro informa ao Engine, e o comando `pull`/`run` falha com o erro "no matching manifest..." (como na Figura 16.3).
*   O exercício demonstra isso tentando baixar uma imagem exclusivamente Windows (`nanoserver`) em um Engine Linux. A falha é esperada.
*   A página menciona algumas exceções (Linux tentando rodar CPU errada, LCOW), mas conclui que a melhor prática é sempre rodar imagens na arquitetura correspondente, e que imagens multi-arch são a forma de fazer isso de maneira transparente para o usuário.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Testar Falha de Pull (Modo Linux):** (Certifique-se que Docker Desktop está no modo Linux)
    ```powershell
    docker image pull mcr.microsoft.com/windows/nanoserver:1809
    # Deve falhar com erro "no matching manifest for linux/amd64..."
    ```
2.  **Testar Falha de Pull (Modo Windows):** (Troque Docker Desktop para modo Windows)
    ```powershell
    docker image pull python:3.10-alpine
    # Deve falhar com erro "no matching manifest for windows/amd64..." (pois alpine é Linux)
    # Volte para o modo Linux após o teste.
    ```

---

📄 Página 325

🔍 Resumo dos principais pontos:

*   **16.2 Building multi-arch images from one or more Dockerfiles** (Construindo imagens multi-arch a partir de um ou mais Dockerfiles)
*   **Abordagem 1: Dockerfile Único (Cross-Platform).**
    *   Como no exemplo `whoami`.
    *   Escrever Dockerfile multi-estágio.
    *   Usar imagens base (SDK, runtime) que *já são* multi-arch e suportam todas as suas targets.
    *   Construir o *mesmo* Dockerfile em máquinas (ou emulação) de arquiteturas diferentes.
    *   Benefício: Manutenção de 1 Dockerfile só.
*   Diagrama (Figura 16.4): Ilustra abordagem com Dockerfile único para .NET Core SDK (base `mcr.microsoft.com/dotnet/sdk` é multi-arch).
*   **Abordagem 2: Dockerfiles Múltiplos (Específicos por Arch).**
    *   Usar quando imagem base *não* é multi-arch, ou não suporta *todas* as targets, ou precisa de lógica *diferente* por OS/Arch.
    *   Ex: Um `Dockerfile.linux-amd64`, outro `Dockerfile.windows-amd64`, outro `Dockerfile.linux-arm`.
    *   Desvantagem: Mais arquivos para manter.
    *   Vantagem: Mais flexibilidade para adaptar a cada target.
*   Diagrama (Figura 16.5): Ilustra abordagem com Dockerfiles múltiplos para Maven (talvez imagem base Maven oficial não tenha Windows, ou precise de passos diferentes).

🧠 Explicação detalhada e didática:

*   Esta seção apresenta as duas estratégias principais para *criar* o conteúdo das diferentes variantes de uma imagem multi-arquitetura:
    1.  **Dockerfile Único:** Se a sua aplicação e as imagens base que você usa (`FROM ...`) são inerentemente multiplataforma (como .NET Core, Go moderno, Python, Node.js rodando sobre bases multi-arch como Debian, Alpine, Ubuntu, ou as oficiais da Microsoft/linguagens), você pode ter um único `Dockerfile`. Você então executa `docker build --platform <target>` (usando máquinas reais ou emulação) para cada plataforma desejada, usando sempre o *mesmo* Dockerfile. O resultado de cada build é uma variante da imagem para aquela plataforma específica. Esta é a abordagem preferida pela simplicidade de manutenção. (Figura 16.4)
    2.  **Dockerfiles Múltiplos:** Se você precisa de passos de construção diferentes para cada plataforma (ex: instalar dependências de formas diferentes no Linux e Windows), ou se as imagens base que você precisa usar só existem para plataformas específicas, então você cria Dockerfiles separados para cada target (ex: `Dockerfile.windows`, `Dockerfile.linux`). Cada arquivo contém as instruções otimizadas ou necessárias para aquela plataforma específica. Você constrói cada Dockerfile na plataforma correspondente (ou via emulação). (Figura 16.5)
*   Ambas as abordagens resultam em múltiplas imagens variantes (ex: `myapp:linux-amd64`, `myapp:windows-amd64`, `myapp:linux-arm64`). O próximo passo (Seção 16.3) será como "unir" essas variantes sob um único nome multi-arch (ex: `myapp:latest`) usando um manifesto.

💻 Exemplos práticos (para meu ambiente Windows):

*   O exemplo `whoami` (p.323) usou a Abordagem 1.
*   O próximo exercício (`folder-list`) usará a Abordagem 2, com Dockerfiles separados.

---

📄 Página 326

🔍 Resumo dos principais pontos:

*   Exemplo `folder-list`: App simples (imprime info runtime, lista pasta). Usa Abordagem 2 (Dockerfiles múltiplos).
*   4 Dockerfiles: Win/Intel, Linux/Intel, Linux/Arm32, Linux/Arm64.
*   Pode-se usar Docker Desktop (Linux mode + emulação) para construir 3 variantes Linux.
*   **Exercício Try-it-now:** Construir variantes Linux do `folder-list`.
    *   Navegar para `ch16/exercises/folder-list`.
    *   Build `linux-amd64` (nativo): `docker build -t ...:linux-amd64 -f ./Dockerfile.linux-amd64 .`
    *   Build `linux-arm64` (emulação): `docker build -t ...:linux-arm64 -f ./Dockerfile.linux-arm64 --platform linux/arm64 .`
    *   Build `linux-arm` (emulação): `docker build -t ...:linux-arm -f ./Dockerfile.linux-arm --platform linux/arm .`
    *   Rodar contêineres de cada variante para verificar.
        *   `docker run ...:linux-amd64`
        *   `docker run ...:linux-arm64`
        *   `docker run ...:linux-arm`

🧠 Explicação detalhada e didática:

*   A página introduz um novo exemplo, `folder-list`, que será construído usando a **Abordagem 2 (Dockerfiles Múltiplos)**. Existem arquivos separados (`Dockerfile.linux-amd64`, `Dockerfile.linux-arm64`, `Dockerfile.linux-arm`, e provavelmente um `Dockerfile.windows-amd64` não usado neste exercício) para cada plataforma alvo.
*   O exercício instrui a construir as três variantes Linux usando o Docker Desktop (que deve estar no modo Linux).
    *   O build para `linux-amd64` será nativo e rápido, pois corresponde à arquitetura do host (Intel/AMD).
    *   Os builds para `linux-arm64` e `linux-arm` usarão a flag `--platform` e rodarão via **emulação**, sendo visivelmente mais lentos.
*   Após construir as três imagens variantes (cada uma com uma tag específica indicando a plataforma), o exercício pede para rodar um contêiner de cada uma (`docker run ...`). Graças à emulação do Docker Desktop, mesmo as imagens Arm devem conseguir rodar na sua máquina Intel/AMD (embora possam rodar mais lentamente e ter limitações). O objetivo é verificar se cada variante se identifica corretamente na saída.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch16\exercises\folder-list
    ```
2.  **Construir Variantes Linux:** (Certifique-se que Docker Desktop está em modo Linux e experimental/buildx ativo)
    ```powershell
    # Nativo amd64
    docker image build -t diamol/ch16-folder-list:linux-amd64 -f ./Dockerfile.linux-amd64 .
    # Emulado arm64 (lento)
    docker image build -t diamol/ch16-folder-list:linux-arm64 -f ./Dockerfile.linux-arm64 --platform linux/arm64 .
    # Emulado arm (lento)
    docker image build -t diamol/ch16-folder-list:linux-arm -f ./Dockerfile.linux-arm --platform linux/arm .
    ```
3.  **Rodar e Verificar:**
    ```powershell
    docker container run diamol/ch16-folder-list:linux-amd64
    # Deve imprimir "Built as: linux/amd64" e info x86_64

    docker container run diamol/ch16-folder-list:linux-arm64
    # Deve imprimir "Built as: linux/arm64" e info aarch64

    docker container run diamol/ch16-folder-list:linux-arm
    # Deve imprimir "Built as: linux/arm" e info armv7l
    ```

---

📄 Página 327

🔍 Resumo dos principais pontos:

*   Figura 16.6: Mostra saída dos `docker run` das 3 variantes Linux, confirmando que rodam (via emulação) e se identificam corretamente.
*   Dockerfiles Linux são similares (só muda string hardcoded).
*   Dockerfile Windows tem mesma funcionalidade, mas usa comandos Windows diferentes (`echo %VAR%`, `dir`). Vantagem de Dockerfiles separados.
*   **Listagem 16.1:** Compara Dockerfile `linux-arm64` e `windows-amd64`.
    *   `FROM` diferente (base específica).
    *   Windows usa `escape=` para permitir `\` em paths.
    *   Comandos `CMD` são diferentes (`uname` vs `echo %PROC...%`, `ls` vs `dir`).
*   Caso de uso para Dockerfiles múltiplos: Suportar OS diferente (Win/Linux), ou base multi-arch não existe/serve. Ex: Golden images Prometheus/Grafana do autor (oficiais só Linux -> Windows precisa de Dockerfile separado).

🧠 Explicação detalhada e didática:

*   A página analisa o resultado do exercício anterior (Figura 16.6), confirmando que a emulação do Docker Desktop permite rodar as imagens Arm e que cada uma reporta a arquitetura para a qual foi construída.
*   Compara (Listagem 16.1) o `Dockerfile.linux-arm64` com o `Dockerfile.windows-amd64` (que não foi construído no exercício anterior, mas existe nos fontes). A comparação destaca por que Dockerfiles separados são às vezes necessários:
    *   Usam imagens `FROM` diferentes e específicas da plataforma.
    *   Usam sintaxe e comandos específicos do Sistema Operacional (`uname`/`ls` vs `echo %VAR%`/`dir`).
    *   O Dockerfile Windows usa a diretiva `escape=\`` para facilitar o uso de barras invertidas nos comandos `CMD`.
*   Reafirma os cenários onde múltiplos Dockerfiles são a abordagem correta: suporte a OS fundamentalmente diferentes (Linux vs. Windows) ou quando uma imagem base multi-arch adequada não está disponível.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Construir Variante Windows (Opcional):**
    1.  Troque Docker Desktop para o modo Windows Containers.
    2.  Navegue para `ch16\exercises\folder-list`.
    3.  Construa: `docker image build -t diamol/ch16-folder-list:windows-amd64 -f ./Dockerfile.windows-amd64 .`
    4.  Rode: `docker container run diamol/ch16-folder-list:windows-amd64` (Deve mostrar "Built as: windows/amd64" e info de CPU Windows).
    5.  Volte para o modo Linux Containers.

---

📄 Página 328

🔍 Resumo dos principais pontos:

*   Para apps próprios, tentar ter Dockerfile único é mais fácil (mas cuidado com comandos OS-specific não portáveis).
*   Risco: `RUN` falha no build se comando inválido; `CMD` não é verificado, pode falhar só no `run`.
*   **Exercício Try-it-now:** Demonstrar falha de Dockerfile "multi-arch" mal escrito.
    *   Dockerfile padrão (`.` ou sem `-f`) na pasta `folder-list` tenta ser multi-arch mas mistura comandos Linux/Windows.
    *   1. Buildar imagem com Dockerfile padrão: `docker image build -t diamol/ch16-folder-list .` (Implicitamente usa `Dockerfile`). Build deve *passar*.
    *   2. Tentar rodar a imagem: `docker container run diamol/ch16-folder-list`. Deve *falhar*.
*   Figura 16.7: Mostra que build passa, mas `run` falha (erro no `CMD`).
*   Importante testar imagens em todas as targets.

🧠 Explicação detalhada e didática:

*   A página adverte sobre os riscos ao tentar criar um **Dockerfile único** que funcione para múltiplas plataformas (especialmente Linux e Windows): é fácil incluir acidentalmente um comando em uma instrução `RUN` ou `CMD` que só funciona em um dos sistemas operacionais.
*   Comandos `RUN` inválidos geralmente causam falha durante o `docker build`. No entanto, comandos dentro de `CMD` ou `ENTRYPOINT` não são executados durante o build; eles só rodam quando um contêiner é iniciado (`docker run`). Portanto, um `CMD` com comandos incompatíveis pode resultar em uma imagem que parece construir com sucesso, mas que **falha imediatamente** ao tentar rodar um contêiner.
*   O exercício demonstra isso usando o `Dockerfile` padrão na pasta `folder-list`, que foi intencionalmente escrito para falhar. O build provavelmente passará, mas o `docker run` falhará porque o `CMD` mistura comandos Linux e Windows.
*   A lição é: sempre **teste** suas imagens construídas em **todas as plataformas de destino** para garantir que elas não apenas construam, mas também rodem corretamente.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar (se necessário):**
    ```powershell
    cd C:\DockerLabs\diamol\ch16\exercises\folder-list
    ```
2.  **Construir Imagem "Ruim":**
    ```powershell
    docker image build -t diamol/ch16-folder-list . # Usa 'Dockerfile' por padrão
    # Build deve completar sem erros.
    ```
3.  **Tentar Rodar (Deve Falhar):**
    ```powershell
    docker container run diamol/ch16-folder-list
    # Deve sair com erro, mostrando a falha ao executar o CMD misturado.
    ```

---

📄 Página 329

🔍 Resumo dos principais pontos:

*   Figura 16.7: Confirma build OK, run falha.
*   **Arquiteturas Docker e Nomes:**
    *   Tabela 16.1: Mapeia OS, CPU, Word Length, Nome CPU Docker, Aliases.
        *   Windows / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Arm / 64-bit -> `arm64` (ou `aarch64`, `armv8`)
        *   Linux / Arm / 32-bit -> `arm` (ou `arm32v7`, `armv7`, `armhf`)
*   Docker suporta mais (x86 32-bit, Mainframes IBM Z/POWER/PowerPC com Linux

**You**

🧠 Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), página por página, a apartir da página 329.

Para cada página, siga este modelo de resposta:

📄 Página X

🔍 Resumo dos principais pontos:

\[bullet points com os tópicos abordados na página\]

🧠 Explicação detalhada e didática:

\[explique os conceitos em português claro, com boa didática, como um professor\]

💻 Exemplos práticos (para meu ambiente Windows):

\[comandos ou trechos de código aplicáveis no Windows, usando Docker Desktop e Visual Studio Code\]

💡 Analogias ou observações úteis (se houver):

\[comparações simples ou observações para reforçar o entendimento\]

✅ Importante: percorra automaticamente todas as páginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "próxima".

Quando terminar todas as páginas, diga:

"✅ Fim da leitura. Todas as páginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Ok, Alexandre, continuando a análise a partir da página 329.

---

📄 Página 329

🔍 Resumo dos principais pontos:

*   Figura 16.7: Confirma build OK, run falha.
*   **Arquiteturas Docker e Nomes:**
    *   Tabela 16.1: Mapeia OS, CPU, Word Length, Nome CPU Docker, Aliases comuns.
        *   Windows / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Intel-AMD / 64-bit -> `amd64` (ou `x86_64`)
        *   Linux / Arm / 64-bit -> `arm64` (ou `aarch64`, `armv8`)
        *   Linux / Arm / 32-bit -> `arm` (ou `arm32v7`, `armv7`, `armhf`)
*   Docker suporta mais arquiteturas (ex: Intel x86 32-bit, Mainframes IBM Z/POWER/PowerPC com Linux).
*   `amd64`: Padrão para desktops/laptops/servidores Intel/AMD.
*   `arm`/`arm64`: Comuns em celulares, IoT, R-Pi (Pi4 é 64-bit).
*   **16.3 Pushing multi-arch images to registries with manifests** (Enviando imagens multi-arch para registros com manifestos)
    *   Builds (nativos ou emulados) criam variantes separadas (ex: `:linux-amd64`, `:linux-arm64`).
    *   Para uni-las sob um nome único multi-arch (ex: `:latest` ou `:2.0`), usa-se um **Manifest** (ou Manifest List).
    *   Manifesto: Metadado no registro que aponta para as diferentes variantes da imagem.
    *   Criado com CLI `docker` e enviado (`push`) ao registro.
    *   Contém lista das variantes; variantes precisam existir no registro *antes* de criar/enviar manifesto.
    *   Fluxo: Criar/Push variantes -> Criar/Push manifesto.

🧠 Explicação detalhada e didática:

*   A página finaliza a discussão sobre Dockerfiles multi-plataforma apresentando uma tabela útil (Tabela 16.1) que resume os nomes e aliases comuns usados pelo Docker para identificar as principais combinações de Sistema Operacional e Arquitetura de CPU (como `amd64` para Intel/AMD 64-bit, `arm64` para Arm 64-bit, etc.). Conhecer esses nomes é importante ao usar a flag `--platform` ou ao inspecionar imagens.
*   A seção 16.3 introduz o passo final para criar uma imagem verdadeiramente multi-arquitetura: o **Manifesto** (ou, mais precisamente, a **Lista de Manifestos**).
*   Até agora, você construiu imagens *separadas* para cada plataforma (ex: `minha-app:linux-amd64`, `minha-app:linux-arm64`). O objetivo é ter um único nome, como `minha-app:1.0`, que funcione em qualquer plataforma.
*   O Manifesto é a "cola" que faz isso acontecer. É um pequeno arquivo de metadados armazenado no registro Docker que diz: "A imagem `minha-app:1.0` na verdade consiste nestas variantes: a imagem X é para linux/amd64, a imagem Y é para linux/arm64, a imagem Z é para windows/amd64, etc.".
*   A ferramenta de linha de comando `docker` (com features experimentais ou `buildx`) permite criar e enviar (`push`) esse manifesto para o registro.
*   **Fluxo Essencial:**
    1.  Construa cada variante da imagem (ex: `meu-app:linux-amd64`, `meu-app:linux-arm64`).
    2.  Envie (`push`) *cada uma* dessas variantes para o registro.
    3.  Use o comando `docker manifest create ...` para criar um manifesto local que lista todas essas variantes sob o nome multi-arch desejado (ex: `meu-app:1.0`).
    4.  Use o comando `docker manifest push ...` para enviar o manifesto para o registro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você já construiu e tagueou as variantes Linux (`:linux-amd64`, `:linux-arm64`, `:linux-arm`) para `diamol/ch16-folder-list` (e talvez a `:windows-amd64`).
*   O próximo passo será enviar essas variantes para o Docker Hub e depois criar/enviar o manifesto.
*   Os comandos serão `docker manifest create` e `docker manifest push`.

💡 Analogias ou observações úteis (se houver):

*   Manifesto Multi-Arch: É o índice ou sumário de um manual de instruções multilíngue. O nome na capa é "Manual do Produto X" (`minha-app:1.0`). O índice (manifesto) diz: "Para Inglês/PC, veja página 1 (imagem linux/amd64). Para Português/Tablet, veja página 50 (imagem linux/arm). Para Japonês/Celular, veja página 100 (imagem windows/amd64)". Quando você pede o manual, a editora (registro) consulta o índice e te envia apenas a seção relevante para você.

---

📄 Página 330

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Enviar variantes `folder-list` para Docker Hub.
    *   1. Taguear variantes com seu Docker ID (se ainda não feito).
        *   `docker image tag diamol/...:linux-amd64 "$dockerId/ch16-folder-list:linux-amd64"` (e para arm64, arm).
    *   2. Fazer push de todas as tags para seu repositório no Hub.
        *   `docker image push "$dockerId/ch16-folder-list"` (push sem tag específica envia todas as tags locais para aquele repo).
        *   OU `docker image push --all-tags "$dockerId/ch16-folder-list"`.
*   Registros são agnósticos à arquitetura; armazenam e servem qualquer tipo. Sabem arch da imagem para match no pull.
*   Figura 16.8: Mostra saída do `push`, indicando arquitetura de cada imagem enviada.
*   **Comandos `docker manifest`:**
    *   Feature nova/experimental da CLI. Precisa habilitar "experimental" na CLI.
    *   Pode ser via Settings Docker Desktop (Command Line) ou editando `~/.docker/config.json`.

🧠 Explicação detalhada e didática:

*   O primeiro passo para publicar a imagem multi-arch `folder-list` é enviar (`push`) todas as variantes individuais (que você construiu nas páginas anteriores e tagueou com seu `$dockerId`) para o Docker Hub.
*   O exercício mostra como re-taguear as imagens `diamol/...` locais para incluir seu Docker ID e, em seguida, usar `docker image push "$dockerId/ch16-folder-list"`. Quando `docker push` é usado com um nome de repositório base (sem tag específica), ele tenta enviar todas as tags locais associadas àquele repositório para o registro remoto. Alternativamente, `--all-tags` faz isso explicitamente.
*   Após o push, todas as variantes (`:linux-amd64`, `:linux-arm64`, `:linux-arm`) existirão no seu repositório no Docker Hub, mas ainda como imagens separadas.
*   Para usar os comandos `docker manifest` (necessários para criar a lista de manifestos multi-arch), a página informa que pode ser necessário habilitar as features experimentais na **CLI Docker**, não apenas no Engine (como foi feito para emulação). Isso pode ser feito nas configurações do Docker Desktop ou editando o arquivo de configuração da CLI (`config.json`).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Taguear com Seu ID:** (Se `$dockerId` está definido)
    ```powershell
    docker image tag diamol/ch16-folder-list:linux-amd64 "$dockerId/ch16-folder-list:linux-amd64"
    docker image tag diamol/ch16-folder-list:linux-arm64 "$dockerId/ch16-folder-list:linux-arm64"
    docker image tag diamol/ch16-folder-list:linux-arm "$dockerId/ch16-folder-list:linux-arm"
    # Se construiu a Windows:
    # docker image tag diamol/ch16-folder-list:windows-amd64 "$dockerId/ch16-folder-list:windows-amd64"
    ```
2.  **Fazer Login (se necessário):** `docker login -u $dockerId`
3.  **Enviar Variantes:**
    ```powershell
    docker image push --all-tags "$dockerId/ch16-folder-list"
    ```
    Observe a saída enviando as diferentes tags.
4.  **Habilitar Experimental CLI (se necessário):**
    *   **Docker Desktop:** Settings > Command Line > Marque "Enable experimental features" > Apply & Restart (se pedir).
    *   **(Alternativa) Editar `config.json`:**
        *   Encontre o arquivo: `$HOME\.docker\config.json` (geralmente `C:\Users\<seu_user>\.docker\config.json`).
        *   Abra no VS Code. Adicione ou modifique para ter `"experimental": "enabled"`. Exemplo:
          ```json
          {
            "auths": { ... },
            "credsStore": "desktop.exe",
            "experimental": "enabled"
          }
          ```
        *   Salve o arquivo. (Reiniciar Docker não deve ser necessário para config da CLI).

---

📄 Página 331

🔍 Resumo dos principais pontos:

*   Figura 16.9: Mostra opção "Enable Experimental Features" na CLI do Docker Desktop.
*   Se usar Docker Engine (não Desktop), editar `~/.docker/config.json` para adicionar `"experimental":"enabled"`.
*   Com CLI experimental ativa, comandos `docker manifest ...` ficam disponíveis.
*   `docker manifest inspect <imagem>:<tag>`: Lê metadados do manifesto (ou lista de manifestos) do registro. Não precisa ter imagem local.
*   **Exercício Try-it-now:** Inspecionar manifesto da imagem `diamol/base` (que é multi-arch).
    *   Comando: `docker manifest inspect diamol/base`.

🧠 Explicação detalhada e didática:

*   A página detalha como habilitar as features experimentais especificamente para a **CLI Docker**, o que é necessário para usar os comandos `docker manifest`. Mostra a opção na interface do Docker Desktop (Figura 16.9) e como fazer editando o arquivo `config.json` manualmente para quem não usa o Desktop.
*   Uma vez habilitado, o comando `docker manifest inspect <imagem_no_registro>` torna-se disponível. Este comando é muito útil, pois ele consulta diretamente o registro Docker (sem baixar a imagem) e exibe o conteúdo do manifesto (ou da lista de manifestos) para a imagem e tag especificadas. Para uma imagem multi-arch, isso mostrará as diferentes variantes (OS/Arch) que ela suporta.
*   O exercício usa `docker manifest inspect diamol/base` para exemplificar. Como `diamol/base` é multi-arch, a saída mostrará várias entradas, cada uma descrevendo uma variante diferente (ex: linux/amd64, linux/arm64, windows/amd64).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Experimental CLI:** Verifique se habilitou na página anterior.
2.  **Inspecionar Manifesto:**
    ```powershell
    docker manifest inspect diamol/base
    ```
3.  **Analisar Saída:** Você verá uma estrutura JSON. O campo principal é `"manifests"`, que é uma lista `[]`. Dentro da lista, haverá vários objetos `{}`. Cada objeto representa uma variante e terá um campo `"platform"` com `"os"` e `"architecture"`. Procure pelas variantes `linux/amd64`, `linux/arm`, `linux/arm64`, `windows/amd64`. (A próxima figura mostra isso).

---

📄 Página 332

🔍 Resumo dos principais pontos:

*   Figura 16.10: Mostra saída (filtrada com `jq`) do `manifest inspect diamol/base`. Confirma múltiplas entradas no array `manifests`, cada uma com `digest`, `platform.os`, `platform.architecture`.
*   `manifest inspect` não tem filtro; retorna tudo.
*   **Criando Manifesto (Lista):**
    *   Comando `docker manifest create`. Une imagens variantes sob tag única multi-arch.
    *   Manifesto é criado localmente primeiro, depois enviado (`push`).
    *   **Manifest List:** Nome técnico para o manifesto multi-arch. Contém lista de manifestos individuais das imagens variantes.
*   Diagrama (Figura 16.11): Relação Imagem <-> Manifesto <-> Lista de Manifestos no Registro.
    *   Cada imagem/variante (ex: `:linux-amd64`) tem seu próprio manifesto individual no registro.
    *   A Lista de Manifestos (ex: `:latest`) aponta para os manifestos individuais das variantes.

🧠 Explicação detalhada e didática:

*   A Figura 16.10 confirma a saída esperada do `manifest inspect` para uma imagem multi-arch como `diamol/base`, mostrando as diferentes plataformas suportadas.
*   A página então explica o processo de **criação** da Lista de Manifestos usando `docker manifest create`.
*   **Conceito:** Você cria um novo objeto (a Lista de Manifestos) que atuará como o "índice" para a sua imagem multi-arch. Este índice conterá referências às imagens variantes individuais (que já devem existir no registro).
*   A Figura 16.11 ajuda a visualizar: cada imagem concreta (ex: a versão linux/amd64, a versão linux/arm64) tem seu próprio manifesto descrevendo suas camadas. A Lista de Manifestos é um nível acima: ela não descreve camadas, apenas aponta para os manifestos das variantes que compõem a imagem multi-arch.
*   O nome que você dá à Lista de Manifestos (`docker manifest create <nome_lista> ...`) torna-se o nome/tag da sua imagem multi-arch no registro (ex: `meu-app:1.0`).

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício executará `docker manifest create` usando as variantes `folder-list` que você enviou para o Hub.

---

📄 Página 333

🔍 Resumo dos principais pontos:

*   Nome da manifest list vira a tag multi-arch.
*   Pode-se usar tag `latest` (padrão se omitido no `create`) ou tag de versão (ex: `:1.0`).
*   **Exercício Try-it-now:** Criar e enviar manifesto para `folder-list`.
    *   1. Criar manifesto local (nome `$dockerId/ch16-folder-list`, tag `latest` implícita), listando as 3 variantes Linux enviadas anteriormente.
        ```bash
        docker manifest create "$dockerId/ch16-folder-list" \
          "$dockerId/ch16-folder-list:linux-amd64" \
          "$dockerId/ch16-folder-list:linux-arm64" \
          "$dockerId/ch16-folder-list:linux-arm"
        ```
    *   2. Enviar manifesto para Docker Hub:
        ```bash
        docker manifest push "$dockerId/ch16-folder-list"
        ```
    *   3. Verificar no Docker Hub (navegar para sua página).

🧠 Explicação detalhada e didática:

*   Esta página mostra como usar os comandos `docker manifest create` e `docker manifest push`.
*   **`docker manifest create <nome_manifest_list> <imagem_variante_1> <imagem_variante_2> ...`**:
    *   `<nome_manifest_list>`: O nome completo (incluindo registro/usuário e tag opcional) que você quer dar à sua imagem multi-arch (ex: `alexandre/ch16-folder-list:latest`).
    *   `<imagem_variante_...>`: Os nomes completos das imagens variantes individuais (que já devem estar no registro) que farão parte desta imagem multi-arch.
    *   Este comando cria a lista de manifestos *localmente* (em memória ou cache, não como um arquivo visível).
*   **`docker manifest push <nome_manifest_list>`**:
    *   Envia a lista de manifestos criada localmente para o registro Docker.
*   O exercício cria um manifesto chamado `$dockerId/ch16-folder-list` (tag `:latest` implícita) que agrupa as três variantes Linux (`:linux-amd64`, `:linux-arm64`, `:linux-arm`) previamente enviadas. Em seguida, envia esse manifesto para o Docker Hub.
*   O resultado é que agora existe uma imagem `$dockerId/ch16-folder-list:latest` no Docker Hub que é multi-arch para essas três plataformas Linux.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Manifesto Local (PowerShell):** (Use seu `$dockerId`)
    ```powershell
    # Use backticks ` para quebrar linha no PowerShell
    docker manifest create "$dockerId/ch16-folder-list" `
      "$dockerId/ch16-folder-list:linux-amd64" `
      "$dockerId/ch16-folder-list:linux-arm64" `
      "$dockerId/ch16-folder-list:linux-arm"
    # (Se você também enviou :windows-amd64, adicione-o à lista)
    ```
2.  **Enviar Manifesto para o Hub:**
    ```powershell
    docker manifest push "$dockerId/ch16-folder-list"
    ```
3.  **Verificar no Hub:**
    *   Use o comando `echo "https://hub.docker.com/r/$dockerId/ch16-folder-list/tags"` para obter o URL.
    *   Acesse no navegador. Você deve ver a tag `latest` e, ao clicar nela ou inspecioná-la, verá as diferentes arquiteturas (linux/amd64, linux/arm64, linux/arm) associadas a ela.

---

📄 Página 334

🔍 Resumo dos principais pontos:

*   Figura 16.12: Mostra a tag `latest` no Docker Hub com múltiplas arquiteturas (OS/ARCH).
*   Qualquer Docker Engine Linux pode agora rodar `$dockerId/ch16-folder-list`, baixando a variante correta (amd64, arm64, arm).
*   Builds Arm via emulação são lentos e nem sempre 100% precisos.
*   Para builds rápidos/precisos: **Build Farm** (máquinas reais com diferentes OS/Arch).
    *   Exemplo do autor: R-Pis + outros SBCs para buildar imagens do livro. Pipeline CI conecta a cada máquina, builda variante, push, depois cria/push manifesto.
*   **16.4 Building multi-arch images with Docker Buildx** (Construindo imagens multi-arch com Docker Buildx)
    *   Alternativa mais eficiente e fácil para build farm: **Buildx**.
    *   Extensão dos comandos `docker build`. Usa novo engine de build otimizado.
    *   Mesma entrada (`Dockerfile`), mesma saída (imagem).
    *   Brilha em builds cross-platform: integra com Contexts Docker, distribui builds em múltiplos servers com 1 comando.

🧠 Explicação detalhada e didática:

*   A página confirma o resultado no Docker Hub (Figura 16.12): uma única tag (`latest`) agora representa múltiplas arquiteturas.
*   Discute a limitação da **emulação** (usada para construir as variantes Arm no Docker Desktop): é lenta e pode não ser perfeita. Para builds frequentes, rápidos e confiáveis em produção, a solução ideal é ter uma **Build Farm** com máquinas físicas (ou VMs) reais para cada arquitetura alvo (ex: um servidor Linux/Intel, um R-Pi para Linux/Arm, um servidor Windows). O pipeline de CI/CD então orquestraria os builds em cada nó da farm.
*   A seção 16.4 introduz o **Docker Buildx** como uma ferramenta mais moderna e integrada para gerenciar esse processo de build multi-arquitetura, simplificando a necessidade de uma build farm complexa.
*   **Buildx:**
    *   É um plugin da CLI Docker (`docker buildx build ...`) que substitui/estende o `docker build`.
    *   Usa um backend de build mais avançado (BuildKit) com melhor performance e caching.
    *   Seu grande trunfo é a capacidade de orquestrar builds para **múltiplas plataformas simultaneamente** usando diferentes "builders" (que podem ser Docker Engines locais ou remotos definidos em Docker Contexts) com um único comando.

💻 Exemplos práticos (para meu ambiente Windows):

*   O Docker Desktop moderno já inclui o Buildx. Você pode verificar com `docker buildx version`.
*   Os próximos exercícios usarão `docker buildx` no PWD para simular uma build farm distribuída.

💡 Analogias ou observações úteis (se houver):

*   Build Farm Tradicional: Ter várias cozinhas separadas (máquinas de build), uma para cada tipo de culinária (arquitetura). O chef (pipeline CI) precisa ir a cada cozinha para fazer a parte correspondente do banquete (imagem multi-arch).
*   Buildx: Um chef super moderno que tem assistentes remotos em cada cozinha especializada. O chef envia a mesma receita base (Dockerfile) para todos os assistentes simultaneamente (`docker buildx build --platform ...`), eles cozinham suas partes, enviam de volta, e o chef monta o banquete final (imagem multi-arch com manifesto) automaticamente.

---

📄 Página 335

🔍 Resumo dos principais pontos:

*   **Limitações Buildx (atuais):**
    *   Não funciona com Windows Containers (ainda).
    *   Só suporta build de Dockerfile *único* (não a abordagem de múltiplos Dockerfiles por arch).
*   OK se target for só variantes Linux CPU.
*   **Exemplo Buildx com PWD (Build Farm Simulada):**
    *   Usará PWD para simular múltiplos nós de build.
    *   Passo 1: Criar Docker Contexts para cada nó da "farm".
*   **Exercício Try-it-now:** Configurar PWD e Contexts para Buildx.
    *   1. PWD: Criar 2 instâncias (`node1`, `node2`).
    *   2. No `node1`: Salvar IP do `node2`, testar SSH (`ssh $node2ip`, `exit`).
    *   3. No `node1`: Criar contexto para `node1` (local): `docker context create node1 --docker "host=unix..."`.
    *   4. No `node1`: Criar contexto para `node2` (remoto via SSH): `docker context create node2 --docker "host=ssh://root@$node2ip"`.
    *   5. Listar contextos: `docker context ls`.

🧠 Explicação detalhada e didática:

*   A página menciona as limitações atuais do Buildx: ele é focado em Linux e na abordagem de Dockerfile único multiplataforma. Se você precisa construir para Windows ou usar Dockerfiles separados por arquitetura, o método manual com `docker build` + `docker manifest` (Seções 16.2 e 16.3) ainda é necessário.
*   O restante da seção mostrará como usar o Buildx para construir para múltiplas arquiteturas de CPU Linux (amd64 e 386/i386 - 32-bit Intel) usando um Swarm de 2 nós no PWD como uma build farm simulada.
*   O primeiro passo é configurar **Docker Contexts** no nó que atuará como "cliente" do Buildx (`node1`) para que ele saiba como se conectar aos nós da "farm" (`node1` mesmo e `node2`).
*   O exercício guia você na criação desses dois contextos no `node1` do PWD:
    *   `node1`: Aponta para o Docker Engine local do `node1` via socket Unix.
    *   `node2`: Aponta para o Docker Engine do `node2` via SSH.
*   Com esses contextos definidos, o Buildx poderá usá-los para distribuir os trabalhos de build.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Executar no PWD:**
    1.  Acesse PWD, inicie 2 instâncias (`node1`, `node2`).
    2.  Selecione `node1`, anote o IP de `node2`.
    3.  Execute os comandos do exercício no terminal do `node1`:
        ```bash
        node2ip=<IP_DO_NODE2>
        echo $node2ip
        ssh root@$node2ip # Testar SSH
        exit
        docker context create node1 --docker "host=unix:///var/run/docker.sock"
        docker context create node2 --docker "host=ssh://root@$node2ip"
        docker context ls
        ```
        Você deve ver os contextos `default`, `node1`, e `node2` listados. `default` ou `node1` estará ativo.

---

📄 Página 336

🔍 Resumo dos principais pontos:

*   Figura 16.13: Mostra saída da criação e listagem dos contextos no PWD.
*   Contextos facilitam setup Buildx. Em CI real, criar contextos para cada nó da farm. Nós podem ser standalone (não precisam de Swarm/K8s).
*   **Instalando e Configurando Buildx:**
    *   Buildx é plugin CLI; já vem no Docker Desktop/CE recentes. PWD não tem.
*   **Exercício Try-it-now:** Instalar Buildx no PWD e criar "builder".
    *   1. Baixar binário Buildx e colocar na pasta de plugins: `wget ...`, `chmod ...`.
    *   2. Criar builder multi-nó chamado `ch16`, usando `node1` para `linux/amd64`:
        *   `docker buildx create --use --name ch16 --platform linux/amd64 node1`. (`--use` ativa o novo builder).
    *   3. Adicionar `node2` ao mesmo builder `ch16` para `linux/386`:
        *   `docker buildx create --append --name ch16 --platform linux/386 node2`.
    *   4. Listar builders: `docker buildx ls`.

🧠 Explicação detalhada e didática:

*   Com os contextos prontos, o próximo passo é configurar o **Buildx**.
*   Como o PWD usa uma versão do Docker que pode não incluir o plugin Buildx, o exercício primeiro mostra como baixá-lo do GitHub e instalá-lo manualmente no diretório de plugins da CLI Docker (`~/.docker/cli-plugins/`).
*   Uma vez instalado, o comando `docker buildx create` é usado para definir um **builder**. Um builder é um agrupamento lógico de nós (definidos por contextos) que o Buildx pode usar para realizar builds.
*   O exercício cria um builder chamado `ch16`:
    *   Primeiro, ele é criado usando o contexto `node1` e especificando que este nó deve ser usado para builds `linux/amd64`. A flag `--use` torna este builder o padrão para comandos `buildx`.
    *   Depois, o contexto `node2` é *adicionado* (`--append`) ao mesmo builder `ch16`, especificando que ele deve ser usado para builds `linux/386` (Intel 32-bit).
*   O comando `docker buildx ls` lista os builders configurados e os nós/plataformas associados a cada um.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Executar no PWD (continuando no `node1`):**
    1.  **Instalar Buildx:**
        ```bash
        mkdir -p ~/.docker/cli-plugins
        wget -O ~/.docker/cli-plugins/docker-buildx https://github.com/docker/buildx/releases/download/v0.10.4/buildx-v0.10.4.linux-amd64 # Use uma versão recente, ex v0.10.4
        chmod a+x ~/.docker/cli-plugins/docker-buildx
        docker buildx version # Verificar se instalou
        ```
    2.  **Criar Builder:**
        ```bash
        docker buildx create --use --name ch16 --platform linux/amd64 node1
        ```
    3.  **Adicionar Nó ao Builder:**
        ```bash
        docker buildx create --append --name ch16 --platform linux/386 node2
        ```
    4.  **Listar Builders:**
        ```bash
        docker buildx ls
        ```
        Você deve ver o builder `ch16` listado, com `node1` (linux/amd64) e `node2` (linux/386) como nós, e um asterisco indicando que ele é o builder ativo.

---

📄 Página 337

🔍 Resumo dos principais pontos:

*   Figura 16.14: Mostra saída do `buildx ls`, confirmando builder `ch16` com 2 nós/plataformas.
*   Buildx é flexível: pode detectar plataformas dos nós ou pode-se especificar manualmente.
*   Build farm `ch16` agora pode construir para Linux Intel 64-bit e 32-bit, *se* Dockerfile usar bases multi-arch que suportem ambos.
*   Buildx distribui build (Dockerfile + contexto) para nós concorrentemente.
*   **Exercício Try-it-now:** Clonar repo, buildar e enviar imagem multi-arch com Buildx.
    *   1. Clonar repo `diamol` no PWD (`node1`).
    *   2. Navegar para `diamol/ch16/exercises/folder-list-2/` (contém Dockerfile multi-arch).
    *   3. Salvar Docker ID, fazer `docker login`.
    *   4. Usar `docker buildx build` para construir *ambas* as variantes e fazer push *automaticamente*.
        ```bash
        docker buildx build -t "$dockerId/ch16-folder-list-2" \
          --platform linux/amd64,linux/386 --push .
        ```
        *   `-t ...`: Nome da imagem multi-arch final.
        *   `--platform ...`: Lista de plataformas a construir (separadas por vírgula). Buildx usa os nós configurados no builder ativo para cada plataforma.
        *   `--push`: Diz ao Buildx para, após construir as variantes, fazer push delas E criar/fazer push do manifesto multi-arch automaticamente.
        *   `.`: Contexto de build.

🧠 Explicação detalhada e didática:

*   Com o builder `ch16` configurado, a "build farm" de 2 nós está pronta. O Buildx agora sabe que pode usar `node1` para builds `linux/amd64` e `node2` para `linux/386`.
*   O exercício final demonstra o poder do `docker buildx build`:
    1.  Clona-se o repositório para ter acesso ao `Dockerfile` (em `folder-list-2`, que deve ser um Dockerfile único multiplataforma).
    2.  Faz-se login no Docker Hub para permitir o push.
    3.  Executa-se `docker buildx build` com parâmetros chave:
        *   `-t "$dockerId/..."`: Define o nome final da imagem multi-arch que será criada no registro.
        *   `--platform linux/amd64,linux/386`: Especifica as duas plataformas alvo. O Buildx automaticamente direcionará o build amd64 para `node1` e o build 386 para `node2`, executando-os em paralelo.
        *   `--push`: Esta flag é crucial. Ela combina múltiplos passos: constrói todas as variantes especificadas, envia cada variante construída para o registro, cria a lista de manifestos multi-arch referenciando essas variantes, e envia a lista de manifestos para o registro sob a tag especificada em `-t`. Tudo em um comando!
        *   `.`: O diretório de contexto contendo o Dockerfile.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Executar no PWD (continuando no `node1`):**
    1.  **Clonar Repo:**
        ```bash
        git clone https://github.com/sixeyed/diamol.git
        ```
    2.  **Navegar:**
        ```bash
        cd diamol/ch16/exercises/folder-list-2/
        ```
    3.  **Login Docker Hub:** (Substitua pelo seu ID)
        ```bash
        dockerId=<your-docker-id>
        docker login -u $dockerId # Digite a senha
        ```
    4.  **Build e Push Multi-Arch com Buildx:**
        ```bash
        docker buildx build -t "$dockerId/ch16-folder-list-2" --platform linux/amd64,linux/386 --push .
        ```
        Observe a saída detalhada e colorida do Buildx, mostrando o progresso em ambos os nós (targets). Aguarde a conclusão (pode levar alguns minutos).

---

📄 Página 338

🔍 Resumo dos principais pontos:

*   Figura 16.15: Mostra final da saída do `buildx build --push` e a imagem resultante no Docker Hub (tag `latest` com 2 arquiteturas: amd64, 386).
*   Buildx simplifica builds multi-arch:
    *   Combina build das variantes, push das variantes, criação do manifesto, push do manifesto em 1 comando (`--push`).
    *   Distribui trabalho entre nós da farm.
*   Diferença no Hub: Buildx (com `--push`) cria só a tag multi-arch (ex: `latest`), não as tags individuais por variante (ex: `:linux-amd64`). Mais limpo para usuários.
*   Conclusão: Se não precisar de Windows, Buildx é a melhor forma atual para multi-arch Linux.

🧠 Explicação detalhada e didática:

*   A página analisa o resultado do comando `buildx build --push` (Figura 16.15). A ferramenta orquestrou todo o processo: enviou o contexto e o Dockerfile para `node1` (para build amd64) e `node2` (para build 386), executou os builds em paralelo, coletou os resultados, enviou as imagens variantes resultantes para o Docker Hub e, finalmente, criou e enviou a lista de manifestos sob a tag `$dockerId/ch16-folder-list-2:latest`.
*   Uma diferença notável em relação ao método manual (build + push variantes + manifest create + manifest push) é que o Buildx, por padrão com `--push`, não cria as tags intermediárias por arquitetura (como `:linux-amd64`) no registro, apenas a tag final multi-arch (`:latest` ou a especificada em `-t`). Isso torna a visualização no Docker Hub mais limpa para os consumidores finais da imagem.
*   A conclusão é que, para cenários focados em múltiplas arquiteturas de CPU **Linux**, o Buildx é a ferramenta preferida devido à sua simplicidade e eficiência, superando o método manual com `docker manifest`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar no Hub:**
    *   `echo "https://hub.docker.com/r/$dockerId/ch16-folder-list-2/tags"`
    *   Acesse o URL. Você deve ver a tag `latest` com as arquiteturas `linux/amd64` e `linux/386`.
2.  **(Opcional) Remover Builder/Contexts/Nós PWD:**
    *   No `node1` do PWD:
        ```bash
        docker buildx rm ch16 # Remove o builder
        docker context rm node1 node2 # Remove os contexts
        # Feche a sessão PWD quando terminar.
        ```

---

📄 Página 339

🔍 Resumo dos principais pontos:

*   **16.5 Understanding where multi-arch images fit in your roadmap** (Entendendo onde imagens multi-arch se encaixam no seu roadmap)
*   Talvez não precise agora, mas vale a pena conhecer. Pode se tornar necessário (IoT, custo nuvem Arm, suporte Windows).
*   Diagrama (Figura 16.16): Evolução de um projeto suportando mais arquiteturas ao longo do tempo (começa Linux/Intel -> adiciona Arm -> adiciona Windows).
*   **Como se preparar / Future-proofing:**
    1.  **Regra 1:** Sempre usar imagens base (`FROM`) que *já são* multi-arch nos seus Dockerfiles.
    2.  **Regra 2:** Evitar comandos específicos de OS em `RUN`/`CMD`. Se precisar de lógica complexa, criar utilitário na mesma linguagem do app (portável).

🧠 Explicação detalhada e didática:

*   Esta seção discute a importância estratégica de considerar o suporte multi-arquitetura, mesmo que não seja uma necessidade imediata. O cenário de TI está mudando, com Arm ganhando espaço em servidores e a necessidade de suportar Windows Containers persistindo em muitos lugares. Estar preparado para adicionar suporte a novas plataformas no futuro é uma boa prática.
*   A Figura 16.16 ilustra como um projeto pode começar suportando apenas Linux/Intel e, posteriormente, adicionar suporte para Arm e/ou Windows conforme os requisitos mudam.
*   Para facilitar essa transição futura, a página sugere duas regras simples ao escrever *todos* os seus Dockerfiles *desde já*:
    1.  **Use Bases Multi-Arch:** Sempre que possível, escolha imagens na sua instrução `FROM` que já sejam multi-arch e suportem as plataformas que você *pode* vir a precisar no futuro (pelo menos Linux/Intel, Linux/Arm64, Windows/Intel). A maioria das imagens oficiais populares já são assim.
    2.  **Evite Comandos Específicos:** Tente não usar comandos nos seus `RUN` ou `CMD` que só funcionem em um sistema operacional (como `apt-get` vs. `choco`, ou `uname` vs. `ver`). Se precisar de lógica complexa, encapsule-a em um script ou pequeno utilitário escrito na mesma linguagem da sua aplicação (que geralmente é multiplataforma), em vez de usar scripts shell específicos do OS.
*   Seguir essas regras tornará muito mais fácil adicionar suporte a uma nova arquitetura posteriormente, muitas vezes apenas exigindo a configuração do pipeline de CI para construir também para a nova plataforma, sem grandes mudanças no Dockerfile.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Verificar Base:** Ao escolher uma imagem `FROM`, verifique sua página no Docker Hub para ver as "Supported Architectures". Prefira imagens que listem `amd64`, `arm64`, `windows/amd64` (se relevante).
*   **Exemplo Comando Portável vs. Não-Portável:**
    *   Não-Portável: `RUN apt-get install -y curl` (Só Linux Debian/Ubuntu)
    *   Mais Portável (se app for .NET): `RUN dotnet tool install --global portable.runtime.tool` (Ferramenta .NET roda onde .NET roda)
    *   Portável (se usar base multi-arch com ferramenta): `RUN apk add --no-cache curl` (Alpine Linux, pode existir em base multi-arch Alpine)

---

📄 Página 340

🔍 Resumo dos principais pontos:

*   Imagens oficiais no Hub são geralmente multi-arch -> boas bases (diretas ou para golden images).
*   Golden images do livro (`diamol/...`) são multi-arch. Código fonte em `images/` como exemplo.
*   Plataformas de app modernas (Go, Node.js, .NET Core, Java) são multi-arch. BD: Postgres é boa opção multi-arch.
*   **Build Multi-Arch em Serviços CI Gerenciados:**
    *   Limitado atualmente. Alguns suportam Linux + Windows.
    *   Para suporte Arm, geralmente precisa de build farm própria (VMs na nuvem ou hardware físico).
    *   Opções: AWS (VMs Linux/Win/Arm), Azure DevOps/GitHub Actions (Linux/Win, talvez Arm no futuro?).
*   Importante: Não assumir que nunca precisará. Seguir boas práticas facilita adicionar suporte depois.
*   **16.6 Lab** (Laboratório)
    *   Objetivo: Corrigir Dockerfile para ser multi-arch e otimizado.
    *   Ponto de Partida: Dockerfile (`ch16/lab`) que funciona mas usa base específica de arch e comandos OS-specific. Imagem resultante é grande.
*   **Tarefa:** Refatorar Dockerfile para:
    1.  Construir variantes para Linux/Intel, Linux/Arm, Windows/Intel.
    2.  Ser otimizado (provavelmente multi-estágio).

🧠 Explicação detalhada e didática:

*   A página reforça a disponibilidade de imagens base multi-arch (oficiais e as `diamol/` do livro) e plataformas de aplicação que facilitam a criação de aplicações multiplataforma. Menciona Postgres como um bom exemplo de banco de dados multi-arch.
*   Discute a realidade atual dos **serviços de build gerenciados na nuvem**: o suporte nativo para build multi-arquitetura completo (incluindo Arm) ainda é limitado. Embora muitos suportem Linux e Windows, construir para Arm frequentemente exige que você configure sua própria "build farm" (conjunto de máquinas de build) com hardware Arm real (ex: VMs Arm na AWS).
*   O **Lab do Capítulo 16** foca em refatorar um Dockerfile existente que *não* foi escrito pensando em multiplataforma. O Dockerfile original provavelmente usa uma imagem `FROM` específica de uma arquitetura e comandos `RUN`/`CMD` que só funcionam nela.
*   Sua tarefa é reescrever este Dockerfile para que ele possa ser usado (talvez com pequenas variações ou na abordagem de Dockerfile único com build cross-platform) para gerar imagens funcionais e otimizadas para Linux (Intel/Arm) e Windows (Intel). Isso provavelmente envolverá:
    *   Mudar para uma imagem `FROM` que seja multi-arch.
    *   Substituir comandos OS-specific por alternativas portáveis ou usar build multi-estágio para isolar passos específicos.
    *   Aplicar otimizações de tamanho/cache.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 16:**
    1.  Navegue para `ch16\lab`. Examine o `Dockerfile` atual. Identifique a imagem `FROM` e os comandos `RUN`/`CMD`.
    2.  **Escolha a Abordagem:**
        *   **Dockerfile Único:** Encontre uma imagem base multi-arch que substitua a atual e funcione em Linux/Win/Arm (se possível). Reescreva os comandos `RUN`/`CMD` para serem portáveis (talvez usando um script ou utilitário se a lógica for complexa). Use build multi-estágio para otimizar. Construa com `docker buildx build --platform linux/amd64,linux/arm,windows/amd64 ...` (ou `docker build --platform ...` repetido).
        *   **Dockerfiles Múltiplos:** Crie `Dockerfile.linux`, `Dockerfile.windows` (e talvez `Dockerfile.arm`). Cada um usa a melhor base para sua plataforma. Adapte os comandos `RUN`/`CMD` para cada OS. Use build multi-estágio em cada um. Construa cada um separadamente (`docker build -f ...`). Depois crie/push o manifesto manualmente (`docker manifest create/push`).
    3.  Verifique se as imagens resultantes funcionam e são otimizadas em tamanho.

---

📄 Página 341

🔍 Resumo dos principais pontos:

*   **Dicas Lab 16:**
    *   Instruções Dockerfile (ex: `COPY`, `WORKDIR`) são cross-platform. Comandos OS em `RUN` podem não ser.
    *   Alguns comandos Windows são iguais a Linux. Golden images `diamol/base` podem ter aliases para outros.
*   Link para solução no GitHub.

🧠 Explicação detalhada e didática:

*   Fornece dicas para o Lab 16, sugerindo focar em substituir comandos `RUN` específicos do OS por instruções Dockerfile (que são inerentemente multiplataforma) ou por comandos que funcionem em ambos os ambientes. Menciona que as imagens base `diamol/` podem incluir alguns truques (aliases) para fazer comandos Linux comuns funcionarem no Windows.
*   Oferece o link para a solução de referência.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Exemplo de Substituição:**
    *   Ruim (Linux): `RUN mkdir /app/data`
    *   Ruim (Windows): `RUN mkdir C:\app\data`
    *   Bom (Cross-platform Dockerfile): `WORKDIR /app`, `RUN mkdir data` (relativo ao WORKDIR) - Ou até melhor, se possível, usar `VOLUME /app/data` se for para dados persistentes.
*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch16/lab/README.md

---

📄 Página 342

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, separando a Parte 3 da Parte 4.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 343

🔍 Resumo dos principais pontos:

*   **Início da Parte 4: Getting your containers ready for production** (Preparando seus contêineres para produção)
*   Quase no fim; últimos tópicos importantes antes de ir para produção.
*   Foco Parte 4: Boas práticas essenciais para produção.
    *   Otimização de imagens Docker (tamanho, velocidade, segurança).
    *   Integração com plataforma Docker: ler config, escrever logs.
    *   Abordagens arquiteturais úteis com Docker:
        *   Reverse Proxy.
        *   Message Queue.

🧠 Explicação detalhada e didática:

*   Esta página introduz a Parte 4 e final do livro, que se concentra em refinar as imagens e aplicações Docker para estarem verdadeiramente prontas para um ambiente de produção.
*   Os tópicos cobrirão otimizações finais nas imagens, como garantir que as aplicações se integrem bem com os mecanismos de configuração e logging da plataforma Docker, e a introdução de dois padrões de arquitetura muito comuns e poderosos no mundo dos contêineres:
    *   **Proxy Reverso:** Para gerenciar tráfego de entrada, segurança (SSL) e performance (cache).
    *   **Fila de Mensagens (Message Queue):** Para comunicação assíncrona, desacoplamento e escalabilidade.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você revisitará a otimização de imagens (Cap. 17).
*   Você aprenderá padrões de configuração e logging mais avançados (Caps. 18, 19).
*   Você rodará e configurará proxies reversos como Nginx ou Traefik (Cap. 20).
*   Você rodará e usará filas de mensagens como Redis ou NATS (Cap. 21).

---

📄 Página 344

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, separando a introdução da Parte 4 do início do Capítulo 17.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 345

🔍 Resumo dos principais pontos:

*   **Capítulo 17: Optimizing your Docker images for size, speed, and security** (Otimizando suas imagens Docker para tamanho, velocidade e segurança)
*   Após containerizar e rodar em cluster, otimização é passo importante antes de ir para produção.
*   Por quê otimizar? Builds/deploys rápidos, segurança, evitar problemas (ex: disco cheio).
*   Dockerfile parece simples, mas esconde complexidade -> entender para otimizar.
*   Capítulo expande Cap. 3 (imagens = múltiplas camadas).
*   **17.1 How you optimize Docker images** (Como otimizar imagens Docker)
    *   Formato Docker já é otimizado (compartilhamento de camadas -> reduz build, rede, disco).
    *   Mas Docker *não* remove imagens/camadas antigas automaticamente. Limpeza (`prune`) é manual.
    *   Disco pode encher com camadas não usadas (especialmente em dev/teste com updates frequentes).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 17, focado na **otimização de imagens Docker**. Argumenta que, mesmo após ter a aplicação rodando em contêineres, há um trabalho de refinamento importante a ser feito na *imagem* para torná-la eficiente e segura para produção.
*   Os objetivos da otimização são:
    *   **Tamanho:** Imagens menores são mais rápidas de baixar, enviar e ocupam menos espaço em disco nos nós do cluster e nos registros.
    *   **Velocidade (Build):** Otimizar o uso do cache de build acelera o pipeline de CI/CD.
    *   **Segurança:** Imagens menores geralmente contêm menos software, reduzindo a "superfície de ataque" para vulnerabilidades.
*   A seção 17.1 começa relembrando que o sistema de camadas do Docker já oferece otimizações, mas o acúmulo de camadas não utilizadas ainda é um problema comum se a limpeza não for feita.

💻 Exemplos práticos (para meu ambiente Windows):

*   O comando `docker system prune` (e `docker image prune`, `docker container prune`, `docker volume prune`) será seu amigo para limpeza.
*   O capítulo focará em como *construir* imagens que sejam inerentemente menores e mais rápidas de construir/transferir.

---

📄 Página 346

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Verificar uso de disco com `docker system df`.
*   Figura 17.1: Exemplo de saída `system df`, mostrando muitos GBs usados por imagens inativas.
*   Servidores antigos podem desperdiçar centenas de GBs. `docker system prune` ajuda, mas otimizar imagens é melhor.
*   **Otimização 1: Não incluir arquivos desnecessários.**
    *   Comum copiar pastas inteiras (`COPY . .`) que incluem docs, imagens, binários não usados em runtime.
    *   Seja explícito no `COPY`.
*   **Listagem 17.1:** Comparação de Dockerfiles.
    *   `v1`: `COPY . .` (copia tudo, incluindo pasta `docs`).
    *   `v2`: `COPY . .` seguido por `RUN rm -rf docs` (tenta remover depois).

🧠 Explicação detalhada e didática:

*   O exercício com `docker system df` serve para conscientizar sobre quanto espaço imagens e outros artefatos Docker podem ocupar se não forem gerenciados. A limpeza periódica com `prune` é recomendada.
*   A primeira técnica de otimização abordada é a mais básica: **evitar incluir lixo na imagem**. É muito comum usar `COPY . .` no Dockerfile por conveniência, mas isso copia *tudo* do diretório de contexto, incluindo arquivos que não são necessários para a aplicação rodar (código fonte após compilação, arquivos de teste, documentação, etc.).
*   A Listagem 17.1 apresenta um cenário problemático:
    *   `v1` copia tudo.
    *   `v2` copia tudo e depois tenta *remover* os arquivos desnecessários (`docs`) em um passo `RUN` subsequente.
*   A intuição poderia dizer que `v2` seria menor, mas como veremos, devido ao funcionamento das camadas, isso não acontece.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Uso de Disco:**
    ```powershell
    docker system df
    # Anote o tamanho das Imagens (SIZE e RECLAIMABLE)
    ```
2.  **(Opcional) Limpeza:** Se o espaço recuperável (RECLAIMABLE) for grande, você pode limpar:
    ```powershell
    docker image prune -a # Remove imagens não usadas (cuidado!)
    docker system prune # Remove containers parados, redes/caches não usados
    docker system df # Verifique novamente
    ```
3.  **Preparar para Próximo Exercício:** Você precisará dos Dockerfiles da Listagem 17.1 e do conteúdo da pasta `ch17/exercises/build-context` (que deve ter subpastas `app` e `docs`).

---

📄 Página 347

🔍 Resumo dos principais pontos:

*   Tentar remover em `v2` não funciona: `rm` em camada posterior só *oculta* arquivos da camada `COPY`, não reduz tamanho total.
*   **Exercício Try-it-now:** Construir v1 e v2, comparar tamanhos.
    *   Navegar para `ch17/exercises/build-context`.
    *   `docker build -t ...:v1 .`
    *   `docker build -t ...:v2 -f ./Dockerfile.v2 .`
    *   `docker image ls -f reference=diamol/ch17*`.
*   Resultado esperado (Figura 17.2): v1 e v2 têm **exatamente o mesmo tamanho**. `rm` não adiantou.
*   Explicação: Camadas são unidas. Arquivos de `COPY` ainda existem na camada 1, só estão escondidos pela camada 2 (`RUN rm`).
*   **Exercício Try-it-now:** Provar que arquivos "deletados" ainda existem em camada anterior.
    *   1. Rodar container da imagem final v2: `docker run ...:v2`. (Deve mostrar só `app`, não `docs`).
    *   2. Ver histórico de v2: `docker history ...:v2`. Anotar ID da camada *anterior* ao `RUN rm`.
    *   3. Rodar container a partir da *camada anterior*: `docker run <id_camada_anterior>`. (Deve mostrar `app` *e* `docs`).

🧠 Explicação detalhada e didática:

*   Esta página revela a "pegadinha" da Listagem 17.1: tentar remover arquivos em uma instrução `RUN` *depois* de terem sido copiados em uma instrução `COPY` anterior **não reduz o tamanho final da imagem**.
*   **Por quê?** Cada instrução (`COPY`, `RUN`) cria uma camada. A imagem final é a união de todas as camadas. A camada criada pelo `COPY . .` contém as pastas `app` e `docs`. A camada criada pelo `RUN rm -rf docs` contém uma informação que diz "oculte a pasta `docs` da camada anterior". O resultado visível no contêiner final não mostra `docs`, mas os arquivos físicos de `docs` *ainda estão presentes* na camada do `COPY`, ocupando espaço.
*   O primeiro exercício comprova isso mostrando que as imagens `v1` e `v2` têm o mesmo tamanho.
*   O segundo exercício demonstra isso de forma mais concreta:
    *   Executa-se um contêiner da imagem `v2` final; ele não mostra a pasta `docs`.
    *   Usa-se `docker history` para encontrar o ID da camada criada *antes* da execução do `RUN rm`.
    *   Executa-se um contêiner *diretamente* a partir daquele ID de camada anterior. Como este contêiner só "vê" as camadas até aquele ponto, a instrução `rm` ainda não foi aplicada, e a pasta `docs` está visível.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\build-context
    ```
2.  **Construir e Comparar Tamanhos:**
    ```powershell
    docker image build -t diamol/ch17-build-context:v1 .
    docker image build -t diamol/ch17-build-context:v2 -f ./Dockerfile.v2 .
    docker image ls -f reference='diamol/ch17-build-context:*'
    # Confirme que v1 e v2 têm o mesmo SIZE.
    ```
3.  **Verificar Camadas:**
    ```powershell
    # Rodar v2 final (não mostra 'docs')
    docker container run diamol/ch17-build-context:v2

    # Obter ID da camada ANTES do RUN rm (geralmente a 2ª linha de cima para baixo)
    docker history diamol/ch17-build-context:v2
    # Anote o IMAGE ID da linha abaixo da que contém 'rm -rf docs'

    # Rodar container da camada anterior (mostra 'docs')
    docker container run <ID_DA_CAMADA_ANTERIOR>
    ```

---

📄 Página 348

🔍 Resumo dos principais pontos:

*   Figura 17.3: Mostra saída do `docker run <camada_anterior>`, confirmando que `docs` ainda estava lá.
*   **Lição 1: Otimizar cada camada.** Não copiar lixo esperando limpar depois.
*   Melhor: Ser preciso no `COPY`.
*   **Listagem 17.2:** Dockerfile `v3` otimizado. Só copia pasta `app`.
    ```dockerfile
    FROM diamol/base
    CMD echo app- && ls app && echo docs- && ls docs
    COPY ./app ./app # Copia apenas o necessário
    ```
*   **Otimização 2: Contexto de Build (`.dockerignore`).**
    *   `docker build` primeiro comprime e envia o **contexto de build** (pasta do build) para o Docker Engine.
    *   Contexto pode conter arquivos desnecessários para o build (ex: `.git`, logs, binários locais). Envio consome tempo/banda.
    *   Solução: Arquivo `.dockerignore` na raiz do contexto. Lista arquivos/pastas a serem **excluídos** do envio para o Engine.
*   **Exercício Try-it-now:** Construir v3 com e sem `.dockerignore`.
    *   1. Build v3 (sem `.dockerignore`): `docker build -t ...:v3 -f ./Dockerfile.v3 .`. Observar tamanho do contexto enviado (ex: 2MB).
    *   2. Renomear arquivo preparado: `mv rename.dockerignore .dockerignore`.
    *   3. Ver conteúdo do `.dockerignore` (`cat .dockerignore`). Ex: `docs/`, `Dockerfile*`.
    *   4. Build v3 novamente (com `.dockerignore`): `docker build -t ...:v3 -f ./Dockerfile.v3 .`. Observar tamanho do contexto (ex: 4KB).

🧠 Explicação detalhada e didática:

*   A página conclui a lição sobre limpeza de camadas: a única forma eficaz é **não adicionar o lixo em primeiro lugar**. Use instruções `COPY` mais específicas em vez de `COPY . .` sempre que possível. A Listagem 17.2 mostra a versão `v3` corrigida, que só copia a pasta `app`, resultando em uma imagem menor.
*   Introduz a segunda grande otimização relacionada ao build: o **Contexto de Build** e o arquivo `.dockerignore`.
*   Quando você executa `docker build ... .`, o `.` diz ao Docker para usar o diretório atual como contexto. A *primeira coisa* que a CLI Docker faz é empacotar *todo* o conteúdo desse diretório (e subdiretórios) e enviá-lo para o Docker Engine (que pode estar na mesma máquina ou remotamente). O Engine então usa esse contexto para executar o build.
*   Se o diretório de contexto contiver muitos arquivos grandes que não são realmente necessários para construir a imagem (como a pasta `.git`, dependências baixadas localmente, arquivos de log, outras imagens Docker exportadas, etc.), esse envio inicial pode ser muito lento e consumir muita rede/disco.
*   O arquivo `.dockerignore` (colocado na raiz do contexto, com sintaxe similar ao `.gitignore`) resolve isso. Ele instrui a CLI Docker a **ignorar** certos arquivos e pastas ao criar o pacote de contexto a ser enviado para o Engine. Arquivos ignorados simplesmente não fazem parte do contexto que o Engine recebe.
*   O exercício demonstra o impacto disso:
    *   O primeiro build `v3` envia um contexto grande (2MB no exemplo, por causa de um arquivo de imagem na pasta `docs`).
    *   Após criar/renomear `.dockerignore` para excluir `docs/` e os próprios `Dockerfile`s, o segundo build `v3` envia um contexto minúsculo (4KB), tornando o início do build muito mais rápido.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar (se necessário):** `cd C:\DockerLabs\diamol\ch17\exercises\build-context`
2.  **Construir v3 (Sem Ignore):**
    ```powershell
    docker image build -t diamol/ch17-build-context:v3 -f ./Dockerfile.v3 .
    # Observe a linha "Sending build context to Docker daemon X.XMB"
    ```
3.  **Criar/Renomear `.dockerignore`:**
    ```powershell
    Rename-Item -Path ./rename.dockerignore -NewName .dockerignore
    # Verificar conteúdo:
    Get-Content .dockerignore
    # Deve listar 'docs/' e 'Dockerfile*'
    ```
4.  **Construir v3 (Com Ignore):**
    ```powershell
    docker image build -t diamol/ch17-build-context:v3 -f ./Dockerfile.v3 .
    # Observe a linha "Sending build context...". O tamanho deve ser muito menor (kB).
    # O tamanho final da imagem (docker image ls) também será menor que v1/v2.
    ```

---

📄 Página 349

🔍 Resumo dos principais pontos:

*   Figura 17.4: Mostra saída dos builds v3, destacando redução drástica no tamanho do contexto enviado com `.dockerignore`.
*   `.dockerignore` economiza tempo (envio do contexto) e pode economizar espaço na imagem (evita cópia acidental).
*   Formato similar a `.gitignore`. Usar templates GitHub como base é bom (incluir `.git/` se Dockerfile na raiz).
*   Próximo passo na otimização: Escolher a imagem base correta.
*   **17.2 Choosing the right base images** (Escolhendo as imagens base corretas)
    *   Escolha da base impacta **segurança**, tamanho e tempo de transferência.
    *   Base OS grande (ex: Ubuntu completo, Windows Server Core) pode ter ferramentas extras -> brechas de segurança em container.
    *   Ex: Se imagem base tem `curl`, atacante que invadir container pode usá-lo para baixar malware ou exfiltrar dados.

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre `.dockerignore`, ressaltando seus benefícios duplos: acelera o início do build (menos dados para enviar) e ajuda a prevenir que arquivos indesejados sejam incluídos na imagem acidentalmente por um `COPY` abrangente.
*   A seção 17.2 move o foco da otimização para a **escolha da imagem base** (`FROM ...`). Esta é uma das decisões mais impactantes.
*   O argumento principal é que imagens base "grandes" (como uma imagem completa de servidor Ubuntu ou Windows Server Core) trazem muitos utilitários e bibliotecas que são úteis em um servidor normal, mas desnecessários e potencialmente **perigosos** dentro de um contêiner focado em rodar uma única aplicação.
*   Cada ferramenta extra presente na imagem base aumenta a **superfície de ataque**. O exemplo dado é o `curl`: se sua aplicação web não precisa do `curl` para funcionar, mas ele está presente na imagem base, um invasor que consiga executar comandos no seu contêiner comprometido pode usar o `curl` para baixar mais ferramentas de ataque ou enviar dados roubados para fora. O mesmo vale para compiladores, shells avançados, editores de texto, etc.

💻 Exemplos práticos (para meu ambiente Windows):

*   Pense nas imagens base que você já usou:
    *   `diamol/base`: Provavelmente mínima.
    *   `diamol/node`, `diamol/openjdk`, `diamol/golang`: Contêm runtimes/SDKs, mas idealmente sobre uma base OS mínima.
    *   `debian:stretch-slim`: Exemplo de base Linux mínima (Debian).
    *   `mcr.microsoft.com/windows/nanoserver`: Base Windows mínima.
    *   `ubuntu`, `mcr.microsoft.com/windows/servercore`: Bases maiores.
*   A recomendação será usar as bases *menores* possíveis que ainda permitam sua aplicação funcionar.

💡 Analogias ou observações úteis (se houver):

*   Escolher Imagem Base: É como escolher o carro para uma viagem específica.
    *   Base Grande (Ubuntu/Server Core): Um caminhão com todas as ferramentas possíveis. Ótimo se você precisar consertar tudo na estrada, mas pesado, lento e chama atenção.
    *   Base Pequena (Alpine/Nano): Um carro compacto e eficiente. Leva você ao destino, é rápido, gasta pouco, discreto, mas não tem ferramentas extras se algo muito inesperado acontecer. Para contêineres (que devem ser substituídos, não consertados), o compacto é geralmente melhor.

---

📄 Página 350

🔍 Resumo dos principais pontos:

*   Vale também para imagens base de plataforma de app (ex: Java JDK vs JRE).
*   Tabela 17.1: Compara tamanhos de imagens OpenJDK 11 (JDK vs JRE vs Slim/Nano).
    *   Linux: JDK=296MB, JRE=103MB, JRE-Slim=69MB.
    *   Windows: JDK=2.4GB, JRE=2.2GB, JRE-Nano=277MB.
*   Diferenças enormes! Usar variantes `*-slim` (Linux/Debian), `*-alpine` (Linux/Alpine), `*-nanoserver` (Windows) reduz muito tamanho e superfície de ataque.
*   Nem todo app funciona nas variantes mínimas, mas vale a pena testar.
*   Risco de ter SDK na imagem final: Atacante pode compilar/executar código arbitrário dentro do container comprometido.
*   **Exercício Try-it-now:** Demonstrar risco do SDK na imagem final.
    *   App: API Java simples (`diamol/ch17-truth-app`) construída com base JDK (`:11-jdk`).
    *   1. Buildar imagem: `docker build -t diamol/ch17-truth-app .`
    *   2. Rodar container: `docker run -d -p 8010:80 --name truth ...`
    *   3. Testar API: `curl http://localhost:8010/truth` (deve retornar `true`).

🧠 Explicação detalhada e didática:

*   A página estende o argumento sobre bases mínimas para as imagens que fornecem **runtimes ou SDKs** de linguagens (Java, .NET, Python, etc.). Muitas vezes, essas imagens vêm em múltiplas variantes.
*   A Tabela 17.1 é um exemplo chocante com OpenJDK 11:
    *   A imagem com o JDK completo (necessário para *compilar* Java) é muito maior que a imagem apenas com o JRE (necessário para *rodar* Java compilado).
    *   Existem variantes ainda menores (`-slim` no Linux, `-nanoserver` no Windows) que usam um OS base mínimo e um JRE enxuto.
*   A recomendação é clara: para a imagem *final* da sua aplicação (o último estágio do build multi-estágio), use a imagem base **mais minimalista possível** que contenha apenas o **runtime** necessário, não o SDK completo. As bases recomendadas são Alpine ou Debian Slim para Linux, e Nano Server para Windows.
*   Além do tamanho, o risco de **segurança** de incluir o SDK na imagem de produção é destacado: se um invasor ganhar acesso ao contêiner, ele pode usar o compilador (JDK) presente na imagem para criar e executar seu próprio código malicioso dentro do seu ambiente.
*   O exercício prepara a demonstração desse risco: constrói e roda uma API Java usando uma imagem base que inclui o JDK completo.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\truth-app
    ```
2.  **Construir Imagem (Base JDK):**
    ```powershell
    docker image build -t diamol/ch17-truth-app .
    # Observe o Dockerfile, ele usa 'FROM diamol/openjdk' que provavelmente é baseado no JDK.
    # Verifique o tamanho da imagem: docker image ls diamol/ch17-truth-app
    ```
3.  **Rodar Container:**
    ```powershell
    docker container run -d -p 8010:80 --name truth diamol/ch17-truth-app
    ```
4.  **Testar API:**
    ```powershell
    Invoke-WebRequest http://localhost:8010/truth
    # Ou curl http://localhost:8010/truth
    # A saída (Content) deve ser 'true'.
    ```

---

📄 Página 351

🔍 Resumo dos principais pontos:

*   Container `truth` tem API Java rodando, mas também ferramentas para compilar outros apps Java.
*   Suposição: Invasor consegue acesso shell ao container. Imagem "acidentalmente" inclui arquivo de código fonte de teste (`FileUpdateTest.java`).
*   Invasor pode usar JDK (presente na imagem) para compilar e rodar esse código, alterando comportamento do app.
*   **Exercício Try-it-now:** Simular invasão e execução de código.
    *   1. Conectar ao shell do container `truth`: `docker exec -it truth sh` (Linux) ou `cmd` (Windows).
    *   2. Dentro do container, compilar e rodar o código Java de teste: `javac FileUpdateTest.java`, `java FileUpdateTest`.
    *   3. Sair do container: `exit`.
    *   4. Testar API novamente: `curl http://localhost:8010/truth`.

🧠 Explicação detalhada e didática:

*   A página descreve o cenário de ataque: um invasor consegue executar comandos dentro do contêiner `truth`. Como a imagem foi construída sobre uma base com o JDK completo, o invasor encontra as ferramentas `javac` (compilador) e `java` (runtime) disponíveis.
*   O cenário é facilitado por um arquivo de código fonte (`FileUpdateTest.java`) "esquecido" na imagem. O invasor compila esse arquivo usando `javac` e o executa usando `java`. Esse código de teste maliciosamente (ou acidentalmente) altera algum estado ou configuração da aplicação principal que está rodando no mesmo processo ou ambiente.
*   O exercício simula exatamente isso:
    1.  Usa `docker exec -it` para obter um shell dentro do contêiner `truth` que já está rodando.
    2.  Dentro do shell do contêiner, executa os comandos `javac` e `java` para compilar e rodar o código malicioso/teste.
    3.  Sai do contêiner.
    4.  Acessa a API `/truth` novamente pelo host. A expectativa é que a resposta tenha mudado (de `true` para `false`), demonstrando que o código executado pelo "invasor" afetou a aplicação principal.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Conectar ao Container:** (Assumindo imagem base Linux)
    ```powershell
    docker container exec -it truth sh
    ```
2.  **Dentro do Container (`/app #`):**
    ```bash
    ls # Você deve ver FileUpdateTest.java e talvez outros arquivos .class/.jar
    javac FileUpdateTest.java # Compila
    ls # Agora deve ter FileUpdateTest.class
    java FileUpdateTest # Executa (pode imprimir algo ou não)
    exit
    ```
3.  **Testar API Novamente:**
    ```powershell
    Invoke-WebRequest http://localhost:8010/truth
    # Ou curl http://localhost:8010/truth
    # A saída (Content) agora deve ser 'false'.
    ```

💡 Analogias ou observações úteis (se houver):

*   Deixar SDK na imagem de produção: É como deixar a caixa de ferramentas completa do mecânico dentro do carro depois que ele terminou o conserto. Se um ladrão arrombar o carro, ele pode usar as ferramentas para desmontar o carro por dentro ou fazer outras modificações indesejadas. Leve apenas o estepe e o macaco (o runtime), não a oficina inteira (o SDK).

---

📄 Página 352

🔍 Resumo dos principais pontos:

*   Figura 17.5: Mostra saída antes (`true`) e depois (`false`) da "invasão".
*   Exemplo é simplificado, mas ilustra risco real. Ataque funcionaria mesmo se container tivesse rede restrita.
*   Lição: Imagem base deve ter só o necessário para rodar app, sem ferramentas de build (exceto linguagens interpretadas onde build tools = runtime tools).
*   **Golden Images:** Revisitadas como solução.
    *   Time dedicado escolhe base mínima oficial, adiciona configs/ferramentas *essenciais* da empresa.
    *   Imagem `diamol/openjdk` do autor segue essa prática (multi-arch, base mínima).
    *   Pode-se integrar **varredura de segurança** (ex: Anchore) no build da golden image.
*   **Anchore:** Ferramenta open source para análise de imagem Docker.
    *   Roda em containers (mas não multi-arch atualmente).
    *   Exercício requer Linux/Intel (Docker Desktop ou PWD).
*   **Exercício Try-it-now:** Usar Anchore para analisar golden image `diamol/openjdk`.
    *   Navegar para `ch17/exercises/anchore`.
    *   1. Iniciar stack Anchore: `docker-compose up -d`.
    *   2. Esperar download do BD de vulnerabilidades (pode levar 15 min): `docker exec ... anchore-cli system wait`.
    *   3. Copiar Dockerfile da golden image para container Anchore API: `docker cp ...`.
    *   4. Adicionar imagem + Dockerfile para análise: `docker exec ... anchore-cli image add diamol/openjdk --dockerfile /Dockerfile`.
    *   5. Esperar análise completar: `docker exec ... anchore-cli image wait diamol/openjdk`.

🧠 Explicação detalhada e didática:

*   A página conclui a demonstração do risco do SDK e reforça a importância de usar imagens base mínimas contendo apenas o runtime.
*   Reintroduz as **Golden Images** (Cap. 5) como a melhor prática para gerenciar imagens base em uma organização. Uma equipe centralizada pode criar e manter essas golden images, garantindo que usem bases oficiais mínimas, adicionem apenas o necessário e, crucialmente, passem por **varreduras de segurança** regulares.
*   Introduz o **Anchore Engine** como uma ferramenta open source popular para realizar essas varreduras de segurança em imagens Docker. O Anchore analisa todas as camadas da imagem, identifica o software instalado (pacotes do SO, bibliotecas de linguagem) e compara com bancos de dados de vulnerabilidades conhecidas (CVEs).
*   O exercício instrui a iniciar o Anchore (que roda como múltiplos contêineres definidos em um `docker-compose.yml`) e usá-lo para analisar a golden image `diamol/openjdk` do autor. O processo envolve:
    1.  Iniciar os contêineres do Anchore.
    2.  Esperar que ele baixe seu banco de dados de vulnerabilidades (demorado na primeira vez).
    3.  Adicionar a imagem a ser analisada (`diamol/openjdk`) ao Anchore, opcionalmente fornecendo seu Dockerfile para análise extra.
    4.  Esperar que a análise seja concluída.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Importante:** Anchore Engine (nesta versão do livro) não roda bem em Windows Containers ou Mac. Use **Docker Desktop no modo Linux** ou **Play With Docker (PWD)**.
1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\anchore
    ```
2.  **Iniciar Anchore:**
    ```powershell
    docker-compose up -d
    ```
3.  **Esperar BD (em outro terminal):** Pode levar bastante tempo!
    ```powershell
    docker exec anchore_engine-api_1 anchore-cli system wait
    # Mantenha este terminal esperando. Continue os próximos passos em outro.
    ```
4.  **Copiar Dockerfile (em outro terminal):**
    ```powershell
    # O path pode precisar de ajuste dependendo de onde está a pasta 'images'
    docker container cp "$(pwd)/../../../images/openjdk/Dockerfile" anchore_engine-api_1:/Dockerfile
    ```
5.  **Adicionar Imagem para Análise:**
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image add diamol/openjdk --dockerfile /Dockerfile
    ```
6.  **Esperar Análise:**
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image wait diamol/openjdk
    ```
    Aguarde este comando retornar antes de prosseguir para a próxima página.

---

📄 Página 353

🔍 Resumo dos principais pontos:

*   Figura 17.6: Mostra comando `image add` e `image wait` do Anchore.
*   Após análise, Anchore conhece muito sobre a imagem:
    *   Licenças open source usadas.
    *   Detalhes do OS e plataforma de app.
    *   Vulnerabilidades de segurança (CVEs) nos binários.
*   Resultados podem ser usados como quality gate para aceitar/rejeitar update de imagem base.
    *   Ex: Falhar build se licença proibida ou CVE crítica encontrada.
*   Anchore tem plugins CI/CD (Jenkins) para automação de políticas.
*   Pode-se consultar resultados via API Anchore.
*   **Exercício Try-it-now:** Consultar resultados da análise Anchore.
    *   1. Checar componentes Java encontrados: `docker exec ... anchore-cli image content diamol/openjdk java`.
    *   2. Checar vulnerabilidades conhecidas: `docker exec ... anchore-cli image vuln diamol/openjdk all`.

🧠 Explicação detalhada e didática:

*   Com a análise do Anchore concluída, a página explica os tipos de informações que ele extrai da imagem Docker: detalhes sobre o software instalado, suas licenças e, mais importante, quaisquer **vulnerabilidades de segurança conhecidas** associadas a esses componentes.
*   Essas informações são valiosas para tomar decisões informadas sobre quais imagens base usar e quando atualizá-las. Uma organização pode definir políticas (ex: "não usar imagens com vulnerabilidades críticas ou altas não corrigidas", "não usar software com licença X") e usar o Anchore (integrado ao pipeline de CI/CD) para **automatizar a verificação** dessas políticas. Builds de imagens que violarem as políticas podem ser automaticamente reprovados.
*   O exercício mostra como consultar manualmente alguns desses resultados usando a CLI do Anchore (executada dentro do contêiner da API do Anchore):
    *   `image content ... java`: Lista os pacotes/componentes relacionados a Java encontrados na imagem.
    *   `image vuln ... all`: Lista todas as vulnerabilidades (CVEs) encontradas em *qualquer* componente da imagem.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Consultar Conteúdo Java:** (Após `image wait` completar na p.352)
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image content diamol/openjdk java
    ```
2.  **Consultar Vulnerabilidades:**
    ```powershell
    docker container exec anchore_engine-api_1 anchore-cli image vuln diamol/openjdk all
    ```
    Observe a saída. Pode haver uma lista de CVEs. Preste atenção na coluna `Severity` (Negligible, Low, Medium, High, Critical).

---

📄 Página 354

🔍 Resumo dos principais pontos:

*   Figura 17.7: Mostra saída (parcial) do `image content` e `image vuln`.
*   Resultados para `diamol/openjdk`: detalhes Java, lista de CVEs (maioria `Negligible`). Saída inclui link para detalhes da CVE.
*   Resultados aceitáveis pois usou base OpenJDK *mínima*.
*   Se analisasse `openjdk:11-jdk` (completo), veria mais CVEs, incluindo `Low` em lib SSL. Poderia ser inaceitável.
*   Scan reforça valor de usar bases mínimas e golden images.
*   Outras ferramentas de scan: Clair (open source), Aqua, Twistlock (comerciais, integram com registro).
*   Ferramentas ajudam a entender segurança e dão confiança nas golden images.
*   Pode-se escanear imagens de app também; política pode exigir que `FROM` seja golden image.
*   **17.3 Minimizing image layer count and layer size** (Minimizando contagem e tamanho das camadas da imagem)
    *   Após escolher base segura/mínima, próximo passo: otimizar o que *você* adiciona.
    *   Foco: Instalar software. Processos de instalação podem deixar "resíduos" (listas de pacotes cacheadas, dependências recomendadas desnecessárias).

🧠 Explicação detalhada e didática:

*   A página analisa os resultados do scan do Anchore para a imagem `diamol/openjdk` (Figura 17.7). Como esperado de uma imagem baseada em um JRE mínimo, as vulnerabilidades encontradas são poucas e de baixa severidade ("Negligible"). A página contrasta isso com o que seria encontrado na imagem JDK completa, que teria mais software e, consequentemente, mais vulnerabilidades potenciais. Isso reforça a recomendação de usar bases mínimas e o valor das golden images (que podem ser escaneadas e aprovadas).
*   Menciona outras ferramentas de mercado (open source e comerciais) que fazem análise de segurança similar. O importante é integrar *alguma* forma de varredura no seu processo. Escanear as imagens finais das aplicações (e não apenas as golden images) também é recomendado, e pode-se criar políticas para garantir que elas usem as golden images aprovadas como base.
*   A seção 17.3 volta o foco para a otimização do **seu próprio Dockerfile**, especificamente em como você **adiciona software** (ex: usando gerenciadores de pacote como `apt-get`, `apk`, `yum`) ou arquivos às suas imagens. A meta é garantir que cada camada adicione o mínimo necessário, evitando "resíduos" que aumentam o tamanho.

💻 Exemplos práticos (para meu ambiente Windows):

*   **(Opcional) Parar Anchore:** Se não for usar mais:
    ```powershell
    # Estando em ch17\exercises\anchore
    docker-compose down -v # -v remove os volumes com o BD do Anchore
    ```
*   A próxima seção mostrará otimizações para `apt-get` (Linux). Os princípios se aplicam a outros gerenciadores (ex: `choco` no Windows, `apk` no Alpine).

---

📄 Página 355

🔍 Resumo dos principais pontos:

*   **Otimização com Gerenciadores de Pacote (Exemplo: `apt-get` Debian/Linux):**
*   **Exercício Try-it-now:** Comparar install não otimizado vs. otimizado. (Requer host Linux ou Docker Desktop modo Linux).
    *   Navegar para `ch17/exercises/socat`.
    *   `v1`: Instala `curl` e `socat` com comandos `apt-get` padrão em `RUN`s separados.
        *   `docker build -t ...:v1 .`
    *   `v2`: Usa tweaks (`--no-install-recommends`, `rm` cache `apt`) em `RUN` único.
        *   `docker build -t ...:v2 -f Dockerfile.v2 .`
    *   Comparar tamanhos: `docker image ls -f reference=diamol/ch17-socat`.
*   Resultado esperado (Figura 17.8): Imagem `v2` significativamente menor (ex: 20MB a menos, >20% de redução).

🧠 Explicação detalhada e didática:

*   Esta página foca em como otimizar a instalação de pacotes usando gerenciadores como o `apt-get` (comum em Debian/Ubuntu). O exercício compara duas abordagens:
    *   **Não Otimizada (`v1`):** Usa comandos `apt-get update` e `apt-get install` separados em múltiplas instruções `RUN`. Isso é ineficiente porque:
        1.  `apt-get update` baixa listas de pacotes que ficam cacheadas em uma camada.
        2.  Cada `apt-get install` pode baixar o pacote e suas dependências (incluindo as "recomendadas", nem sempre necessárias) em camadas separadas.
        3.  Os arquivos de cache do `apt` permanecem nas camadas intermediárias, mesmo que não sejam necessários na imagem final.
    *   **Otimizada (`v2`):** Aplica várias técnicas em uma **única instrução `RUN`** (para evitar camadas intermediárias desnecessárias):
        1.  Chama `apt-get update` no início.
        2.  Usa `apt-get install -y --no-install-recommends ...` para instalar os pacotes desejados (`curl`, `socat`) *sem* puxar dependências opcionais/recomendadas.
        3.  No final do *mesmo* `RUN`, executa `rm -rf /var/lib/apt/lists/*` para limpar o cache de listas de pacotes baixado pelo `update`.
*   Como tudo acontece em uma única camada `RUN`, a limpeza do cache efetivamente remove esses arquivos do resultado final, e a opção `--no-install-recommends` evita baixar pacotes extras. O resultado é uma imagem consideravelmente menor.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Requer Docker Desktop MODO LINUX.**
1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\socat
    ```
2.  **Construir v1:**
    ```powershell
    docker image build -t diamol/ch17-socat:v1 .
    ```
3.  **Construir v2:**
    ```powershell
    docker image build -t diamol/ch17-socat:v2 -f Dockerfile.v2 .
    ```
4.  **Comparar Tamanhos:**
    ```powershell
    docker image ls -f reference=diamol/ch17-socat
    # Observe a diferença significativa no SIZE entre v1 e v2.
    ```

---

📄 Página 356

🔍 Resumo dos principais pontos:

*   Figura 17.8: Confirma redução de tamanho de ~20MB com otimizações `apt-get`.
*   **Listagem 17.3:** Compara Dockerfiles `v1` (não otimizado) e `v2` (otimizado) do `socat`. Destaca `RUN` único, `--no-install-recommends`, `rm` do cache apt.
*   **Benefício `RUN` Único:** Cria 1 camada só. Reduz contagem total (máximo ~127), mas principal vantagem é **clareza/atomicidade** e permitir limpeza eficaz (como `rm` do cache apt na mesma camada da instalação).
*   **Erro Comum:** Colocar `rm` em `RUN` separado -> não reduz tamanho (só oculta).
*   **Otimização 3: Download + Extração + Limpeza em Camada Única.**
    *   Cenário comum: Baixar arquivo comprimido (zip, tar.gz), extrair, deletar arquivo original.
    *   Temptador: Fazer em `RUN`s separados durante dev (para cache do download).
    *   Otimização Final: Combinar download, extração e `rm` do arquivo baixado em um **único `RUN`**.
*   **Exercício Try-it-now:** Exemplo com dataset ML (grande).
    *   Navegar para `ch17/exercises/ml-dataset`.
    *   `v1`: `RUN wget ...`, `RUN tar ...`, `RUN rm ...` (separados).
        *   `docker build -t ...:v1 .`
    *   `v2`: Otimizado (download, extração *seletiva*, `rm` em `RUN` único).
        *   `docker build -t ...:v2 -f Dockerfile.v2 .`
    *   Comparar tamanhos: `docker image ls -f reference=diamol/ch17-ml-dataset`.

🧠 Explicação detalhada e didática:

*   A Listagem 17.3 deixa clara a diferença entre as abordagens `apt-get`. A chave da otimização `v2` é realizar o `update`, `install` (com `--no-install-recommends`) e a limpeza (`rm -rf /var/lib/apt/lists/*`) **dentro da mesma instrução `RUN`**. Isso garante que os arquivos temporários do cache do `apt` sejam removidos *antes* da camada ser finalizada, efetivamente reduzindo o tamanho.
*   A página então generaliza esse princípio para outro cenário comum: baixar um arquivo compactado, extrair seu conteúdo e remover o arquivo compactado original. Se esses passos forem feitos em instruções `RUN` separadas, o arquivo compactado original (potencialmente grande) permanecerá em uma camada intermediária, inflando o tamanho final da imagem, mesmo que tenha sido removido depois.
*   A **otimização** é combinar tudo em um único `RUN`: `RUN wget ... && tar ... && rm ...`. O `&&` garante que os comandos rodem em sequência na mesma camada.
*   O exercício compara uma versão não otimizada (`v1`) com uma otimizada (`v2`) para baixar e extrair (parcialmente) um dataset de Machine Learning. A versão `v2` não só combina os passos, mas também usa uma opção do `tar` para extrair *apenas* o arquivo necessário, evitando extrair GBs de dados desnecessários para depois deletá-los (o que também não funcionaria bem).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\ml-dataset
    ```
2.  **Construir v1 (Lento/Grande):**
    ```powershell
    docker image build -t diamol/ch17-ml-dataset:v1 .
    # Este pode demorar e ocupar bastante espaço temporário durante o build.
    ```
3.  **Construir v2 (Otimizado):**
    ```powershell
    docker image build -t diamol/ch17-ml-dataset:v2 -f Dockerfile.v2 .
    ```
4.  **Comparar Tamanhos:**
    ```powershell
    docker image ls -f reference=diamol/ch17-ml-dataset
    # A diferença de SIZE deve ser GIGANTESCA.
    ```

---

📄 Página 357

🔍 Resumo dos principais pontos:

*   Figura 17.9: Mostra comparação de tamanho das imagens ML dataset (v1 ~2.5GB, v2 ~24MB). Diferença enorme.
*   Não é exemplo tão artificial; comum separar `RUN`s durante dev para debug/cache. Precisa otimizar (combinar) depois.
*   **Listagem 17.4:** Dockerfile otimizado (`v2`) do ML dataset. Combina `wget`, `tar` (extração seletiva), `rm` em `RUN` único.
    *   Extração seletiva (`tar -xf ... <arquivo_especifico>`) também crucial para evitar inflar camada.
*   **Alternativa Melhor: Multi-Stage Builds para Otimização.**
    *   Revisita multi-stage (Cap. 4). Ideal para otimizar imagem final.
*   **Listagem 17.5:** Refatoração do ML dataset usando multi-stage (`v3`).
    ```dockerfile
    # Estágio 1: Download
    FROM diamol/base AS download
    ARG DATASET_URL=...
    RUN wget -O dataset.tar.gz ${DATASET_URL}

    # Estágio 2: Extração Completa (em ambiente temporário)
    FROM diamol/base AS expand
    COPY --from=download dataset.tar.gz .
    RUN tar xvzf dataset.tar.gz

    # Estágio 3: Final (copia SÓ o arquivo necessário)
    FROM diamol/base
    WORKDIR /dataset/url_svmlight
    COPY --from=expand url_svmlight/Day1.svm . # Copia só Day1.svm
    ```
*   Benefícios multi-stage aqui:
    *   Dockerfile mais legível (cada estágio tem propósito claro).
    *   Não precisa de otimizações complexas nos comandos (`tar` pode extrair tudo no estágio `expand`).
    *   Imagem final só contém o que foi explicitamente copiado (`Day1.svm`).
    *   Tamanho final de `v3` será pequeno como `v2` otimizado.

🧠 Explicação detalhada e didática:

*   A página confirma a drástica redução de tamanho obtida com a otimização do `RUN` único e extração seletiva (Figura 17.9, Listagem 17.4).
*   Introduz uma abordagem **ainda melhor** e mais legível para cenários como este: usar **builds multi-estágio**.
*   A Listagem 17.5 reescreve o processo de obtenção do arquivo `Day1.svm` usando três estágios:
    1.  **`download`:** Usa `wget` para baixar o arquivo `.tar.gz` completo.
    2.  **`expand`:** Copia o `.tar.gz` do estágio `download` e usa `tar` para extrair **todo** o conteúdo do arquivo compactado (sem se preocupar com tamanho aqui, pois este estágio é temporário).
    3.  **`final` (sem nome):** Usa uma imagem base limpa, define o `WORKDIR` e copia **apenas** o arquivo específico (`Day1.svm`) que realmente interessa, a partir do estágio `expand`.
*   **Vantagens:**
    *   **Legibilidade:** Cada estágio tem uma responsabilidade clara.
    *   **Simplicidade:** Os comandos dentro de cada estágio podem ser mais diretos (ex: não precisa da extração seletiva do `tar`).
    *   **Otimização Automática:** A imagem final só terá o `Day1.svm` porque apenas ele foi copiado. Todo o "lixo" (arquivo `.tar.gz`, outros arquivos extraídos) dos estágios `download` e `expand` é descartado automaticamente. O resultado é uma imagem final pequena, obtida de forma mais clara.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você criará um `Dockerfile.v3` com o conteúdo da Listagem 17.5 (na pasta `ml-dataset`).
*   O próximo exercício construirá essa versão `v3` e suas sub-partes.

---

📄 Página 358

🔍 Resumo dos principais pontos:

*   Build multi-stage `v3` será pequeno como `v2`, mas mais fácil de debugar.
*   **Build por Estágio (Debugging):** Pode-se construir até um estágio específico usando `docker build --target <nome_estagio>`.
*   **Exercício Try-it-now:** Construir `v3` completo e também até estágios intermediários.
    *   Navegar para `ml-dataset`.
    *   1. Build completo `v3`: `docker build -t ...:v3 -f Dockerfile.v3 .`
    *   2. Build até `download`: `docker build -t ...:v3-download -f Dockerfile.v3 --target download .`
    *   3. Build até `expand`: `docker build -t ...:v3-expand -f Dockerfile.v3 --target expand .`
    *   4. Comparar tamanhos: `docker image ls -f reference='diamol/ch17-ml-dataset:v3*'`.
*   Figura 17.10: Mostra tamanhos: `v3` (final) pequeno (24MB), `v3-download` médio (200MB+), `v3-expand` enorme (2GB+).
*   Confirma que multi-stage otimiza tamanho final e permite inspecionar estágios intermediários para debug.

🧠 Explicação detalhada e didática:

*   A página introduz a flag `--target <nome_estagio>` para o comando `docker build`. Isso permite interromper um build multi-estágio em um estágio intermediário específico (usando o nome `AS <nome_estagio>` definido no Dockerfile). O resultado é uma imagem que contém apenas as camadas até aquele estágio.
*   Isso é extremamente útil para **debugging**. Se um passo em um estágio posterior está falhando, você pode construir a imagem até o estágio *anterior*, rodar um contêiner interativo a partir dela (`docker run -it <imagem_do_estagio> sh`), e inspecionar o filesystem ou tentar rodar o comando problemático manualmente para entender o erro.
*   O exercício demonstra isso construindo a imagem `v3` completa e também versões paradas nos estágios `download` e `expand`.
*   A comparação dos tamanhos (Figura 17.10) ilustra perfeitamente o processo:
    *   `v3-download`: Contém a base + o arquivo `.tar.gz` baixado (~200MB).
    *   `v3-expand`: Contém a base + *todo* o conteúdo extraído (~2GB).
    *   `v3` (final): Contém a base + *apenas* o `Day1.svm` copiado (~24MB).
*   Isso confirma que multi-stage é a melhor abordagem: legível, fácil de depurar (com `--target`) e otimizado automaticamente para o tamanho final.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:** `cd C:\DockerLabs\diamol\ch17\exercises\ml-dataset`
2.  **Criar `Dockerfile.v3`:** Salve o conteúdo da Listagem 17.5 como `Dockerfile.v3`.
3.  **Construir Estágios:**
    ```powershell
    docker image build -t diamol/ch17-ml-dataset:v3 -f Dockerfile.v3 .
    docker image build -t diamol/ch17-ml-dataset:v3-download -f Dockerfile.v3 --target download .
    docker image build -t diamol/ch17-ml-dataset:v3-expand -f Dockerfile.v3 --target expand .
    ```
4.  **Comparar Tamanhos:**
    ```powershell
    docker image ls -f reference='diamol/ch17-ml-dataset:v3*'
    # Confirme os tamanhos dramaticamente diferentes.
    ```
5.  **(Opcional) Debugar Estágio:**
    ```powershell
    # Rodar container do estágio 'expand' para ver todos os arquivos extraídos
    docker run -it diamol/ch17-ml-dataset:v3-expand sh
    # Dentro do container: ls -R / (ou onde o WORKDIR estava)
    # exit
    ```

---

📄 Página 359

🔍 Resumo dos principais pontos:

*   **Vantagem Final Multi-Stage: Cache por Estágio.**
    *   Cada *estágio* tem seu próprio cache independente.
    *   Se mudar algo no estágio `expand`, o estágio `download` (anterior) ainda virá do cache.
*   **Otimização 4: Uso do Cache (Velocidade de Build).**
    *   Revisita caching (maximizar reuso).
    *   Regra: Ordenar instruções no Dockerfile da menos frequente para a mais frequente.
    *   Static setup (`EXPOSE`, `ENV`, `ENTRYPOINT`) no início.
    *   Binários/config da app (`COPY`) no final.
    *   Pode exigir iteração para acertar ordem.
*   **Exercício Try-it-now:** Demonstrar otimização de cache com multi-stage e ordem.
    *   App: Instalação Jenkins mínima (só baixa JAR, copia config).
    *   Navegar para `ch17/exercises/jenkins`.
    *   `v1` (`Dockerfile`): Ordem ruim - `COPY config` *antes* de `RUN wget` (download grande).
    *   `v2` (`Dockerfile.v2`): Ordem boa + multi-stage - `RUN wget` em estágio inicial, `COPY config` no estágio final.
    *   1. Build v1 e v2: `docker build -t ...:v1 .`, `docker build -t ...:v2 -f ...v2 .`
    *   2. Modificar arquivo de config: `echo 2.0 > jenkins.install.UpgradeWizard.state`.
    *   3. Re-build v1 e v2. Comparar tempos de build.

🧠 Explicação detalhada e didática:

*   A página revela outra vantagem crucial do multi-estágio: o **cache de build funciona por estágio**. Se você tem estágios `A -> B -> C` e faz uma mudança apenas no estágio `C`, os estágios `A` e `B` virão inteiramente do cache quando você reconstruir, tornando o build muito mais rápido.
*   Isso se conecta à otimização final discutida: maximizar o uso do cache para **acelerar os builds**. A regra geral (já vista no Cap. 3) é ordenar as instruções dentro de cada estágio (ou no Dockerfile inteiro, se não for multi-estágio) da **menos volátil para a mais volátil**.
*   O exercício compara duas versões de um Dockerfile para instalar o Jenkins:
    *   `v1` (não otimizado): Copia um arquivo de configuração local (`COPY ...config...`) *antes* de baixar o JAR grande do Jenkins (`RUN wget ...`).
    *   `v2` (otimizado): Usa multi-estágio. O download (`RUN wget`) acontece em um estágio inicial. O estágio final copia o JAR baixado do estágio anterior e *depois* copia o arquivo de configuração local (`COPY ...config...`).
*   O teste envolve construir ambas as versões, depois modificar o arquivo de configuração local e reconstruir ambas. A expectativa é:
    *   Re-build `v1`: Como o `COPY config` veio antes do `wget`, a mudança no config quebra o cache *antes* do download. O `wget` (lento) terá que ser executado novamente. Build lento.
    *   Re-build `v2`: A mudança no config só afeta o último `COPY` no estágio final. O estágio inicial (com o `wget` lento) virá inteiramente do cache. Build rápido.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch17\exercises\jenkins
    ```
2.  **Build Inicial v1 e v2:**
    ```powershell
    docker image build -t diamol/ch17-jenkins:v1 .
    docker image build -t diamol/ch17-jenkins:v2 -f Dockerfile.v2 .
    ```
3.  **Modificar Config:**
    ```powershell
    Set-Content -Path ./jenkins.install.UpgradeWizard.state -Value "2.0"
    # Ou edite no VS Code e salve
    ```
4.  **Re-Build v1 (Medir Tempo):**
    ```powershell
    Measure-Command { docker image build -t diamol/ch17-jenkins:v1 . }
    # Observe a saída do build (deve mostrar o wget rodando) e o tempo total.
    ```
5.  **Re-Build v2 (Medir Tempo):**
    ```powershell
    Measure-Command { docker image build -t diamol/ch17-jenkins:v2 -f Dockerfile.v2 . }
    # Observe a saída (deve usar cache para o download) e o tempo total (deve ser bem menor).
    ```

---

📄 Página 360

🔍 Resumo dos principais pontos:

*   Figura 17.11: Mostra comparação de tempos de re-build (v1 lento >10s, v2 rápido <1s) confirmando benefício da otimização de ordem/multi-stage para cache.
*   Uso bom de cache -> builds CI/CD rápidos.
*   **Cuidado: Over-caching.**
    *   Se `RUN` instala/baixa software (ex: `apt-get install pacote` *sem* versão), cache pode impedir pegar updates.
    *   `RUN` é cacheado até instrução mudar *ou* cache quebrar antes.
*   **Solução:** Sempre usar **versões explícitas** ao instalar/baixar software em `RUN`.
    *   Ex: `apt-get install curl=7.52.1...` (Exemplo socat, Listagem 17.3).
    *   Ex: Usar `ARG VERSAO=...` e `wget .../$VERSAO/...` (Exemplo Jenkins).
    *   Permite usar cache até *você* decidir atualizar a versão no Dockerfile/build args.

🧠 Explicação detalhada e didática:

*   A Figura 17.11 comprova experimentalmente a enorme diferença no tempo de build ao otimizar a ordem das instruções e usar multi-estágio para aproveitar o cache. Builds rápidos são essenciais para pipelines CI/CD ágeis.
*   A página, no entanto, alerta sobre um risco: o **excesso de cache (over-caching)**. Isso acontece quando uma instrução `RUN` que baixa ou instala software (ex: `apt-get install meu-pacote`) é cacheada. Se você não especificou uma versão exata, o cache fará com que builds futuros continuem usando a versão antiga que foi baixada na primeira vez, mesmo que uma versão mais nova (com correções de segurança!) esteja disponível nos repositórios. A camada cacheada só será invalidada se a própria linha `RUN` no Dockerfile mudar ou se uma instrução anterior quebrar o cache.
*   A **solução** para evitar over-caching e garantir controle sobre as dependências é sempre especificar **versões explícitas** ao instalar software via `RUN`:
    *   Para gerenciadores de pacote: `apt-get install <pacote>=<versao>`, `apk add <pacote>=<versao>`, `npm install <pacote>@<versao>`, etc.
    *   Para downloads diretos (wget/curl): Inclua a versão na URL ou passe-a via `ARG`, como no exemplo do Jenkins.
*   Isso garante que você obtenha os benefícios do cache, mas apenas até o momento em que *você* decidir atualizar a versão no Dockerfile (ou no argumento de build), garantindo builds reprodutíveis e controlados.

💻 Exemplos práticos (para meu ambiente Windows):

*   Revise a Listagem 17.3 (p.356): Note que `apt-get install` usava versões explícitas (`curl=7.52.1...`, `socat=1.7.3...`). Isso é bom.
*   Revise o Dockerfile do Jenkins (`ch17\exercises\jenkins\Dockerfile.v2`). Ele usa `ARG JENKINS_VERSION=...` e `ARG URL=.../${JENKINS_VERSION}/jenkins.war`. Isso também é bom, pois a versão é controlada externamente ou pelo default no `ARG`.

---

📄 Página 361

🔍 Resumo dos principais pontos:

*   **17.5 Understanding why optimization counts** (Entendendo por que a otimização conta)
*   Boas práticas resumidas:
    1.  Escolher base correta (idealmente golden images curadas).
    2.  Usar multi-stage (exceto para apps mais simples).
    3.  Não adicionar lixo (foco no tamanho da camada).
    4.  Ordenar instruções por frequência de mudança (maximizar cache).
*   Otimização impacta todo ciclo de vida.
*   Diagrama (Figura 17.12): Impactos da otimização no ciclo de vida da imagem.
    *   **Build:** Cache rápido, contexto pequeno -> CI rápido.
    *   **Push:** Camadas menores, mais reuso -> Push rápido, menos banda/storage no registro.
    *   **Pull:** Camadas menores, mais reuso -> Pull rápido nos nós de produção/teste.
    *   **Scan:** Menos software -> Scan rápido, menos vulnerabilidades.
    *   **Run:** Menos software -> Menor superfície de ataque.
*   **17.6 Lab** (Laboratório)

🧠 Explicação detalhada e didática:

*   A seção 17.5 resume as quatro principais práticas de otimização de imagem Docker abordadas no capítulo: escolher a base certa, usar multi-stage, minimizar o conteúdo de cada camada e ordenar instruções para o cache.
*   A Figura 17.12 é um excelente sumário visual dos **benefícios** da otimização em **todo o ciclo de vida** de uma imagem Docker:
    *   **Build:** Imagens otimizadas constroem mais rápido (cache, contexto menor).
    *   **Distribuição (Push/Pull):** Imagens menores e com mais camadas compartilhadas são mais rápidas de enviar para registros e de baixar nos servidores, economizando banda e espaço.
    *   **Segurança (Scan/Run):** Imagens menores contêm menos software, o que significa menos vulnerabilidades potenciais para serem encontradas em varreduras e menor superfície de ataque se o contêiner for comprometido em tempo de execução.
*   Esses benefícios combinados justificam o esforço investido na otimização das suas imagens.
*   Prepara para o Lab do Capítulo 17.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. É uma revisão conceitual dos benefícios da otimização.

---

📄 Página 362

🔍 Resumo dos principais pontos:

*   Figura 17.12: Diagrama dos benefícios da otimização.
*   **Continuação do Lab 17:**
    *   Objetivo: Otimizar imagem que instala CLI `docker`.
    *   Ponto de Partida: Dockerfiles (Linux/Windows) em `ch17/lab` que funcionam mas geram imagens grandes.
*   **Tarefa:** Refatorar Dockerfile(s) para atingir metas:
    1.  Tamanho Otimizado: `< 80MB` (Linux), `< 330MB` (Windows).
    2.  Cache Otimizado: Re-builds devem ser `< 1s`.
    3.  Funcionalidade: `docker run <imagem> docker version` deve funcionar (mostrar versão CLI, erro no Server é OK).
*   Dicas: Pensar criativamente, talvez não só otimizar instruções existentes, mas repensar o processo.

🧠 Explicação detalhada e didática:

*   Esta página detalha o **Lab do Capítulo 17**. O desafio é pegar Dockerfiles funcionais, mas não otimizados, que instalam a CLI do Docker e refatorá-los para atender a metas específicas de tamanho e velocidade de build, mantendo a funcionalidade principal.
*   As metas são:
    1.  **Tamanho:** Reduzir drasticamente o tamanho final da imagem. Isso provavelmente exigirá usar uma base mínima e técnicas para evitar camadas infladas durante a instalação da CLI (talvez baixando o binário diretamente em vez de usar um gerenciador de pacotes que traga muitas dependências, ou usando multi-stage para limpar arquivos temporários).
    2.  **Velocidade (Cache):** Garantir que re-builds (sem mudanças reais) sejam quase instantâneos, indicando bom uso do cache. Isso requer ordenar as instruções corretamente.
    3.  **Funcionalidade:** A imagem resultante ainda deve ser capaz de executar o comando `docker version`.
*   A dica sugere que pode ser necessário mais do que apenas reordenar ou combinar `RUN`s; talvez uma abordagem completamente diferente para obter a CLI Docker seja necessária para atingir as metas de tamanho.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 17:**
    1.  Navegue para `ch17\lab`. Examine o `Dockerfile` (Linux) e/ou `Dockerfile.windows`.
    2.  Construa a versão original (`docker build -t lab17-v1 ...`) e anote o tamanho.
    3.  Analise os passos. Provavelmente está usando `apt-get` ou `curl/tar` de forma não otimizada, ou uma base grande.
    4.  **Refatoração:**
        *   **Base:** Troque `FROM` para uma imagem mínima (ex: `alpine` para Linux, `mcr.microsoft.com/windows/nanoserver` para Windows).
        *   **Obter CLI Docker:** Em vez de `apt-get install docker-ce-cli`, talvez seja melhor usar `curl` ou `wget` para baixar o binário estático da CLI diretamente de `download.docker.com` em um estágio multi-stage inicial. Verifique se a base mínima tem `curl` ou `wget`, ou adicione/remova em um `RUN` único nesse estágio.
        *   **Estágio Final:** Copie *apenas* o binário `docker` baixado do estágio anterior para a imagem base final mínima (ex: em `/usr/local/bin/`). Defina `ENTRYPOINT ["docker"]` ou `CMD ["docker"]`.
    5.  Construa a versão otimizada (`docker build -t lab17-v2 ...`) e compare tamanho.
    6.  Teste o cache reconstruindo `v2`.
    7.  Teste a funcionalidade: `docker run --rm lab17-v2 version`.

---

📄 Página 363

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 17 no GitHub.
*   **Capítulo 18: Application configuration management in containers** (Gerenciamento de configuração de aplicação em contêineres)
*   Apps precisam carregar config do ambiente (env vars, arquivos).
*   Docker cria ambiente (seta env vars, constrói filesystem de várias fontes - imagem, volumes, configs, secrets).
*   Peças existem para config flexível (mesma imagem, configs diferentes por ambiente).
*   Precisa de trabalho para unir peças: app precisa mesclar config de múltiplas fontes.
*   Foco Cap. 18: Abordagem recomendada (+ alternativas) com exemplos .NET Core, Java, Go, Node.js. Requer colaboração Dev/Ops.
*   **18.1 A multi-tiered approach to app configuration** (Uma abordagem multi-camadas para configuração de app)
*   Modelo deve refletir estrutura dos dados de config:
    1.  **Nível Release:** Igual para todos ambientes de uma versão (ex: nome do app).
    2.  **Nível Ambiente:** Diferente por ambiente (ex: string conexão BD, nível log).
    3.  **Nível Feature:** Para ligar/desligar features (feature flags).
*   Fontes podem ser estáticas (imagem) ou dinâmicas (env var, arquivos montados).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 18, focado em como as **aplicações dentro dos contêineres** devem lidar com suas **configurações**.
*   Relembra que o Docker fornece vários mecanismos para injetar configuração no ambiente do contêiner (variáveis de ambiente, montagem de arquivos via volumes/bind mounts/configs/secrets).
*   O desafio é fazer com que a *aplicação* utilize esses mecanismos de forma flexível, permitindo que a mesma imagem se adapte a diferentes ambientes. Isso geralmente requer que a aplicação seja capaz de **ler e mesclar configurações de múltiplas fontes**, com uma ordem de precedência definida (ex: variável de ambiente sobrescreve arquivo, que sobrescreve default da imagem).
*   A seção 18.1 propõe uma **abordagem multi-camadas** para pensar sobre a configuração:
    *   **Defaults (Nível Release):** Configurações básicas que raramente mudam e podem vir embutidas na imagem Docker.
    *   **Ambiente:** Configurações específicas do ambiente (Dev, Test, Prod) que devem ser injetadas externamente (via arquivos montados - Configs/Secrets/Volumes - ou `env_file`).
    *   **Overrides/Features:** Configurações individuais que precisam ser facilmente alteradas (ex: para debugging, A/B testing, feature flags) e são ideais para serem passadas via variáveis de ambiente (`environment:` no Compose ou `-e` no `docker run`).

💻 Exemplos práticos (para meu ambiente Windows):

*   O capítulo usará bibliotecas de configuração populares em diferentes linguagens (node-config, .NET Core config, Viper Go) para demonstrar como implementar esse carregamento multi-fonte.
*   Você usará `docker run -e VAR=valor` e `docker run -v /path/host:/path/container` (ou equivalentes Compose) para injetar as configurações.

💡 Analogias ou observações úteis (se houver):

*   Config Multi-Camadas: É como configurar seu telefone.
    *   Defaults (Imagem): Configurações de fábrica.
    *   Ambiente (Arquivo Montado): Configurações que você define para "Casa" vs "Trabalho" (Wi-Fi, Não Perturbe).
    *   Overrides (Env Var): Ajustes rápidos que você faz na hora (Brilho da tela, Volume).

---

📄 Página 364

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 18.1): Ilustra hierarquia de config: Defaults (Imagem) <- Arquivos (Filesystem) <- Variáveis de Ambiente (Env Vars). Env Vars têm maior precedência.
*   **Exemplo Node.js:** Usa biblioteca `node-config`.
    *   Lê config de múltiplas fontes hierárquicas + overrides de env vars.
    *   App `access-log` configurado para ler de:
        1.  `/app/config` (padrões, empacotado na imagem).
        2.  `/app/config-override` (vazio na imagem; pode ser montado de volume/config/secret).
*   **Exercício Try-it-now:** Rodar `access-log` com default e com override.
    *   1. Rodar com default: `docker run -d -p 8080:80 diamol/ch18-access-log`.
    *   2. Rodar com override (bind mount de pasta `config/dev` local para `/app/config-override`): `docker run -d -p 8081:80 -v "$(pwd)/config/dev:/app/config-override" diamol/ch18-access-log`.
    *   3. Verificar config via API (`/config`) em ambos: `curl :8080/config`, `curl :8081/config`.

🧠 Explicação detalhada e didática:

*   A Figura 18.1 estabelece a **ordem de precedência** típica e recomendada para configuração: Variáveis de Ambiente (maior prioridade) > Arquivos de Configuração Montados > Arquivos de Configuração Padrão (na imagem). Isso permite flexibilidade máxima.
*   O primeiro exemplo usa Node.js com a biblioteca `node-config`. Essa biblioteca é configurada para procurar arquivos de configuração em uma ordem específica de diretórios e, por fim, permite que variáveis de ambiente sobrescrevam tudo.
*   O app `access-log` (uma nova versão para este capítulo) procura por:
    *   Arquivos em `/app/config` (contém `default.json` na imagem).
    *   Arquivos em `/app/config-override` (está vazio na imagem, mas é o local onde montaremos arquivos específicos do ambiente).
*   O exercício demonstra isso:
    *   O primeiro contêiner usa apenas o `default.json` da imagem.
    *   O segundo contêiner usa um **bind mount** para mapear a pasta `config/dev` do host (que contém um arquivo como `local.json` com settings de dev) para `/app/config-override` dentro do contêiner. O `node-config` detecta e carrega esse arquivo, sobrescrevendo ou adicionando configurações ao `default.json`.
    *   A API `/config` (exposta pela aplicação de exemplo) permite verificar quais configurações estão ativas em cada contêiner.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\access-log
    # Verifique a pasta 'config\dev' e o arquivo dentro dela (ex: local.json)
    ```
2.  **Rodar Container Default:**
    ```powershell
    docker container run -d -p 8080:80 --name accesslog-default diamol/ch18-access-log
    ```
3.  **Rodar Container com Override:** (Ajuste $TargetPath se imagem for Windows)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override' # Para imagem Linux
    docker container run -d -p 8081:80 --name accesslog-dev -v "$SourcePath`:$TargetPath" diamol/ch18-access-log
    ```
4.  **Verificar Configs:**
    ```powershell
    curl http://localhost:8080/config ; echo "" # Default
    curl http://localhost:8081/config ; echo "" # Com override Dev
    ```
    Compare as saídas. A segunda deve mostrar `environment: DEV` e talvez `metrics.enabled: false` (dependendo do `local.json`). A primeira mostrará `environment: UNKNOWN`.

---

📄 Página 365

🔍 Resumo dos principais pontos:

*   Figura 18.2: Mostra saída da API `/config` para os dois contêineres, confirmando que o override de arquivo foi aplicado no segundo.
*   Fonte do override (volume, config obj, secret) é transparente para o app, desde que montado no path esperado (`/app/config-override`).
*   Nuança: Montar diretório é mais flexível que arquivo único (montagem de arquivo único não funciona em Win Container). Nomes de arquivos na origem devem bater com os esperados pelo app.
*   `node-config` também suporta override via **variáveis de ambiente** (maior precedência).
    *   Formato incomum: Precisa passar string JSON na variável `NODE_CONFIG`.
*   **Exercício Try-it-now:** Rodar 3ª versão com override de arquivo + override de env var.
    *   Objetivo: Usar config `dev` (metrics=false), mas ligar métricas via env var.
    *   Comando: `docker run -d -p 8082:80 -v ...:/app/config-override -e NODE_CONFIG='{\"metrics\": {\"enabled\":\"true\"}}' diamol/ch18-access-log`.
    *   Verificar config: `curl http://localhost:8082/config`.

🧠 Explicação detalhada e didática:

*   A página confirma o sucesso do override via arquivo montado e generaliza que a *origem* desse arquivo (bind mount local, Volume Docker, Config Object Swarm, Secret Swarm) não importa para a aplicação, desde que o arquivo apareça no caminho que ela espera (`/app/config-override`). Menciona a preferência por montar diretórios em vez de arquivos únicos por portabilidade (especialmente devido à limitação do Windows).
*   Introduz a camada final de override para `node-config`: variáveis de ambiente. A biblioteca `node-config` tem uma maneira específica (e um pouco estranha) de fazer isso: você não define variáveis individuais (como `METRICS_ENABLED=true`), mas sim uma única variável chamada `NODE_CONFIG` contendo uma *string JSON* que representa as configurações a serem sobrescritas.
*   O exercício demonstra isso:
    *   Inicia um contêiner montando a pasta `config/dev` (que define `metrics.enabled: false`).
    *   **E também** define a variável de ambiente `-e NODE_CONFIG='{\"metrics\": {\"enabled\":\"true\"}}'`. (Note as barras invertidas `\` para escapar as aspas duplas dentro da string JSON no comando shell).
    *   Como a variável de ambiente tem a maior precedência no `node-config`, a configuração final para `metrics.enabled` deve ser `true`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar Container com Override Duplo:** (Ajuste $TargetPath se necessário)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override'
    # No PowerShell, aspas simples preservam a string JSON literal.
    # Se usar aspas duplas, pode precisar escapar as aspas internas com ` ou \.
    $JsonOverride = '{\"metrics\": {\"enabled\":\"true\"}}'
    docker container run -d -p 8082:80 --name accesslog-env -v "$SourcePath`:$TargetPath" -e NODE_CONFIG=$JsonOverride diamol/ch18-access-log
    ```
2.  **Verificar Config:**
    ```powershell
    curl http://localhost:8082/config ; echo ""
    ```
    Verifique se `environment` é `DEV` (do arquivo) mas `metrics.enabled` é `true` (do env var).

---

📄 Página 366

🔍 Resumo dos principais pontos:

*   Figura 18.3: Mostra saída do 3º container, confirmando override da env var sobre o arquivo (`metrics.enabled: true`).
*   Padrão Multi-Tiered (Imagem -> Arquivo -> EnvVar) é recomendado.
*   Flexível para dev (ex: ligar/desligar métricas) e ops (deploy em ambientes diferentes).
*   Detalhes de implementação (paths, formato env var) importam e podem gerar erros se não comunicados/documentados.
*   **Alternativa:** Empacotar config de *todos* os ambientes *dentro* da imagem.
*   **18.2 Packaging config for every environment** (Empacotando config para cada ambiente)
    *   Frameworks suportam carregar `config.{Environment}.ext` baseado em variável `ENVIRONMENT_NAME`.
    *   Ex: .NET Core: `appsettings.json` (default) + `appsettings.{Environment}.json` (override) + Env Vars (final override).
*   App To-Do (`diamol/ch18-todo-list`) usa essa abordagem .NET Core.
    *   Imagem contém `appsettings.json`, `appsettings.Development.json`, `appsettings.Test.json`, `appsettings.Production.json`.
    *   Variável `DOTNET_ENVIRONMENT` (ou `ASPNETCORE_ENVIRONMENT`) define qual arquivo `.Environment.` carregar.

🧠 Explicação detalhada e didática:

*   A página conclui a demonstração do padrão multi-camadas (padrão na imagem, arquivo externo, variável de ambiente) como sendo flexível e recomendado, mas alerta para a necessidade de boa documentação e comunicação sobre como a configuração funciona (paths esperados, formato das variáveis, etc.) para evitar erros.
*   A seção 18.2 introduz uma **abordagem alternativa**: em vez de injetar arquivos de configuração específicos do ambiente em tempo de execução, **empacotar os arquivos de configuração para *todos* os ambientes *dentro* da própria imagem Docker**.
*   Muitos frameworks de aplicação modernos suportam isso nativamente. O exemplo dado é o **.NET Core**:
    1.  Ele sempre carrega um arquivo base (ex: `appsettings.json`).
    2.  Ele verifica uma variável de ambiente (geralmente `ASPNETCORE_ENVIRONMENT` ou `DOTNET_ENVIRONMENT`) para saber em qual ambiente está rodando (ex: "Development", "Test", "Production").
    3.  Ele então carrega um arquivo específico para aquele ambiente (ex: `appsettings.Production.json`), cujos valores sobrescrevem os do arquivo base.
    4.  Finalmente, ele ainda permite que variáveis de ambiente individuais sobrescrevam qualquer configuração vinda dos arquivos.
*   A vantagem dessa abordagem é que a imagem já contém "tudo" que precisa para rodar em qualquer ambiente; você só precisa definir *uma* variável de ambiente (`DOTNET_ENVIRONMENT`) para selecionar o conjunto correto de configurações. A desvantagem será discutida depois (segurança, gerenciamento de config).
*   A nova versão do app To-Do (`diamol/ch18-todo-list`) implementa esse padrão .NET Core.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício rodará essa imagem To-Do, alternando o ambiente via variável de ambiente.

💡 Analogias ou observações úteis (se houver):

*   Empacotar Todas Configs na Imagem: É como um manual de instruções que já vem com seções separadas para "Montagem Rápida", "Montagem Completa", "Montagem Profissional". Você só precisa dizer qual seção seguir (definir `DOTNET_ENVIRONMENT`), e todas as instruções já estão lá dentro do manual (imagem).

---

📄 Página 367

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar To-Do com config default (Development) e Test.
    *   1. Rodar default: `docker run -d -p 8083:80 diamol/ch18-todo-list`. (Dockerfile seta `DOTNET_ENVIRONMENT=Development` como padrão).
    *   2. Rodar Test: `docker run -d -p 8084:80 -e DOTNET_ENVIRONMENT=Test diamol/ch18-todo-list`.
*   Containers rodam mesma imagem, mas carregam arquivos `.json` diferentes de dentro da imagem (`appsettings.Development.json` vs `appsettings.Test.json`).
*   Sem necessidade de montar volumes externos para mudar ambiente.
*   Verificar: Acessar `/diagnostics` endpoint em ambos (`:8083` e `:8084`).
*   Figura 18.4: Mostra saída da API `/diagnostics`, confirmando `EnvironmentName` diferente (Development vs Test) e outras configs (ex: `Database:Provider`) que mudam entre os arquivos.

🧠 Explicação detalhada e didática:

*   O exercício demonstra a abordagem de "configuração por ambiente empacotada".
*   O primeiro contêiner é iniciado sem especificar a variável `DOTNET_ENVIRONMENT`. Como o Dockerfile desta imagem define `Development` como o valor padrão para essa variável, a aplicação carrega `appsettings.Development.json` para sobrescrever os defaults de `appsettings.json`.
*   O segundo contêiner é iniciado com `-e DOTNET_ENVIRONMENT=Test`. Isso sobrescreve o padrão do Dockerfile. A aplicação agora carrega `appsettings.Test.json` para sobrescrever os defaults.
*   Ambos os contêineres usam a **mesma imagem**, mas exibem configurações diferentes porque selecionaram arquivos diferentes que já estavam *dentro* da imagem. Não foi necessário montar nenhum arquivo externo.
*   Acessar o endpoint `/diagnostics` (específico desta aplicação de exemplo) permite visualizar a configuração ativa em cada instância. A Figura 18.4 mostra que o ambiente e outras configurações (como o provedor de banco de dados) são de fato diferentes.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar Default (Dev):**
    ```powershell
    docker container run -d -p 8083:80 --name todo-dev-packed diamol/ch18-todo-list
    ```
2.  **Rodar Test:**
    ```powershell
    docker container run -d -p 8084:80 --name todo-test-packed -e DOTNET_ENVIRONMENT=Test diamol/ch18-todo-list
    ```
3.  **Verificar Configs:**
    *   Acesse `http://localhost:8083/diagnostics` no navegador. Observe `EnvironmentName: Development` e outras settings.
    *   Acesse `http://localhost:8084/diagnostics`. Observe `EnvironmentName: Test` e settings diferentes (ex: `Database:Provider: Postgres`).

---

📄 Página 368

🔍 Resumo dos principais pontos:

*   Figura 18.4: Confirma configs diferentes para Dev e Test.
*   Abordagem funciona bem se sistema de gerenciamento de config for separado do source control. Pipeline CI/CD pode injetar os arquivos de config corretos na imagem durante build.
*   **Desvantagem:** **Segurança!** Não se pode empacotar *tudo*. Dados confidenciais (senhas, chaves API) NÃO devem ir na imagem Docker.
*   Assumir que registro pode ser comprometido; não expor segredos em texto plano em arquivos na imagem.
*   Mesmo com essa abordagem, ainda precisa permitir **overrides via arquivos externos** (para segredos) e **via variáveis de ambiente** (para flexibilidade).
*   App To-Do ainda suporta isso: carrega arquivos de `/app/config-overrides` (se existir) e lê env vars (padrão .NET Core).
*   Permite cenários como rodar "produção" localmente, mas sobrescrevendo para usar BD de arquivo (SQLite).
*   **Exercício Try-it-now:** Rodar em modo `Production` com override de arquivo para usar SQLite.
    *   Modo Production (sem override) falharia (espera BD remoto).
    *   Navegar para `ch18/exercises/todo-list`.
    *   Comando: `docker run -d -p 8085:80 -e DOTNET_ENVIRONMENT=Production -v "$(pwd)/config/prod-local:/app/config-override" diamol/ch18-todo-list`.
    *   Monta arquivo local (`prod-local/local.json`?) que provavelmente define `Database:Provider=SQLite`.

🧠 Explicação detalhada e didática:

*   A página discute as implicações da abordagem de "empacotar todas as configs". Pode funcionar se houver um processo externo (talvez manual ou via pipeline) que insere os arquivos de configuração corretos na imagem durante o build, mantendo a separação entre código e config.
*   No entanto, a principal **desvantagem é a segurança**. É inaceitável colocar dados sensíveis (senhas, chaves) diretamente em arquivos dentro de uma imagem Docker, pois qualquer pessoa que obtiver a imagem (ex: de um registro comprometido ou por acesso indevido) poderá extrair esses segredos.
*   Portanto, mesmo que se use essa abordagem de empacotar arquivos por ambiente, ainda é **necessário** ter um mecanismo para injetar configurações **externamente** em tempo de execução, pelo menos para os dados sensíveis (usando Secrets) e, idealmente, também para permitir overrides rápidos (usando variáveis de ambiente).
*   Felizmente, o app To-Do (`diamol/ch18-todo-list`) foi construído para suportar isso: além de carregar `appsettings.{Environment}.json`, ele também procura por arquivos em `/app/config-override` (onde podemos montar secrets ou configs externos) e também lê variáveis de ambiente (comportamento padrão do .NET Core).
*   O exercício demonstra isso rodando o app no modo `Production` (que normalmente esperaria um BD Postgres remoto), mas usando um bind mount para injetar um arquivo de override que o força a usar SQLite, permitindo rodar localmente sem o BD externo.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\todo-list
    # Verifique a pasta config\prod-local e o arquivo dentro (ex: local.json)
    # Ele deve conter algo como { "Database": { "Provider": "Sqlite" } }
    ```
2.  **Rodar Prod com Override Local:** (Ajuste target se imagem for Linux)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\prod-local"
    $TargetPath = 'C:\app\config-override' # Para imagem Windows
    # $TargetPath = '/app/config-override' # Para imagem Linux
    docker container run -d -p 8085:80 --name todo-prod-local -e DOTNET_ENVIRONMENT=Production -v "$SourcePath`:$TargetPath" diamol/ch18-todo-list
    ```
3.  **Verificar:** Acesse `http://localhost:8085/diagnostics`. Deve mostrar `EnvironmentName: Production` mas `Database:Provider: Sqlite`. Acesse `http://localhost:8085/` e tente adicionar um item; deve funcionar.

---

📄 Página 369

🔍 Resumo dos principais pontos:

*   Figura 18.5: Mostra UI /diagnostics confirmando `Environment: Production` mas `Database: Sqlite` devido ao override.
*   App ainda suporta override final via variáveis de ambiente individuais (padrão .NET Core).
*   **Exercício Try-it-now:** Rodar Prod local (com override de arquivo) + override de env var.
    *   Objetivo: Mudar nome do release via env var.
    *   Comando: `docker run -d -p 8086:80 -e DOTNET_ENVIRONMENT=Production -e release=CUSTOM -v ...:/app/config-override diamol/ch18-todo-list`.
    *   `-e release=CUSTOM`: Sobrescreve chave `release` vinda dos arquivos JSON.
*   Verificar: Acessar `http://localhost:8086/diagnostics`. Ver release customizado.
*   Figura 18.6: Mostra UI /diagnostics com `release: CUSTOM`.
*   **Crítica do Autor:** Não gosta da abordagem de empacotar múltiplos arquivos de config na imagem.
    *   Risco de incluir dados semi-sensíveis (nomes de servidor, URLs, etc.).
    *   Separação confusa entre config no source control vs. config em sistema externo.

🧠 Explicação detalhada e didática:

*   A página demonstra a camada final de precedência na configuração do .NET Core: variáveis de ambiente individuais sobrescrevem configurações vindas de *todos* os arquivos JSON carregados (base, ambiente, overrides externos).
*   O exercício define a variável de ambiente `release=CUSTOM`. O framework .NET Core automaticamente mapeia essa variável para a chave de configuração `release` e sobrescreve o valor que veio dos arquivos `appsettings...json`.
*   Acessar `/diagnostics` confirma que a variável de ambiente teve a maior prioridade.
*   O autor então expressa sua **opinião pessoal contrária** à prática de empacotar arquivos de configuração específicos de ambiente dentro da imagem Docker. Ele argumenta que é difícil garantir que nenhum dado, mesmo que não seja uma senha direta (como nomes de servidores internos ou URLs de APIs privadas), que possa ser útil a um atacante seja incluído acidentalmente. Além disso, cria uma gestão dividida da configuração (parte no código/imagem, parte em sistemas externos). Ele implicitamente prefere o modelo anterior (padrões na imagem, todo o resto injetado externamente).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar Prod Local com Override Env Var:** (Use $SourcePath, $TargetPath de antes)
    ```powershell
    docker container run -d -p 8086:80 --name todo-prod-custom -e DOTNET_ENVIRONMENT=Production -e release=CUSTOM -v "$SourcePath`:$TargetPath" diamol/ch18-todo-list
    ```
2.  **Verificar:** Acesse `http://localhost:8086/diagnostics`. Confirme que `release` mostra `CUSTOM`.

---

📄 Página 370

🔍 Resumo dos principais pontos:

*   Figura 18.6: Confirma override da env var `release=CUSTOM`.
*   Opinião autor: Prefere padrão mais consistente (não empacotar configs de ambiente na imagem).
*   Contêineres permitem escolher padrão preferido.
*   Problema: Múltiplas stacks tecnológicas -> Múltiplas abordagens de config.
*   **Exemplo Go:** Biblioteca `Viper`.
    *   Similar a .NET Core / node-config.
    *   Especifica paths de config, lê env vars para override.
*   App Image Gallery (`diamol/ch18-image-gallery`) usa Viper. Hierarquia:
    1.  `/app/config` (padrões da imagem).
    2.  `/app/config-override` (vazio na imagem, para montar).
    3.  Variáveis de Ambiente (override final).
*   **Formato TOML:** Viper suporta JSON, YAML, mas TOML é popular em Go. Fácil de ler/escrever, mapeia bem para código.
*   **Listagem 18.1:** Exemplo de arquivo config `config.toml`.
    ```toml
    release = "19.12"
    environment = "UNKNOWN"

    [metrics]
    enabled = true

    [apis.image]
    url = "http://iotd/image"

    [apis.access]
    url = "http://accesslog/access-log"
    ```

🧠 Explicação detalhada e didática:

*   A página conclui a crítica à abordagem de empacotar configurações e transiciona para o próximo exemplo usando Go, destacando que a diversidade de tecnologias muitas vezes leva a abordagens de configuração ligeiramente diferentes, mesmo que o padrão geral (múltiplas fontes com precedência) seja o mesmo.
*   Introduz a biblioteca `Viper` como uma solução popular de configuração em Go, que oferece funcionalidades semelhantes às vistas para Node.js e .NET Core (ler múltiplos arquivos, sobrescrever com variáveis de ambiente).
*   A versão `diamol/ch18-image-gallery` usa Viper com a mesma estrutura de diretórios (`/app/config` para defaults da imagem, `/app/config-override` para montagens externas).
*   Apresenta o formato **TOML** (Tom's Obvious, Minimal Language) como uma alternativa legível a JSON e YAML, frequentemente usada na comunidade Go. A Listagem 18.1 mostra um exemplo de arquivo TOML com seções (`[metrics]`, `[apis]`) e subseções (`[apis.image]`) que são fáceis de entender.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará a imagem `diamol/ch18-image-gallery` e injetará configuração via bind mount (com arquivo TOML) e variáveis de ambiente nos próximos exercícios.
*   Abra `ch18\exercises\image-gallery\config\default\config.toml` no VS Code para ver o arquivo TOML padrão da imagem.

💡 Analogias ou observações úteis (se houver):

*   TOML: Outro formato para escrever sua lista de compras ou configurações. Pode ser mais agradável de ler e escrever para humanos do que JSON (muitas chaves e aspas) ou YAML (sensível à indentação), especialmente para configurações mais simples.

---

📄 Página 371

🔍 Resumo dos principais pontos:

*   TOML é fácil de ler/debugar/comparar.
*   **Exercício Try-it-now:** Rodar app Go com config default (da imagem).
    *   Comando: `docker run -d -p 8086:80 diamol/ch18-image-gallery`. (Nota: Usa porta 8086, que já pode estar em uso pelo ex. anterior. Usarei 8087).
    *   Verificar config via API: `curl http://localhost:8087/config`.
*   Figura 18.7: Mostra config default (release, environment UNKNOWN, URLs API).
*   **API `/config`:** Útil para debug, mas precisa ser **segura**.
    *   Não expor segredos.
    *   Proteger endpoint (autenticação/autorização).
    *   Tornar a própria API `/config` opcional (via config).

🧠 Explicação detalhada e didática:

*   O exercício instrui a rodar a aplicação Go (`image-gallery`) usando apenas a configuração padrão empacotada na imagem (o `config.toml` dentro de `/app/config`).
*   Assim como os outros exemplos, este app expõe um endpoint `/config` que retorna a configuração ativa no momento em formato JSON. Acessá-lo permite verificar quais configurações estão sendo usadas.
*   A página aproveita para dar conselhos importantes sobre **segurança** ao implementar um endpoint como `/config` que expõe a configuração da aplicação:
    1.  **Nunca inclua segredos:** A API não deve retornar senhas, chaves API, etc.
    2.  **Proteja o Acesso:** O endpoint deve exigir autenticação e autorização, não ser público.
    3.  **Torne-o Opcional:** Idealmente, a exposição desse endpoint deve ser controlada por uma configuração (ex: habilitado apenas em ambiente de desenvolvimento ou teste).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Limpar Porta Anterior (se usou 8086):**
    ```powershell
    docker rm -f todo-prod-custom # Ou o nome que deu
    ```
2.  **Rodar Go App (Default Config) na porta 8087:**
    ```powershell
    docker container run -d -p 8087:80 --name gallery-default diamol/ch18-image-gallery
    ```
3.  **Verificar Config Default:**
    ```powershell
    curl http://localhost:8087/config ; echo ""
    # Verifique os valores padrão (environment=UNKNOWN, etc.)
    ```

---

📄 Página 372

🔍 Resumo dos principais pontos:

*   Figura 18.7: Confirma config default.
*   Abordagem Image Gallery: Defaults na imagem, mas *espera* override por ambiente.
*   **Exercício Try-it-now:** Rodar app Go com override de arquivo (Dev).
    *   Navegar para `ch18/exercises/image-gallery`.
    *   Comando: `docker run -d -p 8087:80 -v "$(pwd)/config/dev:/app/config-override" diamol/ch18-image-gallery`. (Nota: Reusa porta 8087, precisa parar anterior. Usarei 8088).
    *   Verificar config: `curl http://localhost:8088/config`.
*   Figura 18.8: Mostra config mesclada (environment=DEV).

🧠 Explicação detalhada e didática:

*   A página explica que o design da configuração da imagem Go `image-gallery` é ligeiramente diferente dos anteriores: ela fornece defaults, mas *assume* que um arquivo de override específico do ambiente será fornecido em `/app/config-override`.
*   O exercício demonstra isso rodando o contêiner novamente, mas desta vez usando um bind mount para mapear a pasta local `config/dev` (que contém um `config.toml` com `environment = "DEV"`) para `/app/config-override` dentro do contêiner.
*   O Viper (biblioteca Go) detecta e carrega este arquivo de override, mesclando-o com o default da imagem. Acessar a API `/config` agora deve mostrar a configuração combinada, incluindo `environment = "DEV"`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Parar/Remover Container Anterior:**
    ```powershell
    docker rm -f gallery-default
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\image-gallery
    # Verifique pasta config\dev e arquivo config.toml dentro dela.
    ```
3.  **Rodar Go App com Override de Arquivo (porta 8088):** (Ajuste target se imagem for Win)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override' # Para imagem Linux
    docker container run -d -p 8088:80 --name gallery-dev -v "$SourcePath`:$TargetPath" diamol/ch18-image-gallery
    ```
4.  **Verificar Config Mesclada:**
    ```powershell
    curl http://localhost:8088/config ; echo ""
    # Verifique se environment=DEV e outros valores estão corretos.
    ```

---

📄 Página 373

🔍 Resumo dos principais pontos:

*   Figura 18.8: Confirma config DEV mesclada.
*   Variações nos modelos de config entre apps/linguagens são comuns. Difícil padronizar 100%.
*   Exemplo Go: Override final via Env Vars usa convenção diferente.
*   **Exercício Try-it-now:** Rodar Go com override de arquivo + override de env var.
    *   Viper espera prefixo + separador `_` + nome config em MAIÚSCULAS (pode variar). App usa prefixo `IG`.
    *   Comando: `docker run -d -p 8088:80 -v ...:/app/config-override -e IG_METRICS.ENABLED=TRUE diamol/ch18-image-gallery`. (Nota: Reusa porta 8088, precisa parar anterior. Usarei 8089. Nome da var pode precisar ser `IG_METRICS_ENABLED`).
    *   Verificar config: `curl http://localhost:8089/config`.
*   Figura 18.9: Mostra config final com `metrics.enabled: true` vindo da env var.

🧠 Explicação detalhada e didática:

*   A página reconhece que, apesar do padrão geral ser similar (múltiplas fontes com precedência), os **detalhes** de como as bibliotecas de configuração em diferentes linguagens lidam com overrides (especialmente via variáveis de ambiente) podem variar. Isso reforça a necessidade de documentação clara (idealmente nos arquivos Compose/manifestos).
*   No caso do Viper (Go), uma convenção comum (e configurável) é que as variáveis de ambiente precisam ter um **prefixo** específico (definido pelo app, aqui `IG`), seguido por `_`, e o nome da chave de configuração (às vezes em maiúsculas, com `.` substituído por `_`). Ex: Para sobrescrever `metrics.enabled`, a variável pode ser `IG_METRICS_ENABLED`.
*   O exercício tenta sobrescrever a configuração de métricas (que veio do arquivo) usando uma variável de ambiente.
    *   **Correção Provável:** A variável no exercício `-e IG_METRICS.ENABLED=TRUE` provavelmente deveria ser `-e IG_METRICS_ENABLED=TRUE` para seguir a convenção de substituir `.` por `_` e usar maiúsculas, se o Viper estiver configurado assim. Assumirei a versão com `_`.
*   O comando roda o contêiner com o bind mount do arquivo `dev` E a variável de ambiente. A API `/config` deve mostrar o resultado final, com a variável de ambiente tendo a maior precedência.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Parar/Remover Container Anterior:**
    ```powershell
    docker rm -f gallery-dev
    ```
2.  **Rodar Go App com Override Duplo (porta 8089):**
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/app/config-override'
    # Usando a provável variável correta para Viper
    docker container run -d -p 8089:80 --name gallery-env -v "$SourcePath`:$TargetPath" -e IG_METRICS_ENABLED=true diamol/ch18-image-gallery
    ```
3.  **Verificar Config Final:**
    ```powershell
    curl http://localhost:8089/config ; echo ""
    # Verifique se environment=DEV e metrics.enabled=true.
    ```

---

📄 Página 374

🔍 Resumo dos principais pontos:

*   Figura 18.9: Confirma override da env var (`metrics.enabled: true`).
*   Recapitulação: 3 apps, 3 stacks, mesmo padrão config (multi-tier), mas detalhes diferentes. Gerenciável via documentação nos manifestos.
*   **Último Exemplo: Aplicações Legadas.**
*   **18.4 Configuring legacy apps in the same way as new apps** (Configurando apps legados da mesma forma que apps novos)
    *   Apps legados têm ideias próprias sobre config (ex: XML em paths fixos, sem ler env vars).
    *   Pode-se aplicar mesmo padrão multi-tier, mas requer **trabalho extra** no Dockerfile/imagem.
    *   **Abordagem:** Empacotar utilitário/script na imagem que roda *na inicialização* do container, *antes* do app legado.
        *   Utilitário lê config das fontes "modernas" (arquivos montados, env vars).
        *   Mescla/transforma essas configs.
        *   Escreve o resultado final no formato e local que o app legado espera (ex: gera o XML).

🧠 Explicação detalhada e didática:

*   A página resume os exemplos anteriores, destacando a consistência do padrão multi-camadas apesar das variações de implementação entre linguagens/bibliotecas.
*   A seção 18.4 aborda um desafio comum: como aplicar esses padrões modernos de configuração a **aplicações legadas** que não foram projetadas para ler variáveis de ambiente ou arquivos de configuração em locais flexíveis?
*   A solução proposta é inteligente: em vez de modificar o código legado (o que pode ser impossível ou arriscado), modifica-se a **imagem Docker** e o **processo de inicialização** do contêiner.
*   **Passos:**
    1.  Cria-se um pequeno **utilitário ou script** (na linguagem que for mais conveniente para manipular a configuração) que *entende* o padrão moderno (ler de `/config-override`, ler env vars com prefixo `LEGACYAPP_...`, etc.).
    2.  Esse utilitário é incluído na imagem Docker junto com a aplicação legada.
    3.  O `CMD` ou `ENTRYPOINT` do Dockerfile é modificado para **primeiro executar o utilitário de configuração** e **depois executar a aplicação legada**.
    4.  O utilitário lê as configurações modernas injetadas no contêiner (via volumes, secrets, env vars), processa-as e **gera o arquivo de configuração no formato exato (ex: XML) e no local exato** que a aplicação legada espera encontrar.
*   Dessa forma, a aplicação legada roda sem modificações, lendo sua configuração da maneira antiga, mas essa configuração foi dinamicamente gerada na inicialização com base nos padrões modernos injetados pelo Docker/Compose/Swarm.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício usará uma versão "legada" (simulada) da API Java IOTD, junto com um utilitário Java `ConfigLoader` que fará essa transformação na inicialização.

💡 Analogias ou observações úteis (se houver):

*   Config para App Legado: É como ter um aparelho antigo que só entende fitas K7. Você quer tocar um MP3 moderno nele. A solução é usar um "adaptador" (o utilitário) que lê o MP3 (config moderna) e o grava na fita K7 (formato legado) na hora, para que o aparelho antigo possa tocar.

---

📄 Página 375

🔍 Resumo dos principais pontos:

*   Exemplo "Legado": API Java IOTD (`diamol/ch18-image-of-the-day`), imagem construída com padrão legado (requer utilitário `ConfigLoader` para gerar config final).
*   Utilitário lê override de arquivo (path via env var `CONFIG_SOURCE_PATH`) e env vars (prefixo `IOTD_`). Escreve config final em path fixo (via `CONFIG_TARGET_PATH`).
*   **Exercício Try-it-now:** Rodar app "legado" com default e com override de arquivo.
    *   1. Rodar default: `docker run -d -p 8089:80 diamol/ch18-image-of-the-day`.
    *   2. Rodar com override: `docker run -d -p 8090:80 -v "$(pwd)/config/dev:/config-override" -e CONFIG_SOURCE_PATH="/config-override/application.properties" diamol/ch18-image-of-the-day`.
    *   3. Verificar configs via API `/config`: `curl :8089/config`, `curl :8090/config`.
*   Experiência do *usuário* é similar (monta volume, seta env var), mesmo que mecanismo interno seja diferente. Única diferença: precisa setar `CONFIG_SOURCE_PATH`.

🧠 Explicação detalhada e didática:

*   A página aplica a abordagem "legada" à API Java `image-of-the-day`. A imagem `diamol/ch18-image-of-the-day` contém a API e um utilitário `ConfigLoader`. O `CMD` desta imagem primeiro executa `ConfigLoader` e depois a API.
*   O `ConfigLoader` foi programado para:
    *   Ler um arquivo de propriedades de override de um caminho especificado pela variável de ambiente `CONFIG_SOURCE_PATH`.
    *   Ler variáveis de ambiente que começam com `IOTD_`.
    *   Mesclar essas configurações com alguma base (talvez interna ou de outro arquivo).
    *   Escrever o resultado final em um arquivo no caminho especificado por `CONFIG_TARGET_PATH` (que a API Java principal espera ler).
*   O exercício demonstra o uso:
    *   O primeiro contêiner roda sem overrides externos. O `ConfigLoader` roda, mas não encontra fontes externas, e provavelmente gera um arquivo de configuração padrão para a API.
    *   O segundo contêiner:
        *   Monta a pasta `config/dev` local em `/config-override`.
        *   Define `CONFIG_SOURCE_PATH` para apontar para o arquivo `application.properties` dentro dessa pasta montada.
        *   O `ConfigLoader` agora lê esse arquivo de override, mescla e gera a configuração final para a API usar.
*   O ponto principal é que, do ponto de vista de quem *opera* o contêiner (definindo volumes e env vars), a experiência é muito parecida com a dos apps modernos, apesar da complexidade extra na inicialização interna. A única diferença notável é a necessidade de fornecer o `CONFIG_SOURCE_PATH`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises\image-of-the-day
    # Verifique a pasta config\dev e o arquivo application.properties dentro.
    ```
2.  **Rodar Default (Porta 8089):**
    ```powershell
    docker container run -d -p 8089:80 --name iotd-legacy-default diamol/ch18-image-of-the-day
    ```
3.  **Rodar com Override de Arquivo (Porta 8090):** (Ajuste target se imagem for Win)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/config-override' # Para imagem Linux
    $SourceFileInContainer = '/config-override/application.properties'
    docker container run -d -p 8090:80 --name iotd-legacy-dev -v "$SourcePath`:$TargetPath" -e CONFIG_SOURCE_PATH=$SourceFileInContainer diamol/ch18-image-of-the-day
    ```
4.  **Verificar Configs:**
    ```powershell
    curl http://localhost:8089/config ; echo "" # Default
    curl http://localhost:8090/config ; echo "" # Com override Dev
    ```
    Compare as saídas (ex: `environment`).

---

📄 Página 376

🔍 Resumo dos principais pontos:

*   Figura 18.10: Mostra saída das APIs `/config`, confirmando que override de arquivo funcionou para app "legado".
*   **Utilitário de Config:**
    *   Acontece na inicialização (atrasa um pouco app ficar pronto, adiciona ponto de falha -> usar health checks!).
    *   Permite estender imagem para fazer app antigo se comportar como novo.
    *   Controla lógica de inicialização (`CMD`/`ENTRYPOINT`).
*   `ConfigLoader` também suporta override via Env Vars (com prefixo `IOTD_`).
*   **Exercício Try-it-now:** Rodar "legado" com override de arquivo + override de env var.
    *   Objetivo: Mudar `environment` via env var `IOTD_ENVIRONMENT`.
    *   Comando: `docker run -d -p 8091:80 -v ...:/config-override -e CONFIG_SOURCE_PATH=... -e IOTD_ENVIRONMENT="custom" diamol/ch18-image-of-the-day`.
    *   Verificar config: `curl http://localhost:8091/config`.
*   Figura 18.11: Mostra `environment: custom` vindo da env var.

🧠 Explicação detalhada e didática:

*   A página confirma que o override de arquivo funcionou para a aplicação "legada" simulada. Reitera que adicionar essa lógica de inicialização customizada (o utilitário `ConfigLoader`) tem pequenas desvantagens (atraso no startup, mais um ponto de falha) que devem ser mitigadas com health checks apropriados.
*   Explica que o utilitário `ConfigLoader` também foi programado para seguir o padrão completo, permitindo que **variáveis de ambiente** (com prefixo `IOTD_`) sobrescrevam as configurações vindas dos arquivos.
*   O exercício demonstra essa camada final de override:
    *   Roda o contêiner com o bind mount do arquivo `dev` (que define `environment=DEV`).
    *   Mas também define a variável de ambiente `-e IOTD_ENVIRONMENT="custom"`.
    *   O `ConfigLoader`, ao rodar, detecta a variável `IOTD_ENVIRONMENT` e usa seu valor ("custom") com precedência sobre o valor ("DEV") vindo do arquivo `application.properties`.
*   O resultado (Figura 18.11) mostra que a configuração final reflete o valor da variável de ambiente, completando o padrão multi-camadas mesmo para a aplicação "legada".

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Parar/Remover Anterior (se usou 8090):**
    ```powershell
    docker rm -f iotd-legacy-dev
    ```
2.  **Rodar com Override Duplo (Porta 8091):**
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "config\dev"
    $TargetPath = '/config-override'
    $SourceFileInContainer = '/config-override/application.properties'
    docker container run -d -p 8091:80 --name iotd-legacy-env -v "$SourcePath`:$TargetPath" -e CONFIG_SOURCE_PATH=$SourceFileInContainer -e IOTD_ENVIRONMENT="custom" diamol/ch18-image-of-the-day
    ```
3.  **Verificar Config:**
    ```powershell
    curl http://localhost:8091/config ; echo ""
    # Verifique se environment=custom.
    ```

---

📄 Página 377

🔍 Resumo dos principais pontos:

*   Figura 18.11: Confirma `environment: custom`.
*   Conclusão: App "legado" agora usa mesmo padrão config multi-tier dos outros. Transparente para usuário/operador.
*   **Documentação de Config:** Fazer via arquivos de manifesto (Compose).
*   **Exemplo Compose Completo (Image Gallery):** Reúne todos os componentes com config Dev + métricas habilitadas via env var.
*   **Listagem 18.3:** Trechos de `docker-compose.yml` configurando os 3 apps.
    *   `accesslog`: Usa `-e NODE_CONFIG=...` e `secrets:` (com `file:` apontando para `dev/local.json`).
    *   `iotd`: Usa `-e IOTD_MANAGEMENT...` e `secrets:` (com `file:` apontando para `dev/application.properties`).
    *   `image-gallery`: Usa `-e IG_METRICS.ENABLED...` e `secrets:` (com `file:` apontando para `dev/config.toml`).
    *   Seção raiz `secrets:` define as fontes `file:` para cada secret.

🧠 Explicação detalhada e didática:

*   A página conclui o exemplo do app legado, mostrando que a abordagem do utilitário permite um comportamento de configuração externo consistente com apps modernos.
*   Enfatiza que a melhor forma de **documentar** as configurações necessárias (e as variações entre ambientes) é no próprio **arquivo de manifesto** da aplicação (o `docker-compose.yml` para Compose/Swarm, ou manifestos K8s).
*   A Listagem 18.3 exemplifica isso reunindo as configurações para rodar a aplicação Image Gallery completa (accesslog, iotd, image-gallery) em um modo "Dev com métricas". O arquivo Compose mostra claramente para cada serviço:
    *   Quais variáveis de ambiente (`environment:`) são necessárias (e seus formatos específicos, como a string JSON para `NODE_CONFIG` ou o prefixo `IOTD_` ou `IG_`).
    *   Quais arquivos de configuração externos (`secrets:` com `source:`) são esperados e onde serão montados (`target:`).
    *   A seção `secrets:` raiz define a origem (`file:`) desses arquivos de configuração externos no host para este ambiente específico (Dev).
*   Este arquivo Compose serve tanto para **executar** a configuração quanto para **documentá-la**.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará um `docker-compose.yml` similar à Listagem 18.3 (provavelmente em `ch18/exercises/docker-compose.yml`) para rodar a stack completa no próximo exercício.
*   Note o uso de `secrets:` aqui para carregar os arquivos de config `dev`. Isso funciona bem no Compose e prepara para usar secrets reais no Swarm/K8s para configurações mais sensíveis.

---

📄 Página 378

🔍 Resumo dos principais pontos:

*   Análise Listagem 18.3: Padrão similar, detalhes diferentes (formato env var, tipo arquivo secret). Uso de `secrets` funciona como volumes/configs.
*   **Experiência Usuário/Ops:** Boa (fácil mudar ambiente/features), mas Ruim (precisa saber quirks de cada app).
    *   Mitigado por ter overrides Compose pré-feitos para cada config comum.
*   **Exercício Try-it-now:** Rodar app completo com config fixa do Compose file.
    *   1. Limpar containers.
    *   2. Navegar para `ch18/exercises`.
    *   3. Rodar `docker-compose up -d` (usará `docker-compose.yml` padrão com config da Listagem 18.3).
    *   4. Verificar configs via APIs `/config` de cada serviço (ports 8010, 8020, 8030 - *Nota: verificar ports reais no .yml*).
*   Figura 18.12: Mostra saída das 3 APIs `/config`, confirmando `environment: DEV`, `release: 19.12`, `metrics: enabled: true` em todos.

🧠 Explicação detalhada e didática:

*   A página conclui a análise da Listagem 18.3, reforçando a consistência do padrão, mas também as inevitáveis diferenças de implementação. A recomendação é encapsular essas configurações em arquivos Compose de override bem definidos para cada cenário comum.
*   O exercício final do capítulo instrui a rodar a aplicação Image Gallery completa usando o `docker-compose.yml` que implementa a configuração "Dev com Métricas" da Listagem 18.3.
*   O objetivo é verificar se todos os três componentes (`access-log`, `iotd`, `image-gallery`) carregaram corretamente a combinação de configurações:
    *   `release: 19.12` (do `default.json/.properties/.toml` na imagem).
    *   `environment: DEV` (do arquivo `dev/...` carregado via `secrets:`).
    *   `metrics.enabled: true` (da variável de ambiente definida em `environment:` no Compose).
*   A verificação é feita acessando a API `/config` de cada serviço (nas portas publicadas por eles no `docker-compose.yml`). A Figura 18.12 mostra o resultado esperado.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Opcional: docker volume prune -f
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch18\exercises
    ```
3.  **Verificar `docker-compose.yml`:** Abra o arquivo. Confirme que ele define os 3 serviços, a rede, os secrets (com `file:` apontando para subpastas `dev`), e as variáveis `environment:` para ligar métricas. Anote as portas publicadas para cada serviço (ex: 8010, 8020, 8030).
4.  **Iniciar Stack:**
    ```powershell
    docker-compose up -d
    ```
5.  **Verificar Configs:** (Use as portas corretas do seu `.yml`)
    ```powershell
    curl http://localhost:8010/config ; echo "" # Gallery (Go)
    curl http://localhost:8020/config ; echo "" # IOTD (Java)
    curl http://localhost:8030/config ; echo "" # AccessLog (Node)
    ```
    Confirme que todos mostram `DEV`, `19.12`, e `metrics enabled true` (ou equivalente).
6.  **Testar App/Métricas (Opcional):** Acesse `http://localhost:8010`. Acesse os endpoints `/metrics` de cada serviço (ex: `:8010/metrics`, `:8020/actuator/prometheus`, `:8030/metrics`).

---

📄 Página 379

🔍 Resumo dos principais pontos:

*   Figura 18.12: Confirma configs aplicadas via Compose.
*   Fim da parte prática de config. Próximo: entender benefícios do modelo flexível.
*   **18.5 Understanding why a flexible configuration model pays off** (Entendendo por que um modelo de configuração flexível compensa)
*   Revisita CI/CD (Caps 11, 15): Builda imagem única, promove entre ambientes.
*   Modelo config multi-tier suporta isso: mesma imagem, comportamento diferente por ambiente.
*   Benefícios de usar env vars para overrides finais (feature flags):
    *   Reagir rápido a problemas prod (desligar log verboso, desligar feature bugada).
    *   Replicar prod localmente (usar config prod, mas sobrescrever BD com env var para usar SQLite).
*   **Payback:** Habilidade de rodar *exatamente a mesma imagem* em qualquer ambiente.
*   Diagrama (Figura 18.13): Ciclo de vida da imagem: CI/CD -> Smoke -> UAT -> Prod -> Dev (debug). Mesma imagem, configs diferentes.

🧠 Explicação detalhada e didática:

*   A seção 18.5 resume o **valor estratégico** de investir tempo em um modelo de configuração flexível e multi-camadas (Padrão na Imagem -> Arquivo Externo -> Variável de Ambiente) para suas aplicações conteinerizadas.
*   O benefício principal é habilitar o fluxo de trabalho ideal de CI/CD: **construir a imagem Docker uma única vez e promover essa mesma imagem imutável através de todos os ambientes**. A flexibilidade na configuração permite que essa imagem única se adapte às necessidades de cada ambiente (conectar ao BD de teste, usar chaves de API de produção, ter logging diferente, etc.) sem precisar reconstruir a imagem.
*   O uso de variáveis de ambiente como a camada final de override é particularmente poderoso para:
    *   **Operações:** Permitir ajustes rápidos em produção (ex: diminuir nível de log se estiver causando sobrecarga) sem precisar de um novo deploy completo da aplicação.
    *   **Feature Flags:** Ligar ou desligar funcionalidades específicas da aplicação dinamicamente via variáveis de ambiente.
    *   **Debugging:** Facilitar a reprodução de um problema de produção no ambiente de desenvolvimento, usando a configuração de produção mas sobrescrevendo apenas o necessário (como a conexão do banco de dados) via variáveis de ambiente.
*   A Figura 18.13 ilustra esse fluxo virtuoso da imagem única através dos diferentes ambientes, cada um aplicando sua própria configuração.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando novo. É a justificativa final para os padrões mostrados no capítulo. Pense em como você gerenciaria diferentes configurações para seus próprios projetos usando essas camadas.

---

📄 Página 380

🔍 Resumo dos principais pontos:

*   Figura 18.13: Diagrama do ciclo de vida da imagem.
*   Modelo flexível é **future-proof**:
    *   Runtimes de contêiner (Compose, Swarm, K8s) suportam carregar arquivos (configs/secrets) e setar env vars.
    *   Mesma imagem/modelo funciona em todos.
    *   Também funciona em PaaS e Serverless (usam mesmos mecanismos de config).
*   **18.6 Lab** (Laboratório)
    *   Objetivo: Praticar configuração de app com múltiplas fontes.
    *   App: Image Gallery novamente.
    *   Ponto de Partida: `docker-compose.yml` em `ch18/lab` com serviços definidos, mas *sem* configuração aplicada.
*   **Tarefa:** Modificar `docker-compose.yml` para configurar *todos* os componentes para:
    1.  Usar **Volumes** (Docker Volumes nomeados) para carregar arquivos de override.
    2.  Carregar os overrides do ambiente **Test**.
    3.  Sobrescrever o ciclo de release para ser `"20.01"` (em vez de "19.12").

🧠 Explicação detalhada e didática:

*   A página finaliza a seção de benefícios, destacando que o modelo de configuração multi-camadas baseado em arquivos e variáveis de ambiente é **portátil** não apenas entre diferentes orquestradores de contêiner (Compose, Swarm, K8s), mas também para outras plataformas modernas como PaaS (Platform-as-a-Service) e FaaS (Functions-as-a-Service / Serverless), que geralmente usam os mesmos mecanismos. Investir nesse modelo agora facilita migrações futuras.
*   O **Lab do Capítulo 18** pede para você aplicar esse modelo completo ao configurar a aplicação Image Gallery. Você recebe um `docker-compose.yml` básico e precisa modificá-lo para:
    1.  **Usar Volumes:** Em vez de bind mounts (como nos exercícios) ou secrets (como no exemplo final), usar **Volumes Docker Nomeados** para fornecer os arquivos de configuração de override para cada serviço. Isso envolverá:
        *   Definir volumes nomeados na seção raiz `volumes:`.
        *   Montar cada volume no `target` apropriado (`/app/config-override`) dentro de cada serviço (`access-log`, `iotd`, `image-gallery`).
        *   *Pré-popular* esses volumes com os arquivos de configuração do ambiente de **Teste** (provavelmente copiando os arquivos de `ch18/exercises/.../config/test` para o local dos volumes no host antes de iniciar, ou usando um container temporário para copiar os arquivos para o volume - esta parte pode ser tricky). *Alternativa mais simples (talvez o que o lab espera):* Usar bind mounts para os arquivos de teste, mas **declarar** volumes nomeados no YAML (mesmo que não usados diretamente para config) como parte do exercício de sintaxe. **VERIFIQUE A SOLUÇÃO DO LAB se esta parte for confusa.** O mais provável é usar bind mount para os arquivos de teste, mas a instrução pede Volumes. Talvez o objetivo seja só declarar o volume e ainda usar bind mount para os arquivos, ou usar um container inicializador. Vamos assumir bind mount por simplicidade na estratégia.
    2.  **Carregar Config Test:** Usar os arquivos das pastas `config/test` de cada app como `source` para os bind mounts (ou para popular os volumes).
    3.  **Sobrescrever Release:** Usar a seção `environment:` em cada serviço para definir a variável de ambiente apropriada (`release=20.01`, `NODE_CONFIG='{"release":"20.01"}'`, `IG_RELEASE="20.01"`) para sobrescrever o ciclo de release para "20.01".

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 18 (Usando Bind Mounts para simplicidade, apesar da instrução ambígua sobre Volumes):**
    1.  Navegue para `ch18\lab`. Copie o `docker-compose.yml` inicial.
    2.  Adicione seção `networks:` (ex: `app-net:`). Conecte todos os serviços a ela.
    3.  Para cada serviço (`access-log`, `iotd`, `image-gallery`):
        *   Adicione `ports:` se necessário para acessar APIs `/config`.
        *   Adicione seção `environment:` e defina a variável para setar `release` para `"20.01"` (formato varia por app!).
        *   Adicione seção `volumes:` (ou `--mount`) para fazer bind mount da pasta de config de **Teste** apropriada (ex: `../exercises/access-log/config/test`) para `/app/config-override` (ou path correto). Use paths relativos ou absolutos corretos.
    4.  **Rodar:** `docker-compose up -d`.
    5.  **Verificar:** Acesse as APIs `/config` de cada serviço e confirme que `environment` é `TEST` e `release` é `20.01`.

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch18/lab/README.md (Verifique como eles lidaram com a questão Volume vs. Bind Mount para os arquivos de config).

---

📄 Página 381

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 18 no GitHub.

🧠 Explicação detalhada e didática:

*   Fornece o link para a solução completa do Lab 18 no repositório do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 18, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch18/lab/README.md

---

📄 Página 382

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco, separando o Lab 18 do início do Capítulo 19.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 383

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 384

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Página em branco.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Página 385

🔍 Resumo dos principais pontos:

*   **Capítulo 19: Writing and managing application logs with Docker** (Escrevendo e gerenciando logs de aplicação com Docker)
*   Logging: Geralmente chato, mas não com Docker.
*   **Princípio Básico:** App precisa escrever logs para **Standard Output (stdout)** ou **Standard Error (stderr)**. Docker coleta dessas streams.
*   Capítulo cobre como fazer isso e o que fazer *depois*.
*   **Docker Logging Framework:** É plugável. Uma vez que logs saem do container (via stdout/stderr), Docker pode enviá-los para diferentes destinos (drivers de log).
*   Permite modelo poderoso: logs de todos os containers centralizados em log store com UI de busca (ex: EFK stack - Elasticsearch, Fluentd, Kibana). Tudo open source, rodando em containers.
*   **19.1 Welcome to stderr and stdout!** (Bem-vindo ao stderr e stdout!)
    *   Imagem Docker = Filesystem + Metadado (qual processo iniciar).
    *   Processo roda em foreground, como comando no terminal.
    *   Saída do processo (logs, erros) vai para stdout e stderr.
    *   Docker *assiste* stdout/stderr e coleta essa saída como "logs do contêiner".

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 19, focado em **logging** no contexto Docker.
*   O ponto fundamental e mais importante é: para que o Docker possa gerenciar os logs da sua aplicação, a aplicação precisa ser configurada para escrever suas mensagens de log nas **saídas padrão (standard output - stdout)** ou de **erro padrão (standard error - stderr)**. Estas são as mesmas streams que um programa de linha de comando usa para imprimir texto no seu terminal.
*   Quando um contêiner está rodando, o Docker monitora essas duas streams do processo principal. Tudo que é escrito nelas é capturado pelo Docker e se torna o que chamamos de "logs do contêiner" (acessíveis via `docker logs`).
*   A grande vantagem disso é que, uma vez que os logs estão sendo capturados pelo Docker, ele pode usar **drivers de log** diferentes para enviá-los a diversos destinos. O padrão é salvar em um arquivo JSON no host, mas pode-se configurar para enviar para Fluentd, Splunk, Syslog, AWS CloudWatch, etc., permitindo a centralização e análise de logs de múltiplos contêineres.
*   A seção 19.1 revisita como o processo principal do contêiner se relaciona com stdout/stderr.

💻 Exemplos práticos (para meu ambiente Windows):

*   Qualquer comando que você roda no PowerShell/CMD que imprime texto na tela está usando stdout/stderr.
*   Aplicações .NET Core, Java (com Logback/Log4j configurados para ConsoleAppender), Node.js (com `console.log`), Python (`print`) geralmente escrevem para stdout/stderr por padrão ou podem ser facilmente configuradas para isso.

💡 Analogias ou observações úteis (se houver):

*   stdout/stderr: Pense neles como os dois "megafones" que o processo dentro do contêiner pode usar para falar. stdout é para mensagens normais, stderr é para erros. O Docker fica "ouvindo" esses megafones.
*   Logging Drivers: São como "serviços postais" diferentes que o Docker pode usar para pegar as mensagens dos megafones e enviá-las para destinos diferentes (arquivo local, servidor de log central, nuvem, etc.).

---

📄 Página 386

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar `timecheck` (Cap 15) em foreground para ver logs em stdout.
    *   Comando: `docker run diamol/ch15-timecheck:3.0` (sem `-d`).
    *   Terminal fica preso, mostrando logs da app (timestamps) periodicamente.
    *   Sair com `Ctrl+C`.
*   Figura 19.1: Mostra terminal com logs do `timecheck` rodando em foreground.
*   Este é o modelo operacional padrão: processo foreground, logs via stdout/stderr, Docker coleta.
*   Todos os apps do livro seguem este padrão.
*   Diagrama (Figura 19.2): Ilustra App -> stdout/stderr -> Docker coletando.
*   **Armazenamento Padrão:** Docker salva logs coletados em arquivos **JSON** no host.
    *   Logs persistem para containers detached ou `Exited`.
    *   Arquivos JSON têm mesmo ciclo de vida do container (removidos com `docker rm`).

🧠 Explicação detalhada e didática:

*   O exercício demonstra o comportamento padrão: rodar um contêiner em **primeiro plano (foreground)** (sem `-d`). O terminal que iniciou o contêiner fica vinculado às streams stdout/stderr do processo dentro do contêiner, exibindo diretamente qualquer saída que ele produza. Isso é útil para depuração rápida, mas não para serviços que precisam rodar continuamente.
*   A Figura 19.2 reforça visualmente: a aplicação escreve para stdout/stderr, e o Docker Engine captura essa saída.
*   Explica como o Docker armazena esses logs por padrão quando *não* está rodando em foreground: ele usa um **driver de log** chamado `json-file`. Esse driver escreve cada linha de log (junto com metadados como timestamp e stream) como um objeto JSON em um arquivo específico para aquele contêiner no disco do host Docker.
*   **Importante:** Esses arquivos JSON são gerenciados pelo Docker e são **removidos** quando o contêiner correspondente é removido com `docker rm`. Se você precisar de retenção de logs a longo prazo, não pode depender apenas desses arquivos locais; precisará usar um driver de log que envie os dados para um sistema externo.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar em Foreground:**
    ```powershell
    # Use uma imagem simples que imprime e sai, ou a timecheck
    docker container run diamol/ch15-timecheck:3.0
    # Observe os logs aparecerem. Pressione Ctrl+C para parar.
    ```
2.  **Localização dos Arquivos JSON (Informativo):**
    *   No Docker Desktop, os arquivos estão dentro da VM Linux ou nos dados do serviço Windows, e não são facilmente acessíveis diretamente do host Windows.
    *   Em um servidor Linux Docker nativo, estariam tipicamente em `/var/lib/docker/containers/<container-id>/<container-id>-json.log`.
    *   Em um servidor Windows com Windows Containers, estariam em `C:\ProgramData\docker\containers\<container-id>\...`.
    *   Você normalmente interage com os logs via `docker logs`, não acessando esses arquivos diretamente.

---

📄 Página 387

🔍 Resumo dos principais pontos:

*   Figura 19.2: Diagrama App -> stdout/stderr -> Docker.
*   **Exercício Try-it-now:** Rodar `timecheck` em background (`-d`), verificar logs, parar container, verificar logs de novo, inspecionar path do log file.
    *   `docker run -d --name timecheck ...`
    *   `docker logs --tail 1 timecheck` (ver último log rodando)
    *   `docker stop timecheck`
    *   `docker logs --tail 1 timecheck` (ver último log mesmo parado)
    *   `docker inspect --format='{{.LogPath}}' timecheck` (ver caminho do arquivo JSON no host/VM)
*   Figura 19.3: Mostra saída dos comandos e exemplo de path do log file no Windows.
*   Acessar log file diretamente não é comum/fácil (especialmente no Docker Desktop). `docker logs` é a interface padrão.

🧠 Explicação detalhada e didática:

*   O exercício demonstra que os logs capturados pelo Docker (usando o driver `json-file` padrão) persistem mesmo quando o contêiner está rodando em background (`-d`) ou quando está parado (`Exited` após `docker stop`). O comando `docker logs` ainda consegue ler o conteúdo do arquivo JSON associado ao contêiner.
*   O comando `docker inspect --format='{{.LogPath}}'` revela o caminho exato no sistema de arquivos do host (ou da VM do Docker Desktop) onde o arquivo `...-json.log` para aquele contêiner específico está armazenado.
*   A página reitera que, embora seja bom saber que os logs são armazenados em arquivos, a maneira normal e recomendada de acessá-los é através do comando `docker logs`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar em Background e Ver Logs:**
    ```powershell
    # Limpar anterior se existir
    docker rm -f timecheck timecheck2 timecheck3 timecheck4 timecheck5

    # Rodar timecheck v3 em background
    docker container run -d --name timecheck diamol/ch15-timecheck:3.0

    # Esperar um pouco e ver último log
    Start-Sleep -Seconds 5
    docker container logs --tail 1 timecheck

    # Parar o container
    docker container stop timecheck

    # Verificar que log ainda existe
    docker container logs --tail 1 timecheck

    # Ver path do log file (informativo)
    docker container inspect --format='{{.LogPath}}' timecheck
    # A saída será um path dentro da VM do Docker Desktop (ex: /var/lib/docker/...)
    ```

---

📄 Página 388

🔍 Resumo dos principais pontos:

*   Formato JSON bruto no arquivo de log: `{"log":"...", "stream":"stdout/stderr", "time":"..."}`.
*   Listagem 19.1: Exemplo do JSON bruto.
*   `docker logs` lê e formata esse JSON.
*   **Gerenciamento Tamanho/Rotação de Log:**
    *   Padrão: 1 arquivo JSON por container, cresce indefinidamente (risco de encher disco).
    *   Solução: Configurar Docker para usar **rolling files** com tamanho máximo.
        *   Quando arquivo atinge `max-size`, Docker começa novo arquivo (até `max-file` arquivos).
        *   Quando todos cheios, sobrescreve o mais antigo.
    *   Pode configurar globalmente (Engine `daemon.json`) ou por container (`docker run --log-opt ...`).
    *   Configurar por container é bom para ter controle fino (ex: logs pequenos/rotacionados para app verboso, logs completos para outros).

🧠 Explicação detalhada e didática:

*   A página mostra o formato JSON real (Listagem 19.1) que o driver `json-file` usa para armazenar cada entrada de log. O comando `docker logs` simplesmente lê esse arquivo e extrai o campo `"log"` para exibição (opcionalmente adicionando timestamp ou outros detalhes com flags).
*   Aborda um problema prático importante do driver padrão: o arquivo de log pode crescer sem limite. Para evitar que logs de contêineres muito "falantes" consumam todo o espaço em disco, o Docker permite configurar a **rotação de logs**.
*   As opções principais (para o driver `json-file`) são:
    *   `max-size`: O tamanho máximo que um único arquivo de log pode atingir (ex: `10m` para 10MB, `5k` para 5KB).
    *   `max-file`: O número máximo de arquivos de log rotacionados a serem mantidos.
*   Quando um arquivo atinge `max-size`, o Docker o renomeia (ex: `log.1`) e começa um novo `log`. Isso continua até atingir `max-file` arquivos (`log`, `log.1`, ..., `log.N-1`). Quando o `log` atual enche novamente, o arquivo mais antigo (`log.N-1`) é excluído, os outros são renomeados (`log.N-2` -> `log.N-1`, etc.), e um novo `log` é iniciado.
*   Essas opções podem ser definidas globalmente para todos os contêineres no `daemon.json` ou individualmente para um contêiner específico usando a flag `--log-opt` no comando `docker run` (ou `logging: options:` no Docker Compose).

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício usará `--log-opt` no `docker run`.
*   **Exemplo de Configuração Global (`daemon.json`):**
    ```json
    {
      "log-driver": "json-file",
      "log-opts": {
        "max-size": "10m",
        "max-file": "3"
      }
      // ... outras configs ...
    }
    ```
    *(Lembre-se de reiniciar o Docker Engine após mudar `daemon.json`)*.

---

📄 Página 389

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Rodar `timecheck` com opções de log por container.
    *   Comando: `docker run -d --name timecheck2 --log-opt max-size=5k --log-opt max-file=3 -e Timer__IntervalSeconds=1 diamol/ch15-timecheck:3.0`.
    *   `--log-opt max-size=5k`: Limita cada arquivo a 5 KB.
    *   `--log-opt max-file=3`: Mantém no máximo 3 arquivos.
    *   `-e Timer__IntervalSeconds=1`: Faz app logar mais rápido (a cada 1s).
    *   Esperar alguns minutos para logs acumularem/rotacionarem.
    *   Inspecionar log path: `docker inspect --format='{{.LogPath}}' timecheck2`.
*   Saída do `inspect` ainda mostra path de 1 arquivo, mas Docker gerencia rotação com sufixos (`.1`, `.2`).
*   Se host permite acesso (Win Container, Linux CE), pode-se listar diretório e ver arquivos rotacionados.
*   Figura 19.4: Mostra exemplo de arquivos rotacionados (`...-json.log`, `...-json.log.1`, `...-json.log.2`) no host.

🧠 Explicação detalhada e didática:

*   O exercício demonstra a configuração de rotação de logs *por contêiner* usando as flags `--log-opt`.
*   Um novo contêiner (`timecheck2`) é iniciado com limites pequenos (`max-size=5k`, `max-file=3`) e configurado para logar mais frequentemente (`-e Timer__IntervalSeconds=1`) para acelerar o processo de rotação.
*   Após esperar um tempo, o comando `docker inspect` ainda mostrará o caminho para o arquivo de log "principal" (sem sufixo). No entanto, por baixo dos panos, o Docker Engine está gerenciando os arquivos rotacionados (adicionando `.1`, `.2`, etc.) naquele mesmo diretório e respeitando os limites definidos.
*   A Figura 19.4 ilustra como esses arquivos apareceriam no sistema de arquivos do host (se diretamente acessível).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar com Rotação:**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck timecheck2

    docker container run -d --name timecheck2 --log-opt max-size=5k --log-opt max-file=3 -e Timer__IntervalSeconds=1 diamol/ch15-timecheck:3.0
    ```
2.  **Esperar e Inspecionar:**
    ```powershell
    Write-Host "Aguardando logs rotacionarem..."
    Start-Sleep -Seconds 60 # Espere 1 minuto ou mais

    docker container inspect --format='{{.LogPath}}' timecheck2
    # Anote o caminho (informativo)

    # Verificar logs via Docker (deve mostrar apenas os logs mais recentes, <= 15KB)
    docker logs timecheck2
    ```
    *Como você está no Docker Desktop, não conseguirá ver os arquivos `.1`, `.2` diretamente no seu filesystem Windows.*

---

📄 Página 390

🔍 Resumo dos principais pontos:

*   Docker tem estágio de coleta/processamento para logs stdout/stderr -> onde opções (`--log-opt`) e drivers (`--log-driver`) atuam.
*   Para aproveitar, app *precisa* enviar logs para stdout/stderr.
*   **Problema:** Nem todo app faz isso.
    *   Apps background (Windows Service, Linux daemon): Processo principal do container não é o app -> sem logs.
    *   Apps com logging frameworks antigos/diferentes: Escrevem para arquivos, Syslog (Linux), Event Log (Windows) -> Docker não vê.
*   **19.2 Relaying logs from other sinks to stdout** (Retransmitindo logs de outros coletores/sinks para stdout)
*   **Exercício Try-it-now:** Demonstrar app que loga em arquivo, não stdout.
    *   Imagem: `diamol/ch19-timecheck:4.0`.
    *   1. Rodar container: `docker run -d --name timecheck3 ...:4.0`.
    *   2. Verificar `docker logs timecheck3` (vazio).
    *   3. Conectar ao container (`docker exec -it ... sh/cmd`).
    *   4. Ler arquivo de log interno: `cat /logs/timecheck.log`. Logs estão lá.

🧠 Explicação detalhada e didática:

*   A página resume que as funcionalidades avançadas de logging do Docker (drivers, opções) dependem fundamentalmente de os logs da aplicação serem direcionados para **stdout/stderr**.
*   Introduce o problema comum: muitas aplicações, especialmente as mais antigas ou aquelas projetadas para rodar como serviços de background, **não** logam para stdout/stderr por padrão. Elas podem:
    *   Rodar como um processo de background, enquanto o processo principal do contêiner é apenas um script de inicialização que termina.
    *   Usar frameworks de logging configurados para escrever em arquivos (`app.log`), no Syslog do sistema (Linux) ou no Event Log (Windows).
*   Em todos esses casos, o Docker Engine não captura os logs da aplicação através do monitoramento de stdout/stderr do processo principal.
*   A seção 19.2 introduz a solução para isso.
*   O exercício demonstra o problema usando uma versão modificada do `timecheck` (`:4.0`) que escreve seus logs para um arquivo `/logs/timecheck.log` *dentro* do contêiner. O resultado esperado é que `docker logs timecheck3` não mostrará nada, mas se você entrar no contêiner (`docker exec`) e ler o arquivo, os logs estarão lá.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar App v4 (Log em Arquivo):**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck3

    docker container run -d --name timecheck3 diamol/ch19-timecheck:4.0
    ```
2.  **Verificar Docker Logs (Vazio):**
    ```powershell
    Start-Sleep -Seconds 5
    docker logs timecheck3 # Deve retornar vazio ou quase vazio
    ```
3.  **Verificar Arquivo Interno:** (Assumindo imagem Linux)
    ```powershell
    docker container exec -it timecheck3 sh
    # Dentro do container:
    ls /logs # Deve mostrar timecheck.log
    cat /logs/timecheck.log # Deve mostrar os logs reais da aplicação
    exit
    ```

---

📄 Página 391

🔍 Resumo dos principais pontos:

*   Figura 19.5: Mostra `docker logs` vazio e `cat /logs/timecheck.log` com conteúdo.
*   Docker só lê stdout/stderr, não outros sinks (arquivos, syslog, etc.).
*   **Padrão "Sidecar" ou Processo Duplo para Relaying:**
    *   Rodar um **segundo processo** no comando de inicialização do container.
    *   Esse segundo processo lê logs do sink original (ex: arquivo, syslog) e os **escreve para stdout**.
    *   Esse relay deve ser o *último* processo iniciado (para ser o processo foreground que Docker monitora).
*   Diagrama (Figura 19.6): Ilustra Padrão Relay.
    *   App escreve em `app.log`.
    *   Relay Utility (rodando no mesmo container) lê `app.log`.
    *   Relay Utility escreve para stdout.
    *   Docker coleta de stdout.
*   **Desvantagens/Cuidados:**
    *   Relay precisa ser robusto (se ele falhar, container sai, mesmo se app OK).
    *   Se app falhar mas relay continuar, container fica Up (precisa de health check no app).
    *   Ineficiente em disco (logs duplicados: no sink original + no JSON do Docker).

🧠 Explicação detalhada e didática:

*   A página apresenta a solução padrão para capturar logs de aplicações que não escrevem em stdout/stderr: adicionar um **processo "relay"** (retransmissor) dentro do mesmo contêiner.
*   **Como funciona:**
    1.  A aplicação principal roda (muitas vezes em background).
    2.  Um segundo processo (um script ou um pequeno utilitário) é iniciado.
    3.  Esse segundo processo é configurado para "seguir" ou ler continuamente o local onde a aplicação principal está escrevendo seus logs (ex: o arquivo `app.log`).
    4.  Tudo que o relay lê, ele imediatamente escreve para seu próprio **stdout**.
    5.  O comando de inicialização do contêiner (`CMD` ou `ENTRYPOINT`) é configurado para iniciar a aplicação principal em background e depois iniciar o processo relay em **foreground**.
    6.  Como o relay está em foreground, o Docker captura o stdout *dele*, que agora contém os logs originais da aplicação. (Figura 19.6 ilustra isso).
*   Essa técnica é às vezes chamada de padrão "sidecar" (embora mais comumente sidecar se refira a um contêiner separado; aqui são dois processos no mesmo contêiner).
*   A página alerta para as **desvantagens**:
    *   Complexidade e fragilidade: O relay precisa ser confiável.
    *   Mascaramento de falhas: Falha do app pode não parar o relay (e vice-versa). Health checks são essenciais.
    *   Duplicação de armazenamento: Os logs existem no local original *e* no arquivo JSON do Docker.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo Dockerfile (`v5`) implementará esse padrão com um utilitário `.NET` chamado `Tail.dll`.
*   Se fosse Linux puro, poderia usar `CMD meu_app & tail -F /var/log/meu_app.log`. O `tail -F` seguiria o arquivo e escreveria para stdout, enquanto `meu_app` rodaria em background (`&`).

---

📄 Página 392

🔍 Resumo dos principais pontos:

*   Ainda assim, relay é padrão útil (especialmente se app roda em background). Melhor ter logs duplicados do que logs invisíveis para Docker.
*   **Exemplo `timecheck:5.0`:** Adiciona utilitário `Tail.dll` para relay.
*   **Listagem 19.2:** Trechos do Dockerfile `v5` multi-stage.
    *   Estágio `builder` compila app principal.
    *   Estágio `utility-builder` compila `Tail.dll`.
    *   Estágio final `app` copia ambos.
    *   **`CMD`s diferentes para Windows/Linux:**
        *   Windows: `CMD start /B dotnet TimeCheck.dll && dotnet Tail.dll /logs timecheck.log`. (`start /B` roda app em background).
        *   Linux: `CMD dotnet TimeCheck.dll & dotnet Tail.dll /logs timecheck.log`. (`&` roda app em background).
    *   Em ambos, `Tail.dll` roda em foreground, lendo o log e escrevendo para stdout.
*   **Exercício Try-it-now:** Rodar v5 e verificar logs via `docker logs` e arquivo interno.
    *   `docker run -d --name timecheck4 diamol/ch19-timecheck:5.0`.
    *   `docker logs timecheck4` (deve mostrar logs).
    *   `docker exec ... cat /logs/timecheck.log` (logs ainda existem no arquivo também).

🧠 Explicação detalhada e didática:

*   A página argumenta que, apesar das desvantagens, o padrão relay é frequentemente a melhor (ou única) solução para integrar aplicações "não-stdout" com o ecossistema de logging do Docker.
*   A Listagem 19.2 mostra como a imagem `timecheck:5.0` implementa isso:
    *   Usa build multi-estágio para compilar tanto a aplicação `TimeCheck.dll` quanto o utilitário `Tail.dll`.
    *   O estágio final copia ambos.
    *   O `CMD` é crucial: ele primeiro inicia a aplicação principal `TimeCheck.dll` **em background** (usando `start /B` no Windows ou `&` no Linux) e depois inicia o utilitário `Tail.dll` **em foreground**, instruindo-o a monitorar o arquivo `/logs/timecheck.log`.
    *   Como `Tail.dll` está em foreground, o Docker monitora o stdout *dele*. O `Tail.dll` lê as linhas do arquivo de log e as imprime em seu stdout, efetivamente retransmitindo os logs para o Docker.
*   O exercício instrui a rodar esta imagem `v5`. A expectativa é que `docker logs timecheck4` *agora funcione*, mostrando os logs da aplicação, mas que os logs *também continuem sendo escritos* no arquivo `/logs/timecheck.log` dentro do contêiner (confirmando a duplicação).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar v5 com Relay:**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck4

    docker container run -d --name timecheck4 diamol/ch19-timecheck:5.0
    ```
2.  **Verificar Docker Logs (Funciona!):**
    ```powershell
    Start-Sleep -Seconds 5
    docker logs timecheck4
    ```
3.  **Verificar Arquivo Interno (Duplicado):** (Assumindo imagem Linux)
    ```powershell
    docker container exec -it timecheck4 sh
    # Dentro do container:
    cat /logs/timecheck.log
    # Logs estão aqui também.
    exit
    ```

---

📄 Página 393

🔍 Resumo dos principais pontos:

*   Figura 19.7: Mostra `docker logs` funcionando para v5, e `cat` mostrando logs duplicados no arquivo.
*   Usou utilitário customizado (`Tail.dll`) para cross-platform; `tail -F` Linux não tem equivalente Windows direto.
*   Utilitário customizado é mais flexível (pode ler de qualquer sink). Cobre qualquer cenário onde logs não vão para stdout.
*   Com todos os apps escrevendo container logs (direto ou via relay), pode-se usar logging plugável do Docker.
*   **19.3 Collecting and forwarding container logs** (Coletando e encaminhando logs de contêiner)
    *   Docker adiciona camada de gerenciamento consistente (start, stop, inspect, logs).
    *   Útil com **logging centralizado**.
    *   Exemplo: **Fluentd**.
        *   Camada de logging unificada, open source, projeto CNCF, maduro, flexível.
        *   Ingere logs de várias fontes.
        *   Filtra/enriquece logs.
        *   Encaminha para vários destinos.
        *   Roda em container.
    *   **Fluxo:**
        1.  Rodar Fluentd container (ouve por logs).
        2.  Rodar app containers usando **driver de log `fluentd`** do Docker (`--log-driver=fluentd`).
        3.  Logs do app (stdout/stderr) são enviados pelo Docker Engine para Fluentd (em vez do `json-file`).

🧠 Explicação detalhada e didática:

*   A página conclui a discussão sobre o padrão relay, justificando o uso do utilitário customizado pela portabilidade e flexibilidade. O objetivo principal é garantir que **todos** os seus contêineres, independentemente de como logavam originalmente, agora emitam seus logs de uma forma que o Docker possa capturar (via stdout/stderr do processo foreground).
*   A seção 19.3 introduz o **próximo nível** de gerenciamento de logs: usar o sistema plugável de **logging drivers** do Docker para enviar os logs capturados para um **sistema de logging centralizado**.
*   **Fluentd** é apresentado como uma solução popular e poderosa para isso:
    *   É um agregador/processador/encaminhador de logs muito versátil.
    *   Roda como um contêiner.
    *   O Docker Engine tem um `log-driver` embutido chamado `fluentd`. Quando você inicia um contêiner com `--log-driver=fluentd`, o Docker Engine, em vez de escrever os logs em um arquivo JSON, os envia (geralmente via rede UDP ou TCP) para um endereço onde o serviço Fluentd está ouvindo.
    *   O Fluentd recebe esses logs, pode processá-los (adicionar metadados, filtrar, transformar) e então enviá-los para um ou mais destinos finais (Elasticsearch, Splunk, S3, etc.).

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker run --log-driver=fluentd ...` para configurar contêineres a enviarem logs para o Fluentd.
*   Você rodará o Fluentd em um contêiner, configurando-o (via volume mount de arquivo de config) para receber logs e fazer algo com eles (inicialmente, apenas imprimir no próprio stdout do Fluentd).

💡 Analogias ou observações úteis (se houver):

*   Log Driver `fluentd`: É como dizer ao Docker: "Em vez de arquivar os logs deste contêiner localmente, por favor, envie-os para este endereço postal especial onde o carteiro Fluentd os recolherá".
*   Fluentd: O carteiro/centro de distribuição de logs. Ele recebe cartas (logs) de vários remetentes (containers), pode carimbar informações extras (enriquecer), separar por CEP (filtrar) e entregar em diferentes destinos finais (Elasticsearch, S3, etc.).

---

📄 Página 394

🔍 Resumo dos principais pontos:

*   Fluentd usa arquivo de config para processar logs.
*   **Exercício Try-it-now:** Rodar Fluentd (config simples: ecoar logs para stdout) e app enviando logs para ele.
    *   Navegar para `ch19/exercises/fluentd`.
    *   1. Rodar Fluentd: `docker run -d -p 24224:24224 --name fluentd -v "$(pwd)/conf:/fluentd/etc" -e FLUENTD_CONF=stdout.conf diamol/fluentd`.
        *   `-p 24224`: Porta padrão que Fluentd ouve.
        *   `-v ...`: Monta pasta `conf` local (com `stdout.conf`) para config do Fluentd.
        *   `-e FLUENTD_CONF`: Diz ao Fluentd qual arquivo de config usar.
    *   2. Rodar `timecheck` usando driver `fluentd`: `docker run -d --log-driver=fluentd --name timecheck5 diamol/ch19-timecheck:5.0`.
    *   3. Verificar logs `timecheck5`: `docker logs timecheck5` (deve dar erro ou vazio).
    *   4. Verificar logs `fluentd`: `docker logs --tail 1 fluentd` (deve mostrar log do timecheck).
*   Observação: `docker logs` não funciona com driver `fluentd` (logs não estão mais no host local). Precisa ver no destino (stdout do Fluentd, neste caso).
*   Figura 19.8: Mostra erro do `logs timecheck5` e log do timecheck aparecendo nos logs do `fluentd`.

🧠 Explicação detalhada e didática:

*   O exercício demonstra a configuração básica do driver `fluentd`.
    1.  Inicia-se um contêiner `fluentd`. Ele é configurado (via volume mount `-v` e variável de ambiente `-e`) para usar um arquivo `stdout.conf`. Este arquivo de configuração instrui o Fluentd a simplesmente pegar qualquer log que ele receba na porta 24224 e imprimi-lo em seu próprio stdout.
    2.  Inicia-se um contêiner `timecheck5` com a opção `--log-driver=fluentd`. O Docker Engine agora enviará os logs (stdout/stderr) deste contêiner para o endereço padrão do Fluentd (que o driver conhece, geralmente `localhost:24224` do ponto de vista do Engine).
    3.  Tentar usar `docker logs timecheck5` **não funciona** (ou retorna vazio/erro), porque o driver `json-file` não está mais sendo usado para este contêiner; os logs não estão sendo armazenados localmente associados a ele.
    4.  Em vez disso, ao verificar `docker logs fluentd`, você verá os logs do `timecheck5` aparecendo lá, porque a configuração `stdout.conf` do Fluentd os está ecoando.
*   Isso prova que o redirecionamento de logs via driver está funcionando.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch19\exercises\fluentd
    # Verifique a pasta 'conf' e o arquivo 'stdout.conf'
    ```
2.  **Rodar Fluentd (stdout config):** (Ajuste path se necessário)
    ```powershell
    $SourcePath = Join-Path -Path (Get-Location).Path -ChildPath "conf"
    # No Docker Desktop Linux mode, o path do host é montado diretamente.
    docker container run -d -p 24224:24224 --name fluentd -v "$SourcePath`:/fluentd/etc" -e FLUENTD_CONF=stdout.conf diamol/fluentd
    ```
3.  **Rodar Timecheck com Driver Fluentd:**
    ```powershell
    # Limpar anterior
    docker rm -f timecheck5
    docker container run -d --log-driver=fluentd --name timecheck5 diamol/ch19-timecheck:5.0
    ```
4.  **Verificar Logs:**
    ```powershell
    Start-Sleep -Seconds 5
    docker logs timecheck5 # Deve dar erro ou vazio
    docker logs --tail 1 fluentd # Deve mostrar um log do timecheck5
    ```

---

📄 Página 395

🔍 Resumo dos principais pontos:

*   Figura 19.8: Confirma erro em `logs timecheck5` e logs aparecendo em `logs fluentd`.
*   Fluentd adiciona metadados (container ID/name) aos logs para identificação.
*   Usar stdout no Fluentd é só para demo. Uso real: encaminhar para datastore central.
*   **Pilha EFK:** Elasticsearch + Fluentd + Kibana.
    *   **Elasticsearch:** Banco de dados NoSQL de documentos, bom para logs. Roda em container.
    *   **Kibana:** Interface web de busca e visualização para Elasticsearch. Roda em container.
*   Diagrama (Figura 19.9): Arquitetura EFK.
    *   App Containers -> (Docker Engine com driver fluentd) -> Fluentd Container -> Elasticsearch Container(s).
    *   Browser -> Kibana Container -> Elasticsearch Container(s).

🧠 Explicação detalhada e didática:

*   A página explica que o Fluentd automaticamente enriquece os logs recebidos com metadados úteis, como o ID e o nome do contêiner de origem, o que é essencial quando se centraliza logs de múltiplas fontes.
*   Introduz a **Pilha EFK** (Elasticsearch, Fluentd, Kibana) como uma solução open source muito popular para logging centralizado com Docker:
    *   **Fluentd:** Coleta os logs dos contêineres (via driver Docker) e os envia para o Elasticsearch.
    *   **Elasticsearch:** Armazena os logs de forma eficiente e permite buscas rápidas e complexas. É um banco de dados de documentos distribuído e escalável.
    *   **Kibana:** Fornece a interface web para pesquisar, visualizar e criar dashboards a partir dos logs armazenados no Elasticsearch.
*   A Figura 19.9 ilustra como esses três componentes (cada um rodando em seu(s) próprio(s) contêiner(es)) interagem: Fluentd recebe de todos, envia para Elasticsearch; Kibana lê do Elasticsearch para exibir ao usuário.

💻 Exemplos práticos (para meu ambiente Windows):

*   O próximo exercício usará um `docker-compose.yml` para iniciar toda a pilha EFK (Fluentd, Elasticsearch, Kibana) como contêineres interconectados.

💡 Analogias ou observações úteis (se houver):

*   Pilha EFK:
    *   Fluentd: O carteiro que coleta logs de todos os contêineres.
    *   Elasticsearch: O grande arquivo central/biblioteca onde o carteiro armazena todos os logs de forma organizada e indexada.
    *   Kibana: O bibliotecário com um computador super-rápido que permite a você pesquisar e encontrar qualquer log específico nesse arquivo gigantesco, e até criar gráficos sobre os tipos de logs recebidos.

---

📄 Página 396

🔍 Resumo dos principais pontos:

*   Arquitetura EFK parece complexa, mas fácil de iniciar com Docker Compose.
*   Infra de log em containers -> Usa-se driver `fluentd` para apps que querem log centralizado.
*   **Exercício Try-it-now:** Rodar stack EFK e app enviando logs.
    *   1. Limpar containers.
    *   2. Navegar para `ch19/exercises`.
    *   3. Iniciar stack EFK: `docker-compose -f fluentd/docker-compose.yml up -d`. (Define serviços fluentd, elasticsearch, kibana).
    *   4. Rodar `timecheck` com driver `fluentd`: `docker run -d --log-driver=fluentd diamol/ch19-timecheck:5.0`.
*   **Configuração Inicial Kibana:**
    *   Esperar Elasticsearch iniciar (~2 min).
    *   Acessar Kibana: `http://localhost:5601`.
    *   Clicar "Discover".
    *   Pedirá "index pattern". Digitar `fluentd*`. (Figura 19.10).

🧠 Explicação detalhada e didática:

*   A página mostra como iniciar a pilha EFK completa usando um arquivo `docker-compose.yml` fornecido (`fluentd/docker-compose.yml`). Esse arquivo define os três serviços (Fluentd, Elasticsearch, Kibana) e como eles se conectam.
*   Após iniciar a pilha EFK, qualquer contêiner iniciado com `--log-driver=fluentd` terá seus logs automaticamente coletados pelo Fluentd e enviados para o Elasticsearch. O exercício inicia o `timecheck` novamente dessa forma.
*   Como é a primeira vez que se acessa o Kibana (`http://localhost:5601`) após ele começar a receber dados do Elasticsearch, é necessário fazer uma configuração inicial mínima: dizer ao Kibana qual(is) "índice(s)" (coleções de documentos) no Elasticsearch ele deve monitorar.
    *   O Fluentd, por padrão (ou configuração comum), cria índices diários no Elasticsearch com nomes como `fluentd-YYYY.MM.DD`.
    *   Para que o Kibana encontre todos esses índices, usa-se um padrão de nome (index pattern) com curinga: `fluentd*`. Isso fará o Kibana procurar por todos os índices que começam com "fluentd".

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Limpar:**
    ```powershell
    docker container rm -f $(docker container ls -aq)
    # Opcional: docker volume prune -f
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch19\exercises
    ```
3.  **Iniciar EFK Stack:**
    ```powershell
    docker-compose -f fluentd/docker-compose.yml up -d
    ```
    *Isso pode levar algum tempo, especialmente na primeira vez, para baixar as imagens de Elasticsearch e Kibana, que são grandes.*
4.  **Verificar Stack:**
    ```powershell
    docker-compose -f fluentd/docker-compose.yml ps
    # Deve mostrar fluentd, elasticsearch, kibana Up.
    ```
5.  **Rodar App com Driver Fluentd:**
    ```powershell
    docker container run -d --log-driver=fluentd diamol/ch19-timecheck:5.0
    ```
6.  **Configurar Kibana:**
    *   Aguarde 1-2 minutos para Elasticsearch iniciar completamente.
    *   Abra o navegador em `http://localhost:5601`.
    *   Pode levar um tempo para carregar. Se pedir login/setup, procure pela opção de ir para o Discover ou Management > Index Patterns.
    *   Clique em "Discover" (ícone de bússola no menu esquerdo).
    *   Na tela "Define index pattern", digite `fluentd*` no campo "Index pattern name".
    *   Clique "Next step".

---

📄 Página 397

🔍 Resumo dos principais pontos:

*   Figura 19.10: Tela do Kibana pedindo Index Pattern (`fluentd*`).
*   **Config Kibana (cont.):**
    *   Próxima tela: escolher campo de filtro de tempo. Selecionar `@timestamp`. (Figura 19.11).
    *   Clicar "Create index pattern".
*   Setup inicial Kibana pode ser automatizado (mas bom ver manualmente).
*   Cada log do Fluentd vira documento no Elasticsearch (índice `fluentd-{date}`).
*   Kibana Discover:
    *   Mostra histograma de logs ao longo do tempo.
    *   Lista documentos (logs) individuais.
*   Figura 19.12: Tela Discover do Kibana mostrando logs do `timecheck`.

🧠 Explicação detalhada e didática:

*   A página continua a configuração inicial do Kibana:
    *   Após definir o padrão `fluentd*`, o Kibana precisa saber qual campo nos documentos contém a informação de data/hora para poder filtrar e ordenar por tempo. O Fluentd geralmente adiciona um campo chamado `@timestamp` para isso. Seleciona-se esse campo.
    *   Clica-se em "Create index pattern".
*   Com isso, o Kibana está pronto para visualizar os logs. A tela principal "Discover" mostra:
    *   Um gráfico de barras no topo (histograma) indicando quantos logs foram recebidos em diferentes intervalos de tempo.
    *   Abaixo, uma lista dos logs individuais mais recentes, com seus campos (incluindo a mensagem original do log, o `@timestamp`, e metadados adicionados pelo Fluentd/Docker como `container_name`, `container_id`).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Continuar Config Kibana:** (Após digitar `fluentd*` e clicar Next)
    *   Na lista suspensa "Time Filter field name", selecione `@timestamp`.
    *   Clique em "Create index pattern".
2.  **Explorar Logs:**
    *   Você será levado para a tela "Discover".
    *   Ajuste o intervalo de tempo no canto superior direito (ex: "Last 15 minutes") se necessário.
    *   Você deverá ver barras no histograma e, abaixo, os logs do contêiner `timecheck` que você iniciou, com campos como `log`, `container_name`, `@timestamp`.

---

📄 Página 398

🔍 Resumo dos principais pontos:

*   Figura 19.12: Confirma visualização dos logs no Kibana Discover.
*   Kibana permite busca full-text e filtros por campos/data. Tem dashboards (como Grafana) para visualizações (contagem logs/app, erros, etc.).
*   Elasticsearch é escalável para produção. EFK é mais gerenciável que logs em console.
*   **Logando App Multi-Serviço (Image Gallery):**
*   **Exercício Try-it-now:** Rodar Image Gallery com driver `fluentd` para todos os componentes.
    *   Usar `docker-compose -f image-gallery/docker-compose.yml up -d`. (Arquivo já configurado com driver e tags).
    *   Gerar tráfego (`http://localhost:8010`).
*   Config Fluentd para Image Gallery adiciona **tag** ao log (identifica componente). `log_name` no Kibana.
*   Figura 19.13: Kibana filtrado para mostrar só logs de `access-log`, usando campo `log_name`.

🧠 Explicação detalhada e didática:

*   A página destaca os recursos do Kibana além da simples visualização: busca poderosa (como Google, mas nos seus logs) e capacidade de criar dashboards. Confirma que a pilha EFK é adequada para produção.
*   O próximo exercício aplica o logging centralizado à aplicação Image Gallery (3 serviços). Um arquivo `docker-compose.yml` (`image-gallery/docker-compose.yml`) já está preparado para iniciar os 3 serviços configurados para usar o driver `fluentd`.
*   Uma configuração importante nesse arquivo (mostrada na próxima página) é o uso da opção `tag` no `log-driver`. Isso permite adicionar informações estruturadas (como nome do app, nome do serviço) à tag que o Docker envia para o Fluentd. O Fluentd pode então usar essa tag para filtrar ou enriquecer os logs.
*   Após iniciar o Image Gallery e gerar tráfego, os logs de todos os 3 serviços fluirão para o Elasticsearch. A Figura 19.13 mostra como, no Kibana, pode-se então filtrar facilmente para ver logs de apenas um componente específico (ex: `access-log`) usando o campo derivado da tag (chamado `log_name` no exemplo).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Iniciar Image Gallery (com Logging Fluentd):** (Estando em `ch19/exercises`)
    ```powershell
    # Parar/Remover Timecheck (opcional)
    # docker rm -f <id_ou_nome_timecheck>

    # Iniciar Image Gallery
    docker-compose -f image-gallery/docker-compose.yml up -d
    ```
2.  **Verificar Serviços:**
    ```powershell
    docker-compose -f image-gallery/docker-compose.yml ps
    # Deve mostrar accesslog, iotd, image-gallery Up
    ```
3.  **Gerar Tráfego:** Abra `http://localhost:8010` e atualize algumas vezes.
4.  **Explorar no Kibana:**
    *   Volte para `http://localhost:5601` (Discover).
    *   Atualize o intervalo de tempo se necessário. Você deverá ver logs dos 3 serviços intercalados.
    *   Encontre um log do `access-log`. Clique para expandir. Procure por um campo como `log_name` ou `tag` que identifique a origem como `gallery.access-log...`.
    *   Use a barra de filtro no topo para filtrar por esse campo. Ex: `log_name : "gallery.access-log*"` (sintaxe pode variar). Agora só devem aparecer logs do `access-log`.

---

📄 Página 399

🔍 Resumo dos principais pontos:

*   Campo `log_name` no Kibana veio da opção `tag` do driver Fluentd.
*   Pode usar valores fixos ou **template strings** (ex: `{{.ImageName}}`, `{{.ContainerName}}`) na tag.
*   Exemplo usa `gallery.<componente>.{{.ImageName}}`. Identifica app, componente, versão da imagem.
*   **Listagem 19.3:** Trechos do `image-gallery/docker-compose.yml` mostrando seção `logging:` para cada serviço.
    ```yaml
    services:
      accesslog:
        image: diamol/ch18-access-log # Imagem do Cap 18
        logging: # Configuração do driver de log
          driver: "fluentd" # Usa driver fluentd
          options: # Opções para o driver
            tag: "gallery.access-log.{{.ImageName}}" # Define a tag
      iotd:
        image: diamol/ch18-image-of-the-day
        logging:
          driver: "fluentd"
          options:
            tag: "gallery.iotd.{{.ImageName}}"
      image-gallery:
        image: diamol/ch18-image-gallery
        logging:
          driver: "fluentd"
          options:
            tag: "gallery.image-gallery.{{.ImageName}}"
    # ...
    ```
*   Modelo EFK é recomendado para produção. Outros drivers/ferramentas existem (Graylog, Splunk).
*   Configurar driver/opts no `daemon.json` (global) vs. no Compose (por app/ambiente). Compose dá mais clareza por ambiente.
*   Fluentd é boa opção inicial (fácil, escala, flexível - pode enriquecer/filtrar).

🧠 Explicação detalhada e didática:

*   A página explica como as tags foram definidas no Docker Compose para permitir a filtragem vista no Kibana. A seção `logging:` dentro da definição de cada serviço no `docker-compose.yml` permite configurar o driver de log e suas opções.
*   **Listagem 19.3** mostra que para cada serviço:
    *   `driver: "fluentd"` foi definido.
    *   `options: tag: "..."` foi usado para definir um formato de tag.
*   A tag usa uma combinação de texto fixo (`gallery.access-log.`) e **template strings** fornecidas pelo Docker (`{{.ImageName}}`). O Docker substitui esses templates pelos valores reais do contêiner no momento em que envia o log para o Fluentd. `{{.ImageName}}` será substituído pelo nome completo da imagem (ex: `diamol/ch18-access-log:latest`). Outros templates úteis incluem `{{.ContainerName}}`, `{{.ContainerID}}`.
*   Essa tag estruturada é então usada pelo Fluentd (com configuração apropriada, vista depois) para extrair informações e/ou rotear os logs.
*   A página reitera a recomendação da pilha EFK (ou similar) para produção e menciona a flexibilidade de configurar logging drivers globalmente (`daemon.json`) ou por aplicação (Compose), preferindo a segunda por clareza ambiental. Destaca a flexibilidade do Fluentd para processamento avançado de logs.

💻 Exemplos práticos (para meu ambiente Windows):

*   Abra o arquivo `ch19\exercises\image-gallery\docker-compose.yml` no VS Code. Localize a seção `logging:` em cada serviço e veja as tags definidas.
*   No Kibana, expanda um log e procure pelo campo `tag` (pode estar aninhado). Veja como ele corresponde ao formato definido no Compose, com `{{.ImageName}}` substituído.

---

📄 Página 400

🔍 Resumo dos principais pontos:

*   **19.4 Managing your log output and collection** (Gerenciando sua saída e coleta de logs)
*   Balancear info útil vs. volume de dados.
*   Flexibilidade Docker: App pode logar verbosamente (stdout), mas **Fluentd pode filtrar** antes de armazenar.
*   Permite ligar/desligar logs detalhados mudando config do *Fluentd* (não do app -> sem restart do app).
*   **Configuração Fluentd:** Usa `match` para rotear logs baseado na `tag`.
*   **Listagem 19.4:** Config Fluentd (`elasticsearch-filtered.conf`?) que filtra logs de `access-log`.
    ```aconf
    # Bloco para logs com tag iniciando 'gallery.access-log.'
    <match gallery.access-log.**>
      @type copy
      <store>
        @type stdout # Envia SÓ para stdout do Fluentd
      </store>
    </match>

    # Bloco para OUTROS logs com tag iniciando 'gallery.'
    <match gallery.**>
      @type copy
      <store>
        @type elasticsearch # Envia para Elasticsearch
        # ... (config elasticsearch) ...
      </store>
    </match>
    ```
*   Explicação:
    *   Primeiro `<match>` captura logs de `access-log` (tag `gallery.access-log.**`). `@type copy` + `@type stdout` os direciona para a saída padrão do Fluentd.
    *   Segundo `<match>` captura *outros* logs `gallery.**` (iotd, image-gallery). `@type copy` + `@type elasticsearch` os envia para o Elasticsearch.
    *   Ordem importa; primeiro match vence.
*   Config também enriquece logs (split da tag em campos app, service, image).

🧠 Explicação detalhada e didática:

*   Esta seção aborda o gerenciamento do **volume** de logs. Uma estratégia útil é configurar suas aplicações para logar em um nível relativamente detalhado (ex: Debug ou Information) por padrão, mas usar a camada de coleta (Fluentd) para **filtrar** os logs antes de enviá-los para o armazenamento de longo prazo (Elasticsearch).
*   **Vantagem:** Se você precisar ver os logs detalhados temporariamente para diagnosticar um problema, você pode reconfigurar e reiniciar *apenas* o Fluentd para parar de filtrar, sem precisar reiniciar os contêineres da sua aplicação (o que poderia fazer o problema desaparecer).
*   A **Listagem 19.4** mostra um trecho de um arquivo de configuração do Fluentd que implementa esse filtro:
    *   Usa diretivas `<match ...>` para direcionar logs com base em padrões na `tag`. O `**` é um curinga.
    *   O primeiro `<match gallery.access-log.**>` intercepta todos os logs cujo tag começa com `gallery.access-log.`. Dentro dele, `@type stdout` direciona esses logs para a saída padrão do próprio contêiner Fluentd (efetivamente descartando-os do armazenamento principal).
    *   O segundo `<match gallery.**>` captura todos os *outros* logs cuja tag começa com `gallery.` (ou seja, `iotd` e `image-gallery`). Dentro dele, `@type elasticsearch` os encaminha para o Elasticsearch.
*   A ordem dos blocos `<match>` é importante; o primeiro que corresponder será usado.
*   A página também menciona que esta configuração do Fluentd (provavelmente no mesmo arquivo) também adiciona lógica para "parsear" a tag e criar campos separados como `app_name`, `service_name`, `image_name` no documento final do Elasticsearch, facilitando ainda mais a filtragem e agregação no Kibana.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você usará `docker-compose` com um arquivo override para instruir o serviço `fluentd` a usar este novo arquivo de configuração com filtro.
*   Você também usará outro override para (talvez) reconfigurar o app Image Gallery para logar mais verbosamente, para ter algo a filtrar.

---

📄 Página 401

🔍 Resumo dos principais pontos:

*   **Exercício Try-it-now:** Aplicar filtro Fluentd e aumentar log do app.
    *   1. Atualizar Fluentd para usar config filtrada: `docker-compose -f fluentd/... -f fluentd/override-gallery-filtered.yml up -d`. (Override aponta para novo .conf).
    *   2. Atualizar App para logar mais: `docker-compose -f image-gallery/... -f image-gallery/override-logging.yml up -d`. (Override provavelmente seta env var para nível Debug).
*   Resultado: Logs de `access-log` não devem mais aparecer no Kibana, só nos logs do container `fluentd`. Logs de `iotd`/`image-gallery` devem aparecer no Kibana, agora enriquecidos com novos campos (app, service, image).
*   Figura 19.14: Kibana mostrando logs enriquecidos (campos extras), sem logs de `access-log`.

🧠 Explicação detalhada e didática:

*   O exercício aplica a configuração de filtragem do Fluentd e também aumenta o nível de log da aplicação Image Gallery (provavelmente definindo uma variável de ambiente via `override-logging.yml` para que ela gere logs mais detalhados).
*   O comando para Fluentd usa um arquivo override (`override-gallery-filtered.yml`) que deve instruir o serviço `fluentd` a montar ou usar um arquivo de configuração diferente (aquele com os `<match>` da Listagem 19.4). O contêiner Fluentd é recriado com a nova configuração.
*   O comando para o Image Gallery usa outro override (`override-logging.yml`) para recriar os contêineres da aplicação com uma configuração de logging mais verbosa.
*   O resultado esperado é:
    *   No Kibana: Os logs detalhados de `iotd` e `image-gallery` aparecerão, agora com campos extras como `app_name`, `service_name`, `image_name` (extraídos da tag pelo Fluentd). Os logs de `access-log` **desaparecerão** do Kibana.
    *   Nos logs do contêiner `fluentd` (`docker logs fluentd_...`): Os logs de `access-log` (que foram filtrados) **aparecerão** aqui, pois foram direcionados para `@type stdout`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar (se necessário):** `cd C:\DockerLabs\diamol\ch19\exercises`
2.  **Aplicar Overrides:**
    ```powershell
    # Atualizar Fluentd com filtro
    docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery-filtered.yml up -d
    # Atualizar App com logging verbose
    docker-compose -f image-gallery/docker-compose.yml -f image-gallery/override-logging.yml up -d
    ```
3.  **Verificar Kibana:**
    *   Gere tráfego (`http://localhost:8010`).
    *   Acesse Kibana (`http://localhost:5601`). Atualize a busca.
    *   Você deve ver logs de `iotd` e `image-gallery` (talvez mais detalhados). Expanda um log e procure pelos novos campos (`app_name`, etc.).
    *   Confirme que **não** há logs recentes de `access-log` (filtrando por `log_name: "gallery.access-log*"` não deve retornar nada novo).

---

📄 Página 402

🔍 Resumo dos principais pontos:

*   Logs de `access-log` ainda disponíveis no stdout do container Fluentd.
*   **Exercício Try-it-now:** Verificar logs `access-log` no stdout do Fluentd.
    *   Comando: `docker container logs --tail 1 fluentd_fluentd_1`.
*   Figura 19.15: Mostra log de `access-log` no output do `logs fluentd...`, confirmando que foi capturado mas redirecionado. Log ainda está enriquecido.
*   **Simulando Debug em Prod:** Remover filtro para ver logs `access-log` no Kibana novamente.
    *   Problema surgiu -> quer ver logs detalhados -> reconfigura *Fluentd* (não o app).
*   **Exercício Try-it-now:** Reverter filtro Fluentd.
    *   Comando: `docker-compose -f fluentd/... -f fluentd/override-gallery.yml up -d`. (Usa override que aponta para config *sem* filtro para `access-log`).
*   Fluentd reinicia com nova config.

🧠 Explicação detalhada e didática:

*   A página primeiro confirma (com exercício e Figura 19.15) que os logs de `access-log`, embora filtrados do Elasticsearch, ainda estão sendo recebidos pelo Fluentd e direcionados para seu stdout, provando que o app ainda está logando e o Fluentd ainda está coletando.
*   Em seguida, demonstra o benefício da filtragem na camada de coleta: simula-se um cenário onde se precisa reativar a visualização dos logs detalhados de `access-log` no Kibana (talvez para investigar um problema).
*   Em vez de reconfigurar e reiniciar os contêineres do `access-log` (o que poderia alterar o estado e dificultar a depuração), reinicia-se *apenas* o contêiner do Fluentd, fornecendo um arquivo de configuração diferente (`override-gallery.yml`) que agora instrui o Fluentd a enviar *todos* os logs `gallery.**` (incluindo `access-log`) para o Elasticsearch.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Logs Fluentd:**
    ```powershell
    docker logs --tail 1 fluentd_fluentd_1 # (Nome pode variar ligeiramente)
    # Deve mostrar um log recente do access-log
    ```
2.  **Reverter Filtro Fluentd:**
    ```powershell
    docker-compose -f fluentd/docker-compose.yml -f fluentd/override-gallery.yml up -d
    # Isso recria o container fluentd com a config original (sem filtro específico para access-log)
    ```

---

📄 Página 403

🔍 Resumo dos principais pontos:

*   Deploy do Fluentd (sem filtro) -> logs `access-log` voltam a ir para Elasticsearch.
*   Figura 19.16: Kibana agora mostrando logs recentes de `iotd` e `access-log`.
*   **Alerta:** Risco de perda de logs durante restart do Fluentd.
    *   Se containers app enviam logs enquanto Fluentd está reiniciando, logs podem ser perdidos (driver `fluentd` não faz buffer por padrão).
    *   Aceitável em cluster prod (Fluentd pode rodar com HA), mas preferível a reiniciar app (que pode perder estado do problema).
*   **19.5 Understanding the container logging model** (Entendendo o modelo de logging de contêiner)
    *   Flexível, *se* app logar para stdout/stderr (direto ou via relay).
    *   EFK stack é exemplo; muitos drivers/backends possíveis (Graylog, Splunk, CloudWatch...).
    *   EFK roda bem em dev e escala para prod.

🧠 Explicação detalhada e didática:

*   A página confirma que, após reiniciar o Fluentd com a configuração sem filtro, os logs de `access-log` voltam a aparecer no Kibana (Figura 19.16), tudo sem tocar nos contêineres da aplicação.
*   Faz um alerta importante: o driver `fluentd` do Docker, por padrão, opera de forma "não bloqueante". Se ele não conseguir enviar um log para o Fluentd (ex: porque o Fluentd está temporariamente indisponível durante um reinício), ele pode **descartar** esse log para não bloquear a aplicação. Isso significa que pode haver perda de logs durante a reconfiguração do Fluentd. Em ambientes de produção com múltiplos nós e Fluentd rodando com alta disponibilidade, esse risco é menor. Mesmo com o risco, geralmente é preferível perder alguns logs transitórios do que reiniciar a aplicação que você está tentando depurar.
*   A seção 19.5 resume o modelo de logging em contêineres:
    1.  Faça sua aplicação logar para stdout/stderr (diretamente ou via relay).
    2.  Configure o Docker Engine (globalmente ou por contêiner) para usar um **logging driver** que envie esses logs para seu sistema de logging centralizado preferido.
    3.  Use as ferramentas desse sistema (ex: Kibana para EFK) para buscar, visualizar e alertar sobre os logs.
*   A pilha EFK é destacada como uma opção popular e escalável.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Verificar Kibana Novamente:** Gere tráfego (`http://localhost:8010`). Acesse Kibana (`http://localhost:5601`). Confirme que logs de `access-log` estão aparecendo novamente.
2.  **Explorar Outros Drivers (Conceitual):** Pesquise por "docker logging drivers" na documentação oficial para ver as outras opções suportadas nativamente (syslog, journald, gelf, splunk, awslogs, gcplogs, etc.) e como configurá-las.

---

📄 Página 404

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 19.17): Arquitetura EFK em produção.
    *   Fluentd rodando como `daemonset` (ou `global service` Swarm) - 1 instância *por nó*.
    *   Cada Fluentd coleta logs dos containers *no seu nó*.
    *   Envia logs para cluster Elasticsearch (também em containers, escalado).
    *   Kibana conecta ao cluster ES.
*   **Nota de Cautela (Autor):** Alguns preferem que app logue *diretamente* no backend (ex: app envia direto para Elasticsearch API).
    *   Autor **não gosta** dessa abordagem.
    *   Prós (aparentes): Menos camadas/processamento.
    *   Contras (reais): Menos flexibilidade (acopla app ao backend de log), mais complexo para app (precisa lidar com falhas de envio), difícil mudar backend depois.
*   **Recomendação Autor:** Manter simples/flexível -> App loga para stdout, plataforma (Docker + driver + Fluentd/etc) cuida do resto.
*   **19.6 Lab** (Laboratório)

🧠 Explicação detalhada e didática:

*   A Figura 19.17 ilustra como a pilha EFK normalmente é implantada em um **ambiente de produção clusterizado (Swarm/K8s)** para escalabilidade e resiliência:
    *   O **Fluentd** não roda como um único contêiner, mas sim como um **DaemonSet (K8s)** ou **Global Service (Swarm)**. Isso garante que *cada nó* do cluster tenha sua própria instância do Fluentd rodando.
    *   Cada instância do Fluentd é configurada para coletar logs *apenas* dos contêineres que estão rodando *no mesmo nó* que ela (geralmente acessando os arquivos JSON do Docker naquele nó).
    *   Todas as instâncias do Fluentd enviam os logs coletados para um **cluster Elasticsearch centralizado** (que também roda em contêineres, possivelmente com múltiplas instâncias para escalabilidade e redundância de dados).
    *   O Kibana se conecta a esse cluster Elasticsearch.
*   A página conclui com uma **opinião forte do autor**: ele desaconselha fazer com que a aplicação envie logs *diretamente* para o sistema de armazenamento final (como chamar a API do Elasticsearch de dentro do código da aplicação). Embora possa parecer que elimina uma camada (Fluentd), na prática isso acopla fortemente a aplicação ao sistema de logging específico, dificulta mudanças futuras e adiciona complexidade/responsabilidade à aplicação (tratar falhas de envio, buffering, etc.). A recomendação é manter a aplicação simples (logar para stdout) e deixar a plataforma cuidar do transporte e processamento dos logs.
*   Prepara para o Lab do Capítulo 19.

💻 Exemplos práticos (para meu ambiente Windows):

*   Implantar Fluentd como DaemonSet/Global Service está fora do escopo do Compose single-host, mas o princípio é importante para produção.
*   O Lab focará na configuração do Fluentd em si.

---

📄 Página 405

🔍 Resumo dos principais pontos:

*   **Lab 19:** Foco em configurar Fluentd.
*   Ponto de Partida: Compose file para app Random Number (`ch19/lab/numbers.yml`?) e Compose file para EFK stack (`ch19/lab/efk.yml`?).
*   Problemas Iniciais: App não usa driver `fluentd`; config Fluentd não faz enriquecimento.
*   **Tarefa:**
    1.  Modificar Compose do app (`numbers.yml`?) para que *todos* os componentes (`numbers-api`, `numbers-web`) usem driver `fluentd` e definam `tag` estruturada (app, service, image).
    2.  Modificar config Fluentd (`elasticsearch.conf`) para:
        *   Parsear a `tag` vinda do app `numbers` e criar campos `app_name`, `service_name`, `image_name`.
        *   Adicionar bloco `<match>` "catch-all" no final para encaminhar logs de *outras* fontes (não `numbers`) para `stdout` (como fallback/debug).

🧠 Explicação detalhada e didática:

*   O **Lab do Capítulo 19** foca na configuração prática do **Fluentd** e na integração dos logs da aplicação com ele. Você receberá arquivos Compose básicos e precisará modificá-los.
*   **Passos:**
    1.  **Configurar Aplicação:** Edite o `docker-compose.yml` da aplicação Random Number (`numbers-api`, `numbers-web`). Para cada serviço, adicione a seção `logging:` especificando `driver: fluentd` e a opção `tag:` com um formato estruturado, como `"numbers.{{.ServiceName}}.{{.ImageName}}"` (ou similar).
    2.  **Configurar Fluentd (Enriquecimento):** Edite o arquivo de configuração do Fluentd (`elasticsearch.conf`, que provavelmente será montado no contêiner `fluentd` definido no Compose da EFK). Você precisará adicionar um bloco `<filter ...>` ou modificar o `<match>` existente que envia para o Elasticsearch. Dentro desse bloco, use plugins do Fluentd (como `record_transformer` ou parsers específicos) para extrair as partes da `tag` (separada por '.') e criar os novos campos (`app_name`, `service_name`, `image_name`) no registro de log antes de enviá-lo ao Elasticsearch.
    3.  **Configurar Fluentd (Fallback):** Adicione um bloco `<match **>` *no final* do arquivo de configuração do Fluentd. Este match captura *todos* os logs que não foram capturados pelos matches anteriores (como os de `gallery.**` ou `numbers.**`). Dentro deste bloco, use `@type stdout` para que logs inesperados ou de outras fontes sejam impressos nos logs do próprio Fluentd, facilitando a depuração.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 19:**
    1.  Navegue para `ch19\lab`. Examine os arquivos `.yml` e `.conf` fornecidos.
    2.  **Edite `numbers.yml`:** Adicione a seção `logging:` a ambos os serviços (`numbers-api`, `numbers-web`) com `driver: fluentd` e `options: tag: "numbers.{{.ServiceName}}.{{.ImageName}}"`.
    3.  **Edite `elasticsearch.conf`:**
        *   Localize o `<match ...>` que envia para `@type elasticsearch`.
        *   *Antes* dele (ou dentro dele, dependendo da estrutura exata), adicione um bloco `<filter numbers.**>` (para aplicar só aos logs do app numbers).
        *   Dentro do `<filter>`, use `<record_transformer>` para adicionar os novos campos baseados na tag. A sintaxe exata para extrair da tag pode exigir consulta à documentação do Fluentd ou da solução do lab. Exemplo conceitual:
          ```aconf
          <filter numbers.**>
            @type record_transformer
            enable_ruby true
            <record>
              app_name ${tag_parts[0]}
              service_name ${tag_parts[1]}
              image_name ${tag_parts[2]}
            </record>
          </filter>
          ```
        *   *Depois* de todos os matches específicos (como o para Elasticsearch), adicione o fallback:
          ```aconf
          <match **>
            @type stdout
          </match>
          ```
    4.  **Rodar:** Use `docker-compose -f efk.yml -f numbers.yml up -d`.
    5.  **Verificar:** Gere logs do app Numbers. Verifique no Kibana se os logs aparecem e se os novos campos (`app_name`, etc.) estão presentes. Tente rodar outro app (ex: `timecheck`) com driver `fluentd` mas tag *diferente*; seus logs devem aparecer nos logs do `fluentd` container (devido ao match `**`), não no Kibana.

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md

---

📄 Página 406

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 19 no GitHub.

🧠 Explicação detalhada e didática:

*   Fornece o link para a solução completa do Lab 19 no repositório do livro.

💻 Exemplos práticos (para meu ambiente Windows):

*   Se tiver dificuldades com o Lab 19, consulte o link: https://github.com/sixeyed/diamol/blob/master/ch19/lab/README.md

---

📄 Página 407

🔍 Resumo dos principais pontos:

*   **Capítulo 20: Controlling HTTP traffic to containers with a reverse proxy** (Controlando tráfego HTTP para contêineres com um proxy reverso)
*   Docker roteia tráfego externo (via Ingress/Publish), mas só 1 container por porta host.
*   Problema: Em produção, quer-se múltiplos apps (sites, APIs) acessíveis nas portas padrão HTTP (80) e HTTPS (443).
*   Solução: **Reverse Proxy**.
*   Reverse Proxy: Componente de rede que atua como **gateway** para múltiplas aplicações web/HTTP.
*   Foco Cap. 20: Features e padrões habilitados por reverse proxy. Exemplos com Nginx e Traefik (populares, rodam em containers).
*   **20.1 What is a reverse proxy?** (O que é um proxy reverso?)
    *   Proxy "normal" (forward proxy): Intercepta tráfego *saindo* da sua rede (cliente -> proxy -> internet). Usado para controle, cache, segurança na ponta do cliente.
    *   **Reverse Proxy:** Intercepta tráfego *entrando* para seus servidores (internet -> reverse proxy -> app server). Fica na frente dos servidores de aplicação.
*   Diagrama (Figura 20.1): Ilustra Reverse Proxy em container.
    *   Tráfego externo chega SÓ no container do Reverse Proxy (ex: na porta 80/443).
    *   Reverse Proxy decide para qual container de aplicação (web1, web2, api1) encaminhar a requisição (baseado em hostname, path, etc.).
    *   Containers de aplicação ficam "internos", sem portas publicadas diretamente no host.

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 20, focado no **Reverse Proxy**, um componente essencial em arquiteturas de contêineres (e web em geral).
*   O problema que ele resolve é: como expor **múltiplas aplicações web ou APIs** que rodam em contêineres diferentes, mas que precisam ser acessadas através das portas padrão **80 (HTTP)** e **443 (HTTPS)**, sendo que o Docker só permite que um contêiner escute em uma porta específica do host por vez?
*   A solução é o **Reverse Proxy**:
    *   É um servidor especializado (como Nginx, Traefik, HAProxy, Caddy) que roda em um contêiner.
    *   *Apenas* o contêiner do reverse proxy tem as portas 80 e 443 publicadas no host (`-p 80:80 -p 443:443`).
    *   Todo o tráfego HTTP/S externo chega primeiro a ele.
    *   O reverse proxy examina a requisição (geralmente o nome do host/domínio ou o caminho URL) e, com base em regras de configuração, decide para qual contêiner de aplicação *interno* ele deve encaminhar ("proxy") essa requisição.
    *   Os contêineres das aplicações rodam *sem* publicar portas no host, comunicando-se com o proxy através da rede interna do Docker.
*   A Figura 20.1 ilustra essa arquitetura de gateway.
*   O capítulo usará **Nginx** (um clássico poderoso) e **Traefik** (um proxy moderno "cloud-native") como exemplos.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você rodará Nginx e Traefik em contêineres.
*   Você configurará regras de roteamento (via arquivos de config para Nginx, ou via labels Docker para Traefik).
*   Você rodará contêineres de aplicação *sem* a flag `-p`, e os acessará *através* do proxy na porta 80/443 usando nomes de host (que você configurará no seu arquivo `hosts` local para testes).

💡 Analogias ou observações úteis (se houver):

*   Reverse Proxy: É como o recepcionista/telefonista de um grande prédio comercial. Todos os visitantes/ligações (tráfego externo) chegam primeiro na recepção (proxy na porta 80/443). O recepcionista pergunta para qual empresa/departamento (hostname/path) o visitante/ligação se destina e o direciona para o andar/ramal correto (contêiner da aplicação). Os escritórios das empresas (contêineres de app) não têm acesso direto da rua (sem portas publicadas no host).

---

📄 Página 408

🔍 Resumo dos principais pontos:

*   Figura 20.1: Diagrama do Reverse Proxy.
*   Benefícios do Reverse Proxy:
    *   Apps ficam internos -> mais fácil escalar, atualizar, segurar.
*   Reverse proxies não são novos, mas "shift left" com containers (rodam em dev/teste também, não só prod).
*   Exemplo com **Nginx**:
    *   Popular (>30% internet), leve, rápido, poderoso servidor HTTP (serve estáticos + proxy).
*   **Exercício Try-it-now:** Rodar Nginx e ver página default.
    *   1. Criar rede `ch20` (Linux/Windows).
    *   2. Navegar para `ch20/exercises`.
    *   3. Rodar Nginx com Compose (usa override linux/windows para montar pasta `sites-enabled` local): `docker-compose -f nginx/... -f nginx/override-... up -d`.
    *   4. Acessar `http://localhost`.
*   Nginx usa arquivos de config por site. Pasta `sites-enabled` montada está vazia -> Nginx serve página default.
*   Figura 20.2: Mostra página "Welcome to nginx!".

🧠 Explicação detalhada e didática:

*   A página reforça os benefícios de colocar as aplicações atrás de um reverse proxy (melhor gerenciamento, segurança). Menciona que, com contêineres, é fácil usar o mesmo proxy em todos os ambientes (dev, teste, prod), o que não acontecia antigamente ("shift left").
*   Começa o exemplo prático usando **Nginx**.
*   O exercício instrui a:
    1.  Criar uma rede Docker (`ch20`) para os contêineres deste capítulo.
    2.  Usar `docker-compose` para iniciar um contêiner Nginx. O Compose file (`nginx/docker-compose.yml` com override) faz duas coisas importantes:
        *   Publica a porta 80 do contêiner Nginx na porta 80 do host (`-p 80:80`).
        *   Usa um **bind mount** para mapear uma pasta local chamada `nginx/sites-enabled` para o diretório `/etc/nginx/conf.d` (ou similar) dentro do contêiner Nginx. É neste diretório que o Nginx procura por arquivos de configuração (`.conf`) que definem os sites/proxies que ele deve servir.
    3.  Acessar `http://localhost`. Como a pasta `sites-enabled` local está vazia no início, o Nginx não encontra nenhuma configuração customizada e serve sua página de boas-vindas padrão.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Rede:**
    ```powershell
    # Se usando Linux mode no Desktop
    docker network create ch20
    # Se usando Windows Containers mode
    # docker network create --driver=nat ch20
    ```
2.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch20\exercises
    ```
3.  **Rodar Nginx:** (Usando override apropriado)
    ```powershell
    # Se Linux mode
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml up -d
    # Se Windows Containers mode (verifique se override-windows.yml existe)
    # docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-windows.yml up -d
    ```
4.  **Verificar Nginx:**
    ```powershell
    docker ps # Ver container nginx rodando
    ```
5.  **Acessar Página Default:** Abra navegador em `http://localhost`. Você deve ver a página "Welcome to nginx!".

*   **Estrutura Nginx Config:** Nginx geralmente tem pastas como `sites-available` (onde você cria todas as configs) e `sites-enabled` (onde você cria links simbólicos para as configs de `sites-available` que quer ativar). O bind mount direto para `sites-enabled` simplifica o exemplo.

---

📄 Página 409

🔍 Resumo dos principais pontos:

*   Figura 20.2: Página default Nginx.
*   Configurar Nginx como proxy: Adicionar arquivo `.conf` na pasta `sites-enabled`.
*   **Roteamento por Hostname:** Para hospedar múltiplos apps na mesma porta (80), Nginx usa header `Host:` da requisição HTTP para escolher qual config/app servir.
*   Teste Local: Adicionar domínios ao arquivo `hosts` local para simular diferentes hostnames apontando para `127.0.0.1`.
*   **Exercício Try-it-now:** Configurar proxy para app `whoami`.
    *   1. Adicionar `whoami.local` ao `hosts` (apontando para `127.0.0.1`). Comandos Linux/Win fornecidos.
    *   2. Iniciar container `whoami` (Compose file em `whoami/`) *sem* publicar portas, mas na rede `ch20`.
        *   `docker-compose -f whoami/docker-compose.yml up -d`.
    *   3. Copiar config Nginx `whoami.local.conf` da pasta `sites-available` para `sites-enabled` (que está montada no container Nginx).
        *   `cp ./nginx/sites-available/whoami.local ./nginx/sites-enabled/`
    *   4. Reiniciar Nginx para carregar nova config: `docker-compose -f nginx/... restart nginx`.
    *   5. Acessar `http://whoami.local` no navegador.

🧠 Explicação detalhada e didática:

*   A página explica como configurar o Nginx para atuar como um reverse proxy para uma aplicação específica. Isso é feito criando um arquivo de configuração (`.conf`) para essa aplicação e colocando-o no diretório que o Nginx monitora (`sites-enabled`, que mapeamos para uma pasta local).
*   Como queremos que o Nginx sirva múltiplas aplicações na mesma porta 80, precisamos de uma forma de diferenciar as requisições. O método padrão é usar o **nome do host** (domínio) que o cliente (navegador) solicita. O navegador envia esse nome no cabeçalho HTTP `Host:`. O Nginx lê esse cabeçalho e o usa para encontrar o arquivo de configuração correspondente (que terá uma diretiva `server_name` com aquele host).
*   Para **testar isso localmente**, usamos o truque do arquivo `hosts` novamente (como na p.93), adicionando entradas como `127.0.0.1 whoami.local`. Isso faz com que, ao digitar `http://whoami.local` no navegador, a requisição seja enviada para `127.0.0.1` (sua máquina local), mas com o header `Host: whoami.local`.
*   O exercício então executa os passos para configurar o Nginx para servir o app `whoami` (um app simples que mostra info do container):
    1.  Modifica o `hosts` local.
    2.  Inicia o container `whoami` **sem publicar portas**, mas conectando-o à rede `ch20` (para que o Nginx possa encontrá-lo pelo nome `whoami` via DNS Docker).
    3.  Copia o arquivo `whoami.local.conf` (que contém as regras de proxy para `whoami.local`) para a pasta `sites-enabled` local. Como essa pasta está montada no Nginx via bind mount, o Nginx agora "vê" o novo arquivo de configuração.
    4.  Reinicia o serviço `nginx` (usando `docker-compose restart`) para que ele releia a configuração e ative o novo site.
    5.  Acessa `http://whoami.local`. O Nginx deve receber, rotear para o container `whoami`, obter a resposta e enviá-la de volta.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 whoami.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Iniciar App `whoami`:** (Estando em `ch20/exercises`)
    ```powershell
    # Verifique o whoami/docker-compose.yml. Ele deve definir o serviço 'whoami' e conectá-lo à rede 'ch20' (que deve ser definida como externa ou criada neste file).
    docker-compose -f .\whoami\docker-compose.yml up -d
    ```
3.  **Copiar Config Nginx:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\whoami.local -Destination .\nginx\sites-enabled\
    ```
4.  **Reiniciar Nginx:** (Use o comando que iniciou o Nginx)
    ```powershell
    # Se Linux mode
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx
    # Se Windows Containers mode
    # docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-windows.yml restart nginx
    ```
5.  **Acessar no Navegador:** Abra `http://whoami.local`. Você deve ver a saída do aplicativo whoami (hostname do container, IPs, etc.).

---

📄 Página 410

🔍 Resumo dos principais pontos:

*   Figura 20.3: Mostra a saída do app `whoami` sendo servida através do proxy em `whoami.local`.
*   Config Nginx para proxy é simples. **Listagem 20.1:** Config `whoami.local.conf`.
    ```nginx
    server {
        server_name whoami.local; # Responde a este hostname
        location / { # Para qualquer path sob o domínio
            # Encaminha para o serviço 'whoami' na rede Docker (porta 80 http)
            proxy_pass http://whoami;
            # Passa o Host header original do cliente para o app backend
            proxy_set_header Host $host;
            # Adiciona header extra na resposta para debug (mostra qual Nginx serviu)
            add_header X-Host $hostname;
        }
    }
    ```
*   **Explicação Config:**
    *   `server { ... }`: Define um servidor virtual.
    *   `server_name whoami.local;`: Faz este bloco responder a requisições com `Host: whoami.local`.
    *   `location / { ... }`: Aplica regras a todas as requisições (`/`).
    *   `proxy_pass http://whoami;`: A diretiva principal. Diz ao Nginx para encaminhar a requisição para o endereço `http://whoami`. O Nginx usa o DNS Docker para resolver `whoami` para o IP do container `whoami` na rede `ch20` e faz uma requisição HTTP para ele.
    *   `proxy_set_header Host $host;`: Reenvia o cabeçalho `Host` original do cliente para a aplicação backend (`whoami`). Importante para apps que dependem desse header. `$host` é variável Nginx.
    *   `add_header X-Host $hostname;`: Adiciona um cabeçalho customizado (`X-Host`) à *resposta* que volta para o cliente, contendo o hostname do *contêiner Nginx* que processou a requisição. Útil para debug.
*   Config simples atua como "pass-through": só encaminha request/response. Se app backend falhar, Nginx repassa falha.
*   **Exercício Try-it-now:** Proxy para API `numbers` (que falha).
    *   1. Adicionar `api.numbers.local` ao `hosts`.
    *   2. Rodar API `numbers` (Compose file em `numbers/`).
    *   3. Copiar config Nginx `api.numbers.local.conf`.
    *   4. Reiniciar Nginx.
    *   5. Acessar `http://api.numbers.local/rng`. Refrescar até quebrar (ver erro 500 do Nginx, vindo da API).

🧠 Explicação detalhada e didática:

*   A página analisa a configuração básica do Nginx para atuar como reverse proxy (Listagem 20.1). A diretiva chave é `proxy_pass`, que informa ao Nginx para onde encaminhar a requisição recebida. Ele usa o nome do serviço Docker (`whoami`) como hostname, confiando no DNS interno do Docker para resolução. `proxy_set_header Host` também é importante para que a aplicação backend receba o nome de host que o usuário originalmente digitou.
*   Explica que essa configuração básica apenas repassa a comunicação. Se a aplicação por trás do proxy (`whoami` ou `numbers-api`) retornar um erro (como o erro 500 da API `numbers` após algumas chamadas), o Nginx simplesmente enviará essa resposta de erro de volta ao cliente.
*   O exercício aplica a mesma técnica de proxy para a API `numbers-api`, tornando-a acessível via `http://api.numbers.local/rng`. Ao testar, espera-se ver o Nginx retornando o erro 500 após algumas atualizações, demonstrando o comportamento pass-through.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 api.numbers.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Rodar API `numbers`:** (Estando em `ch20/exercises`)
    ```powershell


**You**

🧠 Prompt para o Agent do AI Drive:

Leia completamente este PDF sobre Docker (Learn Docker in a Month of Lunches), página por página, a apartir da página 410.

Para cada página, siga este modelo de resposta:

📄 Página X

🔍 Resumo dos principais pontos:

\[bullet points com os tópicos abordados na página\]

🧠 Explicação detalhada e didática:

\[explique os conceitos em português claro, com boa didática, como um professor\]

💻 Exemplos práticos (para meu ambiente Windows):

\[comandos ou trechos de código aplicáveis no Windows, usando Docker Desktop e Visual Studio Code\]

💡 Analogias ou observações úteis (se houver):

\[comparações simples ou observações para reforçar o entendimento\]

✅ Importante: percorra automaticamente todas as páginas do PDF, aplicando esse modelo para cada uma, sem que eu precise digitar "próxima".

Quando terminar todas as páginas, diga:

"✅ Fim da leitura. Todas as páginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code."



**AI PDF**

Okay, Alexandre, continuando a análise a partir da página 410.

---

📄 Página 410

🔍 Resumo dos principais pontos:

*   Figura 20.3: Mostra a saída do app `whoami` sendo servida através do proxy em `whoami.local`.
*   Config Nginx para proxy é simples. **Listagem 20.1:** Config `whoami.local.conf`.
    ```nginx
    server {
        server_name whoami.local; # Responde a este hostname
        location / { # Para qualquer path sob o domínio
            # Encaminha para o serviço 'whoami' na rede Docker (porta 80 http)
            proxy_pass http://whoami;
            # Passa o Host header original do cliente para o app backend
            proxy_set_header Host $host;
            # Adiciona header extra na resposta para debug (mostra qual Nginx serviu)
            add_header X-Host $hostname;
        }
    }
    ```
*   **Explicação Config:**
    *   `server { ... }`: Define um servidor virtual.
    *   `server_name whoami.local;`: Faz este bloco responder a requisições com `Host: whoami.local`.
    *   `location / { ... }`: Aplica regras a todas as requisições (`/`).
    *   `proxy_pass http://whoami;`: A diretiva principal. Diz ao Nginx para encaminhar a requisição para o endereço `http://whoami`. O Nginx usa o DNS Docker para resolver `whoami` para o IP do container `whoami` na rede `ch20` e faz uma requisição HTTP para ele.
    *   `proxy_set_header Host $host;`: Reenvia o cabeçalho `Host` original do cliente para a aplicação backend (`whoami`). Importante para apps que dependem desse header. `$host` é variável Nginx.
    *   `add_header X-Host $hostname;`: Adiciona um cabeçalho customizado (`X-Host`) à *resposta* que volta para o cliente, contendo o hostname do *contêiner Nginx* que processou a requisição. Útil para debug.
*   Config simples atua como "pass-through": só encaminha request/response. Se app backend falhar, Nginx repassa falha.
*   **Exercício Try-it-now:** Proxy para API `numbers` (que falha).
    *   1. Adicionar `api.numbers.local` ao `hosts`.
    *   2. Rodar API `numbers` (Compose file em `numbers/`).
    *   3. Copiar config Nginx `api.numbers.local.conf`.
    *   4. Reiniciar Nginx.
    *   5. Acessar `http://api.numbers.local/rng`. Refrescar até quebrar (ver erro 500 do Nginx, vindo da API).

🧠 Explicação detalhada e didática:

*   A página analisa a configuração básica do Nginx para atuar como reverse proxy (Listagem 20.1). A diretiva chave é `proxy_pass`, que informa ao Nginx para onde encaminhar a requisição recebida. Ele usa o nome do serviço Docker (`whoami`) como hostname, confiando no DNS interno do Docker para resolução. `proxy_set_header Host` também é importante para que a aplicação backend receba o nome de host que o usuário originalmente digitou.
*   Explica que essa configuração básica apenas repassa a comunicação. Se a aplicação por trás do proxy (`whoami` ou `numbers-api`) retornar um erro (como o erro 500 da API `numbers` após algumas chamadas), o Nginx simplesmente enviará essa resposta de erro de volta ao cliente.
*   O exercício aplica a mesma técnica de proxy para a API `numbers-api`, tornando-a acessível via `http://api.numbers.local/rng`. Ao testar, espera-se ver o Nginx retornando o erro 500 após algumas atualizações, demonstrando o comportamento pass-through.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 api.numbers.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Rodar API `numbers`:** (Estando em `ch20/exercises`)
    ```powershell
    # Assume que numbers/docker-compose.yml define serviço 'numbers-api' e rede 'ch20'
    docker-compose -f .\numbers\docker-compose.yml up -d
    ```
3.  **Copiar Config Nginx:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\api.numbers.local -Destination .\nginx\sites-enabled\
    ```
4.  **Reiniciar Nginx:** (Use o comando que iniciou o Nginx)
    ```powershell
    # Se Linux mode
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx
    # Se Windows Containers mode
    # docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-windows.yml restart nginx
    ```
5.  **Acessar API via Proxy:** Abra navegador em `http://api.numbers.local/rng`. Atualize (F5) 4 ou 5 vezes. Você deve ver o número aleatório nas primeiras vezes e depois uma página de erro (provavelmente Erro 500 ou 502 do Nginx, refletindo a falha da API `numbers-api`).

---

📄 Página 411

🔍 Resumo dos principais pontos:

*   Figura 20.4: Mostra erro 500 servido pelo Nginx vindo da API `numbers`.
*   Reverse proxies oferecem mais que roteamento (ponto central de config).
*   **20.2 Handling routing and SSL in the reverse proxy** (Lidando com roteamento e SSL no proxy reverso)
*   Processo de adicionar apps no Nginx: Start app -> Copy config -> Restart Nginx.
    *   Ordem importa: Nginx checa se upstreams (apps backend) estão acessíveis na inicialização/reload. Se não, Nginx falha ao iniciar.
*   Nginx cria lista interna de roteamento (host -> IP).
*   **Load Balancing:** Se DNS Docker retorna múltiplos IPs para upstream (serviço escalado), Nginx automaticamente faz balanceamento de carga (round-robin por padrão) entre eles.
*   **Exercício Try-it-now:** Proxy para app `image-gallery` web escalado.
    *   1. Adicionar `image-gallery.local` ao `hosts`.
    *   2. Rodar app Image Gallery (Compose file em `image-gallery/`) com escala de 3 para serviço `image-gallery` (`--scale image-gallery=3`).
    *   3. Copiar config Nginx `image-gallery.local.conf`.
    *   4. Reiniciar Nginx.
    *   5. Testar com `curl -i --head http://image-gallery.local` múltiplas vezes.

🧠 Explicação detalhada e didática:

*   A página confirma o resultado do exercício anterior (Nginx repassando erro).
*   A seção 20.2 começa a explorar funcionalidades mais avançadas do reverse proxy, começando pelo **balanceamento de carga (Load Balancing)**.
*   Menciona uma característica do Nginx: ao iniciar ou recarregar configurações, ele tenta resolver os nomes dos servidores `upstream` (definidos em `proxy_pass`). Se um nome não puder ser resolvido (porque o contêiner ainda não iniciou, por exemplo), o Nginx pode falhar ao iniciar. Por isso, a ordem "iniciar app, depois configurar/reiniciar proxy" é geralmente mais segura.
*   A funcionalidade chave é que se o nome do `upstream` (ex: `http://image-gallery` no `proxy_pass`) for resolvido pelo DNS Docker para *múltiplos* endereços IP (porque o serviço `image-gallery` foi escalado com `docker-compose --scale`), o Nginx automaticamente distribuirá as requisições entre esses IPs/contêineres, realizando o balanceamento de carga (por padrão, usando um método round-robin simples).
*   O exercício configura isso:
    1.  Adiciona o domínio `image-gallery.local` ao `hosts`.
    2.  Inicia a aplicação Image Gallery, mas desta vez escalando o serviço web (`image-gallery`) para 3 réplicas usando `--scale`.
    3.  Copia a configuração Nginx para este site.
    4.  Reinicia o Nginx.
    5.  Usa `curl -i --head ...` (que mostra os cabeçalhos da resposta) repetidamente para observar o balanceamento.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Adicionar ao Hosts:** (PowerShell como Admin)
    ```powershell
    Add-Content -Value "127.0.0.1 image-gallery.local" -Path C:\Windows\System32\drivers\etc\hosts
    ```
2.  **Rodar Image Gallery (Escalado):** (Estando em `ch20/exercises`)
    ```powershell
    # Assume que image-gallery/docker-compose.yml define serviços e rede ch20
    docker-compose -f .\image-gallery\docker-compose.yml up -d --scale image-gallery=3
    # Verifique com 'docker-compose -f ... ps'. Deve ver 3 containers image-gallery.
    ```
3.  **Copiar Config Nginx:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery.local -Destination .\nginx\sites-enabled\
    ```
4.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
5.  **Testar Load Balancing:** Execute várias vezes:
    ```powershell
    curl -i --head http://image-gallery.local
    # Procure pelo header 'X-Host:' (do Nginx) e 'Hostname:' (do app whoami, se estiver lá)
    # Ou use Invoke-WebRequest -UseBasicParsing -HeadersOnly http://image-gallery.local
    # A próxima página explicará como ver o IP do backend.
    ```

---

📄 Página 412

🔍 Resumo dos principais pontos:

*   Config Nginx para `image-gallery` (em `image-gallery.local.conf`) é similar, mas adiciona header `X-Upstream` na resposta mostrando IP do container backend que serviu.
*   Figura 20.5: Mostra saída do `curl` com header `X-Upstream` tendo IPs diferentes (ex: 172.20...) em chamadas repetidas, provando load balancing.
*   Benefício: Rodar/testar app com load balancing em máquina única (dev/teste) sem Swarm/K8s. Sem mudanças no código/config do app.
*   **Roteamento por Path:** Proxy pode rotear para diferentes backends baseado no path da URL (além do hostname).
*   Permite expor partes de um app (ex: API) sob o mesmo domínio da UI.
*   **Exercício Try-it-now:** Expor API `iotd` como `/api/image` sob `image-gallery.local`.
    *   1. Remover config Nginx anterior (`image-gallery.local`).
    *   2. Copiar nova config (`image-gallery-2.local`) que inclui regra para `/api/image`.
    *   3. Reiniciar Nginx.
    *   4. Testar API via proxy: `curl -i http://image-gallery.local/api/image`.

🧠 Explicação detalhada e didática:

*   A página explica que a configuração Nginx usada no exercício anterior (`image-gallery.local.conf`) foi ligeiramente modificada para adicionar um cabeçalho de resposta `X-Upstream`. Este cabeçalho é preenchido pelo Nginx com o endereço IP do contêiner backend (`upstream`) que ele escolheu para servir aquela requisição específica. Ao fazer chamadas repetidas com `curl --head`, observar a variação no valor de `X-Upstream` confirma que o Nginx está distribuindo a carga entre as diferentes réplicas do `image-gallery`.
*   Isso demonstra a capacidade do Nginx de fornecer balanceamento de carga localmente.
*   Em seguida, introduz o **roteamento baseado em Path**. Além de escolher o backend com base no `Host:` (ex: `image-gallery.local`), o Nginx pode usar diretivas `location` mais específicas para encaminhar requisições baseadas no caminho da URL (ex: `/api/...` vai para o container da API, enquanto `/` vai para o container da UI).
*   O exercício demonstra isso:
    1.  Remove-se a configuração Nginx anterior.
    2.  Copia-se uma nova configuração (`image-gallery-2.local`) que provavelmente contém:
        *   Um `location / { proxy_pass http://image-gallery; ... }` (para a UI).
        *   Um `location /api/image { proxy_pass http://iotd/image; ... }` (para a API).
    3.  Reinicia-se o Nginx.
    4.  Testa-se o acesso ao path da API (`/api/image`) através do domínio principal (`image-gallery.local`). O Nginx deve rotear essa requisição específica para o contêiner `iotd`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Remover Config Antiga:** (Estando em `ch20/exercises`)
    ```powershell
    Remove-Item -Path .\nginx\sites-enabled\image-gallery.local
    ```
2.  **Copiar Nova Config:**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery-2.local -Destination .\nginx\sites-enabled\image-gallery.local
    # Renomeia para .local no destino para sobrescrever/ativar com nome esperado
    ```
3.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
4.  **Testar Rota API:**
    ```powershell
    curl -i http://image-gallery.local/api/image
    # Você deve obter a resposta JSON da API 'iotd'.
    # Teste também http://image-gallery.local (deve servir a UI de 'image-gallery').
    ```

---

📄 Página 413

🔍 Resumo dos principais pontos:

*   Figura 20.6: Mostra resposta JSON da API vindo de `image-gallery.local/api/image`.
*   Padrão útil para expor partes do app, montar UI de vários componentes sob mesmo domínio.
*   **SSL Termination:** Outra função do proxy: lidar com HTTPS.
    *   Melhor centralizar no proxy do que em cada app.
    *   Nginx pode usar certs reais (Let's Encrypt) ou auto-assinados (não-prod).
*   **Exercício Try-it-now:** Configurar HTTPS para `image-gallery.local` usando certs auto-assinados.
    *   1. Gerar certs auto-assinados usando container `diamol/cert-generator`.
        *   Comando monta pasta local `nginx/certs` em `/certs` no container. `-e HOST_NAME=image-gallery.local` diz para qual domínio gerar.
        *   `docker run -v "$(pwd)/nginx/certs:/certs" ... diamol/cert-generator` (Linux).
        *   `docker run -v "$(pwd)/nginx/certs:C:\certs" ... diamol/cert-generator` (Windows).
    *   2. Remover config Nginx anterior.
    *   3. Copiar nova config (`image-gallery-3.local`) que usa SSL/certs.
    *   4. Reiniciar Nginx.
    *   5. Acessar `http://image-gallery.local` (deve redirecionar para HTTPS). Navegador dará aviso de segurança.

🧠 Explicação detalhada e didática:

*   A página confirma o sucesso do roteamento por path (Figura 20.6) e introduz a próxima função importante do reverse proxy: **Terminação SSL/TLS**.
*   Em vez de configurar cada um dos seus contêineres de aplicação para lidar com HTTPS (o que exigiria gerenciar certificados e chaves privadas em cada um), você configura o **reverse proxy** para:
    1.  Ouvir na porta 443 (HTTPS).
    2.  Possuir o certificado SSL e a chave privada para o domínio (ex: `image-gallery.local`).
    3.  Descriptografar a requisição HTTPS recebida do cliente.
    4.  Encaminhar a requisição (agora como HTTP simples) para o contêiner da aplicação backend apropriado via rede interna Docker.
    5.  Receber a resposta HTTP do backend.
    6.  Criptografar a resposta e enviá-la de volta ao cliente via HTTPS.
*   Isso centraliza todo o gerenciamento de SSL no proxy. Para ambientes de desenvolvimento/teste, podem-se usar certificados **auto-assinados** (gerados localmente, não confiáveis por navegadores). Para produção, usam-se certificados reais de uma Autoridade Certificadora (CA) ou via Let's Encrypt.
*   O exercício demonstra o setup com certificados auto-assinados:
    1.  Usa um contêiner utilitário (`diamol/cert-generator`) que contém ferramentas (como OpenSSL) para gerar um par certificado/chave para o domínio `image-gallery.local` e salvá-los na pasta local `nginx/certs` (que também está montada no contêiner Nginx).
    2.  Substitui a configuração do Nginx por uma (`image-gallery-3.local`) que habilita SSL, aponta para os arquivos de certificado/chave gerados e provavelmente redireciona HTTP para HTTPS.
    3.  Reinicia o Nginx.
    4.  Ao acessar o site via HTTP, espera-se um redirecionamento para HTTPS, onde o navegador exibirá um aviso de segurança porque o certificado auto-assinado não é reconhecido por uma CA pública.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Gerar Certificados:** (Estando em `ch20/exercises`)
    ```powershell
    $CertPath = Join-Path -Path (Get-Location).Path -ChildPath "nginx\certs"
    mkdir $CertPath -ErrorAction SilentlyContinue
    # Se imagem base do generator for Linux (mais provável)
    docker container run --rm -v "$CertPath`:/certs" -e HOST_NAME=image-gallery.local diamol/cert-generator
    # Se imagem for Windows
    # docker container run --rm -v "$CertPath`:C:\certs" -e HOST_NAME=image-gallery.local diamol/cert-generator
    # Verifique se arquivos .pem ou .crt/.key apareceram em nginx\certs
    ```
2.  **Remover Config Antiga:**
    ```powershell
    Remove-Item -Path .\nginx\sites-enabled\image-gallery.local
    ```
3.  **Copiar Nova Config (SSL):**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery-3.local -Destination .\nginx\sites-enabled\image-gallery.local
    ```
4.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
5.  **Acessar no Navegador:**
    *   Vá para `http://image-gallery.local`. Deve redirecionar para `https://image-gallery.local`.
    *   Você verá um aviso de segurança (ex: "Sua conexão não é privada", "Risco de segurança em potencial"). Clique em "Avançado" ou "Detalhes" e procure uma opção para "Continuar para image-gallery.local (não seguro)" ou "Aceitar o risco e continuar".
    *   A aplicação Image Gallery deve carregar via HTTPS.

---

📄 Página 414

🔍 Resumo dos principais pontos:

*   Explica o passo de geração de certificado (contêiner `cert-generator` usa OpenSSL, salva em pasta local montada no Nginx).
*   Figura 20.7: Mostra aviso de segurança do navegador (Firefox) para certificado auto-assinado.
*   Nginx permite config detalhada de SSL (protocolos, ciphers, etc.; ver SSL Labs).
*   **Listagem 20.2:** Config Nginx para HTTPS + Redirect HTTP.
    ```nginx
    server { # Bloco HTTP
        server_name image-gallery.local;
        listen 80;
        return 301 https://$server_name$request_uri; # Redireciona para HTTPS
    }
    server { # Bloco HTTPS
        server_name image-gallery.local;
        listen 443 ssl; # Ouve na porta 443 com SSL
        # Caminhos para cert/key DENTRO do container Nginx
        ssl_certificate /etc/nginx/certs/server-cert.pem;
        ssl_certificate_key /etc/nginx/certs/server-key.pem;
        ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Exemplo de config SSL
        # ... (bloco location com proxy_pass para http://image-gallery) ...
    }
    ```
*   Explica config: Bloco porta 80 redireciona; bloco 443 habilita `ssl`, especifica arquivos de cert/key (no path *interno* do container Nginx, onde foram montados), define protocolos.
*   Certs são por domínio; precisa de par cert/key para cada app (ou cert wildcard/multi-domínio).
*   Em produção: usar Secrets Swarm/K8s para armazenar certs/keys confidenciais.
*   Benefício: Tira complexidade SSL dos apps; devs podem testar com HTTP simples.
*   Próxima feature Nginx: **Caching**.

🧠 Explicação detalhada e didática:

*   A página detalha a configuração do Nginx para habilitar HTTPS (Listagem 20.2). A configuração é dividida em dois blocos `server`:
    1.  **Bloco HTTP (porta 80):** Configurado apenas para capturar requisições HTTP e imediatamente retornar uma resposta de redirecionamento permanente (HTTP 301) para a versão HTTPS do mesmo URL (`https://$server_name$request_uri`).
    2.  **Bloco HTTPS (porta 443):**
        *   `listen 443 ssl;`: Habilita SSL/TLS nesta porta.
        *   `ssl_certificate ...;` e `ssl_certificate_key ...;`: Especificam os caminhos, *dentro do contêiner Nginx*, onde encontrar o arquivo do certificado público e o arquivo da chave privada correspondente (esses arquivos foram colocados lá pelo bind mount da pasta `certs`).
        *   `ssl_protocols ...;`: Exemplo de configuração para definir quais versões do protocolo TLS permitir.
        *   (Oculto na listagem, mas necessário): Este bloco também conteria a(s) diretiva(s) `location ... { proxy_pass ...; }` para encaminhar as requisições (agora descriptografadas) para o serviço backend (`image-gallery`).
*   Reafirma que certificados são por domínio e que em produção devem ser gerenciados como segredos. Centralizar o SSL no proxy simplifica muito as aplicações backend.
*   Introduz a próxima capacidade do Nginx: atuar como um **cache** para as respostas das aplicações backend.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Explorar Config:** Abra o arquivo `ch20\exercises\nginx\sites-available\image-gallery-3.local` no VS Code. Veja a estrutura completa com os dois blocos `server` e as diretivas SSL.
*   O próximo exercício modificará esta configuração para adicionar caching.

---

📄 Página 415

🔍 Resumo dos principais pontos:

*   Figura 20.7: Relembra aviso do navegador.
*   **20.3 Improving performance and reliability with the proxy** (Melhorando desempenho e confiabilidade com o proxy)
*   Nginx é de alta performance; pode servir conteúdo estático muito rápido.
*   Pode usar essa performance para **cachear respostas** dinâmicas das apps backend (upstream).
*   **Proxy Cache:**
    *   Nginx armazena cópia da resposta do upstream (em disco/memória local do Nginx).
    *   Requisições futuras para *mesmo* conteúdo são servidas *diretamente* pelo Nginx (do cache), sem contatar o upstream.
*   Diagrama (Figura 20.8): Ilustra fluxo com cache. Request -> Nginx -> Cache HIT? -> [Sim: Serve do cache] / [Não: Pega do Upstream -> Salva no Cache -> Serve para cliente].
*   **Benefícios Cache:**
    1.  **Menor Latência:** Servir do cache Nginx é mais rápido que app gerar resposta.
    2.  **Menor Carga no App:** Reduz tráfego para app containers -> podem lidar com mais usuários com mesma infra.
*   Cache só para conteúdo não-específico do usuário (ou bypass cache se cookie de auth presente). App Image Gallery é bom candidato.
*   **Exercício Try-it-now:** Configurar Nginx cache para Image Gallery (web e API).
    *   1. Remover config anterior.
    *   2. Copiar nova config (`image-gallery-4.local`) com diretivas de cache.
    *   3. Reiniciar Nginx.
    *   4. Testar com `curl -i --head --insecure https://image-gallery.local` (duas vezes).

🧠 Explicação detalhada e didática:

*   Esta seção introduz o uso do Nginx como um **cache de proxy reverso**. A ideia é que o Nginx pode armazenar temporariamente as respostas que ele recebe das suas aplicações backend (upstreams).
*   **Fluxo com Cache (Figura 20.8):**
    1.  Cliente faz requisição ao Nginx.
    2.  Nginx verifica se tem uma resposta válida e não expirada para essa requisição em seu cache local.
    3.  **Cache Hit:** Se tiver, Nginx envia a resposta do cache diretamente para o cliente (muito rápido!). A aplicação backend nem é contatada.
    4.  **Cache Miss:** Se não tiver, Nginx encaminha a requisição para a aplicação backend.
    5.  Nginx recebe a resposta do backend.
    6.  Nginx **armazena** uma cópia dessa resposta em seu cache (para uso futuro).
    7.  Nginx envia a resposta para o cliente.
*   Isso melhora muito a performance percebida pelo usuário (respostas mais rápidas em hits de cache) e reduz drasticamente a carga nos servidores de aplicação. É preciso ter cuidado para cachear apenas conteúdo que possa ser compartilhado entre usuários (páginas públicas, dados de API genéricos) e definir tempos de expiração apropriados.
*   O exercício configura o Nginx para cachear respostas tanto da UI web quanto da API do Image Gallery. O teste usa `curl` duas vezes para o mesmo URL; a primeira vez deve ser um "MISS" (Nginx busca no backend e popula o cache), a segunda deve ser um "HIT" (Nginx serve do cache).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Remover Config SSL:** (Estando em `ch20/exercises`)
    ```powershell
    Remove-Item -Path .\nginx\sites-enabled\image-gallery.local
    ```
2.  **Copiar Nova Config (Cache):**
    ```powershell
    Copy-Item -Path .\nginx\sites-available\image-gallery-4.local -Destination .\nginx\sites-enabled\image-gallery.local
    ```
3.  **Reiniciar Nginx:**
    ```powershell
    docker-compose -f .\nginx\docker-compose.yml -f .\nginx\override-linux.yml restart nginx # Ou -windows
    ```
4.  **Testar Cache:** Execute o comando duas vezes seguidas:
    ```powershell
    curl -i --head --insecure https://image-gallery.local
    # Ou Invoke-WebRequest -Uri https://image-gallery.local -HeadersOnly -SkipCertificateCheck
    ```
    Analise os cabeçalhos da resposta (próxima página explica o header `X-Cache`).

---

📄 Página 416

🔍 Resumo dos principais pontos:

*   Nova config Nginx adiciona header `X-Cache` (MISS/HIT) na resposta.
*   Figura 20.9: Mostra saídas do `curl`. Primeira tem `X-Cache: MISS` e `X-Upstream`. Segunda tem `X-Cache: HIT` e *sem* `X-Upstream` (não contatou backend).
*   Nginx permite tunar cache (ex: tempo de expiração).
*   Config (`image-gallery-4.local`) usa cache `SHORT` (1 min) para API, `LONG` (6h) para Web UI.
*   **Listagem 20.3:** Trechos da config Nginx com cache.
    ```nginx
    # Bloco location para API
    location = /api/image {
        proxy_pass http://iotd/image;
        proxy_cache SHORT; # Usa zona de cache nomeada 'SHORT'
        proxy_cache_valid 200 1m; # Cacheia respostas 200 por 1 minuto
        # ... proxy_set_header, add_header ...
    }
    # Bloco location para UI Web
    location / {
        proxy_pass http://image-gallery;
        proxy_cache LONG; # Usa zona de cache 'LONG'
        proxy_cache_valid 200 6h; # Cacheia 200 por 6 horas
        # Usa cache VENCIDO se backend der erro/timeout/etc:
        proxy_cache_use_stale error timeout invalid_header updating
                              http_500 http_502 http_503 http_504;
        # ... proxy_set_header, add_header ...
    }
    ```
*   Zonas de cache `SHORT`/`LONG` definidas na config Nginx core (`diamol/nginx`).
*   **`proxy_cache_use_stale`:** Feature de **resiliência**. Permite Nginx servir conteúdo do cache *mesmo que esteja expirado* (`stale`) se o servidor backend (`upstream`) estiver indisponível ou retornando certos erros (configurados na diretiva). Mantém app parcialmente online.

🧠 Explicação detalhada e didática:

*   A página analisa o resultado do teste de cache (Figura 20.9), confirmando o funcionamento (MISS -> HIT) através do cabeçalho customizado `X-Cache`.
*   Explica que a configuração Nginx usada (Listagem 20.3) define diferentes políticas de cache para a API e para a UI:
    *   `proxy_cache SHORT/LONG;`: Referencia "zonas" de cache pré-definidas (na imagem `diamol/nginx`) que especificam onde e como armazenar os dados cacheados (memória/disco, tamanho).
    *   `proxy_cache_valid 200 <tempo>;`: Define por quanto tempo uma resposta HTTP 200 (sucesso) deve ser considerada "fresca" no cache (1 minuto para API, 6 horas para UI). Após esse tempo, a resposta fica "stale" (vencida).
*   Introduz a diretiva `proxy_cache_use_stale`. Ela é muito importante para **resiliência**. Ela diz ao Nginx: "Se você precisar buscar conteúdo do backend, mas ele falhar (com erro, timeout, etc.), verifique se você tem uma versão *antiga* (stale) desse conteúdo no cache. Se tiver, sirva essa versão antiga mesmo assim." Isso pode manter partes do seu site funcionando para o usuário final, mesmo que a aplicação backend esteja temporariamente fora do ar.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Explorar Config:** Abra `ch20\exercises\nginx\sites-available\image-gallery-4.local` no VS Code. Veja as diretivas `proxy_cache`, `proxy_cache_valid`, `proxy_cache_use_stale`.
*   O próximo exercício testará a funcionalidade `proxy_cache_use_stale`.

💡 Analogias ou observações úteis (se houver):

*   `proxy_cache_valid`: Data de validade no produto da prateleira do supermercado (cache).
*   `proxy_cache_use_stale`: Política do gerente: "Se o caminhão de entrega (backend app) atrasar ou não vier, podemos vender o produto que venceu ontem (cache stale) para não deixar o cliente de mãos vazias, mas só para certos tipos de problema (tipos de erro configurados)".

---

📄 Página 417

🔍 Resumo dos principais pontos:

*   `proxy_cache_use_stale` mantém app online (com dados antigos) mesmo se containers app caírem. Útil para falhas transitórias ou rollbacks.
*   **Exercício Try-it-now:** Testar `use_stale`.
    *   1. Acessar site e API para popular cache: `curl ... https://...`, `curl ... https://.../api/image`.
    *   2. Matar containers web: `docker rm -f $(docker container ls -f name=imagegallery_image-gallery_* -q)`.
    *   3. Tentar acessar web de novo: `curl -i --head --insecure https://image-gallery.local`. Deve funcionar (HIT, servido do cache stale).
    *   4. Matar container API: `docker rm -f image-gallery_iotd_1`.
    *   5. Tentar acessar API de novo: `curl -i --head --insecure https://image-gallery.local/api/image`. Deve falhar (Erro 502), pois API tinha cache curto e *não* `use_stale` configurado (ou com menos condições).
*   Figura 20.10: Mostra saída: Web retorna 200 (cache stale), API retorna 502 (Bad Gateway).

🧠 Explicação detalhada e didática:

*   O exercício demonstra o poder (e a configuração) do `proxy_cache_use_stale`.
*   Primeiro, acessa-se a UI e a API para garantir que o Nginx tenha respostas cacheadas para ambas.
*   Depois, os contêineres da aplicação web (`image-gallery`) são **removidos forçadamente**.
*   Ao tentar acessar a UI web novamente, como a configuração dela incluía `proxy_cache_use_stale error timeout ...`, o Nginx detecta que o backend (`http://image-gallery`) está inacessível (erro), mas encontra a resposta antiga (agora "stale") no cache `LONG` e a serve mesmo assim. O usuário vê o site, embora possa estar desatualizado.
*   Em seguida, o contêiner da API (`iotd`) é removido.
*   Ao tentar acessar a API (`/api/image`), a configuração do cache `SHORT` para ela provavelmente *não* incluía `proxy_cache_use_stale` (ou tinha um tempo de validade muito curto que já expirou). O Nginx tenta contatar o backend `http://iotd`, falha (contêiner não existe), não encontra ou não pode usar uma resposta stale, e retorna um erro **HTTP 502 Bad Gateway** para o cliente, indicando que o proxy não conseguiu obter uma resposta do servidor upstream.
*   A Figura 20.10 mostra exatamente esses resultados diferentes, destacando como `use_stale` pode adicionar resiliência seletiva.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Popular Cache:**
    ```powershell
    curl -s --insecure https://image-gallery.local | Out-Null
    curl -s --insecure https://image-gallery.local/api/image | Out-Null
    ```
2.  **Matar Containers Web:**
    ```powershell
    # Encontrar IDs dos containers web e remover
    $webContainers = docker container ls -f name=image-gallery_image-gallery_* -q
    if ($webContainers) { docker container rm -f $webContainers }
    ```
3.  **Testar Web (Deve Funcionar - Cache Stale):**
    ```powershell
    Invoke-WebRequest -Uri https://image-gallery.local -SkipCertificateCheck -HeadersOnly -UseBasicParsing
    # Verifique o status 200 e o header X-Cache (deve ser HIT ou STALE)
    ```
4.  **Matar Container API:**
    ```powershell
    # Encontrar nome/ID do container iotd e remover
    $apiContainer = docker container ls -f name=image-gallery_iotd_* -q # Nome pode variar
    if ($apiContainer) { docker container rm -f $apiContainer }
    ```
5.  **Testar API (Deve Falhar - 502):**
    ```powershell
    Invoke-WebRequest -Uri https://image-gallery.local/api/image -SkipCertificateCheck -HeadersOnly -UseBasicParsing
    # Deve falhar com status 502 Bad Gateway
    ```

---

📄 Página 418

🔍 Resumo dos principais pontos:

*   Figura 20.10: Confirma web OK (stale), API falha (502).
*   Fim dos exercícios Nginx. É capaz (cache, GZip, headers, etc.).
*   Nginx é "container-agnostic" (trabalha com DNS/IP, não integra com API Docker). Requer config manual + reload.
*   Alternativa moderna: Proxy Cloud-Native (container-aware, integra com Docker).
*   **20.4 Using a cloud-native reverse proxy** (Usando um proxy reverso cloud-native)
    *   Revisita CI com Jenkins (Cap 11) rodando em container, que controlava Docker Engine do host via socket/pipe montado.
    *   Mesma técnica permite apps em container consultarem API Docker (descobrir outros containers, etc.).
    *   **Traefik:** Proxy reverso cloud-native que usa essa técnica.
*   **Traefik:**
    *   Não usa arquivos de config estáticos por app.
    *   Usa **labels** nos containers de aplicação para **configuração dinâmica**.
    *   Traefik monitora API Docker; detecta containers com labels Traefik e auto-configura roteamento.

🧠 Explicação detalhada e didática:

*   A página conclui a seção sobre Nginx, resumindo suas capacidades mas apontando sua principal característica (e limitação no mundo dos contêineres): ele opera puramente na camada de rede (resolvendo nomes DNS para IPs), sem ter consciência da plataforma Docker subjacente. Isso significa que a configuração dos sites/proxies é feita em arquivos estáticos que precisam ser gerenciados manualmente, e o Nginx precisa ser recarregado ou reiniciado para aplicar mudanças.
*   A seção 20.4 introduz uma abordagem alternativa, mais moderna e integrada ao ecossistema de contêineres: os **proxies reversos "cloud-native"**, exemplificados pelo **Traefik**.
*   A chave para o Traefik é que ele roda em um contêiner que tem acesso à API do Docker Engine (usando a mesma técnica de montar o socket/pipe que o Jenkins usou no Cap. 11).
*   Em vez de você criar arquivos de configuração para cada aplicação que quer expor, você adiciona **labels** especiais do Traefik diretamente na definição do *contêiner da sua aplicação* (no `docker run` ou, mais comumente, no `docker-compose.yml`).
*   O Traefik continuamente **monitora** a API do Docker em busca de contêineres que possuam essas labels. Quando ele encontra um, ele automaticamente lê as labels e **gera dinamicamente** as regras de roteamento necessárias para expor aquele contêiner, sem necessidade de arquivos de configuração manuais ou reinícios.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você rodará o Traefik em um contêiner, montando o `docker.sock` (Linux) ou `docker_engine` pipe (Windows).
*   Você modificará os arquivos `docker-compose.yml` das aplicações (whoami, image-gallery) para adicionar seções `labels:` com diretivas `traefik...`.
*   Você observará o Traefik Dashboard (`http://localhost:8080`) se atualizando automaticamente à medida que você inicia/para as aplicações.

💡 Analogias ou observações úteis (se houver):

*   Nginx Config: Recepcionista segue um manual impresso e estático. Para adicionar uma nova empresa ou mudar um ramal, precisa atualizar o manual e avisar o recepcionista para reler.
*   Traefik Config (Labels): Recepcionista tem um sistema online conectado ao RH (API Docker). Quando uma nova empresa (contêiner) se registra no prédio e coloca uma placa na porta (label) dizendo "Quero receber visitantes no nome X", o sistema do recepcionista atualiza automaticamente suas regras de encaminhamento.

---

📄 Página 419

🔍 Resumo dos principais pontos:

*   **Config Dinâmica Traefik:** Benefício maior. Não precisa iniciar upstreams antes, não precisa reiniciar Traefik para mudanças.
*   Traefik tem API/UI web (dashboard) para ver regras/estado.
*   **Exercício Try-it-now:** Rodar Traefik e ver dashboard inicial.
    *   1. Limpar containers.
    *   2. Rodar Traefik via Compose (usa override linux/windows para montar socket/pipe Docker).
        *   `docker-compose -f traefik/docker-compose.yml -f traefik/override-... up -d`.
    *   3. Acessar dashboard Traefik: `http://localhost:8080`.
*   Figura 20.11: Mostra dashboard Traefik (inicialmente vazio ou só com serviços internos).
*   **Conceitos Traefik:**
    *   **Entrypoints:** Portas que Traefik ouve (80, 443, 8080 para dashboard).
    *   **Routers:** Regras (Host, Path) que ligam Entrypoint a Service.
    *   **Services:** Upstreams (containers app). Traefik descobre automaticamente via Docker API.
    *   **Middlewares:** Opcional, modifica request/response entre Router e Service (ex: strip prefix, auth, headers).

🧠 Explicação detalhada e didática:

*   A página destaca a **configuração dinâmica** como o principal benefício do Traefik. Ele se adapta automaticamente às mudanças no ambiente Docker (contêineres iniciando, parando, sendo escalados) lendo as labels.
*   O exercício instrui a iniciar o contêiner do Traefik usando `docker-compose`. O arquivo Compose (`traefik/docker-compose.yml` + override) configura o Traefik, publica suas portas (80, 443 para tráfego de app, 8080 para o dashboard) e, crucialmente, monta o socket/pipe do Docker para que o Traefik possa monitorar a API Docker.
*   Após iniciar, acessar o dashboard (`http://localhost:8080`) permite visualizar a configuração atual do Traefik. Inicialmente (Figura 20.11), ele não terá nenhuma regra de roteamento para aplicações externas, apenas talvez para seus próprios serviços internos (como o próprio dashboard).
*   A página então define os **conceitos chave** usados pelo Traefik para configurar o roteamento:
    *   `Entrypoint`: Por onde o tráfego entra (Porta 80, Porta 443).
    *   `Router`: Regra que diz "Se tráfego chegar no Entrypoint X e corresponder à condição Y (ex: Host for 'a.com'), envie para o Service Z".
    *   `Service`: Representa a aplicação backend (o(s) contêiner(es) que servem o conteúdo). Traefik descobre os IPs dos contêineres automaticamente.
    *   `Middleware`: Funções opcionais que podem ser inseridas entre o Router e o Service para modificar a requisição ou resposta (ex: adicionar cabeçalhos, fazer autenticação, reescrever URL).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Limpar:** `docker container rm -f $(docker container ls -aq)`
2.  **Navegar:** `cd C:\DockerLabs\diamol\ch20\exercises`
3.  **Rodar Traefik:** (Use override apropriado)
    ```powershell
    # Linux mode
    docker-compose -f .\traefik\docker-compose.yml -f .\traefik\override-linux.yml up -d
    # Windows mode
    # docker-compose -f .\traefik\docker-compose.yml -f .\traefik\override-windows.yml up -d
    ```
4.  **Verificar:** `docker ps` (ver container traefik).
5.  **Acessar Dashboard:** Abra navegador em `http://localhost:8080`. Explore a UI (Entrypoints, Routers, Services - devem estar quase vazios).

---

📄 Página 420

🔍 Resumo dos principais pontos:

*   Config mais simples: só Router (com regra) ligado a Service.
*   **Exercício Try-it-now:** Deployar `whoami` com labels Traefik.
    *   1. Deployar `whoami`: `docker-compose -f whoami/docker-compose.yml -f whoami/override-traefik.yml up -d`. (Override adiciona labels).
    *   2. Verificar dashboard Traefik (`:8080`). Procurar router `whoami@docker`.
    *   3. Testar rota: `curl -i http://whoami.local`.
*   Figura 20.12: Mostra dashboard Traefik agora com router `whoami@docker` configurado (Host `whoami.local` -> service `whoami`).
*   **Listagem 20.4:** Labels no `override-traefik.yml` para `whoami`.
    ```yaml
    services:
      whoami:
        labels:
          - "traefik.enable=true" # Habilita Traefik para este serviço
          # Define router 'whoami', regra é match no Host header
          - "traefik.http.routers.whoami.rule=Host(`whoami.local`)"
    ```
*   Explicação Labels: `traefik.enable=true` ativa. `traefik.http.routers.<nome_router>.rule=...` define a regra de roteamento. Traefik infere o serviço backend a partir do próprio container/serviço onde a label está.

🧠 Explicação detalhada e didática:

*   O exercício demonstra a configuração dinâmica do Traefik na prática.
    1.  Inicia-se a aplicação `whoami` usando um arquivo Compose (`override-traefik.yml`) que contém **labels** específicas para o Traefik (Listagem 20.4).
    2.  O contêiner `traefik`, que está monitorando a API Docker, detecta o novo contêiner `whoami` e suas labels.
    3.  Ele lê as labels e **automaticamente** cria a configuração interna necessária: um `Router` chamado `whoami` que corresponde ao `Host: whoami.local` e o direciona para o `Service` `whoami` (o contêiner que acabou de iniciar).
    4.  Ao verificar o dashboard do Traefik (Figura 20.12), essa nova rota aparece.
    5.  Ao acessar `http://whoami.local`, o Traefik (que está ouvindo na porta 80) aplica a regra e encaminha a requisição para o contêiner `whoami`.
*   A Listagem 20.4 mostra as labels mínimas necessárias:
    *   `traefik.enable=true`: Sinaliza ao Traefik para gerenciar este serviço.
    *   `traefik.http.routers.<nome_router>.rule=<regra>`: Define um roteador HTTP. `<nome_router>` é um nome que você escolhe (aqui, `whoami`). `<regra>` especifica como identificar as requisições para este router (aqui, `Host(\`whoami.local\`)` - note as backticks ` ou aspas simples para a string dentro da regra).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir Traefik Rodando:** (Passo da p.419).
2.  **Deployar `whoami` com Labels:** (Estando em `ch20/exercises`)
    ```powershell
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml up -d
    ```
3.  **Verificar Dashboard:** Atualize `http://localhost:8080`. Vá para HTTP > Routers. Você deve ver o router `whoami@docker`. Clique nele para ver detalhes (regra Host). Verifique também HTTP > Services; deve haver um serviço `whoami`.
4.  **Testar Rota:**
    ```powershell
    curl -i http://whoami.local
    # Deve retornar a saída do whoami.
    ```

---

📄 Página 421

🔍 Resumo dos principais pontos:

*   Figura 20.12: Confirma router `whoami` no dashboard.
*   Traefik suporta roteamento sofisticado (Host, Path, PathPrefix) + Middlewares (StripPrefix).
*   Exemplo: Expor API Image Gallery (`iotd`) via path `/api/` no domínio principal.
    *   Traefik recebe `/api/image`, middleware remove `/api`, envia `/image` para serviço `iotd`.
*   **Exercício Try-it-now:** Deployar Image Gallery com labels Traefik para UI e API (com StripPrefix).
    *   1. Deployar app: `docker-compose -f image-gallery/... -f image-gallery/override-traefik.yml up -d`. (Override contém labels para `image-gallery` e `iotd`, incluindo middleware).
    *   2. Testar UI: `curl --head http://image-gallery.local`.
    *   3. Testar API via path: `curl -i http://image-gallery.local/api/image`.
*   Figura 20.13: Mostra resposta JSON da API via path `/api/image`.

🧠 Explicação detalhada e didática:

*   A página introduz o uso de **Middlewares** no Traefik para manipulações mais complexas, como o **StripPrefix**. O cenário é o mesmo da p.412 com Nginx: queremos que `http://image-gallery.local/api/image` seja roteado para o serviço `iotd`, mas o serviço `iotd` espera receber a requisição apenas como `/image` (sem o `/api`).
*   A configuração via labels no Traefik (dentro de `override-traefik.yml`) fará o seguinte:
    1.  Define um Router para a API (`iotd-api`) que corresponde a `Host(\`image-gallery.local\`) && PathPrefix(\`/api\`)`.
    2.  Define um Middleware chamado `api-strip` do tipo `StripPrefix`, configurado para remover o prefixo `/api`.
    3.  Associa o middleware `api-strip` ao router `iotd-api`.
    4.  Associa o router `iotd-api` ao serviço `iotd`.
    5.  (Também define um router `ig-web` para `Host(\`image-gallery.local\`)` ligado ao serviço `image-gallery` para a UI).
*   O exercício implanta a aplicação com essas labels. Traefik detecta e configura tudo dinamicamente. O teste com `curl` para `/api/image` deve funcionar, pois o Traefik fará a remoção do prefixo antes de chamar o backend `iotd`.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Parar `whoami` (Opcional):**
    ```powershell
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml down
    ```
2.  **Deployar Image Gallery (Traefik):** (Estando em `ch20/exercises`)
    ```powershell
    docker-compose -f .\image-gallery\docker-compose.yml -f .\image-gallery\override-traefik.yml up -d
    # Isso iniciará os 3 serviços do app (image-gallery, iotd, accesslog)
    ```
3.  **Verificar Dashboard:** Atualize `http://localhost:8080`. Você deve ver novos Routers (ex: `ig-web`, `iotd-api`), Services e o Middleware `api-strip`. Explore as configurações deles.
4.  **Testar Rotas:**
    ```powershell
    curl --head http://image-gallery.local # Deve funcionar (UI)
    curl -i http://image-gallery.local/api/image # Deve funcionar (API via path)
    ```

---

📄 Página 422

🔍 Resumo dos principais pontos:

*   Figura 20.13: Confirma resposta API via path.
*   Config StripPrefix é "críptica" (define router, define middleware, associa middleware ao router, associa router ao serviço via labels).
*   Transparente para consumidor.
*   **Comparativo Traefik vs Nginx (cont.):**
    *   Traefik (v2.1) não tem cache; Nginx é melhor para isso.
    *   Traefik tem melhor suporte SSL (auto Let's Encrypt, default self-signed).
*   **Exercício Try-it-now:** Adicionar HTTPS (com cert default Traefik) para Image Gallery.
    *   1. Deployar app com `override-traefik-ssl.yml`. (Labels adicionam redirect HTTP->HTTPS e configuram router para usar entrypoint HTTPS).
    *   2. Testar HTTPS web: `curl --head --insecure https://image-gallery.local`.
    *   3. Testar HTTPS API: `curl --insecure https://image-gallery.local/api/image`.
*   Esperar aviso navegador (cert default não confiável). Não precisa gerar certs.
*   Figura 20.14: Mostra `curl --insecure` funcionando via HTTPS.

🧠 Explicação detalhada e didática:

*   A página reconhece que configurar middlewares como StripPrefix via labels pode ser um pouco verboso, mas funciona.
*   Faz a comparação de features: Nginx ganha em **caching**, Traefik ganha em **gerenciamento de SSL**.
*   O Traefik pode ser configurado para:
    *   Gerar certificados **auto-assinados** automaticamente (bom para dev/teste, como no exercício).
    *   Integrar-se com **Let's Encrypt** para obter e renovar certificados SSL válidos automaticamente para seus domínios públicos (muito útil para produção).
*   O exercício demonstra a facilidade de habilitar HTTPS com o certificado padrão do Traefik:
    1.  Usa-se um novo override (`override-traefik-ssl.yml`) que adiciona mais labels. Essas labels provavelmente:
        *   Definem um middleware para redirecionar HTTP para HTTPS.
        *   Associam esse middleware ao router que ouve no entrypoint HTTP (porta 80).
        *   Modificam/criam routers para também ouvirem no entrypoint HTTPS (porta 443) e habilitam TLS (`routers.<name>.tls=true`). Como nenhum certificado específico é fornecido nas labels, o Traefik usa seu certificado default auto-assinado.
    2.  Após o deploy, testa-se o acesso via `https://...`. O `curl` precisa da flag `--insecure` (ou `-k`) para ignorar o erro de certificado não confiável. O navegador mostraria o aviso de segurança.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Deployar com SSL Labels:** (Estando em `ch20/exercises`)
    ```powershell
    # Parar anterior (opcional)
    # docker-compose -f .\image-gallery\docker-compose.yml -f .\image-gallery\override-traefik.yml down
    # Deploy com SSL
    docker-compose -f .\image-gallery\docker-compose.yml -f .\image-gallery\override-traefik-ssl.yml up -d
    ```
2.  **Verificar Dashboard:** Atualize `http://localhost:8080`. Veja os routers; eles agora devem ter configuração TLS e talvez referência ao middleware de redirect.
3.  **Testar HTTPS:**
    ```powershell
    curl --head --insecure https://image-gallery.local
    # Deve retornar status 200 OK

    curl --insecure https://image-gallery.local/api/image
    # Deve retornar JSON da API
    ```
4.  **Testar Redirect (Opcional):**
    ```powershell
    curl -i http://image-gallery.local
    # Deve retornar status 3xx Redirect com Location: https://...
    ```

---

📄 Página 423

🔍 Resumo dos principais pontos:

*   Figura 20.14: Confirma acesso HTTPS via `curl --insecure`.
*   Recapitulação features proxy: Roteamento, Load Balancing, SSL Termination. Traefik faz via labels dinâmicas.
*   Última feature: **Sticky Sessions (Afinidade de Sessão)**.
    *   Necessário para apps legados (stateful) que requerem que mesmo cliente sempre acesse mesma instância backend.
    *   Habilitado via config de load balancer do serviço Traefik (via label).
*   **Exercício Try-it-now:** Demonstrar sticky sessions com `whoami`.
    *   1. Escalar `whoami` para 3 réplicas (usando `override-traefik.yml` - sem sticky).
    *   2. Fazer chamadas `curl` repetidas usando cookies (`-c c.txt -b c.txt`). Observar que Hostname na resposta muda (load balancing normal).
    *   3. Deployar `whoami` de novo com `override-traefik-sticky.yml` (adiciona label para sticky), escala 3.
    *   4. Fazer chamadas `curl` repetidas (com cookies). Observar que Hostname na resposta permanece o *mesmo*.
*   Figura 20.15: Mostra saída confirmando sticky sessions (mesmo hostname/container servindo requests).

🧠 Explicação detalhada e didática:

*   A página introduz a última funcionalidade a ser explorada: **Sticky Sessions**, também conhecida como afinidade de sessão.
*   Enquanto aplicações modernas devem ser stateless (permitindo que qualquer instância atenda qualquer requisição), aplicações legadas frequentemente armazenam o estado da sessão do usuário na memória do servidor específico que atendeu a primeira requisição. Nesses casos, é crucial que todas as requisições subsequentes *daquele mesmo usuário* sejam direcionadas para a *mesma instância* do servidor backend.
*   Reverse proxies como o Traefik podem habilitar sticky sessions. Geralmente, eles fazem isso inspecionando a primeira resposta do backend, adicionando um cookie especial à resposta que identifica aquele backend específico, e então, para requisições futuras do mesmo cliente (identificado pelo cookie), o proxy usa a informação do cookie para rotear diretamente para o backend original.
*   O exercício demonstra isso com o `whoami`:
    1.  Primeiro, roda-se sem sticky sessions e confirma-se (via `curl` com cookies) que requisições diferentes vão para réplicas diferentes.
    2.  Depois, re-implanta-se usando um override (`override-traefik-sticky.yml`) que adiciona uma label como `traefik.http.services.whoami.loadbalancer.sticky.cookie=true` (ou similar).
    3.  Ao repetir as chamadas com `curl`, observa-se que todas as requisições agora são atendidas pela mesma réplica (o mesmo hostname é retornado), pois o Traefik está usando o cookie para "fixar" a sessão.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Escalar `whoami` (Sem Sticky):** (Estando em `ch20/exercises`)
    ```powershell
    # Parar/remover versão anterior do whoami se houver
    # docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml down
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik.yml up -d --scale whoami=3
    ```
2.  **Testar Load Balancing (Sem Sticky):**
    ```powershell
    # Limpar cookie file se existir
    Remove-Item c.txt -ErrorAction SilentlyContinue
    # Fazer 2 chamadas. Use o header 'Hostname' da resposta do whoami
    curl -c c.txt -b c.txt http://whoami.local
    curl -c c.txt -b c.txt http://whoami.local
    # Hostnames devem (provavelmente) ser diferentes.
    ```
3.  **Deployar com Sticky:**
    ```powershell
    docker-compose -f .\whoami\docker-compose.yml -f .\whoami\override-traefik-sticky.yml up -d --scale whoami=3
    ```
4.  **Testar Sticky:**
    ```powershell
    # Limpar cookie file
    Remove-Item c.txt -ErrorAction SilentlyContinue
    # Fazer 2 chamadas
    curl -c c.txt -b c.txt http://whoami.local
    curl -c c.txt -b c.txt http://whoami.local
    # Hostnames agora devem ser os MESMOS.
    ```

---

📄 Página 424

🔍 Resumo dos principais pontos:

*   Figura 20.15: Confirma sticky sessions (mesmo hostname).
*   Explica funcionamento sticky Traefik: usa cookie (com IP backend?).
*   Limitação: Se container "fixado" morrer, cliente perde sessão (Traefik roteia para outro).
*   Fim Traefik.
*   **20.5 Understanding the patterns a reverse proxy enables** (Entendendo os padrões que um proxy reverso habilita)
*   Proxy é essencial para produção com containers. 3 padrões principais:
    1.  **Virtual Hosting:** Múltiplos apps/domínios (site1.com, site2.com) na mesma porta 80/443 do cluster. Proxy usa `Host` header para rotear. (Figura 20.16).
    2.  **API Gateway / Composição:** Múltiplos microsserviços backend (containers) compõem uma única aplicação/domínio externo. Proxy usa `Path` (ex: `/users`, `/orders`) para rotear para serviço correto. (Figura 20.17).

🧠 Explicação detalhada e didática:

*   A página explica brevemente o mecanismo de sticky session do Traefik (baseado em cookie) e sua limitação inerente no ambiente dinâmico de contêineres (se o contêiner de destino some, a sessão é perdida).
*   A seção 20.5 resume os **principais padrões arquiteturais** que são habilitados ou facilitados pelo uso de um reverse proxy na frente de aplicações conteinerizadas:
    1.  **Hospedagem Virtual (Virtual Hosting):** Permite que você execute múltiplos websites ou aplicações, cada um com seu próprio nome de domínio (ex: `app1.minhaempresa.com`, `app2.minhaempresa.com`), mas todos compartilhando o mesmo endereço IP público e as mesmas portas padrão (80/443). O reverse proxy recebe todas as requisições e usa o cabeçalho `Host` enviado pelo navegador para direcionar a requisição ao contêiner da aplicação correta. (Figura 20.16 ilustra isso).
    2.  **API Gateway / Composição de Microsserviços:** Em uma arquitetura de microsserviços, você pode ter dezenas ou centenas de pequenas APIs internas rodando em contêineres. Um reverse proxy pode atuar como um **API Gateway**, expondo um único ponto de entrada para o mundo externo (ex: `api.minhaempresa.com`). O proxy então usa o **caminho (path)** da URL na requisição (ex: `/api/users/...`, `/api/products/...`) para rotear a chamada para o contêiner do microsserviço apropriado (serviço de usuário, serviço de produto). Isso esconde a complexidade interna da arquitetura de microsserviços dos consumidores externos. (Figura 20.17 ilustra isso).

💻 Exemplos práticos (para meu ambiente Windows):

*   Os exercícios que você fez com Nginx e Traefik usando `whoami.local`, `api.numbers.local`, `image-gallery.local` demonstram o Padrão 1 (Virtual Hosting).
*   O exercício que expôs a API `iotd` como `image-gallery.local/api/image` demonstra o Padrão 2 (API Gateway/Composição via Path).

---

📄 Página 425

🔍 Resumo dos principais pontos:

*   Figura 20.16: Diagrama Virtual Hosting.
*   Figura 20.17: Diagrama API Gateway / Microservice Composition.
*   **Padrão 3: Strangler Fig (Estrangulamento de Monolito).**
    *   Para migrar/modernizar apps monolíticos legados.
    *   Proxy fica na frente do monolito (rodando em container).
    *   Identifica-se uma funcionalidade no monolito. Cria-se um *novo* microsserviço (em container, talvez tecnologia moderna) para essa funcionalidade.
    *   Configura-se o proxy para rotear *apenas* as requisições daquela funcionalidade específica para o *novo* microsserviço. Todas as *outras* requisições continuam indo para o monolito antigo.
    *   Gradualmente, "estrangula-se" o monolito, movendo mais funcionalidades para microsserviços, até que monolito possa ser desativado.
*   Diagrama (Figura 20.18): Ilustra padrão Strangler Fig. Proxy roteia alguns paths para microsserviços, resto para monolito.
*   Padrões não exclusivos; proxy pode suportar todos simultaneamente.
*   **20.6 Lab** (Laboratório)
    *   App novo: Calculadora de Pi (`http://localhost:8031/?dp=50000`). Computacionalmente intensivo.
*   **Tarefa:** Rodar app Pi atrás de reverse proxy (Nginx ou Traefik).
    1.  App acessível via `pi.local` (porta 80).
    2.  Proxy deve **cachear** respostas. Requisições repetidas (mesmo nº de casas decimais `dp`) devem ser rápidas (servidas do cache).
    3.  Proxy deve adicionar **resiliência**: servir do cache **stale** se app container cair.

🧠 Explicação detalhada e didática:

*   A página apresenta o terceiro padrão importante habilitado pelo reverse proxy: o **Padrão Strangler Fig** (Figura de Estrangulamento, nome inspirado em figueiras que crescem em volta de outras árvores). Este padrão é uma técnica poderosa para **modernizar aplicações monolíticas legadas gradualmente**.
    *   Coloca-se o proxy na frente do monolito (que já pode estar rodando em um contêiner).
    *   Escolhe-se uma funcionalidade do monolito para refatorar ou reescrever como um microsserviço separado (em seu próprio contêiner).
    *   Configura-se o proxy para interceptar as URLs específicas daquela funcionalidade e roteá-las para o novo microsserviço. Todo o resto do tráfego continua indo para o monolito.
    *   Iterativamente, mais funcionalidades são migradas para microsserviços, e o tráfego é redirecionado pelo proxy. Com o tempo, o monolito original vai sendo "estrangulado" (recebendo cada vez menos tráfego) até poder ser completamente desativado. O proxy esconde essa transição dos usuários finais. (Figura 20.18 ilustra).
*   O **Lab do Capítulo 20** foca nos benefícios de **caching** e **resiliência** do proxy. Você usará uma aplicação que calcula Pi (lenta) e a colocará atrás de um Nginx (ou Traefik, se preferir) configurado para:
    1.  Servir em `pi.local`.
    2.  Cachear as respostas (baseado na URL, que inclui o parâmetro `dp`). Cálculos repetidos para o mesmo `dp` devem vir do cache rapidamente.
    3.  Servir respostas do cache mesmo que estejam vencidas (`stale`) se o contêiner da aplicação Pi cair (`proxy_cache_use_stale` no Nginx).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 20 (usando Nginx):**
    1.  Navegue para `ch20\lab`. Examine o `docker-compose.yml` para rodar o app `pi-calculator`. Ele deve definir o serviço `pi-calc` e a rede `ch20`.
    2.  Adicione `pi.local` ao seu arquivo `hosts`.
    3.  Crie um arquivo de configuração Nginx (`pi.local.conf`) na pasta `nginx\sites-available`.
        *   Defina `server_name pi.local;`.
        *   Defina `location / { ... }`.
        *   Dentro do `location`:
            *   `proxy_pass http://pi-calc:8080;` (Verifique a porta interna do app pi-calc).
            *   Adicione diretivas de cache: `proxy_cache MYCACHE;` (Use uma zona de cache definida na config Nginx base, ou defina uma), `proxy_cache_valid 200 1h;` (Cache por 1 hora, por exemplo), `proxy_cache_use_stale error timeout http_502 http_503;`.
            *   Adicione `proxy_set_header Host $host;`.
            *   Adicione `add_header X-Cache $upstream_cache_status;` (Variável Nginx para status do cache).
    4.  Copie `pi.local.conf` para `nginx\sites-enabled\`.
    5.  Certifique-se que Nginx está rodando (`docker-compose -f nginx/... up -d`).
    6.  Rode o app Pi: `docker-compose -f pi-calculator/docker-compose.yml up -d`.
    7.  Reinicie Nginx: `docker-compose -f nginx/... restart nginx`.
    8.  **Teste Cache:**
        *   Acesse `http://pi.local/?dp=10000` no navegador (ou `curl -i`). Anote o tempo. Verifique `X-Cache: MISS`.
        *   Acesse a *mesma URL* novamente. Deve ser muito mais rápido. Verifique `X-Cache: HIT`.
    9.  **Teste Stale:**
        *   Pare o container `pi-calc` (`docker stop <container_id>`).
        *   Acesse `http://pi.local/?dp=10000` novamente. Deve funcionar (rápido). Verifique `X-Cache: STALE` ou `HIT` (depende se ainda estava fresco).
        *   Acesse com `dp` diferente, ex: `http://pi.local/?dp=5000`. Deve falhar (502 Bad Gateway), pois não estava no cache stale.

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch20/lab/README.md

---

📄 Página 426

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 20 no GitHub.
*   **Capítulo 21: Asynchronous communication with a message queue** (Comunicação assíncrona com uma fila de mensagens)
*   Último capítulo completo. Introduz nova forma de comunicação: **Filas de Mensagens (Message Queues)**.
*   **Desacoplamento:** Componentes não se comunicam diretamente, mas via fila.
*   Flexibilidade: Fila entrega para 1 ou muitos destinatários.
*   Foco Cap. 21: Cenários habilitados por filas.
    1.  Melhorar performance/escalabilidade.
    2.  Adicionar features com zero downtime.
*   Exemplos com **Redis** e **NATS** (modernas, rodam bem em Docker).
*   **21.1 What is asynchronous messaging?** (O que é mensageria assíncrona?)
    *   Comunicação **Síncrona** (padrão: REST, SOAP, gRPC): Cliente conecta -> Envia request -> **Espera** response -> Desconecta. (Como ligação telefônica).
    *   **Desafios Síncronos:** Ambas as partes precisam estar disponíveis; servidor pode estar offline/ocupado; processamento longo pode dar timeout no cliente; tratamento de falhas de rede é complexo (re-tentativa segura?).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 21, o último com conteúdo técnico principal, abordando **comunicação assíncrona** através de **filas de mensagens (message queues)**.
*   Contrasta a comunicação **síncrona** (a mais comum, como uma chamada API REST onde o cliente espera pela resposta) com a **assíncrona**.
*   **Comunicação Síncrona:**
    *   **Prós:** Simples de entender o fluxo (request -> response).
    *   **Contras:** Fortemente acoplada (cliente depende da disponibilidade *imediata* do servidor), sensível a falhas (rede, servidor lento/ocupado), pode bloquear o cliente enquanto espera, difícil de escalar se servidor for gargalo.
*   **Comunicação Assíncrona (via Fila):**
    *   Cliente envia mensagem para a fila (operação rápida) e *não espera* resposta direta. Continua seu trabalho.
    *   Servidor(es) "ouvem" a fila, pegam mensagens quando podem, processam no seu próprio ritmo.
    *   Se resposta for necessária, servidor envia outra mensagem para outra fila (ou usa padrões request/reply específicos da fila).
    *   **Prós:** Desacoplamento (cliente e servidor não precisam estar online ao mesmo tempo), resiliência (fila guarda mensagem se servidor offline), escalabilidade (múltiplos servidores podem consumir da mesma fila), absorção de picos (fila funciona como buffer).
*   O capítulo usará **Redis** (um data store em memória muito rápido que também tem funcionalidades de pub/sub e filas) e **NATS** (um sistema de mensageria moderno e de alta performance, parte da CNCF) como exemplos de message brokers que rodam em contêineres.

💻 Exemplos práticos (para meu ambiente Windows):

*   Você rodará Redis e NATS em contêineres.
*   Você usará CLIs ou apps de exemplo para publicar mensagens na fila e outros para subscrever/consumir mensagens da fila.

💡 Analogias ou observações úteis (se houver):

*   Síncrono: Ligar para alguém. Precisa que a pessoa atenda na hora, você espera na linha pela resposta. Se ocupado ou não atender, a comunicação falha.
*   Assíncrono (Fila): Mandar um email (ou mensagem WhatsApp). Você envia a mensagem para a "caixa postal" (fila), não espera resposta imediata. O destinatário lê e responde quando puder. A comunicação funciona mesmo que um dos dois esteja offline temporariamente.

---

📄 Página 427

🔍 Resumo dos principais pontos:

*   Diagrama (Figura 21.1): Ilustra comunicação assíncrona via fila (Client -> Queue <- Server).
*   Velocidade da Fila: Enviar msg é rápido (protocolo otimizado, sem processamento complexo na fila). Filas aguentam alta taxa de mensagens.
*   **Exemplo Redis:** Usar Redis como message broker simples (Pub/Sub).
*   **Exercício Try-it-now:** Rodar servidor Redis.
    *   1. Criar rede `ch21`.
    *   2. Rodar container Redis: `docker run -d --name redis --network ch21 diamol/redis`.
    *   3. Verificar logs (Redis pronto para conexões).
*   Figura 21.2: Log Redis indicando "Ready to accept connections".

🧠 Explicação detalhada e didática:

*   A Figura 21.1 visualiza o fluxo assíncrono: o cliente e o servidor interagem com a Fila, não diretamente entre si.
*   A página destaca a **performance** das filas: enviar uma mensagem é tipicamente muito rápido, permitindo que o cliente publique muitas mensagens em pouco tempo, sem ficar bloqueado esperando processamento.
*   Inicia o exemplo prático usando **Redis**. Embora Redis seja mais conhecido como cache ou banco de dados em memória, ele possui funcionalidades de **Publish/Subscribe (Pub/Sub)** que permitem usá-lo como um message broker simples.
*   O exercício instrui a iniciar um contêiner Redis (`diamol/redis`) conectado a uma nova rede Docker (`ch21`). Os logs indicarão quando o servidor Redis estiver pronto para aceitar conexões de clientes.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Criar Rede:**
    ```powershell
    # Se Linux mode
    docker network create ch21
    # Se Windows mode
    # docker network create -d nat ch21
    ```
2.  **Rodar Redis:**
    ```powershell
    docker container run -d --name redis --network ch21 diamol/redis
    ```
3.  **Verificar Logs:** Espere alguns segundos.
    ```powershell
    docker container logs redis --tail 5 # Ver últimas linhas
    # Procure por "Ready to accept connections" ou similar.
    ```

---

📄 Página 428

🔍 Resumo dos principais pontos:

*   Conexão cliente-fila é rápida (envia bytes, espera ACK).
*   Terminologia Mensageria:
    *   **Publisher:** Envia mensagem.
    *   **Subscriber:** Recebe mensagem.
    *   **Channel (Redis):** Tópico/Fila onde mensagens são publicadas/subscritas. Separa fluxos de msg.
*   **Exercício Try-it-now:** Rodar cliente Redis como Publisher.
    *   Usar `diamol/redis-cli` (CLI para Redis).
    *   Comando publica msg "ping" no channel `channel21`, 50x, a cada 5s, em background.
    *   `docker run -d --name publisher --network ch21 diamol/redis-cli -r 50 -i 5 PUBLISH channel21 ping`.
    *   Verificar logs do publisher (`docker logs publisher`). Deve mostrar `0` (OK) a cada envio.
*   Figura 21.3: Mostra logs do publisher com respostas `0`.

🧠 Explicação detalhada e didática:

*   A página introduz a terminologia padrão de mensageria: **Publisher** (quem envia), **Subscriber** (quem recebe). No caso do Redis Pub/Sub, a comunicação ocorre através de **Channels** (canais) nomeados. Publishers enviam mensagens para um channel específico, e Subscribers se inscrevem para receber mensagens daquele channel.
*   O exercício usa uma imagem (`diamol/redis-cli`) que contém a ferramenta de linha de comando do Redis para atuar como um **Publisher**.
*   O comando `docker run ... diamol/redis-cli -r 50 -i 5 PUBLISH channel21 ping` instrui o cliente Redis a:
    *   Conectar-se ao servidor `redis` (encontrado via DNS na rede `ch21`).
    *   Executar o comando `PUBLISH` no channel `channel21` com a mensagem `ping`.
    *   Repetir (`-r`) 50 vezes.
    *   Com intervalo (`-i`) de 5 segundos entre repetições.
    *   O contêiner (`publisher`) roda em background (`-d`).
*   Verificar os logs do `publisher` mostra as respostas do servidor Redis para cada comando `PUBLISH` (o número `0` geralmente indica sucesso no Redis CLI para essa operação).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar Publisher:**
    ```powershell
    docker run -d --name publisher --network ch21 diamol/redis-cli -r 50 -i 5 PUBLISH channel21 ping
    ```
2.  **Verificar Logs Publisher:**
    ```powershell
    docker logs publisher
    # Você verá vários '0' sendo adicionados a cada 5 segundos.
    ```

💡 Analogias ou observações úteis (se houver):

*   Channel (Redis): Como um quadro de avisos específico ou um grupo de chat. Você posta (`PUBLISH`) no quadro/grupo `channel21`, e quem estiver olhando (`SUBSCRIBE`) para `channel21` verá a mensagem.

---

📄 Página 429

🔍 Resumo dos principais pontos:

*   Publisher envia para `channel21`. Subscriber precisa ouvir no mesmo channel.
*   **Exercício Try-it-now:** Rodar cliente Redis como Subscriber.
    *   Usar `diamol/redis-cli` novamente.
    *   Rodar interativamente (`-it`) para ver msgs chegando.
    *   Comando se inscreve no `channel21`: `docker run -it --network ch21 --name subscriber diamol/redis-cli SUBSCRIBE channel21`.
*   Saída do subscriber:
    *   Primeiro, confirmação da subscrição.
    *   Depois, a cada 5s (quando publisher envia), mostra detalhes da mensagem recebida ("message", "channel21", "ping").
*   Figura 21.4: Mostra logs do subscriber recebendo as mensagens "ping".

🧠 Explicação detalhada e didática:

*   Agora que temos um Publisher enviando mensagens para o `channel21`, o exercício instrui a iniciar um **Subscriber** para receber essas mensagens.
*   Usa-se a mesma imagem `diamol/redis-cli`, mas com o comando `SUBSCRIBE channel21`.
*   A flag `-it` é usada para rodar o contêiner `subscriber` em **primeiro plano interativo**, para que possamos ver as mensagens sendo impressas no terminal assim que chegarem.
*   O Redis Pub/Sub funciona da seguinte forma: quando uma mensagem é publicada em um channel, o servidor Redis envia uma cópia dessa mensagem para **todos** os clientes que estão atualmente subscritos naquele channel.
*   A saída no terminal do `subscriber` mostrará primeiro uma confirmação de que ele se conectou e subscreveu com sucesso. Depois, a cada 5 segundos, quando o contêiner `publisher` enviar a mensagem "ping", o servidor Redis a encaminhará para o `subscriber`, e a CLI do Redis no `subscriber` imprimirá os detalhes da mensagem recebida.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Rodar Subscriber:** (Em um novo terminal PowerShell/VS Code, ou no mesmo se o publisher estiver em background)
    ```powershell
    docker run -it --network ch21 --name subscriber diamol/redis-cli SUBSCRIBE channel21
    ```
2.  **Observar:** O terminal ficará ativo. Você verá algo como:
    ```
    Reading messages... (press Ctrl-C to quit)
    1) "subscribe"
    2) "channel21"
    3) (integer) 1
    # (Após ~5 segundos)
    1) "message"
    2) "channel21"
    3) "ping"
    # (Após mais ~5 segundos)
    1) "message"
    2) "channel21"
    3) "ping"
    ... etc ...
    ```
3.  **Parar:** Pressione `Ctrl+C` quando quiser parar de receber mensagens e sair do contêiner subscriber. (O publisher continuará rodando em background até completar 50 mensagens ou ser parado/removido).

---

📄 Página 430

🔍 Resumo dos principais pontos:

*   Sair do subscriber (`Ctrl+C` ou `docker rm -f subscriber`).
*   Exemplo confirma **comunicação assíncrona**: Publisher enviou antes de subscriber existir; subscriber continuaria ouvindo se publisher parasse. Componentes desacoplados via fila.
*   Próximo: Usar fila para melhorar performance/escala do app To-Do.
*   **21.2 Using a cloud-native message queue** (Usando uma fila de mensagens cloud-native)
    *   Revisita app To-Do (Web + SQL DB). Comunicação síncrona Web -> DB limita escala (pool de conexões BD).
    *   **Solução Async para `Save`:**
        *   Web app (usuário adiciona item) -> Publica msg "NewItem" na Fila (rápido).
        *   Fila aguenta mais conexões/carga que BD.
        *   Separar handler lê msg da fila -> Insere no BD (pode ser escalado/rate-limited).
    *   Tecnologia: **NATS**. Projeto CNCF, maduro, usado. Em memória -> muito rápido, bom para comunicação inter-container.
*   **Exercício Try-it-now:** Rodar NATS server e checar API admin.
    *   Navegar para `ch21/exercises/todo-list`.
    *   Iniciar serviço `message-queue` (NATS) via Compose: `docker-compose up -d message-queue`.
    *   Verificar logs: `docker logs todo-list_message-queue_1`.
    *   Verificar conexões via API NATS: `curl http://localhost:8222/connz`.
*   Figura 21.5: Mostra log NATS "Server is ready" e saída JSON vazia de `/connz`.

🧠 Explicação detalhada e didática:

*   A página resume a demonstração do Redis Pub/Sub como exemplo de comunicação assíncrona e desacoplada.
*   A seção 21.2 aplica o conceito a um problema real: o gargalo de escalabilidade no aplicativo To-Do causado pelas chamadas síncronas diretas da aplicação web para o banco de dados. Muitas requisições web simultâneas podem esgotar o pool de conexões do banco de dados.
*   A solução proposta é tornar a operação de **salvar um novo item** assíncrona:
    1.  A aplicação web, ao receber um novo item, em vez de chamar o banco de dados, rapidamente publica uma mensagem (ex: "Novo Item: Comprar Leite") em uma fila de mensagens.
    2.  Um ou mais componentes separados (message handlers) ouvirão essa fila, pegarão as mensagens e realizarão a inserção no banco de dados.
*   Isso desacopla a escrita no banco de dados da requisição web, tornando a UI mais responsiva e permitindo que a inserção no banco seja feita de forma controlada (limitando o número de handlers ou escalando-os conforme a necessidade), protegendo o banco de dados contra picos de carga.
*   Para este exemplo, será usado o **NATS** como message broker. NATS é conhecido por sua alta performance (por ser primariamente em memória) e simplicidade, sendo uma boa escolha para comunicação rápida entre contêineres.
*   O exercício instrui a iniciar o servidor NATS (definido como serviço `message-queue` em um `docker-compose.yml`) e verificar se ele está rodando checando seus logs e acessando sua API de administração (`/connz` na porta 8222) que mostra as conexões ativas (inicialmente zero).

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Navegar:**
    ```powershell
    cd C:\DockerLabs\diamol\ch21\exercises\todo-list
    ```
2.  **Iniciar NATS:** (Assume que `docker-compose.yml` nesta pasta define o serviço `message-queue` usando uma imagem NATS)
    ```powershell
    docker-compose up -d message-queue
    ```
3.  **Verificar Logs NATS:**
    ```powershell
    docker logs todo-list_message-queue_1 # Nome pode variar ligeiramente
    # Procure por "Server is ready" ou similar
    ```
4.  **Verificar Conexões NATS:**
    ```powershell
    curl http://localhost:8222/connz
    # Ou Invoke-WebRequest http://localhost:8222/connz | ConvertFrom-Json
    # Deve mostrar algo como {"num_connections":0, ... "connections":[]}
    ```

---

📄 Página 431

🔍 Resumo dos principais pontos:

*   Figura 21.5: Confirma NATS rodando e sem conexões.
*   Necessário trabalho de dev para mudar para async (app web agora publica msg).
*   **Listagem 21.1:** Trecho código C# (.NET) mostrando publicação da msg `NewItemEvent`.
    ```csharp
    public void AddToDo(ToDo todo)
    {
        // Em vez de chamar DB:
        // _db.Todos.Add(todo); _db.SaveChanges();
        // Agora publica evento na fila NATS:
        MessageQueue.Publish(new NewItemEvent(todo));
        _NewTasksCounter.Inc(); // Incrementa métrica Prometheus
    }
    ```
*   **NATS Subjects:** NATS usa "subjects" (similar a channels/topics) para rotear msgs. String que identifica tipo/contexto.
*   Exemplo subject: `events.todo.newitem`.
*   Subscribers ouvem subjects específicos. Se ninguém ouvir, msg pode ser descartada (depende da config NATS/Redis).
*   **Exercício Try-it-now:** Rodar web app async e DB (sem handler).
    *   1. Iniciar serviços: `docker-compose up -d todo-web todo-db`.
    *   2. Acessar UI (`http://localhost:8080`), adicionar itens.
*   Observação: App funciona (sem erros), mas itens *não* aparecem na lista.
    *   Causa: List page lê do DB; Add page publica msg na fila; nada está lendo da fila e salvando no DB.
*   Figura 21.6: Mostra UI To-Do vazia mesmo após adicionar itens.

🧠 Explicação detalhada e didática:

*   A página mostra um exemplo de como o código da aplicação web (em C#) foi modificado. A função `AddToDo`, em vez de interagir diretamente com o banco de dados, agora chama `MessageQueue.Publish(...)`, enviando um objeto `NewItemEvent` (contendo os dados da nova tarefa) para a fila NATS. (A classe `MessageQueue` seria uma abstração interna do app que encapsula a conexão e publicação no NATS).
*   Explica que NATS usa **Subjects** para categorizar mensagens. São strings hierárquicas (ex: `minhaapp.usuario.criado`, `pedidos.regiao-sul.novo`) que os subscribers usam para filtrar quais mensagens querem receber. O exemplo usa `events.todo.newitem`.
*   Relembra que, com NATS e Redis (no modo Pub/Sub simples), se uma mensagem é publicada mas não há nenhum subscriber ativo para aquele subject/channel naquele momento, a mensagem é geralmente **descartada**.
*   O exercício demonstra o estado intermediário da migração para async:
    1.  Inicia-se a aplicação web (`todo-web`, que agora publica mensagens) e o banco de dados (`todo-db`).
    2.  **Não** se inicia nenhum componente que leia as mensagens da fila.
    3.  Ao usar a UI para adicionar itens, a operação parece funcionar (a web app publica a mensagem com sucesso), mas os itens nunca chegam ao banco de dados e, portanto, não aparecem na lista.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Garantir NATS rodando:** (Comando da p.430).
2.  **Iniciar Web e DB:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    docker-compose up -d todo-web todo-db
    ```
3.  **Verificar:** `docker-compose ps` (deve mostrar `message-queue`, `todo-db`, `todo-web` rodando).
4.  **Testar UI:**
    *   Abra `http://localhost:8080`.
    *   Adicione um item (ex: "Testar NATS"). Clique Save.
    *   A página da lista é exibida, mas o item **não** está lá. Tente adicionar mais, eles não aparecerão.

---

📄 Página 432

🔍 Resumo dos principais pontos:

*   Redis/NATS (pub/sub simples) descartam msgs sem subscribers. Outras filas podem armazenar. Novos subscribers só recebem msgs futuras.
*   Verificar se msgs estão sendo publicadas: Usar ferramenta subscriber NATS.
*   **Exercício Try-it-now:** Usar `nats-sub` para ver mensagens.
    *   1. Rodar `diamol/nats-sub` container (`todo-sub`), subscrito a `events.todo.newitem`.
        *   `docker run -d --name todo-sub --network todo-list_app-net diamol/nats-sub events.todo.newitem`. (Nota: Nome da rede pode ser só `ch21` se não usou Compose para NATS).
    *   2. Verificar logs `todo-sub` (inicialmente vazio ou só msg de conexão).
    *   3. Adicionar itens via web UI (`:8080`).
    *   4. Verificar logs `todo-sub` novamente.
*   Figura 21.7: Mostra logs do `nats-sub` recebendo mensagens JSON dos novos itens.
*   Confirma que web app está publicando corretamente. Falta componente handler.
*   **21.3 Consuming and handling messages** (Consumindo e tratando mensagens)
    *   Introduz **Message Handler**: componente que subscreve e processa mensagens.

🧠 Explicação detalhada e didática:

*   A página explica o comportamento padrão de NATS/Redis Pub/Sub (descartar mensagens sem ouvintes) e introduz uma ferramenta simples (`nats-sub`, empacotada na imagem `diamol/nats-sub`) para **verificar** se as mensagens estão sendo publicadas corretamente pela aplicação web.
*   O exercício instrui a rodar um contêiner (`todo-sub`) usando essa imagem, dizendo a ele para subscrever ao subject `events.todo.newitem`. O nome da rede (`todo-list_app-net`) é importante para que o `todo-sub` consiga encontrar o servidor NATS (`message-queue`) via DNS. (O nome da rede é prefixado pelo nome do projeto Compose, que parece ser `todo-list` aqui).
*   Ao adicionar novos itens na interface web, o `todo-web` publica as mensagens no NATS. O NATS as entrega para o `todo-sub`. Ao verificar os logs do `todo-sub`, você verá as mensagens chegando (provavelmente em formato JSON), confirmando que a parte de publicação está funcionando.
*   Isso prepara para a seção 21.3, que introduzirá o componente que realmente *fará* algo útil com essas mensagens: o **Message Handler**.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Descobrir Nome da Rede:** (Se iniciou com `docker-compose` da p.431)
    ```powershell
    docker network ls | Select-String "todo-list"
    # Anote o nome completo da rede (ex: todo-list_default ou todo-list_app-net)
    $NetName = "todo-list_default" # Ou o nome correto
    ```
2.  **Rodar NATS Subscriber:**
    ```powershell
    docker container run -d --name todo-sub --network $NetName diamol/nats-sub events.todo.newitem
    ```
3.  **Verificar Logs Iniciais:**
    ```powershell
    docker logs todo-sub # Deve mostrar "Listening on [events.todo.newitem]"
    ```
4.  **Adicionar Itens na Web:** Vá para `http://localhost:8080` e adicione 1 ou 2 itens.
5.  **Verificar Logs Finais:**
    ```powershell
    docker logs todo-sub
    # Agora deve mostrar as mensagens JSON recebidas, como na Figura 21.7.
    ```

---

📄 Página 433

🔍 Resumo dos principais pontos:

*   **Message Handler:** Componente que subscreve à fila e processa mensagens. Geralmente 1 handler por tipo de msg (subject/channel).
*   Handler para To-Do: Ouve `events.todo.newitem`, insere dados no BD Postgres.
*   Diagrama (Figura 21.8): Arquitetura completa: Web (Pub) -> NATS -> Handler (Sub) -> DB.
*   **Benefício Escala/Performance:** Fila age como buffer.
    *   Pode ter muitos Web containers, mas poucos Handlers.
    *   Handlers formam **grupo de consumidores**: NATS distribui msgs entre handlers do grupo (cada msg processada por 1 handler só).
    *   Limita carga no BD (nº conexões = nº handlers).
    *   Se carga alta, msgs acumulam na fila (app continua responsivo), handlers processam depois.
*   Handler (`save-handler`) já construído/publicado.
*   **Exercício Try-it-now:** Rodar o `save-handler`.
    *   1. Iniciar handler: `docker-compose up -d save-handler`.
    *   2. Verificar logs handler (conexão).
    *   3. Adicionar itens via web UI (`:8080`).
    *   4. Verificar logs handler novamente (processamento msgs).
*   Observação: **Eventual Consistency**. Item pode não aparecer na lista *imediatamente* (UI carrega antes do handler salvar no BD). Precisa refresh.
*   Figura 21.9: Mostra logs do `save-handler` processando as mensagens.

🧠 Explicação detalhada e didática:

*   Esta página introduz o componente que faltava: o **Message Handler** (`save-handler` neste caso). Sua função é:
    1.  Subscrever ao subject `events.todo.newitem` na fila NATS.
    2.  Receber as mensagens publicadas pelo `todo-web`.
    3.  Para cada mensagem, extrair os dados do novo item e inseri-los no banco de dados (`todo-db`).
*   A Figura 21.8 mostra o fluxo completo agora.
*   Um conceito importante sobre como NATS (e muitas outras filas) lida com múltiplos subscribers para o *mesmo* trabalho é o **Grupo de Consumidores (Consumer Group)** ou **Fila de Entrega (Queue Delivery)**. Se você iniciar várias instâncias do `save-handler` e todas elas subscreverem ao *mesmo* subject *usando o mesmo nome de grupo* (implícito ou explícito), o NATS tratará essas instâncias como um pool de workers. Ele **distribuirá** as mensagens recebidas entre as instâncias disponíveis no grupo, garantindo que cada mensagem seja processada por **apenas uma** delas. Isso permite escalar horizontalmente o processamento das mensagens.
*   O exercício inicia o `save-handler` (provavelmente definido no mesmo `docker-compose.yml`). Agora, ao adicionar itens na UI, eles devem ser salvos no banco de dados pelo handler e aparecer na lista (após um refresh, devido à "consistência eventual").
*   **Consistência Eventual:** É um efeito colateral comum de sistemas assíncronos. Como a UI publica a mensagem e redireciona para a lista *antes* de o handler ter tido tempo de processar a mensagem e salvar no BD, a lista que a UI busca no BD pode ainda não conter o item recém-adicionado. Ele aparecerá *eventualmente*, após um refresh ou alguma atualização automática da UI.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Iniciar Handler:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    # Assume que 'save-handler' está definido no docker-compose.yml e usa a rede correta
    docker-compose up -d save-handler
    ```
2.  **Verificar Logs Handler (Conexão):**
    ```powershell
    docker logs todo-list_save-handler_1 # Nome pode variar
    # Deve mostrar que conectou ao NATS e subscreveu.
    ```
3.  **Adicionar Itens na Web:** Vá para `http://localhost:8080` e adicione novos itens. Observe que eles podem não aparecer na lista imediatamente. Atualize (F5) a página da lista após alguns segundos; eles devem aparecer.
4.  **Verificar Logs Handler (Processamento):**
    ```powershell
    docker logs todo-list_save-handler_1
    # Agora deve mostrar logs indicando que recebeu e processou as mensagens dos itens adicionados.
    ```

---

📄 Página 434

🔍 Resumo dos principais pontos:

*   Figura 21.9: Confirma logs do handler processando msgs.
*   Explica como escalar handlers (`save-handler`) para aumentar throughput.
*   NATS distribui msgs entre instâncias no mesmo grupo (load balancing).
*   **Exercício Try-it-now:** Escalar `save-handler` e observar load balancing.
    *   1. Escalar: `docker-compose up -d --scale save-handler=3`.
    *   2. Verificar logs de nova instância (`_2`).
    *   3. Adicionar itens na web.
    *   4. Verificar logs de *todos* os handlers: `docker-compose logs --tail=1 save-handler`.
*   Figura 21.10: Mostra logs indicando que instâncias `_1` e `_2` processaram msgs, `_3` não (distribuição pode não ser perfeita com poucas msgs).
*   Benefício: Aumentou poder de processamento (3x) sem mudar código.
*   Próximo: Adicionar *novas features* usando mesmo evento/mensagens.
*   **21.4 Adding new features with message handlers** (Adicionando novas funcionalidades com message handlers)
    *   App To-Do agora é parcialmente **Event-Driven Architecture (EDA)**: publica eventos (`NewItemEvent`) sem saber/se importar quem os consome.
    *   Flexibilidade: Pode-se mudar o que acontece em resposta a um evento sem mudar quem o publica.

🧠 Explicação detalhada e didática:

*   A página explica como escalar horizontalmente os message handlers para aumentar a capacidade de processamento. O comando `docker-compose up -d --scale save-handler=3` instrui o Compose (e o Docker) a garantir que 3 réplicas do serviço `save-handler` estejam rodando.
*   Como todas essas réplicas se conectam ao NATS e subscrevem ao mesmo subject (`events.todo.newitem`) usando o mesmo grupo de entrega (implícito ou explícito), o NATS automaticamente **distribui** as mensagens recebidas entre as 3 instâncias. Isso significa que a capacidade de processar novos itens foi efetivamente triplicada sem nenhuma alteração no código, apenas ajustando a contagem de réplicas.
*   O exercício demonstra isso escalando para 3 e verificando (com `docker-compose logs --tail=1 save-handler`) que diferentes instâncias recebem e processam as mensagens.
*   A seção 21.4 introduz outro benefício poderoso da arquitetura orientada a eventos (EDA) que começou a ser implementada: a capacidade de adicionar **novas funcionalidades** simplesmente criando **novos subscribers** para os eventos existentes, sem precisar modificar os componentes que publicam os eventos.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Escalar Handlers:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    docker-compose up -d --scale save-handler=3
    ```
2.  **Verificar (Opcional):**
    ```powershell
    docker-compose ps # Deve mostrar 3 instâncias de save-handler
    docker logs todo-list_save-handler_2 # Ver logs da nova instância
    ```
3.  **Adicionar Itens na Web:** Adicione mais alguns itens em `http://localhost:8080`.
4.  **Verificar Logs Distribuídos:**
    ```powershell
    docker-compose logs --tail=1 save-handler
    # Veja a saída. Deve mostrar logs de diferentes instâncias (ex: _1, _2, _3) indicando qual processou a última msg que recebeu.
    ```

---

📄 Página 435

🔍 Resumo dos principais pontos:

*   Adicionar feature: Criar novo handler (em novo grupo) que ouve *mesmo* evento (`events.todo.newitem`) mas faz algo *diferente*.
*   Exemplo: `audit-handler`. Só loga info do novo item (trilha de auditoria).
*   NATS entrega cópia da msg para CADA grupo de subscrição.
    *   Grupo `save-handler` (com 3 instâncias): 1 instância recebe msg.
    *   Grupo `audit-handler` (com 1 instância): A instância recebe msg.
*   **Exercício Try-it-now:** Deployar `audit-handler`.
    *   Usar override `docker-compose-audit.yml` (define serviço `audit-handler`).
    *   1. Deploy aditivo: `docker-compose -f ... -f ...audit.yml up -d --scale save-handler=3`. (Mantém escala anterior).
    *   2. Verificar logs `audit-handler`.
    *   3. Adicionar itens na web.
    *   4. Verificar logs `audit-handler` novamente (mostra info do item).
*   Figura 21.11: Mostra logs do `audit-handler`.

🧠 Explicação detalhada e didática:

*   Esta página explica como adicionar uma funcionalidade completamente nova (uma trilha de auditoria) ao sistema **sem tocar** nos componentes existentes (`todo-web`, `save-handler`, `todo-db`).
*   A nova funcionalidade é implementada como um novo serviço, `audit-handler`. Este handler também subscreve ao subject `events.todo.newitem` no NATS.
*   **Comportamento de Entrega NATS (Pub/Sub + Queue Groups):**
    *   Quando `todo-web` publica uma mensagem em `events.todo.newitem`.
    *   NATS vê que há dois "grupos" interessados: o grupo dos `save-handler`s e o grupo do `audit-handler`.
    *   NATS entrega a mensagem para **um** membro do grupo `save-handler` (balanceamento de carga dentro do grupo).
    *   NATS entrega uma **cópia** da mesma mensagem para **um** membro do grupo `audit-handler` (que só tem 1 membro).
*   O resultado é que o mesmo evento agora dispara duas ações diferentes e independentes (salvar no BD e registrar auditoria).
*   O exercício usa um arquivo override (`docker-compose-audit.yml`) para adicionar a definição do serviço `audit-handler` ao deploy existente. O comando `docker-compose up -d` é "aditivo": ele verifica o estado atual e apenas cria/atualiza o que for novo ou diferente nos arquivos combinados. Após o deploy, ao adicionar itens na web, os logs do `audit-handler` devem mostrar as entradas de auditoria.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Deployar Audit Handler:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-audit.yml up -d --scale save-handler=3
    ```
    *Nota: Assume que `docker-compose.yml` é o arquivo base com web, db, nats, save-handler. `docker-compose-audit.yml` só adiciona `audit-handler`.*
2.  **Verificar Serviços:** `docker-compose -f ... -f ... ps` (deve mostrar `audit-handler` rodando).
3.  **Verificar Logs Audit (Conexão):**
    ```powershell
    docker logs todo-list_audit-handler_1 # Nome pode variar
    ```
4.  **Adicionar Item na Web:** Vá para `http://localhost:8080` e adicione um novo item (ex: "Verificar Audit").
5.  **Verificar Logs Audit (Processamento):**
    ```powershell
    docker logs todo-list_audit-handler_1
    # Agora deve mostrar o log de auditoria para o item adicionado.
    # Verifique também os logs de um dos save-handlers; ele também deve ter processado.
    ```

---

📄 Página 436

🔍 Resumo dos principais pontos:

*   Figura 21.11: Confirma logs do audit-handler.
*   Deploy de nova feature (audit) com **zero downtime** para app existente. Pode remover feature parando handler.
*   Flexibilidade EDA: Múltiplos subscribers, múltiplos publishers.
*   **Adicionar Novo Publisher:** Criar REST API para adicionar itens (publica mesmo evento `NewItemEvent`).
*   **Exercício Try-it-now:** Deployar `todo-api`.
    *   Usar override `docker-compose-api.yml`.
    *   1. Deploy aditivo da API: `docker-compose -f ... -f ...audit.yml -f ...api.yml up -d todo-api`. (Só inicia/atualiza `todo-api`).
    *   2. Adicionar item via API (`curl POST` para `http://localhost:8081/todo`).
    *   3. Verificar log `audit-handler`.
*   Item adicionado via API deve ser processado por `save-handler` (aparece na UI) e `audit-handler` (aparece nos logs).
*   Figura 21.12: Mostra log de auditoria após POST na API.

🧠 Explicação detalhada e didática:

*   A página reforça o benefício do padrão EDA habilitado pela fila: novas funcionalidades (como o `audit-handler`) podem ser adicionadas (e removidas) dinamicamente sem interromper ou modificar os componentes existentes.
*   Leva o conceito um passo adiante: assim como pode haver múltiplos *subscribers* para um evento, pode haver múltiplos *publishers* do mesmo evento.
*   O exercício introduz um novo componente: `todo-api`. É uma API REST simples que expõe um endpoint `POST /todo`. Quando recebe uma requisição, ela simplesmente **publica o mesmo evento** `events.todo.newitem` na fila NATS, exatamente como a aplicação web (`todo-web`) faz.
*   O exercício usa outro override (`docker-compose-api.yml`) para adicionar este serviço `todo-api` ao ambiente.
*   Após iniciar a API, usa-se `curl` para enviar um novo item através dela. A expectativa é que:
    1.  `todo-api` publique a mensagem no NATS.
    2.  NATS entregue cópias para o grupo `save-handler` e para o `audit-handler`.
    3.  `save-handler` salve no banco de dados.
    4.  `audit-handler` registre no log.
*   Isso demonstra como diferentes interfaces (Web UI, REST API) podem acionar a mesma lógica de negócio (salvar + auditar) de forma desacoplada através da fila de mensagens.

💻 Exemplos práticos (para meu ambiente Windows):

1.  **Deployar API Handler:** (Estando em `ch21/exercises/todo-list`)
    ```powershell
    # Combina base + audit + api overrides, mas só afeta 'todo-api' por causa do argumento final
    docker-compose -f docker-compose.yml -f docker-compose-audit.yml -f docker-compose-api.yml up -d todo-api
    ```
    *Nota: Pode ser mais simples/claro rodar `up -d` sem o nome do serviço para garantir que todos os serviços definidos nos arquivos combinados estejam rodando.*
    ```powershell
    docker-compose -f docker-compose.yml -f docker-compose-audit.yml -f docker-compose-api.yml up -d --scale save-handler=3 # Garante tudo rodando
    ```
2.  **Verificar Serviços:** `docker-compose -f ... ps` (deve mostrar `todo-api` rodando).
3.  **Adicionar Item via API:**
    ```powershell
    curl -X POST http://localhost:8081/todo -d '{\"item\":\"Item via API\"}' -H 'Content-Type: application/json'
    # Ou com PowerShell:
    # Invoke-RestMethod -Method Post -Uri http://localhost:8081/todo -Body '{"item":"Item via API"}' -ContentType 'application/json'
    ```
4.  **Verificar Logs Audit:**
    ```powershell
    docker logs todo-list_audit-handler_1 # Nome pode variar
    # Deve mostrar o log para "Item via API".
    ```
5.  **Verificar UI:** Acesse `http://localhost:8080`. O "Item via API" deve aparecer na lista (após refresh talvez).

---

📄 Página 437

🔍 Resumo dos principais pontos:

*   Figura 21.12: Confirma log de auditoria do item adicionado via API.
*   Confirma que ambos handlers (save, audit) processaram evento da API.
*   Mensageria assíncrona -> apps flexíveis, escaláveis, evolutivos.
*   **21.5 Understanding async messaging patterns** (Entendendo padrões de mensageria assíncrona)
    *   Mensageria é tópico avançado, mas Docker facilita experimentação.
    *   Existem diferentes padrões de envio/recebimento.
    *   Padrão usado no capítulo: **Publish-Subscribe (Pub/Sub)**.
        *   0 ou mais subscribers recebem cópias.
        *   Publisher não sabe quem consome, o que faz, quando termina.
*   Diagrama (Figura 21.13): Ilustra Pub/Sub. Publisher -> Queue -> Múltiplos Subscribers independentes.

🧠 Explicação detalhada e didática:

*   A página conclui o exemplo prático, confirmando que a arquitetura agora suporta múltiplas entradas (UI, API) e múltiplos processamentos (Salvar BD, Auditoria) para o mesmo evento, graças ao desacoplamento da fila NATS.
*   A seção 21.5 introduz uma visão geral de **padrões de mensageria assíncrona**. O objetivo é mostrar que existem diferentes formas de usar filas, adequadas a diferentes necessidades.
*   O padrão implícito nos exemplos com Redis e NATS até agora é o **Publish/Subscribe (Pub/Sub)**:
    *   Um Publisher envia uma mensagem (evento) para um tópico/subject/channel.
    *   A fila entrega uma cópia dessa mensagem para **todos** os Subscribers (ou grupos de subscribers) interessados naquele tópico.
    *   Os Subscribers processam a mensagem de forma independente.
    *   O Publisher não tem conhecimento dos Subscribers nem do resultado do processamento. É uma comunicação "disparada" (broadcast-like).
*   A Figura 21.13 ilustra isso: um publisher, múltiplos subscribers recebendo a mesma mensagem. (Nota: A Figura 21.8 mostrava um mix, onde save-handlers eram um grupo e audit-handler era outro, ambos recebendo cópias).

💻 Exemplos práticos (para meu ambiente Windows):

*   O fluxo com `todo-web` publicando e *ambos* `save-handler` (grupo) e `audit-handler` recebendo é um exemplo de Pub/Sub.

💡 Analogias ou observações úteis (se houver):

*   Pub/Sub: Transmissão de Rádio/TV. A estação (Publisher) transmite o sinal (mensagem) em uma frequência (tópico). Qualquer pessoa com um rádio/TV (Subscriber) sintonizado na frequência correta recebe o sinal. A estação não sabe quantos ou quem está ouvindo.

---

📄 Página 438

🔍 Resumo dos principais pontos:

*   Pub-Sub não serve para tudo (ex: publisher precisa de resposta/confirmação).
*   **Alternativa: Request-Response Messaging**.
    *   Cliente envia msg de *request* para fila, **espera** (talvez não bloqueante) por msg de *response*.
    *   Handler pega request da fila, processa, envia msg de *response* (para fila de resposta ou via correlação).
    *   Fila roteia response de volta para cliente original.
    *   Substitui chamadas síncronas (API, RPC), mas com benefícios async (handler não sobrecarrega, cliente pode fazer outras coisas enquanto espera).
*   Diagrama (Figura 21.14): Ilustra Request-Response via Fila. Client -> Fila Request -> Handler -> Fila Response -> Client.

🧠 Explicação detalhada e didática:

*   A página introduz um segundo padrão comum de mensageria: **Request-Response** (ou Request/Reply).
*   Este padrão é usado quando o componente que inicia a comunicação (o "cliente" ou "requisitante") precisa de uma **resposta** específica do componente que realiza o trabalho (o "servidor" ou "handler").
*   **Fluxo:**
    1.  O Cliente envia uma mensagem de **Requisição** para uma fila específica. A mensagem geralmente contém os dados da requisição e informações de como/onde enviar a resposta (ex: nome de uma fila de resposta temporária, um ID de correlação).
    2.  O Cliente *pode* esperar pela resposta (de forma bloqueante ou não-bloqueante).
    3.  Um Handler pega a mensagem de Requisição da fila.
    4.  O Handler processa a requisição.
    5.  O Handler cria uma mensagem de **Resposta** (com o resultado e/ou o ID de correlação) e a envia para a fila de resposta especificada pelo Cliente (ou uma fila padrão).
    6.  O Cliente, que estava ouvindo na fila de resposta, recebe a mensagem de Resposta correspondente à sua Requisição original.
*   A Figura 21.14 ilustra esse fluxo de ida e volta através de filas.
*   **Vantagem:** Mantém o desacoplamento e a resiliência da mensageria (cliente e handler não precisam estar online ao mesmo tempo, fila age como buffer), mas permite um fluxo de comunicação bidirecional com resultado. É uma alternativa assíncrona a uma chamada API REST síncrona.

💻 Exemplos práticos (para meu ambiente Windows):

*   Implementar Request/Response geralmente requer mais configuração na fila (ex: filas dedicadas para requests e responses) e lógica mais complexa no cliente (para correlacionar respostas com requests) e no handler (para enviar a resposta ao local correto). Tecnologias como RabbitMQ têm suporte mais explícito para este padrão do que Redis/NATS pub/sub simples.

💡 Analogias ou observações úteis (se houver):

*   Request-Response Assíncrono: É como enviar uma pergunta para um departamento por malote interno (fila de request) com um envelope auto-endereçado (fila de response) dentro. O departamento responde quando pode, coloca a resposta no envelope e manda de volta pelo malote. Você não fica esperando na porta do departamento, mas eventualmente recebe a resposta na sua caixa de entrada.

---

📄 Página 439

🔍 Resumo dos principais pontos:

*   Figura 21.14: Diagrama Request-Response.
*   Outros padrões: Fire-and-Forget (comando via msg, sem resposta), Scatter-Gather (publica para muitos, coleta/agrega respostas).
*   Outra tecnologia de fila: **RabbitMQ**.
    *   Mais avançada que Redis/NATS (usados no cap).
    *   Suporta roteamento complexo (exchanges, bindings).
    *   Suporta **mensagens persistentes** (salvas em disco, sobrevivem a restart da fila/container).
    *   Disponível como imagem oficial Docker Hub.
*   Conclusão: Filas liberam design (EDA, escalável, flexível).
*   **21.6 Lab** (Laboratório)
    *   Último lab. "Sneaky" (com pegadinha).
    *   Objetivo: Adicionar *outro* handler para To-Do: `mutating-handler` (muda texto do item *após* ser salvo).
    *   Handler existe: imagem `diamol/ch21-mutating-handler`.
*   **Tarefa:**
    1.  Adicionar serviço `mutating-handler` ao Compose file.
    2.  Handler ouve evento `events.todo.itemsaved`. **Problema:** Nada publica esse evento ainda. Precisa achar config em componente *existente* (provavelmente `save-handler`) para fazê-lo publicar `itemsaved` *após* salvar no BD.
    3.  Handler `mutating-handler` tem config padrão *errada* para endereço NATS. Precisa achar setting de config (provavelmente env var) para corrigir o endereço NATS.
*   Dica: Respostas estão nos Dockerfiles (procurar `ENV`, `CMD`, etc.) ou docs das imagens. Setar valores no Compose file.

🧠 Explicação detalhada e didática:

*   A página menciona brevemente outros padrões de mensageria e introduz **RabbitMQ** como uma alternativa mais robusta e rica em funcionalidades comparada a Redis (simples Pub/Sub) e NATS (alta performance, mas primariamente em memória por padrão). A principal vantagem do RabbitMQ mencionada é o suporte a **mensagens persistentes** (garantia de que mensagens não se perdem se o broker reiniciar) e roteamento avançado.
*   O **Lab do Capítulo 21** apresenta um desafio de configuração e descoberta. O objetivo é integrar um terceiro handler, `mutating-handler`, que modifica os itens da lista *depois* de serem salvos. Isso requer uma cadeia de eventos: Web -> `newitem` -> SaveHandler -> (Salva BD) -> **SaveHandler publica `itemsaved`** -> MutatingHandler -> (Modifica BD).
*   As "pegadinhas" são:
    1.  O evento `events.todo.itemsaved` não está sendo publicado por padrão. Você precisa investigar o `save-handler` (provavelmente seu Dockerfile ou alguma documentação implícita) para descobrir como habilitar a publicação desse evento secundário (talvez via uma variável de ambiente?).
    2.  O novo `mutating-handler` não consegue conectar ao NATS por padrão. Você precisa investigar *esse* handler para descobrir qual variável de ambiente (ou outro mecanismo) ele usa para configurar o endereço do servidor NATS e defini-la corretamente no `docker-compose.yml` para apontar para o serviço `message-queue`.
*   Este lab simula uma situação realista de integrar componentes onde a configuração não é imediatamente óbvia e exige um pouco de investigação (olhar Dockerfiles, experimentar variáveis de ambiente).

💻 Exemplos práticos (para meu ambiente Windows):

*   **Estratégia para o Lab 21:**
    1.  **Investigar `save-handler`:**
        *   `docker run --rm diamol/ch21-save-handler --help` (Ver se tem ajuda)
        *   `docker image inspect diamol/ch21-save-handler` (Procurar `ENV` ou `CMD` no histórico/config)
        *   *Hipótese:* Pode haver uma `ENV PUBLISH_SAVED_EVENT=false` por padrão.
    2.  **Investigar `mutating-handler`:**
        *   `docker run --rm diamol/ch21-mutating-handler --help`
        *   `docker image inspect diamol/ch21-mutating-handler`
        *   *Hipótese:* Pode haver uma `ENV NATS_URL=nats://default-nats:4222` por padrão.
    3.  **Modificar `docker-compose.yml`:** (Baseado nos arquivos dos exercícios anteriores)
        *   No serviço `save-handler`, adicione `environment:` para habilitar o evento (ex: `PUBLISH_SAVED_EVENT=true`).
        *   Adicione um novo serviço `mutating-handler`:
            *   `image: diamol/ch21-mutating-handler`
            *   `networks: [app-net]`
            *   `restart: unless-stopped`
            *   Adicione `environment:` para corrigir o NATS URL (ex: `NATS_URL=nats://message-queue:4222`).
            *   Adicione `depends_on: [message-queue]` (boa prática).
    4.  **Rodar e Testar:** `docker-compose up -d`. Adicione um item na UI. Verifique os logs do `save-handler` (deve mencionar `itemsaved`?), logs do `mutating-handler` (deve receber `itemsaved` e logar modificação), e a UI (o texto do item deve mudar após alguns segundos/refresh).

*   Link da Solução: https://github.com/sixeyed/diamol/blob/master/ch21/lab/README.md

---

📄 Página 440

🔍 Resumo dos principais pontos:

*   Link para solução do Lab 21 no GitHub.
*   **Capítulo 22: Never the end** (Nunca o fim)
    *   Docker é excitante, muitos usos (Git server, migrar legado, cloud-native).
    *   Livro deu a base; capítulo final dá dicas para continuar.
*   **22.1 Run your own proof-of-concept** (Rode sua própria prova de conceito)
    *   Melhor forma de aprender mais: aplicar no seu próprio trabalho/projeto.
    *   PoC para migrar app próprio para Docker -> consolida habilidades.
    *   Escopo recomendado para PoC impactante (além do básico `build`/`run`):
        *   Dockerizar >1 componente (usar Compose, Cap 10).
        *   Seguir boas práticas (multi-stage, otimização, golden images, Cap 17).
        *   Incluir observabilidade (logs centralizados Cap 19, métricas Cap 9).
        *   Montar pipeline CI/CD simples (Jenkins em container, Cap 11).

🧠 Explicação detalhada e didática:

*   Inicia o Capítulo 22, a conclusão do livro. O objetivo é fornecer orientação sobre os **próximos passos** após aprender os fundamentos do Docker.
*   A seção 22.1 sugere fortemente que a melhor maneira de solidificar e expandir o conhecimento é através da **prática em um projeto real**: uma **Prova de Conceito (Proof of Concept - PoC)**.
*   A ideia é escolher uma aplicação existente (ou uma nova simples) do seu trabalho ou interesse pessoal e passar pelo processo de **conteinerizá-la**, aplicando as técnicas aprendidas ao longo do livro.
*   Para que a PoC seja convincente e demonstre o valor real do Docker, o autor recomenda ir além do básico `docker build` e `docker run`, e incluir no escopo da PoC:
    *   Gerenciamento de múltiplos componentes com **Docker Compose**.
    *   Aplicação de **boas práticas** na construção das imagens (multi-stage, otimizações).
    *   Implementação de **observabilidade** (coleta de logs e métricas).
    *   Criação de um **pipeline automatizado** de CI/CD.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Planejar sua PoC:**
    1.  **Escolher App:** Selecione uma aplicação relativamente simples que você conheça bem (pode ser .NET, Java, Node, Python, etc.). Idealmente, uma com 2-3 componentes (ex: Web API + BD, ou Web UI + Web API).
    2.  **Dockerizar Componentes:** Crie `Dockerfile`s (multi-stage, otimizados) para cada componente. Use golden images se apropriado. Adicione `HEALTHCHECK`s.
    3.  **Definir com Compose:** Crie `docker-compose.yml` (e overrides `dev`/`test`?) para rodar a aplicação completa. Configure redes, volumes/bind mounts, configs/secrets.
    4.  **Adicionar Observabilidade:** Instrumente os apps para expor métricas Prometheus. Adicione serviços Prometheus/Grafana ao Compose. Crie um dashboard básico. Configure driver `fluentd` e rode EFK stack (ou outro backend) para logs.
    5.  **Automatizar CI/CD:** Crie um `Jenkinsfile` (ou use GitHub Actions, etc.). Configure pipeline para buildar, testar (mesmo que um teste simples), escanear (opcional), fazer push para um registro (Hub ou local), e talvez deployar para um ambiente "teste" (pode ser no mesmo host Docker via Compose com `-p`).

---

📄 Página 441

🔍 Resumo dos principais pontos:

*   PoC não precisa ser enorme (ex: 5 dias, app simples). Pode ser projeto paralelo inicial.
*   Se Docker bloqueado no trabalho -> Tentar em casa (ex: com Raspberry Pi).
*   **22.2 Make a case for Docker in your organization** (Apresente um caso para Docker na sua organização)
    *   Docker é mudança grande, nem todos aceitam fácil.
    *   Argumentos chave para diferentes stakeholders:
        *   **Desenvolvedores:** Ambiente consistente (igual prod), menos setup, mesmas ferramentas Ops -> colaboração.
        *   **Operações/Admins:** Ferramentas/processos padrão para todos apps, API padrão (log, métrica, config), deploy/rollback automatizado, releases mais frequentes/seguros.
        *   **DBAs:** Self-service de BDs para Dev/Teste via containers. CI/CD para schemas BD.
        *   **Segurança:** Secure supply chain (golden image, scan, sign). Ferramentas runtime (Aqua, Twistlock) para monitorar/bloquear ataques.
        *   **Negócio/Product Owners:** Entendem linguagem de releases. Docker -> melhor qualidade, entrega mais rápida de features.

🧠 Explicação detalhada e didática:

*   A página continua as sugestões para a PoC, dimensionando o esforço e sugerindo alternativas se houver resistência inicial no ambiente de trabalho.
*   A seção 22.2 foca em como **"vender" a ideia do Docker** dentro da sua organização, reconhecendo que diferentes grupos (stakeholders) têm diferentes prioridades e preocupações. O autor fornece argumentos direcionados para cada grupo:
    *   **Desenvolvedores:** Focar na eliminação de problemas de ambiente ("funciona na minha máquina"), na facilidade de setup e na colaboração com Ops.
    *   **Operações:** Destacar a padronização de ferramentas e processos, a automação de deploy/rollback e a melhoria na observabilidade e frequência de releases.
    *   **DBAs:** Mostrar como contêineres agilizam o provisionamento de bancos de dados para ambientes não-produtivos e como podem modernizar o desenvolvimento de BDs.
    *   **Segurança:** Enfatizar as práticas de "secure supply chain" (imagens base controladas, varredura, assinatura) e a possibilidade de usar ferramentas de segurança específicas para contêineres em runtime.
    *   **Negócio:** Traduzir os benefícios técnicos em valor de negócio: software de maior qualidade entregue mais rapidamente ao mercado.

💻 Exemplos práticos (para meu ambiente Windows):

*   Use os resultados da sua PoC (aplicação rodando, dashboard Grafana, pipeline CI/CD) para **demonstrar** esses benefícios concretamente para os diferentes stakeholders na sua empresa. Prepare uma apresentação focada nos ganhos relevantes para cada grupo.

---

📄 Página 442

🔍 Resumo dos principais pontos:

*   Argumentos para stakeholders (cont.):
    *   **Gerência Sênior:** Foco no negócio + Orçamento TI. Containers -> consolidação hardware -> economia de custos (servidores, licenças OS).
    *   **Gerência TI:** Tendência de mercado (containers não vão sumir), nuvens oferecem K8s gerenciado. Adotar Docker moderniza stack, mantém times felizes/atualizados.
*   **22.3 Plan the path to production** (Planeje o caminho para produção)
    *   Importante ter visão clara de onde se quer chegar com Docker.
    *   Revisita 5 cenários do Cap 1 (migrar legado, cloud-native, serverless, etc.). PoC deve se alinhar a um objetivo.
    *   Decisão maior: **Swarm vs. Kubernetes** para orquestração.
        *   Nuvem: K8s gerenciado costuma ser mais fácil.
        *   Datacenter: Swarm mais fácil de operar.
        *   K8s: Curva aprendizado íngreme, YAML diferente, mas mais features.
*   **22.4 Meet the Docker community** (Conheça a comunidade Docker)
    *   Não está sozinho; comunidade Docker é grande, ativa, amigável. Online e presencial.

🧠 Explicação detalhada e didática:

*   Continua os argumentos para convencer diferentes níveis da organização sobre o Docker:
    *   **Alta Gerência:** Focar nos benefícios financeiros (redução de custos de infraestrutura e licenciamento) e de negócio (agilidade).
    *   **Gestores de TI:** Posicionar Docker como uma modernização necessária para acompanhar o mercado, atrair/reter talentos e se preparar para a nuvem.
*   A seção 22.3 enfatiza a necessidade de **planejamento estratégico** ao adotar Docker, especialmente pensando no caminho para produção. É preciso definir o objetivo (ex: migrar X aplicações legadas, construir novo produto Y como cloud-native) e tomar a decisão chave sobre qual **orquestrador** usar em produção: Swarm ou Kubernetes. A escolha depende dos fatores já discutidos (nuvem vs datacenter, complexidade vs features, curva de aprendizado).
*   A seção 22.4 introduz a **comunidade Docker** como um recurso valioso.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Planejamento:** Defina qual(is) aplicação(ões) seriam os melhores candidatos para sua PoC e qual o objetivo de longo prazo (ex: rodar no AKS do Azure? Rodar no datacenter com Swarm?). Isso influenciará as tecnologias a serem focadas após o livro.
*   **Comunidade:** Explore os links fornecidos na próxima página para encontrar fóruns, grupos e eventos.

---

📄 Página 443

🔍 Resumo dos principais pontos:

*   **Recursos da Comunidade Docker:**
    *   Slack Comunitário Docker: `https://dockr.ly/slack`.
    *   Meetups (presenciais/virtuais): `https://events.docker.com`.
    *   Docker Captains (experts reconhecidos): `www.docker.com/community/captains`.
    *   DockerCon (conferência): `https://dockercon.com`.
*   **Contato do Autor:**
    *   Slack: `@eltonstoneman`.
    *   Twitter: `@EltonStoneman`.
*   Encerramento do Capítulo 22.

🧠 Explicação detalhada e didática:

*   Esta página fornece links concretos para se conectar com a **comunidade Docker**. Participar da comunidade é uma ótima maneira de tirar dúvidas, aprender com os outros, compartilhar suas próprias experiências e se manter atualizado sobre as novidades do ecossistema.
*   Os recursos listados incluem:
    *   **Slack:** Canal de chat em tempo real com milhares de usuários Docker.
    *   **Meetups:** Grupos locais (ou virtuais) que se reúnem para discutir Docker.
    *   **Docker Captains:** Programa que reconhece indivíduos que são especialistas e contribuem ativamente para a comunidade (muitos têm blogs, canais no YouTube, etc.).
    *   **DockerCon:** A conferência oficial do Docker, com palestras técnicas, workshops e anúncios.
*   O autor também fornece seus próprios contatos para interação.

💻 Exemplos práticos (para meu ambiente Windows):

*   **Ação:** Explore os links! Entre no Slack, procure por meetups na sua região (ou virtuais), siga alguns Docker Captains no Twitter ou seus blogs.

---

📄 Página 444

🔍 Resumo dos principais pontos:

*   Página em branco, possivelmente antes do índice.

🧠 Explicação detalhada e didática:

*   Página em branco.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

📄 Páginas 455 a 463

🔍 Resumo dos principais pontos:

*   Estas páginas contêm o **índice remissivo** do livro.
*   Listam termos chave, comandos, conceitos e os números das páginas onde são mencionados.

🧠 Explicação detalhada e didática:

*   O índice remissivo é uma ferramenta padrão em livros técnicos para ajudar o leitor a localizar rapidamente informações sobre um tópico específico. Ele lista palavras-chave em ordem alfabética e indica as páginas onde essas palavras aparecem de forma significativa.

💻 Exemplos práticos (para meu ambiente Windows):

*   Se você quiser reler sobre "bind mounts", procure por "bind mounts" no índice e vá para as páginas indicadas (ex: 85-93, 113).
*   Se quiser rever o comando `docker stack deploy`, procure por "stack deploy command" e veja as páginas (ex: 231, 257).

---

📄 Página 464

🔍 Resumo dos principais pontos:

*   **RELATED MANNING TITLES** (Títulos Relacionados da Manning)
*   Lista outros livros da editora Manning sobre temas relacionados:
    *   Kubernetes in Action, Second Edition (Marko Lukša)
    *   Docker in Action, Second Edition (Jeff Nickoloff, Stephen Kuenzli)
    *   Docker in Practice, Second Edition (Ian Miell, Aidan Hobson Sayers)
    *   Cloud Native Patterns (Cornelia Davis)
*   Inclui ISBN, número de páginas, preço e data de publicação (aproximada).
*   Link para informações de compra: www.manning.com.

🧠 Explicação detalhada e didática:

*   Esta página funciona como publicidade para outros livros da mesma editora que complementam ou aprofundam os tópicos abordados. Se você gostou deste livro e quer aprender mais sobre Kubernetes, ou ter uma visão diferente sobre Docker, ou entender padrões de arquitetura cloud-native, essas são sugestões da editora.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando. Apenas referências para leitura adicional.

---

📄 Página 465

🔍 Resumo dos principais pontos:

*   Texto curto, provavelmente da contracapa ou material promocional.
*   "Docker makes it easy to adopt architectures which are usually complex." (Docker facilita adotar arquiteturas geralmente complexas).
*   "A reverse proxy increases performance and reliability, and you can run it in a container alongside your web apps." (Um proxy reverso aumenta performance e confiabilidade, e você pode rodá-lo em um contêiner junto com seus apps web).

🧠 Explicação detalhada e didática:

*   Este é um pequeno trecho de texto, provavelmente usado em marketing, que resume dois dos benefícios e padrões discutidos no livro: a capacidade do Docker de simplificar arquiteturas complexas e o uso de um reverse proxy (rodando em contêiner) para melhorar performance e confiabilidade, como visto no Capítulo 20.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando. Apenas um reforço de conceitos.

---

📄 Página 466

🔍 Resumo dos principais pontos:

*   Página em branco.

🧠 Explicação detalhada e didática:

*   Última página do PDF, provavelmente em branco.

💻 Exemplos práticos (para meu ambiente Windows):

*   Nenhum comando ou exemplo aplicável nesta página.

---

✅ Fim da leitura. Todas as páginas foram explicadas com base no seu ambiente Windows + Docker Desktop + VS Code.

